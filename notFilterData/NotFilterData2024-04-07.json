{
    "1": "За последние пару лет не раз и не два приходилось слышать мнение, что HDD вскоре уступят место твердотельным накопителям. Они становятся всё более ёмкими и быстрыми, а цена на них падает. Но всё не так просто, поскольку технологии в мире HDD тоже не стоят на месте, а продолжают развиваться. На днях стало известно о новом достижении разработчиков из Seagate и учёных из Университета Тохоку. Подробности — под катом. Объединённая команда из Национального института материаловедения (NIMS), компании Seagate Technology и Университета Тохоку провела успешный эксперимент по многоуровневой записи данных с применением трёхмерной магнитной «упаковки информации». В эксперименте использовался подогрев пластин, что в целом новинкой не является, но в сочетании с другими методами он даёт возможность увеличить ёмкость HDD до 60 ТБ уже сейчас. Вполне может быть, что через несколько лет появятся и жёсткие диски ёмкостью 120 ТБ. Так что HDD рано отправлять «на пенсию», этот вид накопителей явно станет развиваться в ближайшем обозримом будущем. Концепция многоуровневой магнитной записи для жёстких дисков существует уже некоторое время, но её практическое применение сдерживается отсутствием подходящих веществ, способных хранить данные на различных уровнях. Исследователи решили эту проблему, создав новый тип покрытия, состоящего из двух наногранулированных железо-платиновых плёнок FePt-C, разделённых промежуточным слоем Ru-C с кубической кристаллической структурой. Это позволяет осуществлять раздельную запись на каждый слой в различных магнитных полях и температурах. Регулируя мощность лазера и характеристики, конфигурацию магнитных полей во время процесса записи, на каждый из слоёв FePt можно записывать независимо, что потенциально удвоит плотность записи и ёмкость жёсткого диска без существенных изменений в материалах магнитных слоев. Исследователи говорят, что такие носители могут обеспечить плотность записи более 10 Тбит/дюйм², т. е. жёсткие диски с 10 пластинами и ёмкостью более 120 ТБ. В ближайшем будущем команда проекта собирается разработать технологию для уменьшения размера зёрен покрытия пластин дисков, оптимизировать работу с магнитными полями, а также увеличить количество слоёв FePt-плёнки для создания структуры носителя данных, способного функционировать не только в лаборатории, но и в реальных условиях. Понятно, что HAMR — аббревиатура, расшифровывается она как Heat-assisted magnetic recording («термомагнитная запись»). Технология позволяет достичь плотности записи в 2,32–7,75 Тбит/см². По словам американских техножурналистов, которые любят сравнения, всего на одном диске объёмом 37 ТБ можно будет записать в несжатом виде все материалы Библиотеки Конгресса США. Суть технологии состоит в локальном нагревании лазером и перемагничивании в процессе записи поверхности пластин жёсткого диска. Встроенный в головку HDD лазер позволяет нагревать покрытие «блина» до 500 градусов Цельсия. Соответственно, нужный участок намагничивается даже относительно слабым магнитом для того, чтобы записать данные. Разработчики технологии сообщают, что при этом не меняются ни физические, ни химические свойства пластины. А вот ёмкость — да, её можно значительно увеличить, причём после охлаждения пластины данные не теряются, они считываются без особых проблем. Сама технология далеко не новая, её разрабатывают в Seagate с 1998 года, этой работой занимается целое подразделение, которое получило название Seagate Research. В 2018 году, т. е. 5 лет назад, был выпущен первый жёсткий диск объёмом 16 ТБ, разработанный по технологии HAMR. И сейчас, как видим, она постепенно совершенствуется. Seagate продолжает создание новых, более совершенных HDD и с использованием других технологий. В частности, SMR и PMR, что даёт возможность выпускать HDD высокой ёмкости без необходимости нагревания пластины лазерами. Что касается SMR, то эту технологию называют «черепичной записью». Благодаря ей плотность записи на пластине составляет около 1 248 Гбит/дюйм². При этом процессом записи можно управлять двумя способами. Первый — с задействованием контроллера диска. Второй — с использованием программного обеспечения системы, к которой накопитель подключён. HDD от Seagate относятся именно ко второму типу. Максимальный их объём — 24 ТБ, но вполне возможно, что в скором будущем появятся SMR HDD и большего объёма. PMR — технология перпендикулярной магнитной записи, которая позволяет размещать данные на параллельных дорожках. Правда, диски, изготовленные по этой технологии, похоже, достигли технологического предела. Не так давно Seagate, выпустив HDD объёмом 24 ТБ, заявила, что для технологии PMR это почти максимум, так что дальнейшая работа в этом направлении смысла не имеет. Насколько можно судить, в ближайшем будущем плотность записи данных на HDD будет расти, а ёмкость самих жёстких дисков — увеличиваться. Так что, вероятно, SSD ещё очень нескоро станут замещать HDD: возможности последних не исчерпаны.",
    "2": "Здравствуйте, меня зовут Дмитрий Карловский и я.. люблю власть. Когда я берусь за клавиатуру, каждый байтик начинает плясать под мою дудку. Но когда этих байтиков становится по настоящему много, уследить за всеми становится сложно. Поэтому давайте сравним популярные паттерны проектирования, позволяющие разделить большое приложение на компоненты, чтобы властвовать над ними максимально эффективно и независимо. Так как одна и та же прикладная сущность встречается в приложении во многих местах, в разных контекстах, и должна иметь разное представление, то базовая декомпозиция заключается в выделении модели предметной области, которая является источником истины для всех мест её отображения. И тут начинаются нюансы.. 💡 Обратите внимание, что далее стрелки показывают не движение данных, как их часто рисуют, а наличие знания одного компонента системы, как работать с другим. В пределе это знание выражается в полном контроле жизненного цикла: от создания, до уничтожения. Отсутствие же знания даёт независимость от конкретной реализации, а значит и возможность работать с разными реализациями. Модель знает как себя по разному представлять. ✅ Удобно из модели получать любые отображения. ❌ Добавление нового отображения требует изменения модели. ❌ Отображение полностью определяется одной основной моделью. ❌ Загрузка модели вытягивает по зависимостям и все её отображения. ❌ Двух слоёв слишком мало на больших масштабах. Код работы с моделями пишется прямо в отображении. ✅ Отображение может использовать произвольные модели. ✅ Легко добавлять новые отображения, не меняя модели. ❌ Для отображения разных моделей необходимо дублировать код отображения. ❌ Изменение интерфейса модели требует обновления всех использующих её отображений. ❌ Двух слоёв слишком мало на больших масштабах. Отображения работают с моделями через посредников, которые трансформируют абстракции предметной области в абстракции отображения и обратно. ViewModel также выступает хранилищем состояния отображения, не связанного с предметной областью. ✅ Отображение может использовать произвольные вьюмодели. ✅ Легко добавлять новые отображения, не меняя ни модели, ни вьюмодели. ✅ Изменение интерфейса модели или отображения требует изменения только лишь вьюмодели. ✅ Одну и ту же вьюмодель можно шарить между несколькими отображениями. ❌ Для отображения разных моделей необходимо дублировать код отображения и вьюмодели. ❌ Трёх слоёв слишком мало на больших масштабах. Контроллер создаёт отображение, и говорит ему с какой моделью работать. Так же он обрабатывает все команды от пользователя, и управляет своими подопечными. ✅ Отображение может использовать произвольные модели с тем же интерфейсом. ✅ Легко добавлять новые отображения, не меняя модели. И наоборот. ❌ Для отображения разных типов моделей необходимо дублировать код отображения. ❌ Изменение интерфейса модели требует обновления всех использующих её отображений и контроллеров. ❌ Трёх слоёв слишком мало на больших масштабах. Модели и отображения пассивны, и не знают друг о друге - они управляются презентером, который выступает и в качестве посредника между ними. ✅ Легко добавлять новые отображения, не меняя модели. И наоборот. ✅ Изменение интерфесов модели или отображения требует изменения только лишь презентеров. ❌ Трёх слоёв слишком мало на больших масштабах. ❌ Для использования состояния одного презентера из другого необходимо искусственное вынесение его в модели. Каждый ModelView выступает в роли модели/контроллера для ведомых ModelView и в качестве отображения для владеющего ModelView. Часть логики может выноситься как в чистые Model, так и в чистые View, которые являются лишь вырожденными случаями ModelView. ✅ Каждый ModelView полностью контролирует внутренние ModelView и ничего не знает про внешние. ✅ Любой ModelView может шариться между разными другими ModelView на любом уровне композиции. ✅ Изменение интерфейса ModelView требует изменения только лишь его владельцев. ✅ Фрактальная структура легко масштабируется на приложения любого размера. $mol_view построен на идеях MVF, так как это наиболее простой паттерн декомпозиции, который легко масштабируется по мере потребности и хорошо разделяет разные уровни абстракции. Актуальный оригинал на $hyoo_page.",
    "3": "Приветствуем вас в четвертой статье цикла «Континент 4 NGFW Getting Started 2.0»! В предыдущем материале мы с Вами рассмотрели функции и настройки межсетевого экрана, а также создали базовые политики межсетевого экранирования. 1. В Менеджере конфигурации создадим сертификат портала аутентификации. Переходим в раздел «Администрирование — Персональные сертификаты — Сертификат». 2. Создаем DNS-запись на DNS-сервере с FQDN, аналогичным названию сертификата. 3. В Менеджере конфигурации создадим сертификат перенаправления на портал аутентификации. Переходим в раздел «Администрирование — Промежуточные центры сертификации — Промежуточный сертификат». Параметры сертификата произвольные, корневой сертификат аналогичный персональному сертификату. 4. Созданные сертификаты прикрепляем к УБ. Переходим в свойства УБ (в нашем случае это УБ с ЦУС), вкладка «Сертификаты». Прикрепляем сертификат «Портала аутентификации» и сертификат «Перенаправление на портал аутентификации». 5. Переходим в свойства УБ и включаем компонент «Идентификация пользователей». В разделе «Идентификация пользователей» включаем портал аутентификации. Выбираем интерфейсы узла, доступные порталу идентификации (внутренние). Диапазон перенаправляемых адресов: LAN1 (192.168.1.0/24). Сохраняем и устанавливаем политику. Теперь при попытке открыть любой сайт с подсети 192.168.1.0/24 нас будет автоматически перенаправлять на портал аутентификации. Это означает, что работает сертификат перенаправления на портал аутентификации. Если открывается портал аутентификации: значит, работает сертификат портала аутентификации. Портал аутентификации настроен и успешно работает. Теперь нам необходимо создать пользователей, которых мы сможем на нем аутентифицировать. Для этого создадим локального пользователя. Перейдем в раздел «Контроль доступа — Список объектов ЦУС — Пользователи» ПКМ на свободную область «Создать». Откроется меню создания локального пользователя. Пароль и/или сертификат как метод аутентификации. Для аутентификации на портале нужен пароль После создания пользователя добавим новое правило, выше ранее созданных правил для выхода в Интернет из локальной сети Центрального офиса. Позволим нашему пользователю открывать и использовать Telegram. Сохраняем и устанавливаем политику. Теперь можно попробовать авторизоваться на портале аутентификации. В случае успеха выведется оповещение со временем до окончания сессии и клавишей «Выйти». Обратите внимание, что нам доступен Telegram, но и доступ до других ресурсов не пропал. Правила срабатывают не только по совпадению пользователя, но и по IP-адресам. Если мы обратимся в Систему мониторинга и найдем срабатывание по шестому правилу, то увидим, что в поле «Имя отправителя» находится логин пользователя. Работа с локальными пользователям удобна в частных случаях (например, для VPN, о котором мы поговорим дальше), или для VPN с сертификатами (подробнее в статье про VPN). При большом количестве пользователей гораздо удобнее работать с доменными группами. Добавление групп из каталога AD возможно с помощью LDAP-коннектора. Сделаем его. Чек-бок включить SSL безопасность: при нажатии на чекбок будет включен режим LDAPS (tcp/636). Без включенного чекбокса будет использоваться режим LDAP (tcp/389); Добавим созданный профиль на УБ во вкладке «Структура»: ПКМ по УБ — «Идентификация пользователей» — «Профиль LDAP». Сохраняем и устанавливаем политику. Возвращаемся к LDAP профилю и нажимаем «Импорт LDAP-групп». Если подключение к AD будет выполнено успешно, то на экране появится окно импорта LDAP-групп. Импортированные группы появятся во вкладке Объектов ЦУС «Пользователи». Мы импортируем ранее созданную группу «Internet Users» в AD и добавляем в ранее созданное правило для локального пользователя. Сохраним и установим политику. Переходим на страницу авторизации. Авторизуемся через доменную учетную запись. Попробуем также открыть Telegram и другие ресурсы. Правила должны успешно отработать. Обратимся к системе мониторинга. События, связанные с сигнатурой telegram, должны быть с именем отправителя. В нашем случае отправитель — доменный пользователь На этом четвертая статья подошла к концу. Мы с вами проделали большую работу и настроили портал аутентификации, создали локального пользователя, рассмотрели работу с пользователями, добавили доменную группу. А также посмотрели, как работают правила с пользователями. Локальных пользователей рекомендуем использоваться для VPN подключения по сертификатам. Если пользователей много, рекомендуем использовать доменные группы с Active Directory; С Active Directory нельзя «подтянуть» конкретного пользователя. Континент 4 NGFW оперирует только доменными группами. В следующей статье мы в подробностях рассмотрим настройки веб-фильтрации.",
    "4": "Эта секция написана уже после статьи, чтобы читатель посмотрел, а надо ли ему что-то отсюда или нет, но это забавное приключение (напоминаю, что статьи в форме (б|в)лога, как всегда. В продолжении к первой публикации про старт инди-дев-(б|в)лога, в котором поделился первыми шагами по созданию кроссплатформенного решения, я понял, что мне негде его обсуждать, а значит нужен добротный, как гнедой конь, решение по коммуникации. Звонки в raidcall, skype, teams, mattermost, google chat, slack, jabber и телеге звучат безумно, особенно, когда нужно быстро подключиться, пошарить экран, а самое главное — не думать о том, что шаришь кому-то телеметрию за проприетарное 3rd-party. Когда ведешь лекции, особенно бесит, что дискорд блочит screen sharing, когда там юзеров >25, а в гугл мит есть лимит по времени. Разумеется, читатель имеет право предъявить за проприетарные ОС, браузеры и прочие ПО, либо, что мне просто лень платить, но скорее просто хочется поискать велосипедов, да и поделиться с вами, что накопал и начал юзать. Как и говорилось ранее, я попытался придумать оправданий для велосипедов, едем-те. Надо пойти посмотреть демку - https://chitchatter.im/Выглядит более чем достаточно, переходя по кнопкам находим, что есть: mardkown-based чат, видео, аудио, шаринг экрана и файлов Тестим и понимаем, что работает вроде как норм, осталось разобраться что под капотом. Копаем про этого зверя и находим репу  trystero, а с ним и вебсайт, обещающий сделать любое решение мультиплеером — https://oxism.com/trystero/ Trystero can connect peers via 🌊 BitTorrent, 🐦 Nostr, 📡 MQTT, 🪐 IPFS, and 🔥 Firebase. Посидел, покурил, не понял, думаю комментаторы лучше объяснят, почему для https://en.wikipedia.org/wiki/Session_Description_Protocol нужен торрент, жду объяснений в комментариях. Учитывая, что все обещают зашифрованный трафик, вроде как звучит норм. Поменять homepage в package.json, чтобы приложение резолвило к себе статику Пишем в Title: DEPLOY_KEYПишем в Key: значение сгнерированного публичного ключа: cat gh-pages.pubСтавим галочку возле Allow write access Оно ищет под https://the-homeless-god.github.io/assets/index-OD1TD8_t.js В любом случае, давайте настроим, чтобы лишний раз убедиться, что всё работает, а вдруг и кому-то достаточно только этого. Так что 1 раз пушим в мастер редактирование в package.json поля homepage на ссылку на наш github page Помогло? Нет.Идем в vite.config.js и указываем ему base Берите тачку где хотите, мне нравятся ruvds, digitalocean и из самых бюджетных — это justhost. Берите бубунту (осуждаю, но для статьи на хабре пойдет). И редачим конфиг: vim /etc/nginx/nginx.conf Дальше в nginx.conf пишем чтобы доступов не было Остаётся только убирать налету информацию про ip адрес в homepage в package.json и удалить содержимое vite.config.ts про base Теперь у вас как и у меня есть свой спрятанный на сервере клиент для коммуникации без проприетарного бугага. Может быть вы что-то накоммуниздите здесь полезного для себя, а может что-то напишите тут на улучшение. Вы можете написать адаптер который вклинивается в этот код с вашими дописками хоть из-под другого npm пакета, хоть из-под git submodule. Я бы написал как это всё дело скрыть под VPN, но мне лень писать статью дольше чем сейчас, так как просто сидел в дискорде и хотел показать коллегам другие способы коммуницировать и шарить экран, а поэтому это тянет для другой статьи организации демо-энва только для режима разработчиков. Среди прочего, упустил немногое про бубунту, просто потому что ну не люблю я её, используйте BSD. А в остальном — ничего нигде не хранится, всё шифруется, да и всё бесплатно, если не нужно приватить доступ к статике. И можно вести лекции не боясь, что качество просядет и общаться с коллегами используя максимальный стриминг. Спасибо за внимание, надеюсь следующая статейка выйдет раньше чем разница между этой и предыдущей.",
    "5": "Предлагаю вашему вниманию не то чтобы что-то уникальное и необычное, но скорее полезное и хозяйственно-применимое. Это небольшая коллекция систем хранения и приспособлений, которые я имплементировал  у себя в мастерской. Всегда любил рассматривать такие решения у других диайвайщиков, теперь поделюсь своими, может кому-то пригодится. Ставим в угол и используем как хранилище для металлических заготовок. По мере отрезания кусочков заготовки мигрируют вниз по течению. Сразу же варим и вторую такую же. Подсказка: при сварке нужно укладывать швы вдоль той трубы, которая является \"спинкой\". Если укладывать поперёк, то \"спинку\" сильно ведёт и потом её приходится долго и мучительно разгибать. На следующем фото левая деталь сварена правильно, а правая - не очень. Ещё понадобится одна аналогичная деталь но с замыкающей второй \"спинкой\", так чтобы получилась полная решётка. Её нужно будет прикрутить к стенке в самом низу (обратите внимание на пару торчащих ножек, которыми она опирается на пол). Две предыдущие детали вешаются на стену повыше. От опрокидывания их удерживают пары упоров, через которые они к стене и прикручены. Также на этом фото хорошо видны ряды коаксиальных отверстий в гребёнках. Теперь можно поставить длинные заготовки вертикально и зафиксировать их, вставив 10мм прутки в отверстия. В данном случае я фиксирую баллон при помощи проволоки. Но если бы я повторял эту конструкцию, то я бы добавил ещё пару \"рожек\" сверху, чтоб баллон не болтался. Рынок предлагает нам широкий выбор стеллажей на любой манер и размер, но если хочется не полочки для вышивки, а настоящее хранилище для тяжёлого железа, то придётся раскошелиться. Бюджетно и сурово можно сварить стеллаж из профиля по-месту, но тогда там же его и красить придётся, ведь он не будет транспортабельным. Я люблю отдавать детали в порошковую покраску, поэтому у меня решение для стеллажей состоит в том, чтобы сварить отдельно рамы полок, просверлив заранее крепёжные отверстия, и отдать их в покраску вместе со стойками: В строительном супермаркете можно заказать порезку OSB согласно чертежу. Ну а затем прикрутить стойки к стене, а рамы к стойкам. Моя оригинальная статья на Medium, где можно также почитать и другие, которые я ещё не успел перевести.",
    "6": "Оценка рабочих процессов через опросы представляет собой мощный инструмент, который позволяет организациям выявлять \"узкие места\", понимать потребности и ожидания своих сотрудников, а также формировать стратегии для повышения общей производительности и эффективности. Статья, которую вы сейчас читаете, адресована HR-специалистам, руководителям отделов и аналитикам, которые ищут способы сделать свои команды и процессы более гибкими и отзывчивыми. В ней мы рассмотрим, как правильно разработать и провести опросы для оценки рабочих процессов, а также как анализировать полученные данные для внедрения необходимых изменений. Эта тема будет интересна всем, кто стремится улучшить эффективность внутренних процессов своей компании. Правильно спланированные и проведенные опросы могут предоставить ценную обратную связь от сотрудников, выявить скрытые проблемы и возможности для улучшения. В результате, компании смогут не только повысить удовлетворенность и мотивацию своих работников, но и достичь значительного прогресса в своих бизнес-показателях. Приступая к чтению, помните, что каждый совет и предложение, изложенные здесь, основаны на реальном опыте и проверены практикой. Мы предлагаем вам взглянуть на привычные вещи под новым углом и обнаружить потенциал для роста и развития вашего бизнеса. Опросы по оценке рабочих процессов играют ключевую роль в стратегии управления и развития компании. Регулярное собирание обратной связи от сотрудников не только способствует созданию культуры открытости и доверия, но и является мощным инструментом для повышения эффективности работы и улучшения рабочего климата. Обратная связь от сотрудников дает руководителям и HR-специалистам возможность увидеть рабочие процессы \"глазами\" самих исполнителей, что часто раскрывает аспекты, недоступные для высшего руководства из-за их удаленности от ежедневных операций. Это позволяет не только адаптировать условия труда к потребностям команды, но и значительно увеличивает лояльность и мотивацию сотрудников, поскольку они видят, что их мнение ценится и на него опираются при принятии решений. Ключевая ценность опросов заключается в способности выявлять \"узкие места\" в рабочих процессах — те аспекты деятельности, которые замедляют работу, снижают качество выполнения задач или порождают конфликты внутри команды. Это могут быть как технические проблемы (например, недостаток оборудования), так и организационные (неэффективное распределение задач, непонимание целей проекта среди сотрудников). Регулярно проводимые опросы создают механизм постоянного совершенствования, позволяя не только определять проблемы, но и отслеживать динамику их решения, а также эффективность внедряемых изменений. Такой подход не только способствует улучшению рабочих процессов, но и стимулирует сотрудников к активному участию в жизни компании, повышая их заинтересованность в общем успехе. Использование сервиса Тестограф для проведения опросов по оценке рабочих процессов позволяет с легкостью создавать и распространять анкеты, а также анализировать полученные данные. Благодаря широкому спектру вопросов и гибким настройкам, каждая компания может адаптировать инструмент под свои уникальные нужды и цели, делая процесс сбора и анализа обратной связи максимально эффективным и удобным. Создание эффективного опроса, направленного на оценку рабочих процессов, требует тщательного планирования и внимания к деталям. Ниже приведена пошаговая инструкция, которая поможет вам разработать опрос, способный дать ценные инсайты и способствовать улучшению производительности и рабочего климата в вашей компании. Перед тем как приступить к составлению вопросов, важно четко определить, что вы хотите узнать с помощью опроса. Цели могут варьироваться от оценки общей удовлетворенности рабочими условиями до выявления конкретных проблем в определенных процессах или отделах. Цели должны быть конкретными, измеримыми, актуальными, реалистичными и ограниченными во времени (SMART). Закрытые вопросы для получения количественных данных (например, оценка удовлетворенности по шкале от 1 до 10). Открытые вопросы для качественных отзывов и предложений по улучшению. Вопросы с множественным выбором для идентификации наиболее распространенных вариантов ответов. Вопросы должны быть ясными и точными, без двусмысленных формулировок, чтобы избежать недопонимания и получить максимально точные ответы. Для упрощения процесса создания опросов Тестограф предлагает готовые шаблоны, специально разработанные для оценки различных аспектов работы компании. Эти шаблоны могут служить отличной отправной точкой, которую можно адаптировать под специфику вашей организации. Использование шаблонов помогает сэкономить время на дизайне опроса и сосредоточиться на анализе полученных данных. Посетите раздел Примеры опросов и шаблоны анкет на сайте Тестографа, чтобы выбрать наиболее подходящий шаблон для вашего опроса по оценке рабочих процессов. Следуя этим шагам, вы сможете создать эффективный инструмент для сбора обратной связи, который поможет вам и вашей команде выявлять и устранять проблемы, повышать эффективность работы и улучшать рабочий климат. Создание вопросов для опроса по оценке рабочих процессов требует глубокого понимания как текущих операционных процессов, так и целей улучшения. Вот несколько примеров вопросов, которые помогут собрать ценную информацию о различных аспектах рабочего процесса: Есть ли в рабочих процессах аспекты, которые вам не нравятся? Пожалуйста, укажите их и предложите, как их можно улучшить. (открытый вопрос) Приведите примеры ситуаций, когда плохая координация задач или недопонимание в коммуникации приводили к проблемам в работе. (открытый вопрос) Какие области рабочих процессов, по вашему мнению, требуют наибольших улучшений? (вопрос с множественным выбором, например: коммуникация, координация задач, IT-инфраструктура, обучение сотрудников и т.д.) Эти вопросы предназначены для глубокого анализа и оценки различных аспектов рабочих процессов, выявления потенциальных проблем и областей для улучшения. Они помогут собрать ценные данные, на основе которых можно будет разработать эффективные стратегии оптимизации и инноваций. Интеграция опросов в рабочий процесс — ключевой шаг для обеспечения непрерывного улучшения и развития компании. Чтобы опросы приносили максимальную пользу, необходимо внедрить их таким образом, чтобы они не отвлекали сотрудников от работы, а скорее дополняли и обогащали их рабочий процесс. Использование сервиса Тестограф позволяет автоматизировать многие аспекты создания, распространения и анализа опросов. Определите оптимальную частоту проведения опросов. Регулярность опросов должна быть достаточной для сбора актуальной информации, но не слишком частой, чтобы не создавать нагрузку на сотрудников. Это может быть ежеквартальный или полугодовой опрос, а также опросы после завершения крупных проектов. Интегрируйте опросы в корпоративные инструменты. Используйте платформы, которыми сотрудники пользуются ежедневно (например, корпоративные мессенджеры, почта, интранет), чтобы уведомлять о новых опросах и напоминать о них. Это сделает участие в опросах удобным и естественным дополнением к рабочему дню. Оптимизируйте время на заполнение. Стремитесь к тому, чтобы заполнение опроса занимало не более 5-10 минут. Четкие и конкретные вопросы помогут сотрудникам быстро ответить на них, не отвлекаясь на длительное размышление. Автоматическая рассылка опросов. Тестограф позволяет настроить автоматическую отправку опросов согласно выбранному графику. Это обеспечивает регулярность сбора данных без необходимости ручного вмешательства. Автоматический анализ результатов. Система аналитики Тестографа автоматически обрабатывает собранные данные, предоставляя визуализации в виде графиков и таблиц. Это позволяет быстро оценить общие тенденции и выявить области, требующие внимания. Интеграция с корпоративными системами. Благодаря возможности интеграции с помощью API, Тестограф может быть встроен в существующие корпоративные инструменты и системы управления ресурсами компании, что упрощает обработку результатов и их использование для принятия управленческих решений. Интегрируя опросы в ежедневный рабочий процесс и используя автоматизацию для их распространения и анализа результатов, компании могут эффективно собирать и использовать ценную обратную связь от сотрудников, минимизируя при этом их отвлечение от работы. Анализ результатов опросов по оценке рабочих процессов и последующее предпринятие действий по улучшению — это ключевые этапы, обеспечивающие непрерывное совершенствование и развитие компании. Тщательный анализ позволяет преобразовать сырые данные в ценные инсайты и конкретные рекомендации для дальнейших действий. Оценка общих тенденций. Сначала рассмотрите общие результаты по всем вопросам, чтобы получить представление о масштабах удовлетворенности или неудовлетворенности рабочими процессами. Высокие и низкие оценки предоставят общее понимание сильных и слабых сторон. Глубокий анализ конкретных вопросов. Далее проанализируйте ответы на конкретные вопросы для выявления специфических проблем или областей, требующих улучшения. Поиск закономерностей и частых упоминаний конкретных аспектов рабочих процессов может указывать на наиболее значимые для сотрудников моменты. Сегментация данных. Разделение данных по отделам, должностям или длительности работы в компании может выявить уникальные проблемы или взгляды определенных групп сотрудников. Это позволит более точно нацелить усилия по улучшению. Качественный анализ открытых ответов. Ответы на открытые вопросы могут содержать богатый материал для анализа, включая предложения по улучшению и описание конкретных проблемных ситуаций. Это требует времени для тщательного рассмотрения, но может дать глубокие инсайты. Разработка программ обучения и развития. Если сотрудники указывают на недостаток навыков или знаний для эффективной работы, можно разработать целевые программы обучения или менторства. Оптимизация инструментов и процессов. Выявление \"узких мест\" в рабочих процессах может привести к внедрению новых инструментов для совместной работы, обновлению программного обеспечения или пересмотру рабочих процедур. Улучшение коммуникации. Если опросы показывают проблемы в коммуникации между отделами или уровнями управления, может потребоваться разработка новой стратегии внутренней коммуникации или внедрение регулярных встреч для синхронизации. Изменение в корпоративной культуре. Ответы на опросы могут указывать на необходимость изменений в корпоративной культуре, например, для укрепления доверия и открытости, или для создания более инклюзивной рабочей среды. Пересмотр политик HR. Результаты опросов могут выявить потребности в изменении политик по найму, удержанию или мотивации сотрудников, например, введение гибкого графика работы или обновление системы поощрений. После анализа результатов и определения направлений для улучшений, важно разработать четкий план действий, включая сроки и ответственных за выполнение, а также предусмотреть механизмы отслеживания прогресса и оценки эффективности предпринятых мер. Опросы по оценке рабочих процессов являются неотъемлемой частью стратегии улучшения и развития любой организации. Они не только предоставляют ценную обратную связь от сотрудников, но и служат инструментом для выявления сильных и слабых сторон в рабочих процедурах, коммуникации и корпоративной культуре. Регулярное проведение и анализ результатов опросов помогают руководству компании и HR-специалистам эффективно реагировать на изменения внутренней среды и внешних условий, адаптироваться к потребностям сотрудников и рынка, способствуя повышению уровня удовлетворенности и производительности труда. Инструменты, такие как Тестограф, делают процесс создания, проведения и анализа опросов максимально удобным и эффективным. С помощью Тестографа вы можете легко разработать опрос, адаптированный под уникальные потребности вашей компании, и получить доступ к авангардным аналитическим инструментам для обработки результатов. Автоматизация рутинных процессов и использование предложенных шаблонов позволяют сосредоточить внимание на самом важном — принятии обоснованных управленческих решений, направленных на улучшение. Помните, что каждый опрос — это шаг к пониманию и улучшению ваших рабочих процессов. Независимо от размера вашей компании или сферы деятельности, сбор и анализ обратной связи от сотрудников играют ключевую роль в создании продуктивной и вовлеченной рабочей среды.",
    "7": "Пространство имен - это важный концепт в программировании, позволяющий группировать элементы и предотвращать конфликты имен. В этом посте мы покажем, как мы применяем этот концепт к API, чтобы облегчить композицию и интеграцию различных сервисов. Мы покажем вам, как интегрировать 8 сервисов: SpaceX GraphQL, 4x GraphQL с использованием Apollo Federation, REST API с использованием OpenAPI Specification, API на основе PostgreSQL и API на основе Planetscale-Vitess (MySQL) всего несколькими строками кода, полностью автоматически, без каких-либо конфликтов. Когда вы устанавливаете пакет npm, он находится в своем собственном пространстве имен. Одним из таких пакетов является axios, очень популярный клиент для выполнения HTTP-запросов. Это устанавливает зависимость axios в вашу папку node_modules и добавляет ее в ваш файл package.json. Импортируйте зависимость, дайте ей имя, в данном случае просто axios, затем используйте ее. Мы также могли бы переименовать axios в bxios. Переименование импорта - это важный элемент управления зависимостями, чтобы избежать конфликтов. Одно из основных правил - у вас не должно быть двух импортов с одним и тем же именем, иначе у вас возникнет конфликт имен, и будет непонятно, как должна выполняться программа. Хорошо, достаточно введения. Вы, вероятно, уже знакомы со всем этим, что это имеет отношение к API? Многое! По крайней мере, я так думаю. Весь этот рабочий процесс потрясающий! Вы можете написать код, упаковать его в виде пакета npm, опубликовать его, и другие смогут импортировать и использовать его очень легко. Это такой приятный способ сотрудничать с помощью кода. Как это выглядит при использовании API? Ну, это не такая смазанная машина. С API мы все еще в каменном веке, когда речь идет об этом рабочем процессе. Некоторые компании предлагают SDK, который вы можете скачать и интегрировать. Другие просто публикуют REST или GraphQL API. У некоторых из них есть спецификация OpenAPI, другие просто предлагают свою собственную пользовательскую документацию по API. Представьте, что вам пришлось бы интегрировать 8 сервисов, чтобы получить данные от них. Почему бы вам просто не запустить что-то похожее на yarn add axios и выполнить работу? Почему так сложно объединить сервисы? Чтобы добиться этого, нам нужно решить ряд проблем. Давайте рассмотрим проблемы по очереди. Первая проблема, которую нужно решить, - это нам нужен общий язык, на котором будет основываться наш подход к реализации. Не отвлекаясь на сторонние темы, позвольте мне объяснить, почему GraphQL отлично подходит для этой цели. GraphQL обладает двумя очень мощными функциями, которые необходимы для нашего случая использования. С одной стороны, он позволяет нам запрашивать именно те данные, которые нам нужны. Это очень важно, когда мы используем много источников данных, так как мы можем легко проникнуть в поля, которые нас интересуют. С другой стороны, GraphQL позволяет нам легко строить и следовать за ссылками между типами. Например, у вас могут быть две конечных точки REST, одна с сообщениями, другая с комментариями. С GraphQL API перед ними, вы можете построить ссылку между двумя объектами и позволить вашим пользователям получать сообщения и комментарии с помощью одного запроса. Кроме того, у GraphQL есть процветающее сообщество, много конференций и людей, активно участвующих, создающих инструменты вокруг языка запросов и многое другое. Стоит отметить, что у GraphQL также есть слабое место, когда речь идет об интеграции API. У него нет концепции пространств имен, что делает его использование для интеграции API немного сложным, до сих пор! Когда речь идет об интеграции сервисов, на данный момент существуют два основных подхода к решению проблемы. Во-первых, это Сшивка схемы (Schema Stitching), а затем - Федерация. С помощью Сшивки схемы вы можете объединять сервисы GraphQL, которые не знают о сшивке. Объединение API происходит в централизованном месте, шлюзе API GraphQL, без ведома сервисов. Федерация, определенная Apollo, с другой стороны, предлагает другой подход. Вместо централизации логики и правил сшивки, федерация распределяет их по всем микросервисам GraphQL, также известным как Подграфы. Каждый Подграф определяет, как он вносит свой вклад в общую схему, полностью осознавая, что существуют другие Подграфы. Здесь нет \"лучшего\" решения. Оба подхода хороши для микросервисов. Они просто разные. Один предпочитает централизованную логику, в то время как другой предлагает децентрализованный подход. Оба подхода имеют свои собственные проблемы. Тем не менее, проблема интеграции сервисов выходит далеко за рамки федерации и сшивки схемы. Ваша компания должна иметь один объединенный граф, а не несколько графов, созданных каждой командой. Имея один граф, вы максимизируете значение GraphQL: Когда команды создают свои собственные индивидуальные графы без координации своей работы, это неизбежно приводит к тому, что их графы начинают перекрываться, добавляя одни и те же данные в граф несовместимыми способами. В лучшем случае, это обходится дорого; в худшем случае, это создает хаос. Этому принципу требуется следовать как можно раньше на пути принятия графа компанией. Давайте сравним этот принцип с тем, что мы узнали о коде выше, вы знаете, пример с axios и bxios. Представьте, что был один гигантский пакет npm на компанию со всеми зависимостями. Если бы вы хотели добавить axios в свой пакет npm, вам пришлось бы вручную скопировать весь код в свою собственную библиотеку и сделать его \"своим\" пакетом. Это было бы не поддерживаемо. Один единый граф звучит здорово, когда вы находитесь в полной изоляции. На самом деле, однако, это означает, что вы должны добавить все внешние API, все \"пакеты\", которые вы не контролируете, в свой один граф. Эту интеграцию должны поддерживать вы сами. Это правда. С помощью всего одного графа мы можем легко делиться запросами между командами. Но является ли это действительно функцией? Если мы разделим наш код на пакеты и опубликуем их отдельно, другим будет легко выбрать именно то, что им нужно. Представьте себе один граф с миллионами полей. Это действительно масштабируемое решение? Как насчет просто выбора подчастей гигантской схемы GraphQL, которые действительно важны для вас? С помощью всего одной схемы у нас может быть централизованный каталог, это правда. Но помните, что этот каталог может представлять только наш собственный API. А как насчет всех других API в мире? Кроме того, почему мы не можем иметь каталог из нескольких API? Просто как пакеты npm, которые вы можете искать и просматривать. Я бы поспорил, что на самом деле обратное верно. Особенно с Федерацией, предложенным Apollo решением для реализации графа, становится гораздо сложнее поддерживать ваш граф. Если вы хотите объявить устаревшими определения типов в нескольких подграфах, вам придется тщательно координировать изменение во всех из них. Микросервисы не действительно микро, если между ними есть зависимости. Этот шаблон скорее называется распределенный монолит. Интересно, что должно быть возможно, но не является реальностью. Мы еще не видели централизованной системы политик контроля доступа, которая добавляет контроль доступа на основе ролей для федеративных графов. О, на самом деле это одна из наших функций, но давайте сегодня не будем говорить о безопасности. Построение одного единого графа звучит как отличная идея, когда вы находитесь в полной изоляции на маленьком острове без интернета. Вероятно, вы не собираетесь использовать и интегрировать сторонние API. Любой другой, кто подключен к интернету, вероятно, захочет интегрировать внешние API. Хотите проверить продажи с помощью API Stripe? Отправлять электронные письма через Mailchimp или Sendgrid? Действительно ли вы хотите вручную добавлять эти внешние сервисы в свой \"Один граф\"? Принцип \"Один граф\" не проходит проверку реальностью. Вместо этого нам нужен простой способ составить несколько графов! Мир разнообразен. Многие отличные компании предлагают действительно хорошие продукты через API. Давайте упростим построение интеграций, не прибегая к ручному добавлению их в наш \"Один граф\". Это приводит нас ко второй проблеме, конфликтам имен. Представьте, что и Stripe, и Mailchimp определяют тип Customer, но у каждого из них свое понимание Customer, с разными полями и типами. Как это достичь? Давайте немного разберем эту проблему. Поскольку у GraphQL нет встроенной функции пространств имен, нам придется быть немного креативными. Во-первых, нам нужно устранить все конфликты имен для типов. Это можно сделать, добавив суффикс к каждому типу \"Customer\" с пространством имен. Так, у нас были бы \"Customer_stripe\" и \"Customer_mailchimp\". Первая проблема решена! Еще одной проблемой, с которой мы могли бы столкнуться, являются конфликты имен полей в корневых типах операций, то есть в типах Query, Mutation и Subscription. Мы можем решить эту проблему, добавив префикс ко всем полям, например, stripe_customer(by: ID!) и mailchimp_customer(by: ID!). Что произойдет, если вы определите директиву под названием @formatDateString и две схемы, но у них разное значение? Не приведет ли это к непредсказуемым путям выполнения? Да, вероятно. Давайте также исправим это. Мы можем переименовать директиву в @stripe_formatDateString и @mailchimp_formatDateString соответственно. Таким образом, мы можем легко различать две директивы. С этим все конфликты имен должны быть решены. Мы уже закончили? На самом деле нет. К сожалению, с нашим решением мы создали много новых проблем! Переименовав все типы и поля, мы на самом деле создали много проблем. Давайте посмотрим на этот запрос: Поле mailchimp_customer не существует в схеме Mailchimp, мы должны переименовать его в customer. Директива mailchimp_formatDateString также не существует в схеме Mailchimp. Мы должны переименовать ее в formatDateString перед отправкой в апстрим. Но будьте осторожны с этим! Убедитесь, что эта директива действительно существует на источнике. Мы автоматически проверяем это, так как вы можете случайно использовать неправильную директиву на неправильном поле. Наконец, определение типа PaidCustomer_mailchimp также не существует в исходной схеме. Мы должны переименовать его в PaidCustomer, иначе источник его не поймет. Звучит как много работы? На самом деле, это уже сделано, и вы можете использовать это прямо сейчас. Просто введите yarn global add @wundergraph/wunderctl в своем терминале, и вы готовы его опробовать! (*deprecated, утилиту переписали с TypeScript на GoLang) С этим мы готовы к фазе реализации. На первом шаге нам нужно \"импортировать\" наши зависимости API. Мы можем сделать это с помощью SDK WunderGraph. Просто \"интроспектируйте\" все различные сервисы и объедините их в \"приложение\". Если вы посмотрите на код, вы, вероятно, заметите ключевое слово apiNamespace несколько раз. apiNamespace гарантирует, что каждый API находится в своей собственной границе. Таким образом, автоматически избегаются конфликты имен. Как только вы проинтроспектировали все зависимости, мы готовы написать запрос, который охватывает все 8 сервисов. Мы хотим получить пользователей из API SpaceX, пользователей из API JSON Placeholder, больше пользователей из нашей базы данных PostgreSQL, еще больше пользователей из базы данных Planetsacle и, наконец, одного пользователя с отзывами и продуктами из федеративного графа. Все это возможно благодаря нашему богатому набору источников данных. Обратите внимание, как он использует префиксированные/пространственные корневые поля. Этот запрос дает нам данные от всех 8 сервисов сразу, это сумасшествие! Теперь запустите wunderctl up, чтобы все это заработало за пару секунд. Это работает на вашем локальном компьютере без вызова каких-либо облачных сервисов. В этом посте мы начали с обсуждения того, как пространства имен облегчают написание и обмен кодом. Затем мы исследовали различия между \"подходом к кодированию\" и необходимостью иметь дело с интеграцией API. Мы изучили Сшивку схем и Федерацию и узнали, что оба подхода хороши, но этого недостаточно. Мы рассмотрели принцип \"Один граф\" и поняли, что у него есть свои недостатки. Наконец, мы представили концепцию пространств имен GraphQL API, что позволяет объединять GraphQL, Federation, REST, PostgreSQL и Planetscale API всего в несколько строк кода. Если вы заинтересованы в том, чтобы увидеть все это в действии, здесь есть видео, где я прохожу через весь процесс: https://youtu.be/jUaJkvPmCSQ Наша цель для WunderGraph - стать \"Менеджером пакетов для API\". Мы еще не совсем там, но в конечном итоге вы сможете запустить wunderctl integrate stripe/stripe, затем написать Запрос или Мутацию, и интеграция будет выполнена.",
    "8": "Миры с ледяными океанами, такие как Европа или Энцелад, являются одними из самых перспективных мест для поиска внеземной жизни в Солнечной системе, поскольку на них есть жидкая вода. Но чтобы определить, скрывается ли что-либо в их инопланетных океанах, нам нужно преодолеть ледяной покров толщиной в десятки километров. Любые роботы, которых мы отправим пробираться через лёд, должны будут проделать большую часть работы самостоятельно, потому что связь с этими лунами занимает до 155 минут. Исследователи, работающие над проектом Лаборатории реактивного движения НАСА по разработке технологии под названием Exobiology Extant Life Surveyor (EELS), возможно, найдут решение обеих этих проблем. Оно заключается в использовании управляемого искусственным интеллектом робота-змеи. И его уже построили. Самой популярной идеей на тему того, как пробиться сквозь ледяной покров Энцелада или Европы, до сих пор было термическое бурение — метод, используемый для исследования ледников на Земле. При этом используется горячий бур, который просто расплавляет лёд. \"Многие люди работают над различными подходами к термическому бурению, но все они сталкиваются с проблемой накопления осадочных пород, что влияет на количество энергии, необходимой для значительного продвижения через ледяной щит\", — говорит Мэтью Глиндер, руководитель проекта EELS. Поэтому вместо того, чтобы бурить новые отверстия во льду, команда EELS сосредоточилась на использовании уже имеющихся. Миссия \"Кассини\" обнаружила похожие на гейзеры струи, выбрасывающие воду в космос из отверстий в ледяном покрове вблизи южного полюса Энцелада. \"Концепция заключалась в том, чтобы были высадиться вблизи жерла гейзера, затем робот должен был проползти по поверхности спутника, спуститься в жерло, исследовать его и через жерло спуститься дальше в океан\", — говорит Мэтью Робинсон, руководитель проекта EELS. Проблема заключалась в том, что лучшие снимки \"Кассини\" той области, где должен был бы приземлиться аппарат, имеют разрешение примерно 6 метров на пиксель, а значит, серьёзные препятствия для посадки могут остаться незамеченными. Ещё хуже то, что эти снимки крупным планом были монокулярными, а значит, мы не могли правильно определить рельеф местности. \"Посмотрите на Марс. Сначала мы отправили орбитальный аппарат. Затем мы отправили посадочный аппарат. Затем мы отправили маленького робота. А потом мы отправили большого робота. Такая парадигма исследования позволила нам получить очень подробную информацию о местности\", — говорит Рохан Тхаккер, руководитель проекта автономного управления EELS. \"Но чтобы добраться до Энцелада, требуется от семи до 11 лет. Если бы мы следовали той же парадигме, нам потребовалось бы столетие\", — добавляет он. Чтобы справиться с незнакомой местностью, команда EELS создала робота, способного пройти практически через всё — универсальную змееподобную конструкцию длиной около 4,4 метра и диаметром 35 сантиметров, вдохновлённую биологическими примерами. Она весит около 100 килограммов (по крайней мере, на Земле). Она состоит из 10 практически идентичных сегментов. \"Каждый из этих сегментов использует комбинацию из привода, меняющего форму, и привода, который вращает винты, установленные на внешней стороне сегментов, чтобы двигать робота по окружающей среде\", — объясняет Глиндер. Используя эти два типа приводов, робот может двигаться, используя то, что команда называет \"кожной тягой\", которая опирается на вращение винтов, или используя один из различных видов движений, основанных на форме, которые опираются на приводы формы. \"Робот способен передвигаться, просто благодаря прижатию к поверхности\", — говорит Глиндер. Стандартный набор датчиков установлен на голове и включает в себя набор стереокамер, обеспечивающих угол обзора в 360 градусов. Есть также инерциальные измерительные блоки (IMU), которые используют гироскопы для оценки положения робота, и лидарные датчики. Кроме того, у робота есть чувство осязания. \"Мы собираемся установить датчики силы крутящего момента в каждом сегменте. Таким образом, у нас будет прямой крутящий момент плюс прямое ощущение сопротивления в каждом суставе\", — объясняет Робинсон. Всё это должно позволить роботу EELS безопасно подниматься и спускаться по жерлам Энцелада, удерживаться на месте в случае извержений, прижимаясь к стенам, и даже ориентироваться только на ощупь, если камеры и лидар не работают. Но, пожалуй, самой сложной частью создания робота EELS был его мозг. О том, чтобы управлять EELS, находящимся на Энцеладе, вручную с Земли, не могло быть и речи из-за огромной задержки связи, поэтому команда пошла на практически полную автономность. Наземное управление будет ограничено выдачей общих команд, таких как \"исследовать эту область\" или \"искать жизнь\". «Представьте себе нечто вроде о программного обеспечения, позволяющего автомобилю Tesla передвигаться автономно – только у вашего автомобиля 48 рулевых колёс, 48 наборов педалей, и перемещается он в пространстве, где нет ни дорог, ни знаков \"стоп\", ни ограничений скорости», — объясняет Тхаккер. ИИ, управляющий роботом EELS, был построен на основе иерархической многоуровневой программной архитектуры, состоящей из двух категорий модулей, называемых оценщиками и контроллерами. Оценщики самого низкого уровня получают информацию от внутренних датчиков, таких как IMU и датчики силы крутящего момента в сегментах, и используют её для определения состояния робота — падает ли он, скользит или ударяется обо что-то. Уровнем выше находятся оценщики, которые строят карту окружающей среды и определяют местоположение робота на основе данных с камер и лидаров. Самый высокоуровневый оценщик учитывает риск и решает, когда нужно двигаться быстро, а когда — осторожно. Контроллеры, отвечающие за принятие мер, варьируются от базовых систем управления приводами на самом низком уровне до планирования задач и движений на самом высоком уровне. \"У вашего разума есть две стороны. Есть интуитивная сторона, которая быстра, предвзята и очень маломощна, и есть логическая сторона, которая задаёт вопросы, оценивает ответы и пытается понять, что на самом деле происходит. Мы пытаемся использовать ту же схему, где есть две такие подсистемы\", — говорит Тхаккер. Интуитивная часть EELS была построена с использованием машинного обучения, в ходе которого робот сам обучился тому, как двигаться. Логическая часть представляет собой модель, основанную на физике, с жёстко закреплёнными правилами безопасности, которые не позволяют роботу превышать определённую скорость, преодолевать склоны с определённым уклоном и так далее. Всё это должно помочь роботу EELS хорошо освоиться на чужих ледяных мирах. Если он вообще туда попадёт. \"В настоящее время мы не являемся частью какой-либо полётной миссии\", — говорит Робинсон. Однако, по его словам, архитектура EELS может быть использована во многих других местах, включая Землю. \"Когда мы тестировали EELS на леднике Атабаска в Канаде, мы использовали его для реальных научных исследований. Мы разработали научный прибор, который измерял содержание соли в воде, текущей в леднике. У робота-змеи есть наземное и космическое применение. Его можно использовать для поисково-спасательных работ, изучения груды обломков и так далее. Но Энцелад остаётся для нас источником вдохновения\", — утверждает Робинсон. Научпоп. Проповедую в храме науки.",
    "9": "В блоге beeline cloud на хабре уже выходила подборка полезных ресурсов по DevOps и Kubernetes для джунов и прожжённых специалистов. Сегодня продолжим тему облачной безопасности и поговорим об открытых проектах для защиты cloud-инфраструктуры и приложений. В сегодняшнем списке — инструменты для управления политиками доступа и сканеры уязвимостей, позволяющие проверить конфигурацию контейнеров на соответствие лучшим ИБ-практикам. Проект запустила компания AccuKnox, которая предлагает продукты для защиты облачных приложений. Он развивается с 2021 года под лицензией Apache 2.0. Для управления политиками безопасности KubeArmor использует LSM (Linux Security Modules) — в частности, AppArmor, SELinux или BPF-LSM. С помощью eBPF инструмент генерирует оповещения о событиях, связанных с контейнерами, подами и пространством имен. В то же время инструмент работает по модели белого списка. Она позволяет реализовать принцип нулевого доверия в кластере. Например, можно запретить запуск исполняемых файлов в определенной директории — другие варианты политик описаны в документации. Технологии eBPF и LSM помогают защитить облачные нагрузки в различных ситуациях, например, обеспечить мониторинг целостности файлов в ОС или запретить изменение корневых сертификатов. Кроме того, инструмент можно использовать в сетях IoT или 5G, поскольку контейнеры все чаще применяют для управления периферийными устройствами. Основное преимущество KubeArmor перед стандартными сканерами уязвимостей — возможность не только детектировать, но и предотвращать вредоносную активность. Для настройки политик можно использовать универсальный синтаксис YAML. По мнению некоторых, это удобнее по сравнению с другими утилитами, такими как AppArmor, имеющими собственный формат. Это еще один open source проект компании AccuKnox, выпущенный в июне 2023 года под лицензией Apache 2.0. Поскольку инструмент вышел меньше года назад, вокруг него не успело сформироваться сколько-нибудь крупное комьюнити. Изначально K8TLS разрабатывался с целью обеспечить безопасность в сетях 5G. Однако позже его стали использовать для защиты обычных кластеров Kubernetes в других сетях. Чтобы обеспечивать безопасный обмен данными между контейнерами и внешним миром, инструмент проверяет конфигурацию TLS-соединения на соответствие стандартам безопасности, таким как PCI-DSS, HIPAA, 3GPP. К параметрам проверки относится версия протокола, набор шифров, хэш-функция, подпись и сертификат. Например, согласно PCI-DSS 3.2, сервер должен поддерживать как минимум TLS 1.1, но предпочтительнее версия 1.2 и выше. Для сканирования не нужен прокси, который обычно снижает производительность всей системы. Плюс — проверку можно интегрировать в CI/CD-пайплайн, чтобы заблаговременно идентифицировать небезопасные порты. Результаты можно выгрузить в формате JSON. Хотя K8TLS и заточен под работу с протоколом Transport Layer Security, он позволяет отслеживать наличие уязвимости под названием Terrapin-SSH, обнаруженной в 2023 году. Она позволяет злоумышленникам проводить MITM-атаки и перехватывать данные в рамках SSH-сессии. Этот открытый инструмент разработала ИБ-компания Deepfence. Релиз состоялся в 2022 году под лицензией Apache 2.0. На данный момент успела выйти вторая полноценная версия решения, а его развитием занимается более 30 разработчиков. Задача ThreatMapper — выявлять уязвимости в программных компонентах облачных продакшн-платформ. Также инструмент позволяет обнаруживать вредоносное ПО, скомпрометированные секреты и несоответствие настроек безопасности требованиям. Проводить проверки можно в средах на базе Kubernetes, Docker и виртуальных машин. Чтобы сканировать рабочие нагрузки с помощью этого инструмента, на хостах нужно установить программные агенты в качестве привилегированных контейнеров. Этим агентам можно отправлять команды из консоли управления, например, на сбор телеметрии. В отличие от всех других инструментов в подборке, ThreatMapper предоставляет разнообразные средства визуализации. С их помощью можно наглядно представить взаимосвязь всех компонентов облачной инфраструктуры и на основе этой топологии исследовать поверхность атаки. Также есть модуль ThreatGraph, который строит графы с угрозами, найденными после сканирования. Дополнительное преимущество в том, что решение приоритезирует уязвимости по уровням опасности. Инструмент помогает не только выявлять уязвимости постфактум, но и действовать на опережение, а именно находить отклонения от лучших ИБ-практик. Например, конфигурацию хостов Kubernetes и Linux можно проверить по стандартам HIPAA, PCI-DSS, GDPR и NIST. По результатам сканирования решение предоставляет соотношение пройденных и проваленных проверок в виде диаграммы. Из других интересных функций ThreatMapper можно отметить автоматическую генерацию спецификации Software Bill of Materials — SBOM. Она представляет собой перечень всех зависимостей, файлов, библиотек и других компонентов ПО. Обычно для составления SBOM используют специальные утилиты — например, Syft — но ThreatMapper предлагает эту возможность из коробки. Что касается мнений сообщества, то на Reddit этот инструмент советуют в качестве сканера уязвимостей наряду с известными проприетарными решениями вроде Nessus. Также корпоративные пользователи отмечают, что ThreatMapper можно развернуть за считаные минуты и потратить освободившееся время на более важные задачи. Kubescape был опубликован на GitHub в 2021 году под лицензией Apache 2.0. Среди всех проектов из нашего списка у него самое масштабное сообщество контрибьюторов — более 100 разработчиков. Инструмент предназначен для обеспечения безопасности на нескольких уровнях: в среде разработки, CI/CD-пайплайне, кластерах Kubernetes. Сканирование на наличие уязвимостей и некорректных конфигураций проходит по фреймворкам: NSA-CISA, MITRE ATT&CK и CIS Benchmark. Рекомендации из этих стандартов собраны и систематизированы в библиотеке Regolibrary. Результаты проверок можно увидеть в командной строке или импортировать в форматах JSON, XML, HTML и PDF. Начать работу с Kubescape можно с помощью CLI или Helm-чарта. В последнем случае есть возможность сгенерировать сетевые политики и настроить непрерывное сканирование. Больше подробностей о преимуществах установки с помощью Helm — в документации. При этом авторы предупреждают, что использование альтернативных методов установки, например, с помощью Kustomize, Helmfile или кастомных скриптов, может привести к непредвиденным проблемам. Из интересных функций решения можно отметить экспериментальную генерацию документов VEX (Vulnerability Exploitability eXchange). В них содержится информация об уязвимостях, как в базе CVE (Common Vulnerabilities and Exposures), но записанная в машиночитаемом формате и с возможностью использовать данные из спецификации SBOM. Это позволяет автоматизировать процессы обнаружения угроз и реагирования на них. В целом пользователи хорошо отзываются об инструменте, но у некоторых возникают вопросы по поводу установки Kubescape. В репозитории указано, что она происходит с помощью скачивания скрипта в curl и его передачи в bash. Один из резидентов Hacker News в тематическом треде выразил сомнения, что такой способ можно считать безопасным. Но в крайнем случае, если и инструмент вызывает недоверие, решение можно развернуть в изолированной виртуальной машине. Разбираем противоречивые мнения об исследованиях ценности open source. Группа ученых представила модель, позволяющую оценить экономическое влияние open source решений на ИТ-отрасль. Работа вызывала противоречивые мнения, и тем интереснее, что это далеко не первая попытка «взвесить» open source. Нужно ли пересматривать модель open source — обсуждаем предложения сообщества. Продолжение темы о влиянии открытого ПО на развитие ИТ-экосистемы. Бизнес давно пытается «нащупать» баланс между коммерческой жизнеспособностью своих продуктов и духом открытого программного обеспечения. И все чаще поднимается вопрос миграции на ПО в формате post open source. Головоломки и среды Kubernetes: подборка новых языков программирования. Наш компактный материал, где мы обсуждаем причины появления новых языков программирования — на примере ЯП, увидевших свет за последние годы. Одни были разработаны энтузиастами для решения головоломок, а другие — для корпоративных задач. beeline cloud — secure cloud provider. Разрабатываем облачные решения, чтобы вы предоставляли клиентам лучшие сервисы.",
    "10": "По результатам опроса, в 2022 году геймдев казался наиболее привлекательной креативной индустрией для трудоустройства россиян от 14 до 35 лет. О желании связать жизнь с этой сферой рассказали 26% опрошенных — каждый четвёртый молодой человек. Тот же 2022 год для геймдева в России стал кризисным: ушли крупные международные компании, геймеры потеряли возможность легально покупать игры на зарубежных площадках — и объём рынка сократился, по данным аналитиков, на 80%. Однако к 2024 году стало ясно, что российский геймдев устоял и даже начал развиваться. За счёт чего восстанавливается индустрия, что сейчас происходит в отрасли и какие у неё перспективы — в нашей статье, которая будет полезна тем, кто задумывается о карьере в геймдеве или просто интересуется видеоиграми. В 2022 году из России ушли крупные международные компании, а российским производителям стало трудно продвигаться на зарубежных рынках. Игровая индустрия устроена таким образом, что не может существовать в изоляции: стоимость разработки и маркетинга не покрывается объёмом локального рынка, поэтому многие студии релоцировались из РФ. Оставшиеся оказались в довольно сложной ситуации: лишились не только выхода на глобальный рынок, но и программного обеспечения, на котором была завязана разработка. Большинство российских студий на самом деле всегда имели иностранное юрлицо в странах с более комфортными налоговыми условиями и для более удобных условий распространения продуктов на мировом рынке. Игры так устроены, что они изначально продукт вне границ. В целом российская игровая индустрия в 2022 году практически обнулилась. Необходимо было переориентироваться на внутреннего потребителя и искать выходы на новые международные рынки, в первую очередь — на китайский. Ситуация осложнялась и тем, что у российского геймдева и до кризиса были преграды для развития. В их числе можно назвать негативное отношение к геймингу в обществе. Помимо того, что геймдев — значимая часть экономики, исследования говорят о социальной пользе от видеоигр. Эксперименты показали, что игры помогают развивать интеллект и такие софт-скиллы, как умение работать в команде, комплексно оценивать проблемы и находить оптимальные пути их решения, планировать стратегически. Исследователи также выяснили, что дети, которые проводит много времени за видеоиграми, обладают более развитыми социальными и когнитивными навыками, чем их сверстники. Возможности видеоигр начинают использовать в медицине: для поддержания тонуса мозговой деятельности, восстановления после инсульта, лечения посттравматического стрессового расстройства и даже для диагностики депрессии. Однако в России до сих пор сильны стереотипы в отношении и геймеров и компаний-разработчиков игр, которые негативно сказываются на образе всей индустрии. Это показало масштабное исследование, которое провели в 2022 году аналитический центр НАФИ и «Организация развития видеоигровой индустрии». Оказалось, что в компьютерные игры играют 60% россиян — около 88 миллионов человек, а у 21% соотечественников геймингом увлекается кто-то из членов семьи. При этом большинство опрошенных полагает, что гейминг приносит человеку больше вреда, чем пользы. Опасения россиян по поводу того, что игры влияют на здоровье и социальную жизнь не лишены оснований: четверть российских геймеров (24%) признали, что им сложно контролировать время, которое они тратят на видеоигры. А у 19% геймеров на почве их увлечения возникают конфликты с другими членами семьи. менее половины россиян (41%) рассмотрели бы компанию, разрабатывающую видеоигры, в качестве желаемого потенциального работодателя. Конечно, не только негативное отношение к геймингу в обществе тормозило развитие игровой индустрии в России. Отрасль нуждалась в системном финансировании и государственной поддержке. И обстоятельства 2022 года подтолкнули к необходимости решать эти проблемы, чтобы вывести российский геймдев из кризиса. Весной 2022 года прошёл Российский интернет-форум, на котором встретились представители игровой отрасли и государства. После форума дискуссия о проблемах и будущем российского геймдева продолжилась, стали разрабатывать инициативы для поддержки отрасли и даже создавать соответствующие организации. Группа предпринимателей и разработчиков создала некоммерческую «Организацию развития видеоигровой индустрии» (РВИ), которая задалась целью объединить профессионалов отрасли — разработчиков, продюсеров, руководителей и собственников компаний, инвесторов — для взаимопомощи и развития российского геймдева. В числе задач РВИ: продвигать значимые для отрасли проекты, создавать положительный образ геймдева в обществе и привлекать в индустрию инвестиции. РВИ помогает студиям получать финансирование и взаимодействовать с бизнесом, научно-образовательными сообществами и государством, которое начало выделять гранты на проекты с социокультурной значимостью. Вот некоторые некоммерческие организации, которые уже помогают РВИ и, соответственно, всей игровой отрасли: фонд «Сколково» и другие. Многие некоммерческие организации и фонды не только финансово поддерживают оставшиеся в России студии, но и оказывают экспертную, технологическую, просветительскую помощь. «Россия — страна возможностей» запустила проект «Начни игру», цель которого — создать основу будущих кадров для индустрии. «Начни игру» — конкурс для начинающих разработчиков игр, художников, гейм-дизайнеров и всех тех, кто хотел бы начать карьеру в геймдеве. В рамках конкурса будущие специалисты погружаются в игровую индустрию и знакомятся с геймдевом изнутри. Всего за два сезона конкурс получил более 170 тысяч заявок из всех регионов России. Очень годный проект — «Начни игру». Хорошие менторы, отличные задания и много талантливых молодых ребят, которые, надеюсь, станут достойными разработчиками и основателями новых российских игровых студий. После того как у российских студий возникли трудности с импортным программным обеспечением и размещением игр на иностранных платформах, стало понятно, что нужны собственные наработки. Весной 2022 года VK объявила о запуске игровой платформы VK Play, которая взяла на себя распространение российских игр в нашей стране и странах СНГ. В 2024 году VK Play, наряду с российским магазином приложений RuStore, должна стать обязательной к предустановке на все устройства, которые продаются в РФ. Ещё VK начала разрабатывать первый российский игровой движок — Nau Engine. Компания инвестирует в проект миллиард рублей и активно набирает специалистов в команду. Анонсируется, что движок будет бесплатным, с открытым кодом. Это значит, что любой разработчик, в том числе начинающий, сможет использовать его, чтобы создать свою игру. Чтобы успешно развиваться, сфера видеоигр нуждается в избавлении от стереотипов и негативного отношения, а также в легитимизации на уровне государства и общества. Некоторые шаги в этом направлении делались ещё до кризиса 2022 года. В 2001 году Россия первой в мире внесла киберспорт в реестр официальных видов спорта. В 2006 году по ряду причин его пришлось исключить, но в 2016 киберспорт в стране признали окончательно. С тех пор любой геймер при желании может стать настоящим спортсменом, участвовать в соревнованиях и выигрывать призовые. Киберспорт может быть достойным ответом окружению геймера, которое со скепсисом относится к его увлечению. При этом с 2022 года киберспортсменов официально обязали заботиться о своей физической форме — Министерство спорта включило бег и приседания в стандарт подготовки киберспортсменов. Можно предположить, что таким образом государство начало работать с опасениями общества по поводу здоровья геймеров и предложило компромисс: хочешь быть киберспортсменом — занимайся физкультурой. В своём желании вытащить геймеров из-за компьютеров и побудить их заняться спортом государство пошло и дальше: в 2022 году в России придумали фиджитал-спорт. Фиджитал-спорт — направление, которое соединяет традиционные виды спорта и компьютерные игры. Например, участники сначала соревнуются на хоккейном симуляторе, а затем выходят из-за компьютеров и продолжают матч уже на льду. В 2023 году фиджитал-спорт включили в реестр официальных видов спорта, а с 21 февраля по 3 марта 2024 года в Казани прошли крупномасштабные международные соревнования — «Игры будущего». В играх приняли участие около двух тысяч спортсменов из более чем двухсот стран. Участники разыграли призовой фонд в десять миллионов долларов. Соревнования транслировались на всю Россию и за рубежом. В рамках «Игр будущего» прошёл и финал Всероссийского конкурса начинающих разработчиков «Начни игру». Таким образом игры стали масштабным национальным проектом, который объединил большое количество разработчиков, киберспортсменов, геймеров и всех причастных к игровой индустрии. «Игры будущего» планируют проводить каждый год. Вероятно, они призваны не только популяризировать кибер- и фиджитал-спорт, но и снять противоречия в обществе, которые касаются гейминга. А ещё вокруг соревнований уже создаётся технологическая, экономическая и образовательная инфраструктура, которая поможет развивать игровые и IT-технологии в России. Планируется, что в будущем основу «Игр будущего» будут составлять российские тайтлы. Организаторы игр начали сотрудничать с несколькими сотнями вузов — в них планируется запустить образовательные программы по фиджитал-спорту. Геймдев в России сегодня — это в большей степени инди-сегмент, то есть небольшие студии и независимые разработчики. Они работают в сложных условиях перестраивающейся отрасли и пытаются развиваться, в том числе за счёт поддержки государства и сообщества, о которых мы рассказали выше. Небольших студий много и становится всё больше. Свято место пусто не бывает, рано или поздно освободившуюся нишу займут другие, времена меняются, и это нормально. Но наиболее значимые, масштабные проекты разрабатывают крупные студии. И несмотря на кризис в индустрии, у нас случаются или вот-вот произойдут релизы ААА-игр. Конечно, это работа не последних двух лет, тем не менее это показывает, что у российского геймдева накопилось достаточно знаний и опыта, есть финансирование. ААА-игры — класс компьютерных игр, которые требуют наиболее высоких затрат на разработку и маркетинг. Они отличаются масштабностью, особой проработанностью мира и высоким качеством графики. В 2023 году вышел шутер Atomic Heart с советским сеттингом от студии Mundfish. Проекту удалось стать одним из самых заметных на мировом рынке. РВИ включила игру в «Зал славы видеоигровой индустрии в России» вместе с «Космическими рейнджерами» (2002) и «Тетрисом» (1984). У студии Owlcat Games в декабре 2023 года вышла ролевая игра по мрачной технофэнтезийной вселенной Warhammer 40,000 — Warhammer 40,000: Rogue Trader. На 4 апреля 2024 запланирован релиз исторической приключенческой игры «Смута» от компании Cyberia Nova. У проекта очевидный образовательный потенциал, в создании игры были задействованы исторические консультанты. Можно предположить, что крупные и дорогие видеоигры будут разрабатываться и дальше. При этом заметен тренд на интерес к истории, культурному коду и идентичности России — на славянский, советский и российский сеттинг. Вероятно, это связано и с государственной поддержкой социокультурных проектов, и с желанием заинтересовать российских геймеров самобытным отечественным продуктом, и с необходимостью выделиться на зарубежных рынках. После обнуления индустрии на пустое поле приходят новые игроки, которые займут лидирующие позиции в будущем. Мы уже видели это в начале 10-х годов, так что ничего нового не происходит. С учётом мировых трендов развития интерактивных развлечений, новых игр потребуется больше, значит, будет больше студий, больше рабочих мест. Что касается рынка игр в России, он формируется. Не так быстро, как хотелось бы, но он будет расти, хотя продукты на этом рынке будут со специфическим контентом, лучше воспринимаемым через нашу идентичность и историю. И это тоже хорошо, так как авторы игр наконец стали больше обращать внимание на социокультурные особенности своего региона, своей страны. Пока появляются первые ростки, впереди нас ждёт расцвет. То, как планируют развивать геймдев в России в ближайшем будущем, показывает «Пятилетняя дорожная карта развития видеоигровой индустрии», которую в марте 2024 года представила РВИ. Как отмечают создатели, это результат двухлетней дискуссии о будущем отрасли между сообществом разработчиков и представителями госсектора: карта основана на рекомендациях и тех и других. развить платформу браузерных видеоигр AG.ru; создать каталог и площадку для продажи российских и зарубежных игр, аналог itch.io; выпустить собственную игровую консоль. включить российские игры в киберспортивные соревнования, интегрировать их в «Игры будущего». снять барьеры для российских разработчиков: лицензировать, адаптировать и продвигать отечественные игры на рынках БРИКС, в первую очередь — в Китае. выпускать учебники по разработке игр и многое другое. Много пунктов дорожной карты посвящены поддержке инди-сектора — очевидно, именно на инди-игры возлагаются большие надежды. И это хорошая новость для начинающих разработчиков и маленьких студий: со своими небольшими проектами они могут претендовать на гранты, получать поддержку, опыт и становиться полноценными участниками рынка. Сейчас время малых проектов: пока индустрия перестраивается, стоит заниматься небольшими короткометражными играми, которые могут принести пусть и небольшую, но достаточную для продолжения деятельности студии прибыль. Кризис 2022 года не мог не отразиться на рынке труда: одна часть специалистов релоцировалась вслед за студиями, другая — теряла заказы. Сейчас ситуация всё ещё остаётся непростой. Если рассматривать российский рынок труда в области гейм-дизайна — он находится в кризисном состоянии. Предложений много больше вакансий, зарплаты снижаются, идёт конкуренция между специалистами за место, а те, кто при деле, стараются держаться за свои места даже несмотря на снижение зарплат. Ситуация получше с релокантами, многие студии предлагают хорошие позиции, но с обязательным условием — переезд в другую страну. Это связано прежде всего с трудностями финансового и юридического обслуживания сотрудников в нашей стране сегодня. Тем не менее, программа развития геймдева призвана выправить ситуацию на рынке труда в ближайшем будущем. Все меры направлены на то, чтобы российских видеоигр на внутреннем и глобальном рынках стало больше. А это значит — со временем потребуется больше специалистов, будет организовано больше рабочих мест. У гейм-дизайнеров пока складывается наиболее напряжённая ситуация с поиском работы: 46 резюме на вакансию, по данным hh.ru: Среди обязанностей гейм-дизайнера: проработка игровых механик, геймплея и сеттинга, постановка задач разработчикам и художникам, помощь в тестировании, продвижении игр, работа с настройками, документацией, аналитикой. Junior-специалист может претендовать на зарплату до 60 тысяч рублей. Зарплата более опытного гейм-дизайнера, с навыками работы над игровым балансом, экономикой и монетизацией проекта уже значительно выше — от 140 тысяч рублей. Гейм-дизайн релевантен и в области геймификации, и тут есть как высокие зарплаты, так и крупные корпорации, создающие целые отделы для геймификации бизнес-процессов. Думаю, в этой области нас ждёт ещё более бурный рост в ближайшие годы. У разработчиков игр, которые объединяют все элементы игры в одно целое, востребованность на рынке труда выше. Правда, вилка зарплат от вакансии к вакансии разнится, а многие работодатели вообще её не указывают. В среднем, разработчик с опытом от года может претендовать на зарплату от 100 тысяч рублей. 2D- и 3D-художники — специалисты, которые отвечают за визуальную часть игры. Их зарплаты в геймдеве, в среднем, чуть ниже зарплат разработчиков. Начинающим художникам предлагают около 60 тысяч, более опытным — порядка 100 тысяч рублей. Всё также востребованы разработчики на основных игровых движках — Unity/Unreal Engine, очень нужны технические художники, моделеры-персонажники и 3D-аниматоры, также недостаёт гейм-дизайнеров с уклоном в баланс и монетизацию. На самом деле всегда востребованы квалифицированные специалисты фактически по всем направлениям, но их, увы, очень мало. Есть к чему стремиться! В 2022 году российский геймдев практически обнулился. Но за счёт принятых мер — поддержки государства и крупного бизнеса, объединения разработчиков в комьюнити для взаимопомощи — отрасль начала восстанавливаться. На ближайшие пять лет запланирована масштабная программа развития геймдева. Ожидается выпуск российских игровых движков, запуск собственных игровых платформ и разработка консоли. Планируется поддержать инди-сегмент и всю отрасль грантами и льготами, образовательными инициативами и ивентами. Я бы рекомендовал начать работать над собственным маленьким проектом или стать участником инди-команды. Возможностей для создания собственных игр сейчас хоть отбавляй. Правда, чтобы сделать действительно заметный и качественный продукт, кто бы мог подумать: надо много учиться, трудиться, развивать креативное мышление и эстетический вкус. В целом, геймдев-сообщество работает над увеличением инвестиций в отрасль, которое должно привести к росту количества разработчиков игр и, соответственно, релизов. Ведётся работа над выводом российских продуктов на глобальный рынок. Рынок труда в области геймдева всё ещё находится в кризисном состоянии: количество резюме превышает количество предложений от работодателей. Однако планы по развитию отрасли позволяют предположить, что в ближайшем будущем ситуация изменится к лучшему. Можно всю жизнь анализировать и обсуждать игры, а можно научиться создавать персонажей и миры по своим правилам. Приходи в Академию разработки игр, если интересно узнать больше про разработку игр с нуля и присоединиться к сообществу единомышленников: Основы геймдизайна: создайте свою игру → бесплатный email-курс.",
    "11": "Идея проекта возникла у меня во время работы над проектом поисковика документов. Существует такая библиотека, как Apache Tika, написанная на Java, которая умеет парсить документы различных типов. Чтобы мой поисковик работал, он должен уметь извлекать текст из документов разных типов (PDF, DOC, XLS, HTML, XML, JSON и т. д.). Сам поисковик я писал на Rust. Но, к сожалению, в мире Rust нет библиотеки, которая умела бы парсить документы всех типов. По этой причине пришлось использовать Apache Tika и вызывать её из моего Rust-кода. Какие недостатки такого решения? 1. Необходимо устанавливать Java на каждом компьютере, где будет запускаться мой поисковик. 2. Очень высокие требования к RAM. Apache Tika использует очень много памяти. Из-за того что в Java есть сборщик мусора,который работает не очень эффективно, приходится выделять очень много памяти для JVM. Я тогда не стал писать библиотеку на Rust, которая умеет парсить документы всех типов, поскольку работодатель не был готов оплатить несколько лет моей разработки. Но идея осталась в голове. И вот, я решился на этот шаг и начал писать свою библиотеку на Rust. На самом деле, задача по созданию такой библиотеки достаточно интересна. Нужно разработать хорошую архитектуру ядра библиотеки, чтобы впоследствии легко было добавлять новые парсеры и генераторы для различных типов документов. Я выбрал подход, основанный на использовании Common Document Model (CDM). То есть код любого парсера должен преобразовывать документ в CDM, а код любого генератора должен преобразовывать CDM в документ. Поскольку в Rust отсутствует Reflection, я решил использовать Any. Таким образом, я могу хранить в одном векторе различные типы элементов документа. При необходимости я могу приводить их к нужному типу через downcast_ref и downcast_mut. Для этого в трейт Element я добавил методы для всех типов элементов (например, paragraph_as_ref, paragraph_as_mut и т.д.). Что бы подключить мою библиотеку в свой проект нужно добавить в Cargo.toml следующую строку: Текущий статус проекта — это MVP (Minimum Viable Product). Я планирую добавить поддержку всех типов документов, которые поддерживает Apache Tika, в течение следующих нескольких лет. А в ближайшее время допишу более глубокую поддержку PDF-документов, так как PDF — это самый популярный тип документов. Также добавлю режим работы в виде веб-сервиса, чтобы можно было использовать мою библиотеку через REST API, а не только через CLI. Исходный код проекта находится на GitHub. Если у вас есть желание помочь мне в разработке, то пишите мне на почту.",
    "12": "Одноклеточное РНК-секвенирование (scRNA-seq) – метод изучения экспрессионных профилей на уровне отдельных клеток, то есть определения, какие РНК присутствуют в каждой клетке и в каком количестве. Это позволяет ученым понимать, как функционирует каждая клетка и какие функции она выполняет. Простыми словами: данный метод помогает понять, какие гены в клетке \"включены\" и \"выключены\" в данный момент. Это важно, потому что активные гены определяют, как клетка будет себя вести, например, будет ли она здоровой, превратится ли в раковую, поможет ли она иммунной системе бороться с инфекцией и так далее. Таким образом, РНК-секвенирование применяют для разработки лекарств, при изучении болезней и их лечении, а также для того, чтобы понять, как развиваются и функционируют различные живые организмы на уровне их клеток. Данное исследование основывается на использовании большой языковой модели GPT-4 для автоматизации процесса аннотирования типов клеток в данных одноклеточного РНК-секвенирования. Одноклеточное РНК-секвенирование – это высокотехнологичный метод исследования, который позволяет ученым заглянуть внутрь отдельных клеток и узнать, какие гены в них активны. Каждый ген, который \"включен\", производит РНК, и именно эту РНК ученые \"читают\" с помощью секвенирования. Это осуществляется с помощью расшифровки последовательностей нуклеотидов! Можно представить, что внутри каждой клетки есть небольшой заводик, где гены – это рабочие, которые выполняют разные задачи. Некоторые рабочие активны в определенное время, а другие отдыхают. Одноклеточное РНК-секвенирование как раз и позволяет выяснить, кто из рабочих сейчас \"на смене\". Это очень важно, поскольку разные типы клеток выполняют разные функции в организме, и активность генов отражает эти функции. Например, клетки печени будут активировать один набор генов, а клетки мозга – совершенно другой :Р Теперь об аннотировании типов клеток. После того как ученые получили данные от одноклеточного РНК-секвенирования, перед ними стоит задача понять, какие клетки они изучали. Ведь образцы для исследования часто берут из тканей, где содержится множество разных клеток. Аннотирование – это процесс, при котором ученые сопоставляют группы клеток с уже известными типами клеток, основываясь на их генной активности. Если вернуться к аналогии с заводом, то это как если бы вы определяли, что за продукцию выпускает завод, глядя на то, какие рабочие сейчас на смене :) GPT-4 способен распознавать и классифицировать различные типы клеток, опираясь на информацию о генах-маркерах, то есть о генах, которые специфичны для определенных типов клеток. Эффективность GPT-4 была проверена на большом количестве типов тканей и клеток, и результаты, полученные моделью, показали высокую степень согласованности с аннотациями, выполненными вручную специалистами. Это означает, что GPT-4 может точно идентифицировать типы клеток внутри сложных биологических образцов, что обычно требует глубоких знаний и много времени при традиционном подходе. Также был создан специальный программный пакет для языка программирования R, названный GPTCelltype. Этот пакет как раз и представляет собой инструмент, который позволяет использовать возможности GPT-4 для автоматической аннотации типов клеток, упрощая и ускоряя этот процесс для исследователей. Таким образом, применение GPT-4 в анализе одноклеточного РНК-секвенирования открывает возможности для уменьшения рабочей нагрузки исследователей и упрощения процесса аннотации, делая его более доступным и менее затратным по времени. GPT-4 обеспечивает экономическую эффективность и непрерывную интеграцию в существующие пайплайны анализа одиночных клеток, такие как Seurat, что исключает необходимость создания дополнительных пайплайнов и сбора высококачественных эталонных наборов данных. Обширный массив обучающих данных GPT-4 позволяет использовать его в широком спектре приложений для различных тканей и типов клеток, а его природа чат-бота дает возможность пользователям уточнять аннотации. Seurat – это пакет программного обеспечения для R, который разработан специально для анализа данных одноклеточного РНК-секвенирования. Seurat предоставляет пользователям инструменты для качественной обработки данных, идентификации и классификации типов клеток, анализа молекулярных путей и других распространенных задач одноклеточной геномики. С его помощью исследователи могут обрабатывать сложные наборы данных одноклеточной последовательности, чтобы выявлять гетерогенность и молекулярные механизмы клеточной функции и взаимодействия. Исследователи систематически оценивали производительность GPT-4 по десяти наборам данных, охватывающим пять видов и сотни типов тканей и клеток, включая как нормальные, так и раковые образцы. Запросы к GPT-4 были выполнены с использованием GPTCelltype. Для сравнения, исследователи также оценили GPT-3.5, предыдущую версию GPT-4, и CellMarker2.0, SingleR и ScType, которые являются автоматическими методами аннотирования типов клеток и предоставляют справочные данные, применимые к большому числу тканей. Аннотации типов клеток, выполненные с помощью GPT-4 или конкурирующих методов, оценивались на основе их соответствия ручным аннотациям, предоставленным первоначальными исследованиями. Степень согласия измерялась с использованием числового балла. Дополнительная таблица ниже представляет пример оценки аннотаций типов клеток GPT-4 в ткани человеческой простаты. Также ученые исследовали различные факторы, которые могут повлиять на точность аннотации GPT-4. Azimuth — это инструмент или платформа для аннотирования типов клеток в данных scRNA-seq. Он использует обширные референсные наборы данных для определения клеточных типов на основе их профилей экспрессии генов. Colon Cancer — это общий термин для рака толстой кишки. В контексте аннотирования типов клеток, исследования могут быть направлены на идентификацию и классификацию различных клеточных типов в тканях, пораженных раком толстой кишки, для лучшего понимания его патогенеза. HCL (Human Cell Landscape) — это проект, предназначенный для картографирования клеточных типов и состояний в различных тканях и органах человека. Он предоставляет богатую информацию, которая может быть использована для аннотирования и сравнения клеточных типов в scRNA-seq данных. Lung Cancer — это рак легких. Аналогично раку толстой кишки, в контексте аннотирования типов клеток, данные исследования направлены на различение клеточных типов в раковых и нормальных образцах легких. BCL (Blood Cancer Lymphoma) — это лимфома или рак крови. В исследованиях типов клеток, это может относиться к анализу клеточной гетерогенности и подтипов рака крови, включая лимфому. GTEx (Genotype-Tissue Expression) — проект и база данных, которые содержат информацию об экспрессии генов в различных тканях нормального человеческого организма. Эти данные могут служить важной референсной информацией для аннотирования и сравнения клеточных типов. MCA (Mouse Cell Atlas) — является атласом клеток мыши, аналогичным Human Cell Landscape, но для модельного организма мыши. TS (Tissue Specificity) — это термин, относящийся к экспрессии генов, которая специфична для определенных тканей. В контексте аннотирования типов клеток, это относится к идентификации клеточных типов, которые характерны для конкретной ткани. Таким образом, исследователи обнаружили, что GPT-4 показывает наилучшие результаты при использовании 10 ведущих дифференциально экспрессированных генов (при сравнении с 20 и 30) и при использовании двустороннего теста Вилкоксона для получения дифференциальных генов. Двусторонний тест Вилкоксона — это непараметрический статистический тест, который используется для сравнения двух выборок, чтобы определить, есть ли между ними значимые различия. Он часто применяется когда данные не соответствуют нормальному распределению, что делает его альтернативой t-тесту Стьюдента, предназначенного для данных с нормальным распределением. В контексте генетических данных, таких как экспрессия генов в различных клеточных типах, тест Вилкоксона может быть использован для определения генов, которые дифференциально экспрессируются между двумя группами, например, между здоровыми и больными образцами или между разными типами клеток. Точность GPT-4 оказалась схожей при использовании различных стратегий подсказок, включая базовую стратегию подсказки, стратегию подсказки, вдохновленную методом цепочки мыслей, которая включает этапы рассуждения, и стратегию повторяющихся подсказок (также отображено на рисунке выше). Базовая стратегия подсказки: Здесь информация предоставлялась GPT-4 в простом и прямом формате, без дополнительных пояснений или указаний. Стратегия подсказки, вдохновленная методом цепочки мыслей: В этом подходе в запрос включались последовательные этапы рассуждений. Это означает, что каждый шаг логического вывода явно формулировался в запросе к модели, давая ей \"подсказку\" о том, как следует рассуждать для достижения вывода. Стратегия повторяющихся подсказок: Это подразумевает повторный запрос одной и той же или измененной информации за несколько попыток, возможно, для углубления анализа или уточнения ответов. В последующих анализах как GPT-4, так и GPT-3.5 использовали базовую стратегию подсказки с десятью ведущими дифференциально экспрессированными генами, полученными с помощью теста Вилкоксона, в качестве входных данных для применимых наборов данных. Аннотации GPT-4 полностью или частично совпадают с ручными аннотациями более чем для 75% типов клеток в большинстве исследований и тканей, что демонстрирует его компетентность в создании аннотаций типов клеток, сопоставимых с экспертными. Это совпадение особенно велико для маркерных генов, найденных в литературе, по крайней мере, в 70% случаев полного совпадения в большинстве тканей. Хотя для генов, идентифицированных путем дифференциального анализа, совпадение меньше, но оно все же остается высоким. Однако результаты из наборов данных, опубликованных до сентября 2021 года, следует интерпретировать с осторожностью, так как они предшествуют дате обучения GPT-4. Исследователи сделали вывод, что GPT-4 лучше справляется с иммунными клетками, такими как гранулоциты, по сравнению с другими типами клеток. Он идентифицирует злокачественные клетки в наборах данных рака толстой кишки и легких, но испытывает трудности с В-лимфомой, возможно из-за отсутствия четких наборов генов. Идентификация злокачественных клеток могла бы извлечь выгоду из применения других подходов, таких как вариация числа копий генов. Также производительность немного снижается в небольших популяциях клеток, состоящих не более чем из десяти клеток, возможно из-за ограниченного количества доступной информации. Аннотации GPT-4 чаще полностью совпадают с ручными аннотациями для основных типов клеток (например, Т-клеток) по сравнению с подтипами (например, памяти Т-клеток CD4), в то время как более 75% подтипов все еще достигают полного или частичного совпадения. Т-клетки, также известные как Т-лимфоциты, являются ключевой составляющей адаптивной иммунной системы у млекопитающих, в том числе у человека. Эти клетки развиваются из стволовых клеток в костном мозге и зреют в тимусе (отсюда и название \"Т\"), который является небольшим органом, расположенным перед грудной клеткой. В зависимости от их функции и типа антигенного рецептора на их поверхности, Т-клетки подразделяются на несколько основных типов. Т-хелперные клетки (CD4+ Т-клетки), раз уж о них зашла речь, помогают активировать другие иммунные клетки, включая В-клетки для производства антител и Т-киллерные клетки для уничтожения инфицированных клеток. Низкий уровень согласия между аннотациями GPT-4 и ручными аннотациями в некоторых типах клеток не обязательно означает, что аннотации GPT-4 неверны. Например, к типам клеток, классифицированным как стромальные клетки, относят фибробласты и остеобласты, экспрессирующие гены коллагена типа I, и хондроциты, экспрессирующие гены коллагена типа II. Фибробласты – это наиболее распространенный тип клеток в соединительной ткани. Они играют ключевую роль в процессах заживления ран и ремонта тканей, производя важные внеклеточные матриксные компоненты, такие как коллаген, фибронектин и гликозаминогликаны. Фибробласты обеспечивают механическую прочность ткани и участвуют в создании внеклеточного матрикса. Остеобласты – это клетки, которые отвечают за образование кости. Они производят и секретируют матрикс кости, который затем минерализуется, превращаясь в твердую костную ткань. Остеобласты также играют важную роль в процессе ремоделирования кости, то есть в постоянном обновлении и восстановлении костной ткани. Хондроциты – это клетки, которые являются основными и единственными клетками хрящевой ткани. Они отвечают за синтез и поддержание внеклеточного матрикса хряща, который включает коллаген (в основном тип II), протеогликаны и другие компоненты. В отличие от остеобластов, которые образуют костную ткань, хондроциты не производят твердого матрикса, который минерализуется, а вместо этого создают гибкий и упругий матрикс, который позволяет хрящу выполнять свои функции. Для клеток, аннотированных вручную как стромальные клетки, GPT-4 присваивает аннотации клеточного типа с более высокой детализацией (например, фибробласты и остеобласты), что приводит к частичным совпадениям и более низкому уровню согласия. Для типов клеток, которые вручную аннотированы как стромальные клетки, но определены GPT-4 как фибробласты или остеобласты, гены коллагена типа I показывают значительно более высокую экспрессию, чем гены коллагена типа II. Это согласуется с наблюдаемой картиной в клетках, которые вручную аннотированы как хондроциты, фибробласты и остеобласты, что предполагает, что GPT-4 предоставляет более точные аннотации типов клеток для стромальных клеток. GPT-4 значительно превосходит другие методы, основанные на средних показателях согласия. Используя GPTCelltype в качестве интерфейса, GPT-4 работает также заметно быстрее, отчасти из-за использования дифференциальных генов из стандартных пайплайнов анализа отдельных клеток, таких как Seurat. Учитывая неотъемлемую роль этих пайплайнов, исследователи рассматривают дифференциальные гены как непосредственно доступные для GPT-4. В отличие от них, другие методы, такие как SingleR и ScType, требуют дополнительных шагов для повторной обработки матриц экспрессии генов. Кроме того, по сравнению с другими бесплатными методами, GPT-4 взимает ежемесячную плату в размере 20 долларов за использование веб-портала. Стоимость GPT-4 API линейно коррелирует с количеством запрашиваемых типов клеток и не превышает $0,1 для всех запросов в данном исследовании. Исследователи также оценили устойчивость GPT-4 в сложных реальных данных с использованием смоделированных наборов данных. GPT-4 может различать чистые и смешанные типы клеток с точностью 93%, а также различать известные и неизвестные типы клеток с точностью 99%. Когда входной набор генов включает меньше генов или содержит шум, производительность GPT-4 снижается, но остается высокой. Эти результаты демонстрируют устойчивость GPT-4 в различных сценариях. Наконец, исследователи оценили воспроизводимость аннотаций GPT-4 с использованием предыдущих смоделированных исследований. GPT-4 генерировал идентичные аннотации для тех же маркерных генов в 85% случаев, что указывает на высокую воспроизводимость. Аннотации двух версий GPT-4 показали идентичные показатели согласия в большинстве случаев с коэффициентом Каппа Коэна равном 0.67, что свидетельствует о значительной согласованности. Коэффициент Каппа Коэна (Cohen’s κ) — это статистическая мера, используемая для оценки степени согласия или надежности между двумя или более оценщиками, которые независимо классифицируют каждый элемент в категориальные шкалы. Хотя GPT-4 превосходит существующие методы аннотирования типов клеток, есть ограничения, которые следует учитывать. Во-первых, нераскрытый характер корпуса обучения GPT-4 затрудняет проверку основы его аннотаций, что требует человеческой оценки для обеспечения качества и надежности. Во-вторых, участие человека в возможной дополнительной настройке модели может повлиять на воспроизводимость из-за субъективности и может ограничить масштабируемость модели в больших наборах данных. В-третьих, высокий уровень шума в данных scRNA-seq и ненадежные дифференциальные гены могут неблагоприятно сказаться на аннотациях GPT-4. Исследователи рекомендуют проводить проверку аннотаций типов клеток GPT-4 экспертами-людьми перед проведением последующих анализов. Несмотря на то, что исследование сосредоточено на стандартной версии GPT-4, дополнительная настройка GPT-4 с использованием высококачественных списков референсных маркерных генов может дополнительно улучшить производительность аннотирования типов клеток, используя такие услуги, как \"GPTs\", предоставляемые OpenAI. Важно отметить, что применение на данном этапе исключительно сил ИИ и GPT-4 недопустимо, т.к. требует доработки и дополнительных проверок, но этот метод действительно имеет отличные перспективы в будущем! Авторами основного исследования являются Wenpin Hou (Департамент биостатистики, Школа общественного здравоохранения Колумбийского университета) и Zhicheng Ji (Кафедра биостатистики и биоинформатики, Медицинская школа Университета Дьюка).",
    "13": "В этой статье мы начнем рассматривать практическое применение библиотеки FinRL для построения торгового агента. В предыдущей статье мы вкратце рассмотрели библиотеку FinRL, предоставляемые ей возможности моделирования рынка и обучения торговых агентов на основании алгоритмов обучения с подкреплением. Это вторая статья нашего обучающего цикла и в ней мы построим примитивного агента, который анализирует поступающие данные о стоимости позиции на рынке и пытается предсказать будущую цену. Вполне очевидно, что результат такого примитивного агента будет весьма далек от приемлемого уровня, но этот шаг поможет нам построить модель рынка с помощью библиотеки FinRL, обучить агента и быть готовыми к построению более сложных и осмысленных моделей. Итак, начнем знакомиться с FinRL на практике. Здесь и далее буду предполагать, что читатель знаком с установкой языка Python, дистрибутива Anaconda, созданием и переключением виртуальных окружений. Процесс установки достаточно подробно документирован на сайте FinRL. Отмечу, что в процессе сборки FinRL часто возникают ошибки из-за несогласованности пакетов при их обновлении. Установку следует производить в отдельно созданное виртуальное окружение с версией Python не ниже 3.8. Для загрузки рыночных данных мы воспользуемся библиотекой YahooFinance как одной из самых распространенных библиотек для Python в этой области. Этот код загрузить данные о торгах акциями “Apple” с 1 по 31 января 2020 года. Просто данных о ценах и объемах торгов совершенно не достаточно для построения сколько-нибудь адекватной модели. Необходимо обогатить их некоторой аналитикой, на основании которой модель могла бы попытаться построить свой прогноз. Одним из самых примитивных подходов в этой области является прогнозирование на основе технических индикаторов. Так же можно использовать и другие показатели. Подробнее про них и другие технические индикаторы можно почитать здесь. Вы можете дополнить список INDICATORS необходимыми для вашей модели значениями. В минимальной конфигурации конструктор принимает на вход DataFrame с данными на основе которых будет построена модель рынка. Так же можно передать словарь с дополнительными параметрами. Для взаимодействия с моделями нам необходимо создать векторную обертку для нашего окружения. Это можно сделать стандартной функцией класса StockTradingEnv: Вторым возвращаемым параметром здесь является состояние среды, возвращаемое функцией env.reset(). Указать ему какой именно алгоритм из библиотеки необходимо использовать. Данный код создаст нам RL агента и загрузит модель “A2C” - т.е actor-critic. Далее нам необходимо обучить нашего агента на созданном окружении. Для этого достаточно вызвать метод train_model передав ему загруженную модель и число эпизодов обучения. Так же можно дополнительно указать параметры логирования результатов, например в формате tensorboard. Для созданного нами простого окружения доступны следующие модели: [\"a2c\",\"ddpg\",\"ppo\",\"td3\",\"sac\"], что соответствует моделям “actor-critic”, “deep deterministic policy gradient”, “twin delayed deep deterministic policy gradient”, “soft actor-critic”. Более подробно об этих моделях вы можете прочитать здесь. Обучение модели процесс достаточно продолжительный, поэтому перед дальнейшим использованием я Вам настоятельно рекомендую сохранить обученного агента. Сначала соберем среду для тестирования. Ранее мы разделили наши данные на тренировочные, на которых мы построили тренировочную среду и обучили нашего агента и торговые, которые мы будем использовать для оценки результатов. Поскольку мы имеем дело не с одной бумагой, а с портфелем инвестиций, то для оценки эффективности действий нашего агента воспользуемся методом MVO (Mean-Variance Optimization) - широко применяемым в теории портфельного управления. Это стратегия, в рамках современной теории управления портфелем, созданной Гарри Марковицем, направлена на создание оптимального инвестиционного портфеля путем балансировки компромисса между ожидаемой доходностью и риском. В MVO инвестор стремится максимизировать ожидаемую доходность портфеля, минимизируя его риск, обычно измеряемый дисперсией или стандартным отклонением портфеля. Основная идея MVO заключается в диверсификации инвестиций по различным активам для достижения оптимального компромисса между риском и доходностью. В качестве точки сравнения также будем использовать среднее значение по портфелю, т.е стратегию если мы купим все бумаги в равных долях и будем удерживать их на всем протяжении тестового периода. Результат работы нашего обученного агента оказался немного лучше, чем средняя стратегия и значительно хуже стратегии MVO. Не расстраивайтесь, в рассмотренном примере агент не обладал никакими данными о рынке, тем не менее он смог научиться вести себя не хуже чем “в среднем” по рынку. Это отличный результат для такой простой модели и у нас есть значительный потенциал роста. В следующих статьях цикла мы рассмотрим пути улучшения торговых стратегий и построения более осмысленных агентов с помощью библиотеки FinRL.",
    "14": "С помощью 5 000 крошечных роботов в телескопе, расположенном на вершине горы, исследователи могут заглянуть на 11 миллиардов лет в прошлое. Свет от далёких космических объектов только сейчас достигает спектроскопического прибора Dark Energy Spectroscopic Instrument (DESI), позволяя нам составить карту космоса, каким он был в юности, и проследить его развитие до того, что мы видим сегодня. Понимание того, как развивалась наша Вселенная, связано с тем, как она закончится, и с одной из самых больших загадок в физике: тёмной энергией, неизвестным ингредиентом, заставляющим нашу Вселенную расширяться всё быстрее и быстрее. Чтобы изучить влияние тёмной энергии за последние 11 миллиардов лет, в DESI создали самую большую трёхмерную карту нашего космоса из когда-либо созданных, с самыми точными измерениями на сегодняшний день. Впервые учёные измерили историю расширения молодой Вселенной с точностью более 1 %, что даёт нам лучшее представление о том, как развивалась Вселенная. Исследователи поделились результатами анализа собранных за первый год данных в нескольких статьях, которые будут опубликованы сегодня на сервере препринтов arXiv, а также в докладах на заседании Американского физического общества в США и на конференции Rencontres de Moriond в Италии. \"Мы невероятно гордимся этими данными, которые позволили получить ведущие в мире результаты по космологии и являются первыми результатами нового поколения экспериментов с тёмной энергией\", — сказал Майкл Леви, директор DESI и учёный из Национальной лаборатории имени Лоуренса Беркли Министерства энергетики, которая руководит проектом. \"Пока что мы в целом наблюдаем согласованность с нашей лучшей моделью Вселенной, но мы также наблюдаем некоторые потенциально интересные различия, которые могут указывать на то, что тёмная энергия эволюционирует со временем. Они могут исчезнуть, а могут и не исчезнуть с увеличением количества данных, поэтому мы с нетерпением ждём начала анализа нашего трёхлетнего набора данных\". Эта анимация показывает, как акустические колебания барионов действуют как космическая линейка для измерения расширения Вселенной. Наша ведущая модель Вселенной известна как Lambda CDM. Она включает в себя как слабо взаимодействующий тип материи (холодная тёмная материя, или CDM), так и тёмную энергию (Lambda). И материя, и тёмная энергия определяют, как расширяется Вселенная, но противоположными способами. Материя и тёмная материя замедляют расширение, а тёмная энергия ускоряет его. Количество каждого из этих компонентов влияет на эволюцию нашей Вселенной. Эта модель хорошо описывает результаты предыдущих экспериментов и то, как выглядит Вселенная в течение времени. Однако, когда результаты первого года работы DESI объединяются с данными других исследований, обнаруживаются некоторые тонкие различия с тем, что предсказывает Lambda CDM. По мере того как DESI будет собирать больше информации в течение пятилетнего исследования, эти первые результаты станут более точными, проливая свет на то, указывают ли данные на другие объяснения наблюдаемых нами результатов или на необходимость обновления нашей модели. Дополнительные данные также улучшат другие ранние результаты DESI, которые касаются постоянной Хаббла (мера того, насколько быстро расширяется Вселенная сегодня) и массы частиц, называемых нейтрино. \"Ни один спектроскопический эксперимент не собирал такого количества данных, и мы продолжаем собирать данные более чем по миллиону галактик каждый месяц\", — говорит Натали Паланке-Делабруй, учёный из Лаборатории Беркли и один из авторов эксперимента. «Поразительно, что, получив данные только за первый год, мы уже можем измерить историю расширения нашей Вселенной на семи различных отрезках космического времени, каждый с точностью от 1 до 3%\". Команда проделала огромную работу по учёту инструментальных и теоретических тонкостей моделирования, что даёт нам уверенность в надёжности наших первых результатов». Общая точность измерения истории расширения за все 11 миллиардов лет, проведённого DESI, составляет 0,5%, а для самой отдалённой эпохи, охватывающей 8-11 миллиардов лет в прошлом, точность составляет рекордные 0,82%. Такое измерение развития нашей молодой Вселенной сделать невероятно сложно. Тем не менее, за один год DESI стал вдвое мощнее в измерении истории расширения в эти ранние времена, чем его предшественник (BOSS/eBOSS Слоановского цифрового обзора неба), на что ушло более десяти лет. \"Мы рады увидеть результаты космологических исследований, полученные в первый год работы DESI\", — сказала Джина Рамейка, заместитель директора по физике высоких энергий в Министерстве энергетики. \"DESI продолжает удивлять нас своей звёздной работой и уже формирует наше понимание Вселенной\". DESI — это международное сотрудничество более чем 900 исследователей из более чем 70 учреждений по всему миру. Прибор установлен на 4-метровом телескопе Николаса У. Мэйолла Национального научного фонда США в Национальной обсерватории Китт-Пик в рамках программы NOIRLab Национального научного фонда США. Глядя на карту DESI, легко увидеть основную структуру Вселенной: нити галактик, сгруппированные вместе, разделённые пустотами с меньшим количеством объектов. Наша ранняя Вселенная, находящаяся далеко за пределами видимости DESI, была совсем другой: горячий, плотный суп из субатомных частиц, движущихся слишком быстро, чтобы образовать стабильную материю, такую как атомы, которые мы знаем сегодня. Среди этих частиц были ядра водорода и гелия, то есть барионная материя. Крошечные флуктуации в этой ранней ионизированной плазме вызывали волны давления, перемещая барионы в виде ряби, похожей на ту, что можно увидеть, бросив горсть камешков в пруд. По мере расширения и охлаждения Вселенной образовались нейтральные атомы, и волны давления прекратились, заморозив рябь в трёх измерениях и увеличив кластеризацию будущих галактик в плотных областях. Спустя миллиарды лет мы всё ещё можем наблюдать слабый рисунок трёхмерной ряби, или пузырьков, в характерном разделении галактик — эта особенность называется барионными акустическими осцилляциями (БАО). Исследователи используют измерения БAO в качестве космической линейки. Измеряя видимый размер этих пузырьков, они могут определить расстояние до материи, ответственной за этот чрезвычайно слабый узор на небе. Картирование пузырей БAO вблизи и на расстоянии позволяет исследователям разделить данные на фрагменты, измерить скорость расширения Вселенной в каждый момент её прошлого и смоделировать, как тёмная энергия влияет на это расширение. \"Мы измерили историю расширения в этом огромном диапазоне космического времени с точностью, превосходящей все предыдущие исследования БAO, вместе взятые\", — говорит Хи-Джонг Со, профессор Университета Огайо и один из руководителей анализа БAO в DESI. «Мы очень рады узнать, как эти новые измерения улучшат и изменят наше понимание космоса. Люди извечно увлечены развитием нашей Вселенной, желая знать, из чего она состоит и что с ней будет дальше». Использование галактик для измерения истории расширения и лучшего понимания тёмной энергии — это один из методов, но он может дать очень многое. В определённый момент свет от обычных галактик становится слишком слабым, поэтому исследователи обращаются к квазарам — чрезвычайно удалённым ярким галактическим ядрам с чёрными дырами в центре. Свет квазаров поглощается при прохождении через межгалактические облака газа, что позволяет исследователям наносить на карту скопления плотной материи и использовать их так же, как и галактики — этот метод известен как использование \"леса Лайман-альфа\". \"Мы используем квазары в качестве подсветки, чтобы увидеть тень промежуточного газа между квазарами и нами\", — говорит Андреу Фонт-Рибера, учёный из Института физики высоких энергий (IFAE) в Испании, который является одним из руководителей анализа леса Лайман-альфа в DESI. \"Это позволяет нам заглянуть дальше, в то время, когда Вселенная была ещё очень молодой. Это очень сложное измерение, и очень здорово, что оно удалось\". Исследователи использовали 450 000 квазаров — самый большой набор, когда-либо собранный для этих измерений леса Лайман-альфа, — чтобы расширить измерения БAO до 11 миллиардов лет в прошлом. К концу исследования DESI планирует составить карту 3 миллионов квазаров и 37 миллионов галактик. DESI — первый спектроскопический эксперимент, в котором проводится полностью \"слепой анализ\", скрывающий от учёных истинный результат, чтобы избежать подсознательного предубеждения в отношении подтверждения. Исследователи работают в темноте с изменёнными данными, составляя код для анализа своих выводов. Когда всё готово, они применяют свой анализ к исходным данным, чтобы узнать реальный ответ. \"То, как мы провели анализ, даёт нам уверенность в наших результатах, и особенно в том, что лес Лайман-альфа является мощным инструментом для измерения расширения Вселенной\", — говорит Жюльен Ги, учёный из Лаборатории Беркли и соруководитель по обработке информации со спектрографов DESI. \"Набор данных, который мы собираем, исключителен, как и скорость, с которой мы его собираем. Это самое точное измерение, которое я когда-либо проводил в своей жизни\". В этом 360-градусном видеоролике можно совершить интерактивный полёт по миллионам галактик, нанесённых на карту с помощью координатных данных DESI. Credit: Fiske Planetarium, CU Boulder и сотрудничество DESI Данные DESI будут использоваться для дополнения будущих обзоров неба, таких как обсерватория Веры К. Рубин и космический телескоп Нэнси Грейс Роман, а также для подготовки к потенциальной модернизации DESI (DESI-II), которая была рекомендована в недавнем докладе Группы по определению приоритетов проектов по физике частиц США. \"Мы находимся в золотой эре космологии: масштабные исследования уже проводятся и вот-вот начнутся, а новые методы разрабатываются, чтобы наилучшим образом использовать эти наборы данных\", — говорит Арно де Маттиа, исследователь из Французской комиссии по альтернативным источникам энергии и атомной энергии (CEA) и соруководитель группы DESI, занимающейся интерпретацией космологических данных. \"Мы все очень заинтересованы в том, чтобы увидеть, подтвердят ли новые данные те особенности, которые мы увидели в нашей выборке первого года, и лучше понять динамику нашей Вселенной\". Научпоп. Проповедую в храме науки.",
    "15": "Интересное в исполнении приложение всегда сможет привлечь внимание, поскольку мы любим, когда красиво. Но что стоит за этим \"красиво\"? И начинка, и внешний вид. Сегодня я бы хотела поговорить о внешнем виде, ведь встречают по одежке. А конкретно - про анимации. Анимации добавляют жизни приложениям и сайтам и делают их простыми в использовании. Мы можем добавлять изменение кнопок при нажатии, всплывающие объекты, эффекты перехода и многое другое, что позволяет пользователю не просто ознакомиться с содержимым, а еще и интересно провести время. Мы поговорим о библиотеках JavaScript, упрощающих добавление этих анимаций для разработчиков, делая приложения более приятными. Three.js – это высокоуровневая JavaScript-библиотека, специализирующаяся на создании 3D-графики и анимаций для веб-приложений. Используя Three.js, мы можем легко конструировать различные трехмерные сцены, от игр и впечатляющих визуализаций до сред виртуальной реальности. Библиотека облегчает процесс добавления объектов, наложения материалов и текстур, создания анимаций, а также интеграции 3D-моделей, созданных в Blender или других инструментах 3D-моделирования. За счет построения на базе WebGL, Three.js предоставляет интуитивно понятный API, позволяя разработчикам сконцентрироваться на дизайне трехмерных сцен без необходимости погружения в технические детали WebGL. Mo.js представляет собой превосходный фреймворк, выделяющийся своей простотой использования и выразительным синтаксисом. Этот фреймворк значительно облегчает нашу работу в области создания анимаций, позволяя нам легко реализовывать всё, от базовых вращений до сложных, многоуровневых анимаций. В то время как некоторые другие библиотеки сфокусированы на функциональности, Mo.js уделяет равное внимание как художественной, так и технической сторонам анимационного процесса. Фреймворк стимулирует к творческому подходу, предоставляя разработчикам инструменты для детальной настройки каждого элемента анимации, включая задержки, продолжительность, эффекты плавности и множество других параметров. Благодаря модульной структуре, Mo.js дает возможность создавать сложные анимационные последовательности для пользовательских интерфейсов, при этом поддерживая полный контроль над тонкостями движения и взаимодействия. AniJS представляет собой элегантную JavaScript-библиотеку, предназначенную для упрощения взаимодействия с элементами пользовательского интерфейса без необходимости глубоких знаний в программировании. Эта библиотека разработана с учетом потребностей дизайнеров, и поэтому её синтаксис использует ясный и понятный английский язык, делая её доступной для понимания широкому кругу пользователей. GSAP (GreenSock Animation Platform) представляет собой мощную библиотеку для создания анимаций, которая открывает широкие возможности для реализации динамичных визуальных эффектов в веб-приложениях, игровых проектах и интерактивных историях. GSAP отличается выдающейся кросс-браузерной совместимостью и высокой скоростью рендеринга, что делает его предпочтительным инструментом для создания профессиональных анимаций. Платформа поддерживает обширный диапазон анимационных эффектов, включая анимации свойств CSS, SVG, элементов на HTML5 Canvas и проектов на WebGL. GSAP известен своей плавной анимацией без мерцаний, обеспечивая стабильность во всех популярных браузерах. С такими продвинутыми функциями, как контроль временной шкалы, механизмы обратного вызова и усовершенствованные опции плавности переходов, GSAP дает возможность тщательно разрабатывать сложные анимационные последовательности. Typed.js является JavaScript-библиотекой, позволяющей имитировать машинописный текст, с возможностью регулировки скорости печати, автоматического удаления уже напечатанного текста и последующего ввода нового текста согласно заданным параметрам. Lottie представляет собой библиотеку, совместимую с платформами Android, iOS, Web и Windows, которая декодирует анимации, созданные в Adobe After Effects и экспортированные в JSON-формате с использованием Bodymovin. Эта библиотека позволяет без труда воспроизводить анимации на мобильных устройствах и веб-сайтах. Благодаря Lottie, дизайнеры получили возможность разрабатывать и передавать сложные анимации без необходимости их ручной переработки разработчиками. Anime.js представляет собой фреймворк, который выигрывает популярность благодаря своей простоте и мощным функциональным возможностям. Он является одновременно интуитивно понятным для новичков и достаточно гибким для опытных аниматоров, позволяя легко воплощать творческие концепции в реальность. Anime.js умело управляет как CSS-анимациями, так и анимациями, основанными на JavaScript, делая его отличным решением для разнообразных анимационных проектов. Его API является простым для освоения, но при этом обладает достаточной мощностью для реализации сложных анимационных эффектов, от простых переходов до продвинутых ключевых кадров и анимаций с использованием временной шкалы, предоставляя обширный спектр возможностей для разработчиков. Popmotion представляет собой библиотеку для создания анимаций в JavaScript, ориентированную на простоту и удобство использования. Она отличается лаконичным и интуитивно понятным API, что облегчает её применение, и обеспечивает совместимость со всеми основными веб-браузерами. Благодаря встроенной системе плагинов, Popmotion предоставляет возможность дополнения стандартного функционала новыми возможностями. Созданная с использованием TypeScript, эта библиотека способна сотрудничать с любыми API, которые могут принимать числовые значения, как, например, React. Кроме того, Popmotion используется в качестве основы для анимаций в популярной библиотеке Framer Motion, о которой поговорим далее) Framer Motion является специализированной библиотекой анимации для веб, разработанной с учетом интеграции в экосистему React. Эта библиотека обеспечивает полный набор инструментов для внедрения анимационных движений в приложения React. Благодаря тому, что Framer Motion сосредоточен на React, её синтаксис оптимизирован под данную платформу, что делает её особенно удобной для опытных разработчиков. Библиотека предлагает набор анимационных возможностей для: Макетов: манипулируя структурой веб-страницы и расположением элементов. Жестов: используя слушатели событий для управления движением элементов в ответ на действия пользователя, такие как наведение курсора, клики или перетаскивание. Прокрутки: управляя анимацией, связанной с действиями прокрутки пользователя. Переходов: определяя способы анимации изменений между состояниями. Реализация подобных анимационных последовательностей с нуля может быть затратной по времени, поэтому использование предварительно настроенных анимаций из Framer Motion предоставляет значительную экономию времени для разработчиков. ScrollMagic - это библиотека JavaScript, специализированная на создании интерактивных сценариев прокрутки, которая отличается лёгкостью настройки и возможностью расширения функционала. Возможности ScrollMagic могут быть расширены за счёт интеграции с различными анимационными фреймворками. В качестве предпочтительного варианта часто выбирается Greensock Animation Platform (GSAP) благодаря её надёжности и обширным анимационным возможностям. Для более простых сценариев доступна поддержка фреймворка VelocityJS. Также, ScrollMagic предоставляет гибкость для реализации пользовательских расширений или для создания анимаций без использования фреймворков, применяя CSS анимации и переключение классов. Подводя итоги, стоит сказать, что JavaScript-анимации представляют собой вид творчества, в котором техническая экспертиза гармонично сочетается с художественным видением. Они способны радикально преобразовать восприятие пользователем веб-пространства, придавая интерфейсам дополнительную привлекательность и улучшая их взаимодействие с пользователем. Мы можем создавать интересные анимации с GSAP для своего приложения или разместить 3D-модель айфона на своем сайте с помощью Three.js. Надеюсь, вам была полезна эта подборка и вы либо открыли для себя что-то новое, либо просто были рады видеть уже знакомые библиотеки.",
    "16": "На днях стало известно о том, что в Германии реализуется проект, цель которого — перевести многие тысячи компьютеров в госучреждениях на открытое ПО. В частности, вместо Windows от Microsoft власти собираются установить один из дистрибутивов Linux, а Microsoft Office заменят на LibreOffice. Инициатива в целом неплохая. Но это уже не первая попытка немцев перейти на открытое ПО. И предыдущие заканчивались не очень хорошо. Сейчас разговор о замене идет лишь в одном из регионов ФРГ — на земле Шлезвиг-Гольштейн. Здесь сразу 30 тысяч компьютеров на Windows переведут на Linux. Интересно, что чиновники до сих пор не выбрали дистрибутив, который планируют устанавливать вместо операционной системы от Microsoft. Перевод систем будет осуществлен примерно за три года — к 2027 году всё должно быть завершено. За это время не только компьютеры модифицируют, но и пользователи-чиновники будут обучены новым для себя навыкам работы с Linux и соответствующим софтом. Впервые о том, что регион собирается перейти на Linux, заявили в 2021 году, и в целом всё идёт по плану. Среди факторов, которые привели к решению заменить ПО от Microsoft на свободное, — экономия и информационная безопасность. В частности, немцы обеспокоены тем, что телеметрия, собираемая Windows, идёт куда-то за пределы Европейского Союза. По мнению немцев, софт корпорации Microsoft сейчас не соответствует требованиям ЕС к безопасности данных. Ну а сэкономить чиновники надеются за счёт отсутствия необходимости платить за лицензии. При этом до сих пор у чиновников нет особого понимания, как и чем заменить некоторые специализированные возможности ОС редмондской компании. В частности, Active Directory, служба каталогов, — она позволяет администратору управлять различными устройствами, учётными записями пользователей, настройками ПО и т. п. Речь идёт о том, что в Германии уже предпринимались попытки переходить на свободное открытое программное обеспечение. Одна из первых стартовала в 2003 году, т. е. 21 год назад. Тогда в течение 10 лет около 15 000 компьютеров чиновников были переведены на Linux. Конечно же, персонал переобучили, софт подготовили — всё шло по плану. В 2012 году, за год до завершения перехода, городской совет Мюнхена опубликовал отчёт о расходах на ПО Microsoft и на свободный софт. В отчёте приводилось сравнение затрат на LiMux с двумя технологически эквивалентными сценариями. Первый — Windows и OpenOffice, второй — Windows и Microsoft Office. Как оказалось, в первом случае экономия составляла 7,1 млн евро, во втором — 11,3 млн евро. Тогда в ряде СМИ публиковались положительные новости, в которых раскрывалась суть перехода, показывалась экономия и т. п. Но, как оказалось, не всё так гладко. Дело в том, что далеко не всё ПО, с которым работают чиновники, можно перенести на Linux. Почтовые серверы в итоге пришлось снова вернуть на Microsoft Exchange. Ещё немного помучившись, чиновники в 2017 году решили осуществить возвращение к Windows и Office. Бесплатное ПО заметно отстаёт по функциональности от ПО Microsoft. Для нормальной работы почты на смартфонах пришлось установить отдельный почтовый сервер (Exchange). Отсутствует ПО для интеграции с почтой, контактами и встречами. Обмен информацией с гражданами и другими муниципалитетами затруднён, поскольку последние применяют ПО Microsoft. Ожидания, что «Linux дешевле», не оправдались, потому что возникла необходимость самостоятельной доработки функционала. Главная проблема — отсутствие в Linux поддержки примерно половины программ, используемых чиновниками ранее. Выяснилось, что экономически выгоднее применять коммерческие решения, уже поддерживающие всё, что требуется, и для которых нужна лишь минимальная настройка (по сравнению с новой экосистемой открытого софта). В ходе реализации проекта по переходу на открытый софт оказалось, что «из коробки» мало что работает. И да, всё вроде бесплатно, но для настройки всего программного обеспечения нужны специалисты, чьё время стоит денег, и немалых. Доработки ПО, системных сервисов и т. п. аккумулируются и превращаются в «снежный ком» задач, требующих всё новых и новых дополнительных работ. А значит, затраты растут, и вместо экономии чиновники получают уже дыры в бюджете. Что касается нового проекта, то пока что неясно, в курсе ли его авторы предыдущих попыток своих коллег, но, возможно, сейчас всё проще, поскольку и Linux стал совершеннее, да и софта под него уже больше. Тот же LibreOffice без проблем использовался в IT-отделе муниципалитета в течение двух последних лет. При этом для чиновников разработчики LibreOffice создали специальный программный интерфейс для взаимодействия программ из пакета офисного ПО с софтом, используемым работниками ведомства. Он тестировался на протяжении шести месяцев, и проблем с ним не было. Кстати, заменят на бесплатные аналоги и другие программы. Например, Zoom будет заменён на бесплатный аналог Jitsy. Изменения, как и говорилось выше, затронут представителей бюджетной сферы.",
    "17": "Для лиги лени: редкий бардак в импортозамещении. Про что текст: я попытался упорядочить мое представление о качестве импортозаместительных серверов. Не вышло. Предисловие-1. Культурологическое.Перед прочтением нужно вспомнить теорию «истинности» в лекциях Техноложки (СПбГТИ(ТУ)) Зачем: чтобы понимать, что 0, иногда, это просто 0. Это не «хорошо» и не «плохо», и тем более «вообще», это 0, и это не 1. Сначала надо принять это как факт, у некоторых комментаторов с этим проблемы. Предисловие-2. Материальное.Своего производства микроэлектроники в РФ практически нет, причем нет на всех стадиях и лучше ситуация с 2008 года (покупка AMD Fab 30 - 200 мм подложки,процесс 130 нм) - не стала. Кому интересно обсуждение и ответное рррряя вретииии, заказано же производство супер-рентген-машины – могут почитать статью Российская микроэлектроника — два года спустя и комментарии. Там же приведены ссылки на производство рассыпухи и прочих мелочей. Ее не то чтобы «нет совсем», но ее объемы и качество годятся только для отчетов про успешно закрытый этап финансирования, или для производства каких-то штучных одноразовых изделий. И то, если хоть чуть-чуть поверить лживой пропаганде стран НАТО, под названием Royal United Services Institute (RUSI) Silicon Lifeline: Western Electronics at the Heart of Russia's War Machine, которой верить ни в коем случае нельзя, то не все так хорошо. Верить этому докладу мы, конечно, не будем.Как комплексное следствие нехватки «всего, кроме неквалифицированных дешевых кадров - этих избыток» – говорить про производство «российских серверов» можно, но. Но учитывая, что там из российского  – ручной поверхностный монтаж элементов на схему. Для современных плат уже идут 20-24 слойные платы, до недавнего времени в РФ делали что-то на 16 слоев. Российский софт по разметке \\ трассировке \\ рисованию дорожек для такой сложности – может и есть, а может кто-то доделал что-то опенсорсное. Но может быть и так, что весь дизайн куплен в Китае или на Тайване у Gigabyte Technology (про это ниже).Станки и линии по сборке – тоже не РФ, возможно даже бессвинцовый припой в РФ не делают. Изагри, может, делает, а может только фасует что-то от KMI Company (KAZAKHSTAN METAL INDUSTRIAL COMPANY). Что остается из работы:- оператор-загрузчик многослойного готового текстолита в машину, - оператор-наладчик установленного шаблона разметки,- оператор - ручной установщик компонентов, - оператор – разгрузчик готовой схемы в коробку. Может быть, что на такие ответственные задачи уже взяли ценных иностранных специалистов (ТМ). Примеры производства плат в новостях: 9 октября 2021 - Высокотехнологичные серверы российского производства начали делать в Новосибирске Новость про следующий успешный успех - «Центр открытых разработок» запустил в Рязани серийный выпуск серверов OpenYard (6 декабря 2023) содержит видеоряд от видео из 2021 года, только кадрированное, сравните 0:32 первого видео и 0:04 второго, и 1:10 первого и 0:15 второго. Хотя, это канал MASH, может взяли что было.После всех процедур получается какой-то Sugon или Inspur, собранный в РФ, со всеми их проблемами – нет документации, нет поддержки, нет сообщества - есть непонятные сбои. Хотя, почему непонятные – неправильная разводка дорожек в слоях, ослабление сигнала, наводки, прочие ошибки проектирования, старый BIOS, старый BMC, несовместимость наводок от установленных дополнительных плат – и на выходе ситуация «сегодня работало, завтра от вибрации что-то где-то пошло не так - и не работает, а встроенных средств диагностики нет, кроме термодатчика». Intel просто так, всем подряд не раздает инструментария для создания чего-то типа FDM (Fault Diagnosis Management) и анализа дампов на уровне CPU\\RAM\\PCIe и DR0-DR3, DR6, DR7, JTAG и прочие udbgrd, udbgwr. Предисловие-3. Общий подход к тестам и уровень брака. Я до недавнего времени верил, что в ИТ в РФ люди различают состояние «включилось», «сразу не умерло» и «90% партии работает без сбоев год, у 10% вышли из строя следующие компоненты .. ». Оказывается, для многих это не так, и даже наоборот: включилось == работает на 5+. Хотя те же самые люди не стесняются писать про 95% импортозамещение ESXi – даже если их продукт не умеет элементарного - загружаться с USB флешки, какие уж там многопользовательские файловые системы.Уровень брака. Для большой четверки (Dell, HPE, Lenovo, Huawei \\ xfusion) и примкнувшим к ним Supermicro, и даже для такого странного продукта, как материнские платы от Intel – известен и понятен уровень брака. Для всех российских серверов – я не нахожу открытой статистики и какого-то опыта эксплуатации. Редко когда есть тесты. Сложно судить не только о каких-то эксплуатационных \\ технических характеристиках, но и о комплексных – что там с условиями поставки по гарантии, тем более вне Москвы и дальше бетонки (Московское Малое Кольцо (А107). Московское Большое Кольцо (А108)), особенно в условиях ухода DNS DFS DSL DHL. Нет информации, что еще уносят за собой в могилу российские сервера – начиная от связки процессор + память и заканчивая уходом всей стойки от, в хорошем случае, выбитых предохранителей PDU. Общий обзор, попытка номер 1На странице Серверы российского производства для госзаказчиков (это один из сайтов \"Карма Групп\" ) предоставлены аж 14! российских производителей. Это: Карма, Крусайдер, Фплюс, ГагаринИЦЛ Тимрей, Иру, Нерпа, Кутех Рикор, Ситроникс, Ютинет, ЯдроАквариус, ГравитонКроме этого, есть еще реестр Минпромторга - https://gisp.gov.ru/pp719v2/pub/prod/куда включено вообще все и всё подряд. На первой странице списка Минпромторга -ОБЩЕСТВО С ОГРАНИЧЕННОЙ ОТВЕТСТВЕННОСТЬЮ \"ПРОИЗВОДСТВЕННАЯ КОМПАНИЯ АКВАРИУС\" с продуктом Плата материнская AQH110M R30 (АМПР.687283.300), строкой ниже ОБЩЕСТВО С ОГРАНИЧЕННОЙ ОТВЕТСТВЕННОСТЬЮ \"СПЕКТР\"- с продуктом Крой фуфайки (футболки) с короткими рукавами для осужденных мужчин Дополнительно поиском нашлись:Депо шторм, Тринити, Дельта,  и кто-то еще, те же OpenYard (кто сказал Gigabyte ?),Промобит (Bitblaze)  - Сервер BITBLAZE Ganymed I3212d У Промобит в каталог включены целых 3 (три) сервера, причем согласно документу – в 3U сервер устанавливается 2 процессора, но максимум 128 Гб (DDR4) памяти. Бизнес Система Телеком  - Сервер «Иридиум» торговой марки «Звезда», модель ИР-224Х БВМТ.466219.006 Какие-то сервера СИЛА СР2-5422 Пока дописывал текст, нашел еще вот такое - Вычислительные серверы «Звезда» -  На сайте Lone Starr Звезда содержится превосходный, отборный копиум … цитата: Модули оперативной памяти Звезда Андромеда DDR4-3200МГц разработаны специалистами компании «Звезда» и производятся на собственных производственных линиях, что гарантирует сроки поставки и качество продукции. Всего на сайте Минпромторга в списке при фильтрации по слову «сервер» находится 690 позиций, но в нем есть и такие позиции:10406138 Корпус сервераМожно поискать отличия 10398458 Сервер Delta Tioga Pass I22W6U0D00 (ВРТГ.467569.017КДDS.SKUTPBASE.1)   от10398459 Сервер Delta Tioga Pass I22W2U0D21 (ВРТГ.467569.017КДDS.SKUTPBASE.1-05) Или найти отличия 10407479 Сервер «Аквариус» T30 S100DC (АМПР.466539.020) (со сроками от 18.04.2023 до 15.04.2026)10407479 Сервер «Аквариус» T30 S100DC (АМПР.466539.020) (со сроками от 28.11.2023 до 25.11.2026) Так что реестр это конечно хорошо, только сделано не для выбора серверов, а для показа реестра и актуальности документов. Листать реестр и перечни интересно, пока не начинаешь искать какие-то детали из списка выше. Например - Сервер KARMA Basis .. на базе Fujitsu RX1330 M4Сервер KARMA Data .. на базе Fujitsu RX2540 M5 Сервер KARMA Neuro ..  на базе решения Fujitsu RX2530 M5.Сервер Crusader Squire 220R - 4 слота для установки модулей DIMM DDR4, 128 Gb RAMСервер Crusader Squire 420R 2U – на C621 , это чипсет 2017 года, времен Dell R740 и HPE DL380 Gen10. Текущее поколение HPE – Gen 11 (ноябрь 2022 для AMD и Ampere, январь 2023 на Intel) на Intel C741, до 8 Tb RAM (32*256) на 2U (8-Channel DDR5 @ 4800 MT) С 4 февраля 2024 доступны HPE ProLiant Gen11 servers featuring 5th Gen Intel® Xeon® Scalable processors. Текущее поколение Dell - PowerEdge R760 (февраль 2023) на Intel C741. Вышел в январе 2023, под LGA 4677 (Socket E) Российский стоечный сервер F+ data FPD-1-SP-A4K2H-CTO-S1029 корпоративного уровня в корпусе 2U .. лицензия iLO Advanced – это HPE DL360Gen10. Российский стоечный сервер F+ data FPD-8-SP-5K288G5-CTO корпоративного уровня в корпусе 4U, разработан на базе платформы 5288V5 – это Huawei \\ xfusion GAGARIN Оракул Gen 2 – это Open Compute Project (OCP). Стандарт интересный, только в обычную 19 стойку не ставится. И не понятно, как их правильно писать – GAGARIN, GAGAR>N, GAGAR>IN, Gagar.IN LLC или как-то еще ? Часть информации, предоставленная на сайте в части серверов, как бы это правильно сказать – не подтверждена ничем. Для тех же teamRAY 2082-2U-M – на сайте заявлена:совместимость с Windows, Linux, VMwareОткрываю VMware Compatibility Guide, ищу -There are no results found for \"teamRAY\" Впрочем, и на официальном сайте то же самое – вот Сервер teamRAY 2122-2U-M, цитата:Совместимость с операционными системами: Microsoft Windows Server 2016, Microsoft Windows Server 2019, Microsoft Hyper-V Server 2016, Red Hat Enterprise Linux 7/8, CentOS 7.0 и выше, Ubuntu 14.10, SuSE Linux 13.1, SuSE SLES 11/12 SP3/12, VMware ESXi 6.5, 6.7, 7.0 В приложенных документах, разумеется, никаких ссылок на тестирование совместимости и ссылок на Windows Server Catalog или VMware Compatibility Guide – нет. Кому не лень, может поискать подтверждение совместимости с RHEL. Для тех же Fujitsu-RX2530M5 из первых строк, все тоже не очень хорошо с совместимостью – заявлена (на VMware Compatibility Guide) совместимость только с ESXi 6.7 U3 (vSAN 6.7 Update 3), EOL\\EOS – последний патч в рамках стандартной поддержки - ESXi 6.7 P08 20497097, 2022/10/06 (Build numbers and versions of VMware ESXi/ESX (2143832) Совместимость с ESXi 7.0 U3 (vSAN 7.0 Update 3) начинается только с M6 - Fujitsu-RX2530M6 Аналогичная ситуация для iRU C2212P (внутри все тот же старый Intel C621) – официальный сайт заявляет:Поддерживаемая операционная средаAlt Server 10, Astra Linux, Microsoft Windows 10 Pro, Microsoft Windows 11 Pro, Microsoft Windows Server 2019 / 2022, Citrix Xen Server, Vmware ESXi, Linux KVM, Windows Hyper-VСнова открываю VMware Compatibility Guide - There are no results found for \"iRU\"Что хотели сказать писатели документа фразой Linux KVM – я не знаю. Есть интересный вопрос. Если производитель пишет про поддержку VMware by Broadcom на официальном сайте – то будет ли он решать проблемы по версиям, пусть не 5.5 и не 6.0, а по работе 7u3 и 8u1 на этом оборудовании? Или нет? Если не будет, то про какую «поддержку» речь ? То же самое касается Windows Hyper-V , что именно там «поддерживается»? Открываю Windows Server Catalog - https://www.windowsservercatalog.com/ - никакого сервера iRu там нет. Есть только iRU ROCK - by Delovoy Office, Ltd, который совместим с Windows Server 2008 R2 x64 . Возник еще один интересный вопрос, пока я листаю официальный сайт iRu - Берем Сервер iRU S3216P под LGA 3647 на все том же C621 -Оперативная память: 16 x RDIMM/LRDIMM slots,DDR4 RDIMM/LRDIMM (2933 MT/s),8, 16, 32, 64, 128 GB DIMMs (max 4TB)Кто-то может объяснить, как 16 слотами и модулями по 128 Gb набрать 4 Тб? Кто там еще пробегал по новостям? 05 декабря 2023: В России запустили массовое производство серверов на территории технопарка «Рязанский». Мощности производителя OpenYard позволяют выпускать 60 тыс. серверных единиц в год.Ведомости пишут, что Гигабайт покинул здание Следующим номером в нашей программе идет Delta - Delta Bright Lake. Просто так открылось, никакого алфавитного или иного порядка тут нет. Кто бы еще обьяснил, почему сервер называется Delta Bright Lake, а не скажем, Добрыня. То есть, DOBRYNYA, конечно. Открываю полное техническое описание в pdf (ссылка), и читаю:Поддерживаемые ОС: Astra Linux, ALT Linux, РЕД ОС, ROSA Linux, Windows Server, VMware, Microsoft Azure, RedHat Enterprise LinuxКак это понимать? Любой Windows Server? Любой RHEL ?VMware – это точно операционная система? Microsoft Azure – где можно подробнее изучить данную ОС? Самое удивительное для меня, что сервера Delta действительно тестировали с тогда еще VMware ESXi, 16.08.2021 было в новостях, и в VMware Compatibility Guide сервер Delta Tioga Pass от DELTA SOLUTIONS LLC - присутствует. Для 7u2 / 7u3, но присутствует. Еще большим удивлением для меня стало, что там же (в Compatibility Guide) есть и Tioga Pass Single Side/HEPB.466216.007 от Gagar.IN LLC и Tioga Pass ORv2-DDR4 RED 5KT от ITRenew Inc.Придется поверить и в заявленное прохождение тестирования Горизонт-ВС (OpenNebula ?)  . Даже на сайте Астры есть, Tioga Pass I22A [8009]  Тут должен был бы быть сарказм «даже документацию нормально сделать не могут, что же с железом», но нет. Документация написана небрежно, но сертификация настоящая. Чего нельзя сказать про часть остальных заявителей. Гагарин (GAGAR>N ? GAGAR>IN ? Gagar.IN LLC ? Как правильно-то?) тестировался в Кроке – есть микро отчет на ХабреВ том же блоге Крока (от 11 дек 2023 ) висит статья Обзор OpenYard RS1B7I. Что скрывается в 1U сервере из семейства RS101? Злые языки пишут, что это уж очень похоже на Gigabyte R182 (в модификации R182-N20). тестирование со странным мнением, цитата: В случае с OpenYard ответы от поддержки приходят быстро. Например, на момент тестирования ни на официальном сайте, ни в открытом доступе мы не нашли документацию и микрокоды компонентов сервера, информацию о гарантийной замене запчастей, матрицу совместимости сервера с ОС и ПО различных производителей.  NB!: И в этом нет ничего удивительного и критичного! Наш опыт работы с российскими вендорами показывает, что отсутствие документации в открытом доступе — это абсолютно нормальная практика. Ничего нормального в отсутствии документации я не вижу. Практика есть, но чего в этом нормального? По степени «нам вообще все равно, что вы о нас думаете» - на первом месте связка Гравитон плюс гелиус.рф  . Идем на https://graviton.ru/, оттуда в продукты – гиперконвергентные решения, и тут же идет переадресация на https://гелиус.рф/Бог с ним с бесплатным сертификатом от LE на три месяца, но каталог с названием «безымянный-1» по ссылке  - это пять. В каталоге есть и малоизвестный производитель Элвис (ну как, малоизвестный. Обещали что-то для спутников, но не срослось даже на 180 нм, в 2022 их обещали покарать). Для как - бы - гражданского рынка – процессоры тоже делали на TSMC, 40 нм, ARM Cortex-A9 - 1892ВМ14Я (MCom-02) , каталог на 2017 год тут. Тем не менее, более-менее полный каталог от Гравитона – лежит как пдф на сайте Гелиуса. Очень, очень очевидное решение. Лучше только заявляемая поддержка 9/5.Что касается каталога, то оформление pdf разъезжается по всему тексту, выполнено в стиле «и так сойдет», разбавленное грамматическими ошибками типа «бущем» или мутантов «Intel® Xeon® ScalaЫe».Вызывает интерес и математика в документе – например, для серверов 1U Как в 8 слотов по 128 Гб можно собрать 4 Тб? И какой процессор, интересно, предлагается ставить из указанных поколений?раздел Администрирование тоже удался. Указаны:Astra Linux® Special Edition 1.6; 1,7 релиз СмоленскАльт 8 СП Сервер; Альт Сервер 9.1РЕД ОС 7.2 МУРОМWindows Server® 2019 StandardWindows Server® 2020 StandardWindows Server® 2016 StandardCentOS 6; CentOS 7; FreeBSD 11; VMware ESXi 6.5; VMware ESXi 6.7 Не очень понятно. Документ из 2022 года, может просто устарел, а поддержка Windows Server Datacenter и Windows Server Datacenter: Azure Edition – слишком сложна. Что есть на самом сайте Гравитона? Посмотрю серверы «Гравитон» С2082И/С2122И/С2242ИЧто пишут:Сервер поддерживает центральные процессоры с тепловыделением 205 Вт, накопители NVMe с малым временем задержки, а также до двух мощных графических процессоров (GPU) в корпусе 2UНюанс из документации: До пяти графических процессоров тепловыделением до 75 Вт в корпусе 2U. Не очень понятно, как это понимать – как 5*75  и всего 375, или как 15*5, и всего 75. одна NVIDIA A10 – это уже 150 W Расширенные возможности: наличие 4 сетевых портов. Нюанс из документации: это 4х 1 Гбит/с RJ-45 Операционная система, цитатаAlt 8 SP Server, Alt Server 9.1, VMware ESXi 6.7, Windows Server® 2016 Standard, Windows Server® 2019 Standard, Ред OС МуромСнова - говорить про поддержку Datacenter и Datacenter: Azure Edition и VMware by Broadcom 7\\8 – не приходится. Зато на сервере есть звук, и, цитата:Слот MicroSD с использованием в качестве загрузочного диска и поддержкой гипервизора, специальные внутренние диски SATA DOMИнтересно, какого именно гипервизора? Их, гипервизоров, чуть больше 1, и даже чуть больше 2. Документации по BIOS\\ BMC\\IPMI - нет никакой. Вообще. Документации по работе, запуску и массовому обновлению - нет никакой. Вообще.Шаблонов под Zabbix или описания SNMP \\ OID - нет никаких. Вообще. ИтогСначала я хотел написать статью «ой все пропало, полный хлам». Но. Ситуация сложнее. Сама техника выпускается, и не «полный хлам». Не «топ 5», но и сразу не выделяет волшебный белый дым.Но все остальное, кроме самой техники – это какой-то квест, и вовсе не уровня «Похищение слона».  Хотя, может, и его – в той части, где надо открыть холодильник. Обзоров почти нет, таблиц совместимости с прочим оборудованием нет. Нет каких-то элементарных вещей, типа SNMP \\ OID. То же самое касается «импортозамещающих деривативов» - что-то из документации по совместимости ОС + сервер есть, но - даже не рядом с Windows Server Catalog или VMware Compatibility Guide по объему и качеству,- отвратительно оформленное, без рекомендаций по настройкам компонентов, версионности, драйверам, etc.У интеграторов иногда все же удается собрать стенды и отловить какие-то проблемы или сравнить настройки, и Ростелеком-ЦОД \\Даталайн очень вовремя опубликовали полезную статью Как мы тестируем серверы, и, похоже, они много знают про больные места.Но без интеграторов заниматься этим всем разнообразием и возможными проблемами, будет не просто сложно, но и дорого – нужны специально обученные люди, а они жадные и на обещания грамоты или права подписи «всамделишный архитектор» не ведутся, как руками не води. Магия такого типа работает в основном на джунов, не на сеньоров тойдарианцев. Тем кто будет с этим работать, я не завидую – денег больше не дадут, а квалификация будет сводится к работе с частными случаями одного из 20 - 30 вендоров. Тупиковая ветка развития для специалиста, которая еще и в любой момент может закрыться. Пока статья писалась, нашлось разное интересное. Вне конкурса в номинации «Что это было, Холмс» выступает Российский научный фонд. В конце 2022 года Фонд разместил Извещение о проведении запроса котировок на поставку сетевого оборудования и выполнение работ для актуализации IT-инфраструктуры # 000152  С такими предметами как: Маршрутизатор Звезда ZRT-8140-12G10XG, 10 ports 10GE(SFP+), 8 ports GE Combo, 4 ports GE Copper, 1*USB3.0, 4*SIC, 2*PS 350W AC, 3m cable RJ45-to-DB9, 3m cable SFP+ 10G CC2P0.254B(S), 2*SFP GE RJ45 Есть еще такой производитель, как Российские коммутаторы доступа - Вектор Технологии с коммутаторами VA2100-48P-4X или VA2100-48T-4X. Интересно, где же я видел маркировку «48T-4X», кто бы это мог быть ? После таких закупок в вакансиях появляются строки типа:Опыт работы с гипервизорами Звезда, маршрутизаторами Звезда. Примечания и дополненияИмпортозамещение на практике. Часть 1. Варианты Импортозамещение на практике. Часть 2. Начало. ГипервизорИмпортозамещение на практике. Часть 3. Операционные системы «Мыши плакали и кололись..» Импортозамещение на практике. Часть 4 (теоретическая, завершающая). Системы и сервисы ПримечаниеКак многократно разоблаченный бдительными комментаторами (почему-то в основном с вчерашней регистрацией или позавчерашней разморозкой и первым комментарием в 2023 году после молчания с 2014-2015 года), не могу не отметить, что «импортозамещение» не то что буксует, а скорее массово не едет никуда дальше выставок, новостей, разговоров и отчетов про успешное внедрение. Дело не столько в наличии коробки под ключ для бизнеса (ихнет), и не в цене (как бы не дороже, чем HPE\\Dell даже сейчас), дело в том что продукты, от железа до софта, действительно «не очень». Продвижение ИЗ никогда не велось сколько-то рыночными методами, но в текущей командно-административной системе потребительские \\ эксплуатационные качества техники не имеют принципиального значения. Что сверху скажут (настоятельно рекомендуют, или сделают предложение, от которого нельзя отказаться) закупить сервер ЛЧШ-РБ-Э-КЛБС – то и придется покупать, и, возможно, даже эксплуатировать. Но. Ввиду архаизации российского ИТ сообщества – открыто делиться опытом не выгодно с точки зрения бесплатной раздачи своего как-бы уникального знания «как работать с этим».  Очень странный это опыт, не очень нужный. Да и, судя по вакансиям, платить за него готовы вне интеграторов только названиями должностей, а не деньгами. Поэтому в паблике будет или реклама, или крики боли.Единственное что могу сказать, что доволен иногда просачивающимися криками боли от того, что продукт импортозаместительный, ИТ-содержащий, не идентичный натуральному, поставил в определенную позицию организацию, причем с полной потерей данных (не остановкой IO) на импортозаместительном СХД. Пара тройка удивлений от того, что бекапы надо не только делать, но и проверять (потому что зеленая пометка в российских продуктах вовсе не означает, что данные действительно забекапились и их можно восстановить*), и хранить офлайн, и иметь схему и процедуру восстановления – и, может быть, чему-то через боль эксплуататоры научатся. * Это не всегда проблема системы бекапа, но давно работающие с бекапом, отлично знают на что отсылка, как этот продукт назывался ранее, как называется теперь. ** Иногда это действительно может быть проблема с CBT, но про ее работу в продуктах импортозаместительных пока никто писать не желает, как и про то, что понимается под 95% аналогом VMFS или ReFS в импортозамещениях.",
    "18": "В прошлом году осень не торопилась прогонять затянувшееся северное лето. Погода ставила очередные температурные рекорды и о конце сентября напоминали только редкие жёлто-красные островки посреди освещённого ярким солнцем зелёного моря. Сейчас за окном апрель, из серых облаков сыплет лохмотьями бесконечный снег. Сугробы скрывают первоцветы, едва проросшие из мёрзлой земли. Над городом застыло низкое беспросветное петербургское свинцовое небо. Мы привыкли к капризам погоды и знакомы с правилами игры. Мир находится в постоянном движении, вид за окном неуловимо меняется каждую секунду и никогда не повторяется дважды. Однако мы верим, что у нашей реальности есть статичная основа, которая меняется только в результате редких, маловероятных стихийных катаклизмов. Мы привыкли к относительной стабильности нашего уютного уголка действительности. Но Вселенная то и дело подсказывает нам, что не существует ничего вечного и незыблемого. Об этом же нам недвусмысленно намекает и весь школьный курс истории. «Личный опыт и знание новейшей истории не могут не убеждать: стабильность преходяща, счастье долго не длится, и вообще вся жизнь состоит из сплошных перемен, преимущественно к худшему. Даже миллионы лет неизменности и процветания ничего не гарантируют, спросите у динозавров. А уж плёнка покоя в десять или пятнадцать лет лопнет обязательно в самом недалёком будущем, да что будущем — в самом недалёком настоящем.» -- Василий Щепетнёв, статья «Опыт катастроф», Компьютерра, 22.11.2010 Будущее пугает нас своей неизвестностью и непредсказуемостью. Мы пытаемся его прогнозировать, строить какие-то долгосрочные планы. Но в любой момент может произойти «маловероятный» катаклизм и мы поймём, что все прогнозы погоды оказались пустышкой. Совсем другое дело — прошлое. Оно кажется нам надёжным и неизменным. По поводу давно минувших веков мы ещё можем сомневаться: кто его знает, было ли на самом деле то, что написано в летописях. Но уж в нашем недавнем «личном» прошлом и тем более настоящем мы уверены твёрдо. Например, мы чётко помним, что некоторое время назад зашли почитать статьи на Хабр (теперь — с чёрными списками и даже с тёмной темой). Мы знаем, что habr.com — это сайт в сети Интернет. Мы представляем себе, как эта сеть устроена. Мы понимаем, что для выхода  в сеть мы используем устройство, работающее на электричестве. Интернет, компьютер, смартфон — всё это привычные нам вещи. Мы знаем историю их появления и развития. У нас есть многолетний опыт их использования. Подобные небольшие кирпичики знаний составляют наше представление о реальности. Но можем ли мы быть уверены, что весь наш жизненный опыт, все эти знания — не иллюзия? Ведь у нас нет никаких доказательств, что наши воспоминания о прошлом и недавнем настоящем истинны. Все эти документы, фотографии, записи, статьи, учебники — не более, чем часть той же изменчивой реальности. Получается, что безоговорочно доверять нашей памяти тоже не стоит. Тем не менее, наша уверенность в стабильности реальности непоколебима. Окружающая действительность жёстко зафиксирована в нашем сознании, она кажется нам логичной, строго последовательной, подчиняющейся причинно-следственным связям. Синие таблетки здравого смысла исправно выполняют свою работу. Но иногда система даёт сбой и мы наблюдаем странные флуктуации и подсознательно начинаем сомневаться в реальности происходящего. Матрица меняется и показывает нам две одинаковые чёрные кошки. Бывает, что изменения происходят почти незаметно. Но иногда возникают явственно ощутимые сдвиги, на которые обращают внимание даже самые убеждённые скептики. Почему же это происходит? Почему мы периодически начинаем сомневаться в незыблемости нашего прошлого и настоящего? Истину мы, конечно, пока не узнаем. По крайней мере до тех пор, пока какой-нибудь суровый персонаж не предложит нам выбор из двух альтернатив. Но никто не мешает нам пофантазировать, тем более, что научная фантастика давно предлагает нам готовый вариант решения. Мы с детства читали истории о том, как кто-то отправляется в прошлое и специально или случайно изменяет будущее. Для этого даже не нужно делать ничего грандиозного — достаточно какого-нибудь незначительного поступка. Например, наступить на пресловутую бабочку, как в классическом рассказе «И грянул гром» Рэя Брэдбери. Что произойдёт со всем нами, оставшимися в настоящем? На этот счёт существует множество теорий, тысячи научно-фантастических произведений пытаются дать ответ на этот вопрос. Давайте рассмотрим только два варианта. Если не учитывать теорию мультивселенных, то с каждой раздавленной бабочкой мы все дружно переезжаем в новую версию реальности. Путешественник возвращается в наш обновлённый мир. Если же Эверетт и Тегмарк правы, и мультивселенные существуют, то раздавленная бабочка вроде как никак не повлияет на наш мир. Альтернативное будущее проявится в параллельной реальности, где живут и действуют наши двойники. Путешественник из прошлого вернётся именно к ним, к этим двойникам. «Экельс медленно вдыхал воздух — с воздухом что-то произошло, какое-то химическое изменение, настолько незначительное, неуловимое, что лишь слабый голос подсознания говорил Экельсу о перемене. И краски — белая, серая, синяя, оранжевая, на стенах, мебели, в небе за окном — они... они... да: что с ними случилось? А тут ещё ощущение. По коже бегали мурашки. Руки дёргались. Всеми порами тела он улавливал нечто странное, чужеродное. Будто где-то кто-то свистнул в свисток, который слышат только собаки. И его тело беззвучно откликнулось. За окном, за стенами этого помещения, за спиной человека (который был не тем человеком) у перегородки (которая была не той перегородкой) — целый мир улиц и людей. Но как отсюда определить, что это за мир теперь, что за люди?» «Мёртвая бабочка — и такие последствия?» Весь мир изменился: цвета, запахи, люди вроде и те же, но стали неуловимо другими — чужими и непривычными. Изменились даже правила орфографии. Путешественник в ужасе. Ну и сам виноват — нечего было давить ни в чём не повинных доисторических насекомых. Нам гораздо интереснее, как подобные изменения будем ощущать мы сами. Что именно произойдёт с нами в момент смены истории? Во всех этих выкладках понятие «мы» довольно условное. Где именно находится настоящий Нео — в матрице или в Зионе? На первый взгляд кажется, что в Зионе. Но где гарантия того, что не существует ещё какого-то, более настоящего «Нео». Если всё зависит от цвета таблетки, то наше «Я» в каждой реальности — это не больше, чем иллюзия. В классическом варианте для нас поменяется и прошлое и настоящее. Ощутим ли мы такое изменение? Вполне возможно, но морок быстро пройдёт и окружающая действительность снова будет казаться нам стабильной и незыблемой — с неизменным прошлым и реальным настоящим. «Вадим дотронулся до стекла, глядя на незнакомый город за окном. — Что за мир мы сотворили теперь? И можем ли мы всё ещё исправить... Борис, в этот момент смотревший на наручные часы, покачал головой. — Смирись, шеф, уже всё. Волна парадоксов нагонит нас через три... — он поднял вверх указательный палец, — две... одну... Сейчас! Вадим моргнул. Голова как-то странно кружилась, но через мгновение непонятное ощущение спало. Макаров удивлённо взглянул на друга. — А ты чего с поднятым пальцем стоишь-то? — осведомился Вадим. Борис поднял взгляд от циферблата и столь же изумлённо уставился на Макарова. — Я... я не знаю, — наконец, выдохнул он. — Чего-то... Чего-то у меня башка болит... А нет, всё, прошла. Наверное, переутомился сегодня. Вадим кивнул и обернулся на раскинувшийся за окном пейзаж Москвы. Над городом алел закат, отражаясь в мириадах стёкол. По небу бежали редкие облачка, предвещая мягкий тёплый вечер. [...] Продолжая улыбаться, Вадим устало вздохнул. Ещё один день был позади, ещё одна операция прошла успешно, ещё один раз они сохранили привычную историю в целости и сохранности». В варианте с мультивселенными все наши многочисленные двойники в различных реальностях образуют своеобразную суперпозицию личности. Поэтому любые изменения с каким-либо из вариантов реальности не могут не повлиять на все другие варианты. В каких-то случаях влияние будет слабее, в каких-то сильнее. Настолько сильное, что никакими средствами его уже будет не заглушить. «Они понимали: реальность не спасти, мир всё дальше смещался от равновесия. [...] Они смотрели, как на глазах менялся мир, а люди продолжали жить обычной жизнью. Смотреть, помня, было жутко. [...] Они стояли у окна и смотрели, а люди внизу занимались своими делами, жили в своём мире, не помнили другого, они привыкли к этим ураганам, этому небу, этим будням, и праздники тоже были, когда все радовались спокойному небу и устойчивым жилищам. Мир был далёк от максимума вероятности...» Мы верим в стабильность своего прошлого и «нормальность» настоящего, каким бы диким оно ни оказалось в очередной итерации изменений, в очередном релизе действительности. Но бывают моменты, когда мы ощущаем, что с нашей реальностью что-то не так — наш мир «как будто кто-то подменил». Мы не находим вещи на привычных местах, не узнаём окружающую действительность, ощущаем странные флуктуации бытия. Что со всем этим можно сделать? Как минимум не верить в неизменность прошлого, настоящего и будущего. Всё вокруг — не более, чем хаос, в котором нам только мерещится смысл. Жить в хаосе непросто, поэтому наш мозг прячется в уютной скорлупке иллюзии стабильности — это встроено в нашу операционную систему. Мы внезапно оказываемся в мире, о котором раньше не подозревали. Нам чужд этот мир, нам непонятны его законы, мы боимся его, поскольку не представляем, как в нём можно существовать. А потому всеми силами пытаемся спасти остатки привычного настоящего: цементируем его трещины, пытаемся хлипкой арматурой связать расползающиеся обломки. Страх перед будущим — вот доминирующее состояние современности. Если фантастическая гипотеза постоянного изменения прошлого верна, то время от времени мы действительно обнаруживаем себя в совершенно другом мире с новыми законами и правилами. Возможно, некоторое время мы ещё храним в памяти смутные воспоминания о «старом добром» прошлом. Но вскоре мы уже убеждены, что всё идёт своим чередом, что ничего в сущности не изменилось и «всегда так было». Возможно, просто голова разболится, но мы привычно спишем это на переутомление, недосып, циклоническую депресиию и магнитные бури. Тут впору погрузиться в экзистенциальный кризис и размышления о «тщете всего сущего». Но есть и другой выход — просто продолжать жить и заниматься своим делом. Принять нелепость, изменчивость и ненадёжность окружающего мира. Творить новые вещи и идеи, писать программы и тексты. Жить здесь и сейчас, и верить в то, что после печальной осени, вслед за долгой, холодной и беспросветной зимой наступит прекрасная долгожданная весна. — Чем же всё это окончится? — Будет апрель.— Будет апрель, вы уверены? — Да, я уверен.Я уже слышал, и слух этот мною проверен,будто бы в роще сегодня звенела свирель. — Что же из этого следует? — Следует жить,шить сарафаны и лёгкие платья из ситца.— Вы полагаете, всё это будет носиться?— Я полагаю, что всё это следует шить.",
    "19": "Входные технические данные. Коротко. В отличии от эмуляции аркадных автоматов или, например, PSX, какой-то особой специфики здесь нет. В случае SNES использовался SNES9x, в случае Genesis - Gens/GS r7. Руководства по настройке можно найти здесь и здесь. В качестве контроллера выступал типовой геймпад Xbox 360. В целях экономии времени, ввиду первого прохождения, пауз по ходу, а также изучения альтернативных путей / механик и паттернов активно использовались сохранения (save states). Один из примечательных эксклюзивов Genesis, вобравших в себя понемногу от достаточно разных тайтлов. Стилистика, антураж, неплохо отрисованные задники и противники напоминают \"Gods\" (1991). Сложность - игры серии \"Castlevania\". Менеджмент инвентаря - лично мне \"The Lost Vikings\" (1993). Наконец отдельные платформенные и прочие элементы \"Vinyl Goddess from Mars\" (1993). Всё упомянутое сопровождено неплохим музыкальным сопровождением. На выбор предлагается три героя. Типовой воин, сильно бьёт, но относительно медленное передвигается. Маг, слаб, но имеет дальнюю атаку, что в случае последнего класса нивелируется где-то с середины игры. 16-битный прототип Цири в красном бикини, отличающийся от двух других повышенной расторопностью в контексте дальности прыжка. Я остановил выбор на последней. Предстояло преодолеть 8 уровней, в конце каждого из которых поджидает босс. Локации по началу линейны, но чем дальше, тем больше становится развилок, не обязательных комнат, посещение которых сулит нахождения дополнительных предметов или серебряных монет, которые затем можно потратить на покупку новых расходников в перерывах между этапами. Геймплей достаточно не спешный. Герой по умолчанию имеет в запасе 2 дополнительных жизни и несколько continue. За каждые 75 000 очков можно получить дополнительное continue, запас жизней при этом увеличить выше чем на 2 нельзя. По ходу, помимо разномастных в контексте вида и поведения противников, встречается множество ловушек, не корректно проведённые прыжки также могут отнимать HP. Боссы, хотя и простые по своей сути, имеют определённые паттерны с которыми нужно разобраться, в ином случае растерять имеющийся запас здоровья не составит труда. В общем сложность можно оценивать как достаточно высокую. По этой причине игру я проходил с сохранениями, попутно затем, чтобы испробовать разные механики и стратегии, а также местами изучить разные углы локаций, что в ином случае вероятно проигнорировал бы. В контексте стратегического подхода можно отметить несколько моментов: фактическую ценность, не в зависимости от того проходится ли игра с сохранениями или без, имеют склянки для пополнения HP. Всё остальное нужно скорее постольку поскольку. Как, например, возможность превратить противников в серебро - количеством больше чем 3 на фрагмент локации они фактически не появляются и особо опасных разновидностей не так много. к началу пятого уровня за 20 монет можно купить броню, что в случае воительницы даёт ей возможность пользоваться арбалетом вместо меча. накопленные походу склянки, активирующие временную защиту, могут наиболее пригодиться при сражениях с боссами, в частности финальным, двуглавым драконом. Ныне бытует расхожее мнение, что хороших игр по Робокопу, равно как и по Терминатору, до двух недавних знаковых проектов практически не выходило. По факту они были и здесь один из тех редких случаев, когда разработчикам удалось воплотить атмосферу не одного, а сразу двух фильмов в формате достаточно бодрого кроссовера. Главным героем выступает Робокоп. В первой части игры ему предстоит бороться с различной уличной шпаной и силами злой корпорации, что в финале увенчается известной знаковой битвой с неповоротливым роботом. Во второй же действо перемещается в постапокалиптическое будущее. Разруха, горящий огонь в телевизорах, терминаторы разных мастей, начиная от чисто железных, человекоподобных, более \"тугих\", \"ржавых\", заканчивая механическими пауками пары видов. Геймплей подобен серии \"Contra\", похожи здесь в т.ч. и разновидности подбираемого оружия, с той разницей, что герой менее поворотлив и не может исполнять кувырки в воздухе. Одновременно с собой можно носить два вида орудий, в зависимости от типа противников будет эффективно либо одно, либо другое. Если с \"обычным\" оружие от пистолета до гранатомёта всё более-менее понятно, то, по аналогии с \"Terminator: Resistance\" (2019), ситуация более интересная - можно подобрать лазеры двух типов. Красный и белый. Красный, как и в упомянутой игре, хотя способен убить терминаторов быстрее \"обычного\" оружия, не ровня белому, что убивает их в считанные секунды. По мере прохождения встречаются склянки для пополнения здоровья, уровни многогранные, есть хитро запрятанные секреты. В случае гибели Робокоп теряет текущее вооружение, как следствие фактической необходимостью становится в случае риска для жизни такового в последний момент, или же соответственно заранее, сменить оружие на то, что не жалко потерять, после чего переключиться на основное. NintendoComplete: \"Finally, let me impress on you just how  incredibly difficult this game is. The first half is actually not bad -  it's quite manageable, even for relatively inexperienced players. By the  time you get to the later stages, though, there's a good chance the  frustration will have you in tears if you haven't just turned it off  already. The final stage in particular is merciless, and that last boss  is probably among the hardest I ever seen in a 16-bit game. I really  loved the game and forced my way through with untold hours of practice,  but if you aren't patient and willing to replay the game dozens of times  to get good enough, I highly doubt you'll finish it without cheating at  it.\". В финале ждал неожиданный твист. Игру я проходил на \"Normal\", по ходу прохождения не скопил особого запаса жизней, проходил игру впервые и в принципе не знал, что может поджидать впереди. Во-первых сам босс. Он не только не прост сам по себе, но и, как оказалось, имеет больше трёх стадий, что в целом довольно не типично для усреднённого проекта тех лет. Во-вторых в течении первых стадий с обоих концов экрана регулярно появляются терминаторы, как следствие чтобы оперативно совладать с таковыми с собой по определению нужно иметь лазер определённого типа. Наконец в третьих, с учётом того, что к последней стадии удастся выжить, есть лазер и удалось разобраться с основными паттернами, в конце динамика возрастает кратно - на экране остаётся примерно одно условно более-менее безопасное место, требующее предельно чёткого тайминга в вопросах реагирования на атаки противника, в ином же случае, что небольшой, а у меня был именно такой, что большой запас жизней можно растерять моментально. По итогу, несмотря на груду сохранений, убил на финального босса около одного часа от общего времени, т.е. 1/3, пока наконец примерно не осознал правильную последовательность действий, как, впрочем, под вечер и под конец захода реакция в принципе стала проседать, что не добавляло шансов на успех. Поначалу думал повырезать фрагменты из видео, но потом решил оставить всё как есть, так сказать для наглядной демонстрации. Выбор остановил на версии для Genesis, т.к. она по ряду аспектов превосходит вариант для SNES. Рассказ об этой игре пожалуй стоит начать с того, что я не фанат файтингов, плохо в них разбираюсь и играю не особо лучше. В сущности весь спектр интересов в этом плане был всегда ограничен серией \"Mortal Kombat\", в основном классическими частями с первой по четвёртую, при этом играл я преимущественно всегда в первую, а также свежей ревизией с девятой по одиннадцатую части. Спонтанно некогда также довелось пробежать сюжетный режим в \"DoA: 5 Last Round\" (2015) и на том я в сущности в таковую более не возвращался. Не смотря на этот фактор, а также аспект того, что Eternal Champions явно затачивался под контроллер с шестью кнопками, этот тайтл оказался в моём списке. Тому есть несколько причин: По имеющимся фишкам он ничем не уступает \"Mortal Kombat I / II\" и другим файтингам того времени, а где-то возможно и превосходит таковые. Если действие \"Street Fighter II\" происходило в разных странах, то здесь акцент сделан на временные эпохи, будь условные США 1920-х или доисторическая эра на фоне вулкана. Красочная графика. Помимо россыпи базовых ударов присутствует неплохой набор спец. ударов, что в сумме можно выстраивать в комбо. 9 по своему колоритных персонажей. В частности: -- Shadow - нинзя, работающая на могущественную корпорацию. По специфики ударов и приёмов походит на Джейд. Основные приёмы связаны с ударами ногами, может использовать сюрикены и схожие предметы, исчезать из поля зрения. --  Larcen - товарищ, прямиком прибывший из нуарных фильмов былых десятилетий. --  Jetta - цирковая акробатка. --  Xavier - маг, может, подобно Скорпиону, притягивать к себе противников, а  также, подобно Морозилле (с) (Sub-Zero) превращать их, нет, не в лёд, а  в золото, как в мифах о Мидасе. Наличествует аналог \"stage fatality\" из серии \"Mortal Kombat\". Есть режим с модификаторами, возможность играть со slow motion скоростью, настроить уровень противника по шкале от 1 до 8. Существует также версия для Sega CD, отличающаяся, помимо мелочей, расширенным ростером бойцов, наличием CD-Audio и видео-вставок и вроде бы как более продвинутой цветовой гаммой. Как понимаю эта игра в своё время позиционировалась как своего рода сиквел, что играется примерно также, с той разницей, что комбинации ударов были пересмотрены, как и местами костюмы персонажей. С ней я ознакомился бегло, т.к. не было интереса заново разбираться со спецификой ударов, поэтому лишь отмечу, что оригинал мне понравился больше. Существуют хаки для обоих релизов. С одной стороны балансирующие сложность основного турнирного режима, а с другой стороны пересматривающие некоторые тонкости игрового процесса, так, например, большинство спец. приёмов теперь расходуют шкалу внутренней силы. Найти их можно здесь - Eternal Champions - Special Edition и здесь - EC CD AI Nerf соответственно. При желании также сложность можно наоборот выкрутить в сторону хардкора - нажатие определённой комбинации выставит уровень противников на 7 из 8. В турниром режиме скорость, судя по всему, регулировать нельзя. В случае поражения в матче игроку предлагается не начинать игру заново, а сразиться с одним или несколькими предыдущими противниками. В конце поджидает Eternal Champion, силе которого мог бы позавидовать Шао Кан. Во-первых бой с ним протекает в течении пяти поединков и во всех из них необходимо победить. Во-вторых, хотя он и наносит обычный урон, на той или иной стадии он может активировать разные способности - многократно ускорять темп своего передвижения и атаки или, например, становиться неуязвимым на какое-то время. Что же касается управления, то с ним пришлось повозиться, дабы адаптировать под адекватную работу с геймпадом Xbox 360, но по итогу результата добиться удалось. Список всех ударов и комбинаций можно найти здесь - Eternal Champions - Move List and Guide - Genesis - By AlaskaFox - GameFAQs. HoldBk - назад (зажать). X - лёгкий удар рукой. Y - средний удар рукой. А - лёгкий удар ногой. B - средний удар ногой. T - вперёд. LB (Z) - сильный удар рукой. RB (С) - сильный удар ногой. U - вверх. D - вниз. Ряд основных комбинаций за \"Тень\", что я регулярно использовал: X+Y+LB одновременно - скрыться в тумане. Затем X+Y+A+B - нанести удар в прыжке. Т.к. контроллер более простую комбинацию мог считывать не всегда верно для простоты решил, что будет проще нажимать четыре кнопки. Вниз + B - нижний кувырок / удар ногами. LB + X - насмешка. Назад (зажать), затем вперёд и X одновременно - кинуть сюрикен. Назад (зажать), затем вперёд и X одновременно. Если Y, то быстрый сюрикен. Назад (зажать), затем вперёд и RB - последовательный кувырок / удар ногами. Вниз + А + B - длинная подсечка. Вниз + RB - шпагат. Вниз + X + B одновременно - подсечка. Ряд особо забористных комбо на таком геймпаде будет выполнить проблематично, но, впрочем, и без них играется нормально, а ставить направления на крестовину вместо стика мне показалось излишним. Как пример - назад (зажать), вверх и вперёд одновременно, затем X. К слову. На базе OpenBOR в своё время кем-то был сделан любительский beat 'em up - Eternal Champions The Thin Strings of Fate. Пожалуй самый необычный вариант арканоида из тех, что я когда-либо видел, не считая трёхмерного \"Breakfree\" (1995). Присутствует небольшой сюжет. Парень и девушка были превращены демоном в... две платформы для игры в арканоид. В сопровождении магического шара необходимо прорваться через семь довольно разноплановых уровней и расквитаться с обидчиком, походу разобравшись ещё с шестью необычными боссами. Управлять предлагают двумя платформами одновременно, при этом ту, что на переднем плане, можно / нужно поворачивать - геймплей здесь идёт не только по горизонтали, но и местами вертикали. Дополнен этот процесс весьма неплохим саундтреком, варьирующимся от условно типовых электронных мелодий до нечто напоминающее dark ambient. Графика также на уровне. Отдельные сегменты уровней можно пропускать, скоротечно прокатившись по предусмотренным для того проходам, или закатив шар в пушку и использовав его как ядро. Есть возможность сбора апгрейдов для платформ, а также шара. В случае подбора последнего тот временно превращается в огненного дракона. Ложкой же дёгтя служит сложность - continue не предусмотрены, в запасе на старте максимум 4 шара и глобально пополнить таковые можно, судя по всему, только ближе к концу, на вулканическом уровне. В придачу к этому неутомимо тикает таймер - так один из этапов я смог пройти имея в запасе порядка лишь 3-4 секунд. А. Играл на \"Easy\". Б. Использовал сохранения, что в частности существенно пригодились в битве с одним из боссов, где нужно уничтожить множество плиток, т.к. перезагружаться пришлось каждые несколько секунд, дополнительных жизней к тому моменту в запасе не было. Жёсткий, но добротный и самобытный shoot 'em up. Когда заходит речь о \"шмапах\" в качестве основных критериев я рассматриваю две составляющие - стилистическую и \"проходную\", т.е. сложность. Первый аспект наделяет тот или иной проект определённой уникальностью, а второй становится критичным с позиции \"входного барьера\". Ввиду этих факторов большинство проектов этого жанра, и SNES здесь, как платформа, не исключение, практически сразу отправляется в корзину, т.к., с одной стороны, с завидной регулярностью они похожи друг на друга как две капли воды и зачастую совсем ничем не выделяются, с другой же сложность, как неписанная норма, зачастую по умолчанию предполагается как сугубо убойная. В случае же данной, хотя очевидно и не простой, игры баланс упомянутого ощущается соблюдённым. Вначале предоставляется выбор из 5 сложностей - помимо стандартного набора от \"Easy\" до \"Hard\" имеются \"Tricky\" и \"Hyper\", причём последние две стоят по левую сторону от \"Easy\". В чём их специфика я, честно говоря, не разобрался / первично на заметил явной разницы в случае \"Tricky\", и остановил свой выбор на \"Normal\". Предстояло преодолеть 12 уровней, отличающихся продолжительностью. Как правило присутствует чередование длинных этапов, в конце которых поджидает \"крупный\" босс, и коротких, где босс будет либо простым, либо его не будет вовсе. Continue бесконечные, помимо этого присутствуют чекпойнты и в случае уничтожения всех запасных кораблей можно загрузиться не с начала игры или определённого этапа, а конкретного сегмента, но при этом боссы в отдельный сегмент не выделены, что в последующем порождало определённые трудности. Примерно первые три-четыре уровня ощущаются умеренно простыми, после чего градус сложность ощутимо накаляется - противников становится больше, отдельные сегменты приходится проходить буквально рассчитывая расстояние до сантиметрам, дабы прорваться по только что пробитой прямой по заполненной секции или скоротечно маневрируя между не уничтожаемыми барьерами. Паттерны боссов требуют определённого запоминания и стратегии и зачастую пройти их с первого раза не возможно физически, ввиду чего с достаточной регулярностью придётся откатываться до ближайшего чекпойнта. С какого-то момента мне эта составляющая серьёзно приелась и я стал налегать на save state'ы, ставшими особенно актуальными ближе к концу. В финальном уровне, помимо основного босса, придётся предварительно сразиться с тремя из предшествующих этапов. Набор вооружений и апгрейдов с одной стороны достаточно стандартен - пули, имеющие более узкий или широкий охват определённой области, лазер, самонаводящиеся ракеты, ограниченный запас бомб, зачищающих ближайшую область. С другой - есть и оригинальные разновидности, вроде сфер, которые при столкновении с препятствием разлетаются на части, или тоже сфер, но служащих своего рода защитным барьером корабля, диаметр которых, по аналогии с другим вооружением, можно увеличить в случае подбора соответствующего бонуса. Запас жизней корабля исходно не велик и максимум он выдерживает порядка трёх попаданий, при учёте того, что вначале снимаются апгрейды с оружия, а потом жизни, при этом без улучшенного вооружения в большинстве ситуаций далеко не уйти. В случае уничтожения и загрузки с чекпойнта все апгрейды обнуляются, а резерв фактически оказывается нулевым. Этот аспект, в свою очередь, даёт путь для стратегических уловок - \"особо забористые\" моменты можно миновать путём целенаправленного уничтожения резервного корабля. Платформер. Отец и сын ведут эксперименты с новой субстанцией, в основу которой лёг пластелин. Как это бывает в таких случаях ситуация идёт не по плану - в процесс вмешивается злодей, требует предоставить ему формулу, в гневе от отказа крадёт субстанцию, отца, а сына превращает в синий шар из этой самой субстанции. В общем страх и ужас, но наш герой не унывает и планирует вернуть всё на круги своя и отомстить обидчику. Начав своё путешествие во дворе дома, далее предстоит посетить ряд локаций, от улиц до морских глубин, до Японии и космической станции. Каждая из них разбита на несколько этапов. По ходу процесса можно собирать другие шары и таким образом превращаться в различных существ - мышь, кошку, рыбу, утку и бурундука. Рыба может плавать под водой, мышь проходить в труднодоступные места, кошка местами может прыгать на чуть более дальнюю дистанцию. В зависимости от ситуации может потребоваться та или иная форма. С противниками предлагается расправляться либо в ближнем бою, либо, подобрав ещё один шар, можно получить дальнюю атаку. В случае же столкновения при наличии вначале теряется данный тип атаки, потом форма животного и только потом отнимается одна из жизней, изображённых здесь в виде звёзд. Присутствуют чекпойнты, continue в игре бесконечные. Между уровнями, в свою очередь, предстоит решать небольшие головоломки по доставке двух дровосеков к ближайшему проходу, предварительно вооружив их инструментом для расчистки такового от завалов, будь то древесина или нечто иное. Если набрано определённое количество очков, а также последовательно собрано слово, то можно заглянуть на бонусные этапы, дабы ещё нарастить число накопленных очков и число запасных жизней. Что касается сложности. Формируется она несколькими факторами. На уровнях нет каких-то обязательных задач, суть сводится лишь к тому, чтобы добраться до выхода, исследование же тех или иных сегментов зависит лишь от воли игрока. При этом сильно мешкать не выйдет - есть таймер, жёстко ограничивающий прохождение каждого участка десятью минутами. Я не ставил себе цели подробно исследовать разные закоулки, этот аспект довольно быстро надоедает и особого практического смысла в нём нет, но при этом всё равно несколько раз терялся в контексте куда идти далее, с учётом в частности того, что в некоторые места можно попасть только по средствам одного единственного где-то расположенного телепорта, а быть при этом их может быть несколько. Бывали ситуации, когда прыжок можно было выполнить только в форме кота и буквально на грани. Оставшегося времени порой оставалось меньше минуты. Отдельно стоит сказать про продолжительность игры. Судя по всему более-менее знакомые с тайтлом люди могут пробежать его за 2-2,5 часа, у меня же ушло порядка 5 часов, при этом системы паролей или иного способа сохранения прогресса в игре нет. Я делал перерывы и использовал сохранения, в ином же случае можно констатировать, что при \"заходе залпом\" ближе к концу игра может приесться, т.к. в определённой степени наблюдаются самоповторы. Тем не менее, проект можно по своему назвать уникальным - само по себе использование пластелина, как элемента игровой стилистики и логики, достаточно редкое явление, а удачное воплощение этой концепции встречается ещё реже. Из в чём-то схожих, но слабо известных, по духу проектов на PC могу порекомендовать \"Puddle\" (2012). Если на NES был \"Monsters in My Pocket\" (1992), то на SNES есть данная, вышедшая эксклюзивно в Японии, игра. Место монстров здесь заняли призраки. Играть предстоит за рыжеволосую колдудью, работающую в агентстве по поимке призраков. Ей предстоит пройти шесть основных этапов, по окончанию которых можно будет собрать шесть камней, вставить их в статую и таким образом получить доступ к последней, потусторонней локации. Я не фанат \"хентайных мультиков с элементами фансервиса\", т.е. аниме, но несмотря на это могу отметить, что местная стилистика, при всей её культурологической особенности, выглядит занятно, а два этапа удались особенно. В одном предстоит покататься на гигантском коте, а другом полетать на метле. Вооружена героиня волшебной палочкой, по ходу прохождения для неё можно находить взаимозаменяемые апгрейды, а также расходуемую остановку времени. В перерывах между этапами присутствуют сюжетные вставки между несколькими действующими лицами. К слову. Для того, чтобы зацепиться за платформу и затем подняться на таковую нужно вначале нажать прыжок (непосредственно находясь под), затем кнопку атаки и направление, т.е. например А, затем стик влево + X, затем ещё раз направление, вверх. На последнем уровне прыгать выше можно только тогда, когда фон загорается красным. Футуристические гонки в мрачноватом будущем. Выбрав один из трёх болидов предстоит пройти серию скоростных заездов. Вначале на время, затем против одного основного соперника - на трассах присутствуют и другой трафик, но он служит здесь лишь декораций / помехой для основного состязания. За победы вручают два вида валюты - деньги и кредиты. Первые можно тратить на непосредственные апгрейды авто, а вторые на вооружение трёх видов - диски, основная фишка которых в том, что они могут рикошетить от бортов трассы, самонаводящиеся ракеты, наиболее эффективное оружие, лишь бы цель была в минимальном поле зрения, и шипастые бомбы. Что касается геймплея и специфики. Вначале пришлось несколько повозиться с настройкой управление. Вышло оно у меня таким: Газ - B. Смена типа оружия - LT. Огонь - RT. Резкий поворот влево - LB. Резкий поворот вправо - RB. Левый стик - вправо, влево. Про кнопку тормоза можно забыть, т.к. по определению необходимо всегда держать гашетку зажатой в пол, иначе за соперниками, которые хотя порой и собирают все углы трассы, но всё равно едут по около идеальной траектории, будет не угнаться, а вот же без нормального прохода поворотов здесь не обойтись. Если на ранних этапах ещё можно проходить по борту, то позже такая стратегия скорее исключается. В контексте апгрейдов я вначале улучшал, пока хватало денег, ускорение, затем переключился на двигатель и только в самом конце на колеса. Все прочие апгрейды полностью проигнорировал, а из оружия пользовался по существу только ракетами, т.к. по большому счёту только они позволяют умерить пыл постоянно уносящихся вдаль соперников. При общей незамысловатой концепции и не низкой сложности игра в немалой степени оказывается интересной своей атмосферой, будто сошедшей с полотен работ участников демосцены, выделяющей её на фоне подавляющего большинства других аркадных гонок на SNES. Электронный саундтрек прилагается. Да, пара слов про отличия \"Normal\" и \"Hard\". Финал в обоих случаях будет одинаковым, с той разницей, что на \"Hard\" игроку сообщат комбинацию для открытия секретного режима (вверх, вниз, LB, RB, Select), где всё тоже самое, но с видом сверху. Сам режим при этом абсолютно бестолковый, объекты слишком мелкие, текстур не наблюдается, т.е. ожидать от него что-то уровня \"Micro Machines\" не стоит.",
    "20": "Работа с pandas.DataFrame может превратиться в неловкую кучу старого (не очень) доброго спагетти-кода. Я и мои коллеги часто используем эту библиотеку, и хотя мы стараемся придерживаться хороших практик программирования, таких как разделение кода на модули и модульное тестирование, иногда мы все равно мешаем друг другу, создавая запутанный код. Я собрала несколько советов и подводных камней, которых следует избегать, чтобы сделать код на pandas чистым. Надеюсь, вам они тоже будут полезны. Также я буду ссылаться на классическую книгу Роберта Мартина «Чистый код: создание, анализ и рефакторинг». не пишите методы, которые изменяют DataFrame и не возвращают его, потому что это сбивает с толку. тестируйте свои функции, потому что это поможет вам создать более чистый код, защититься от ошибок и крайних случаев и документировать его. Для начала рассмотрим несколько ошибочных паттернов, вдохновленных реальной жизнью. Позже мы попробуем улучшить этот код с точки зрения читабельности и контроля над происходящим. Самое главное, что тут нужно вспомнить: pandas.DataFrame — это изменяемые объекты [2, 3]. Когда вы изменяете мутабельный объект, это затрагивает тот же самый экземпляр, который вы изначально создали, и его физическое расположение в памяти остается неизменным. В отличие от этого, когда вы изменяете неизменяемый объект (например, строку), Python создает новый объект в новом месте памяти и меняет ссылку на новый объект. Это очень важный момент: в Python объекты передаются в функцию путем присваивания [4, 5]. Посмотрите на картинку ниже: значение df было присвоено переменной in_df, когда она была передана в функцию в качестве аргумента. И исходное значение df, и in_df внутри функции указывают на одну и ту же область памяти (числовое значение в круглых скобках), даже если они имеют разные имена переменных. Во время модификации атрибутов расположение изменяемого объекта остается неизменным. На самом деле, поскольку мы изменили исходный экземпляр, возвращать DataFrame и присваивать его переменной избыточно. Этот код дает точно такой же эффект: Внимание: функция теперь возвращает None, поэтому будьте осторожны, чтобы не перезаписать df на None, если вы выполните присваивание: df = modify_df(df). Напротив, если объект неизменяемый, он будет менять место в памяти в процессе модификации, как в примере ниже. На картинке ниже, поскольку красная строка не может быть изменена (строки неизменяемы), зеленая строка создается как новый объект, занимающий новое место в памяти. Возвращаемая методом строка не является той же самой строкой, тогда как в случае с DataFrame возвращаемый объект был бы ровно тем же DataFrame. Дело в том, что изменениеDataFrame внутри функций имеет глобальный эффект. Если вы не будете помнить об этом, вы можете: потерять контроль над тем, что и когда добавляется в DataFrame, например, при вызове вложенных функций. Мы исправим эту проблему позже, а сейчас — еще одно «нет», прежде чем мы перейдем к «да». Конструкция из предыдущего раздела на самом деле является антипаттерном, называемым выходным аргументом [1 стр.45]. Как правило, входные данные функции используются для создания выходного значения. Если единственным смыслом передачи аргумента в функцию является его модификация, то есть входной аргумент меняет свое состояние, то это бросает вызов нашей интуиции. Такое поведение называется побочным эффектом (англ. side effect) [1 стр.44] функции, и оно должно быть хорошо задокументировано, а лучше — сведено к минимуму, поскольку заставляет программиста помнить о том, что происходит в фоновом режиме, а значит, повышает вероятность ошибиться. When we read a function, we are used to the idea of information going in to the function through arguments and out through the return value. We don’t usually expect information to be going out through the arguments. [1 p.41] Когда мы читаем функцию, мы привыкли к тому, что информация поступает в функцию через аргументы, а выходит через возвращаемое значение. Обычно мы не ожидаем, что информация будет возвращена через аргументы. [1 p.41] Ситуация становится еще хуже, если функция несет двойную ответственность: и изменяет входные данные, и возвращает выходные. Рассмотрим эту функцию: Как и следовало ожидать, она возвращает значение, но при этом постоянно модифицирует исходный DataFrame. Побочный эффект застает вас врасплох — ничего в сигнатуре функции не указывало на то, что наши входные данные будут затронуты. В следующем шаге мы рассмотрим, как избежать данного антипаттерна. Чтобы устранить побочный эффект, в приведенном ниже коде мы создали новую временную переменную вместо того, чтобы модифицировать исходный DataFrame. Обозначение lengths: pd.Series указывает на тип данных переменной. Такая конструкция функции лучше тем, что она инкапсулирует промежуточное состояние, а не создает побочный эффект. Еще одно предупреждение: пожалуйста, помните о различиях между глубоким и поверхностным копированием [6] элементов из DataFrame. В приведенном выше примере мы изменили каждый элемент исходной серии df[\"name\"], поэтому старый DataFrame и новая переменная не имеют общих элементов. Однако если вы напрямую присвоите один из исходных столбцов новой переменной, базовые элементы по-прежнему будут иметь одинаковые ссылки в памяти. Вот примеры: Вы можете выводить DataFrame после каждого шага, чтобы следить за происходящим. Помните, что при создании глубокой копии будет выделена новая память, и потому стоит задуматься, нужно ли в вашем случае экономить память. Возможно, по какой-то причине вы хотите сохранить результат вычисления длины. Все равно не стоит добавлять его в DataFrame внутри функции из-за возможных побочных эффектов, а также из-за накопления нескольких обязанностей в одной функции. We need to make sure that the statements within our function are all at the same level of abstraction. Mixing levels of abstraction within a function is always confusing. Readers may not be able to tell whether a particular expression is an essential concept or a detail. [1 p.36] Нам нужно убедиться, что все действия внутри нашей функции находятся на одном уровне абстракции. Смешение уровней абстракции в функции всегда приводит к путанице. Читатель может не понять, является ли конкретное выражение существенной концепцией или деталью. [1 стр.36] Также давайте воспользуемся принципом единственной ответственности [1 стр.138] из ООП, хотя сейчас мы не сосредоточены на объектно-ориентированном коде. (И в принципе ООП даже в контексте Python — это последнее, с чем ассоциируется анализ данных с использованием pandas. (примечание автора перевода)) Отдельная задача создания столбца name_len была передана другой функции. Она не изменяет исходный DataFrame и выполняет одну задачу за раз. Позже мы получим максимальный элемент, передав новый столбец другой специальной функции. Есть еще одна вещь, которую можно сделать для повышения уровня повторного использования: передавать имена столбцов в качестве параметров в функции. Рефакторинга уже много, но иногда за гибкость и возможность повторного использования приходится платить. Вы когда-нибудь выясняли, что ваша предобработка была ошибочной, после нескольких недель экспериментов с предварительно обработанным набором данных? Нет? Повезло. На самом деле мне случалось повторять всю серию экспериментов из-за неработающих аннотаций, чего можно было бы избежать, если бы я протестировала всего пару базовых функций. Итак, важные скрипты должны быть протестированы [1, с. 121, 7]. Даже если скрипт — всего лишь помощник, теперь я стараюсь тестировать хотя бы важнейшие, самые низкоуровневые функции. Давайте вернемся к шагам, которые мы сделали с самого начала: Здесь тестируется куча разных функций: вычисление длины имени и агрегирование результата для элемента max . А тест падает c AttributeError: Can only use .str accessor with string values!, хотя мы этого не ожидали, не так ли? Уже гораздо лучше — мы сосредоточились на одной задаче, поэтому тест стал проще. Кроме того, нам не нужно зацикливаться на именах столбцов, как это было раньше. Однако мне всё ещё кажется, что формат данных мешает проверке правильности вычислений. Здесь мы навели порядок и тестируем саму вычислительную функцию без обёртки в виде pandas. Легче придумать крайние случаи, если сосредоточиться на чем-то одном. Я поняла, что хочу проверить значения None, которые могут появиться в DataFrame, и в итоге мне пришлось усовершенствовать свою функцию, чтобы тест прошел. Баг пойман! Еще одно преимущество модульного тестирования, о котором я никогда не забываю упомянуть: это способ документирования вашего кода, поскольку тот, кто не знает, что там происходит (например, вы сам из будущего), может легко определить входные данные и ожидаемые результаты, включая крайние случаи, просто взглянув на тесты. Это несколько приемов, которые я сочла полезными при написании кода и чтении чужого кода. Я не утверждаю, что тот или иной способ кодирования является единственно верным — только вы решаете, нужна ли вам быстрая работа или отполированная и протестированная кодовая база. Но надеюсь, эта статья поможет вам структурировать свои скрипты так, чтобы они были красивее и надёжнее. Буду рада фидбеку и другим комментариям. Счастливого кодинга! [1] Robert C. Martin, Clean code A Handbook of Agile Software Craftsmanship (2009), Pearson Education, Inc. [7] Brian Okken, Python Testing with pytest, Second Edition (2022), The Pragmatic Programmers, LLC. Иллюстрации были созданы с помощью Miro.",
    "21": "Концепция построения  кустов сетецентрической системы управления войсками. Фомичев В.А., полковник в запасе Крюков В.А. «Без применения военной науки победить нельзя»   В.И.Ленин В предыдущей статье «Применение стационарных умных приёмников звука в составе сетецентрической системы» упор делался на одиночном кусте сетецентрической системы уровня батареи. Куст предназначен для установления 3–х мерных координат цели, производящей или отражающей специфический звук. В настоящей статье предпринята попытка выработать принципы взаимодействия кустов единой сетецентрической системы управления [1], например, системы артиллерийской разведки уровня дивизиона. Прогнозируется выбор типовых унифицированных структур, связанных в единую сетецентрическую систему управления войсками. В статье мы не разглашаем никаких секретов, а просто обсуждаем общие концепции. Термин «сетецентрическая система управления войсками» был введен в обиход военными теоретиками США, однако впервые изложил подобную концепцию маршал Н.В. Огарков еще в начале 1980–х годов. С 60–х годов в СССР разрабатывалась командная система боевого управления (КСБУ), способная объединять пункты управления Ракетных войск, флота и авиации. Не хватало только полевой автоматизированной системы управления войсками (АСУВ). По инициативе маршала Огаркова, который был Начальником Генерального штаба Вооружённых сил СССР и первым заместителем Министра обороны СССР, в начале 1980–х приступили к созданию АСУВ с названием «Маневр». Система «Маневр» была использована уже на учениях «Запад-81» и «Щит-82». По мнению специалистов, эта система повышала эффективность вооружения в 3–5 раз. Доктрина маршала Огаркова делала упор на неядерном высокоточном оружии и на повышении мобильности за счет быстрого обмена данными. В основу новой системы должен был лечь комплекс технических средств передачи данных «Базальт», позволявший обмениваться секретной информацией между объектами, удаленными друг от друга на тысячу километров. Внедрение «Маневра» в войсках шло с большим трудом. Многие генералы без особого энтузиазма принимали мудреные системы автоматизированного управления. После прекращения существования Варшавского договора в бывшем ГДР оставался один из комплексов «Маневр». На него–то и обратили внимание американцы. Они использовали комплекс в штабной игре — результат их поразил: благодаря автоматизации управления условная армия Варшавского Договора без применения ядерного оружия разгромила блок НАТО в течение нескольких дней. Алгоритмы «Маневра» легли в основу Концепции сетецентрических боевых действий армии США, изложенной в 1998г., а в 2003г. Соединенные Штаты опробовали сетецентрические систему в Ираке. В живой силе и бронетехнике иракская армия значительно превосходила коалиционные войска, в авиации и ракетах превосходство было уже на стороне НАТО. Однако глобальный перевес коалиционным силам обеспечила информационная система управления войсками уровня связи «бригада – батальон – рота» FBCB2 (Force XXI Battle Command Brigade and Below). В настоящее время в России ведется разработка Единой системы управления тактического звена (ЕСУ ТЗ) «Созвездие», которая предназначена для комплексного управления войсками с использованием систем навигации, спутниковых и беспилотных средств наблюдения. ЕСУ ТЗ «Созвездие» сможет объединить в единую сеть командные пункты, разведку, артиллерию, ПВО, бронетехнику, беспилотники и экипировку «Ратник». В настоящее время сетецентрические системы управления имеют не только военное значение, но и гражданское. 5)    сеть материального–технического обеспечения. Количество составляющих сетей не ограничивается числом 5. Например, можно сходу предложить включить в список сеть медицинского обеспечения. Это означает, что сетецентрическая АСУВ должна обладать свойством открытости и расширяемости. До сих пор мы рассматривали элементы отдельного куста артиллерийской разведки, спроектированного на базе технологии потоковой обработки данных. Рассмотрим более подробно элементы схемы куста приёмников звука в составе пункта управления огнем батареи. Среди датчиков приёмников могут быть не только звуковые датчики, но и, например, световые, что расширяет возможности куста по распознаванию и идентификации цели. Коммуникационный контроллер (КК) обеспечивают потоковую транспортировку данных в репизиторий сигналов. Репозиторий производит сортировку, синхронизацию и сохранение сигналов. Из репозитория сигналов данные поступают на анализатор для вычисления координат цели. Координаты выводятся на экран командира батареи. Командир батареи принимает решение и со своего пульта управления передает координаты на прицеливание. Репозиторий совмещает функции журнала сигналов с БД командирских решений. Одновременно установленные координаты передаются на верхние уровни сетецентрической системы управления войсками. Построим матрицу управления огнем уровня дивизиона, состоящего из трех батарей. См. Рис. 2. Командир дивизиона находится на батарее №2. Между кустами устанавливаются связи. Связь с куста нижнего уровня на верхний куст осуществляется в автоматизированном режиме через пульт управления командира, что отличает систему сетецентрическую систему управления восками от традиционной системы EDA. Данные (донесения) снизу поступает через репозиторий сигналов. Команды сверху передаются непосредственно на пульт управления командира батареей, т.к. данные поступающие с верхнего уровня являются директивными и подлежат непосредственному исполнению без анализа. На Рис. 2 командные линии для управления подчиненными батареями выделены красным цветом. Возможна передача данных по горизонтали между кустами. Например, связь с соседней батареей, которая отвечает за другой участок и географически располагается в нескольких километров. О горизонтальных связях речь пойдет ниже. Чтобы понять как кусты в этой схеме взаимодействуют, проведем такую штабную игру. Вводная: 1)    Батарея №1 получила 3 достоверных сигнала от приёмников звука. Данных не достаточно и анализатор не смог установить координаты объекта цели. Информация выводится на экран пульта управления командира батареи №1. 2)    Батарея №3 получила 4 достоверных сигнала от приёмников звука. Данных не достаточно и анализатор не смог установить координаты объекта цели. Информация выводится на экран пульта управления командира батареи №3. Командиры принимают решение транслировать сигналы на командную батарею дивизиона №2. Консолидированные сигналы обрабатываются в репозитории сигналов и анализаторе батареи №2 и координаты объекта цели успешно устанавливаются. 1)    Нанести огневое поражение силами батареи №2. 2)    Отдать команду нанести огневое поражение силами батареи, наиболее близко расположенной к объекту цели. Описанные шаги должны быть описаны в нормативном документе, типа военного технического регламента (ВТР). Любой артиллерист скажет, что в рассмотренной системе артиллерийской разведки не хватает метеорологического поста, т.е. службы поставляющей артиллерии данные о ветре и температуре. Современные автоматизированные средства метеорологического обеспечения могут быть разного технического исполнения, например, в виде дрона с датчика дистанционного зондирования атмосферы. Ничто не препятствует концептуально, оформить автоматический пост метеорологического обеспечения в виде отдельного куста. Тогда куст метеорологического обеспечения дополнит сетецентрическую систему артиллерийской разведки и станет поставщиком данных для батарей №№1, 2 и 3. Кроме того, можно использовать дополнительные тепловые датчики излучения выстрелов и пусков. Не обязательно, чтобы звуковые датчики были совмещены с тепловыми в одном приёмнике по причине усложнения конструкции и уменьшения надежности системы. Тепловые датчики тоже оформляются в сетецентрические кусты. Взаимодействие кустов распределенных функций организовывается через входные репозитории. Современное состояние теоретической основы сетецентрических систем в России и за рубежом можно представить по работе [3], в которой дается обзор нормативных документов и анализ теоретических исследований интероперабельности в сетецентрических системах. В концепции сетецентрической автоматизированной системы управления выделяют одну важную проблему — интероперабельность. Согласно определению интероперабельность — способность двух или более информационных систем или компонентов к обмену информацией и к использованию информации, полученной в результате обмена. Важность этого вопроса будет показана в следующем разделе. Дальнейший текст отражает личное мнение авторов по построению бесшовного интероперабельного взаимодействия внутри сетецентрической системы АСУВ. Предлагается строить сетецентрическую систему управления войсками из унифицированных типовых кустов на базе технологии обработки данных в потоке. «Потоковая обработка с учетом состояния – это универсальная и гибкая архитектура, которую можно применить во множестве различных сценариев» [4], которые обычно реализуются тремя классами приложений: (1) приложения, управляемые событиями (EDA), (2) приложения конвейера данных и (3) приложения для анализа данных. Рассмотренные сетецентрические кусты нижнего уровня артиллерийской разведки однозначно относятся к приложениям, управляемым событиями. С повышением уровня сетецентрических кустов их функциональность смещается к приложениям анализа данных. Например, куст верхнего уровня может отвечать за сбор данных, анализ и определение базы дислокации гаубичной артиллерии противника. Принципы построения сетецентрической системы АСУВ из сэндвича пирамид показан на Рис. 3 . 1)    Пирамиды из сетецентрических кустов управления составляют плоскости назначения согласно штатного расписания. 2)    Между кустами разных плоскостей назначения выстраиваются межсетевые связи согласно ВТР. 3)    На плоскости назначения по вертикали действуют связи директив и донесений об исполнении. 4)    На плоскости назначения между кустами одного уровня выстраиваются горизонтальные связи боевого взаимодействия. 5)    Командир каждого куста располагает возможностями и имеет право принять самостоятельное решение по обстановке. Между кустами пирамид организуются интероперабельные связи. Заметим, что в программировании EDA вопросы интероперабельности уже переведены в практическую плоскость. В подтверждение наших слов приведем наказ основателя Amazon Джеффа Безоса [5]: 3)      никакой другой формы межпроцессного взаимодействия разрешено не будет: ни прямых ссылок, ни непосредственных чтений из баз данных другой команды, ни моделей разделяемой памяти, никаких обходных трюков. Единственным допустимым способом коммуникации остаются сетевые запросы через сервисный интерфейс. Еще цитата из [5]: «Чётко определённый интерфейс должен описывать, когда и как данные можно получать и обрабатывать.» Назначение, условия исполнения и форматы связи между унифицированными кустами должны описываться в нормативных документах типа ВТР. В основе любой системы управления лежит нормативно–правовой документ. В качестве такого документа обратимся к стратегии и тактике управления войсками, сформулированной генерал-фельдмаршалом Хельмутом фон Мольтке. Мольтке принадлежит заслуга создания Немецкого Генерального штаба и выработки принципа самостоятельного решения на основании полученной задачи (Auftragstaktik) — тактической концепции и одного из принципов германского оперативного искусства, выработанного на базе боевого опыта европейских войн XIX столетия [6]. В современных реалиях идеи Auftragstaktik соотносятся прежде всего с возросшими темпами ведения боевых действий и требованием молниеносной и адекватной реакции на быстро эволюционирующее оперативное окружение. В наши дни концепция Auftragstaktik заложена в основу доктрины «согласованных наземных боевых действий» (Unified Land Operations), которая была принята ВС США в 2011 году. Мы уже упоминали, что американскими силами в Ираке применялась информационной системы управления войсками нижнего уровня «бригада – батальон – рота», и это принесло успех. По–видимому, быстротечность компании с стиле блицкрига не потребовало подключение системы стратегического планирования. Цитата, принадлежащая самому Мольтке: «На войне нет ничего определенного». Мольтке писал, что для успеха кампании исключительно важно сочетание инициативы и дисциплины. Первая является непреложным правилом для каждого офицера; во многих ситуациях он должен действовать по собственному усмотрению, т. е. самостоятельно принимать решение, исходя из ситуации на местах. Фон Мольтке пришел к выводу, что для достижения целей крайне важно, чтобы замысел высшего руководства понимали на всех уровнях. Фон Мольтке стремился не сдерживать инициативу, а ориентировать ее в нужном направлении. Его решение заключалось не в том, чтобы еще больше контролировать младших офицеров, а в том, чтобы вывести интеллектуальную дисциплину офицеров на новый уровень. Основной особенностью Auftragstaktik является передача командных полномочий от старших звеньев управления в младшие, так как именно они имеют возможность наиболее полно учитывать в своих действиях обстоятельства местной оперативной обстановки. Природа понятия Auftragstaktik с трудом поддаётся формализации, а в современной технической и исторической литературе нет его общепринятого определения. И тем не менее возьмем на себя смелость описать условия реализации принцип самостоятельного командирского решения в концепции рассматриваемой системы управления. 1)    иерархичность, что выстраивает командную вертикаль и обеспечивает необходимую дисциплину. 2)    децентрализованность, что смягчает жесткость командной вертикали и позволяет проявлять инициативу командирскому составу на своем уровне. Система управления поддерживает сквозное циркулирование информации между верхом и низом. На уровне батальонного куста информационное окружение должно содействовать командиру в принятии правильного решения моментального оперативного действия. С повышением уровня куста управления реакция пользователей системы на поступающую информацию смещается в сторону планирования. Результаты планирования спускаются в виде директив и приказов. Речь фактически идет об организации организации, вопросу которому посвящаются солидные исследования, например, [7]. Мы ограничимся только небольшим замечанием, что правильная конфигурация связей кустов системы управления может способствовать росту инициативы младшим командирским составом. Дополним рассмотренную схему на Рис. 2 горизонтальными связями между подчиненными кустами. Линия передачи данных между репозиториями кустов одного уровня окрашены на Рис. 4 в зеленый цвет. Такая горизонтальная связь между горизонтальными кустами позволяет: 2)    укрепить слаженность работы смежных подразделений. Такая простая комбинаторика технических средств без участия командного куста верхнего уровня способна простимулировать повышение инициативности младших командиров. На сегодняшний день имеются не только сторонники применения сетецентрической концепции, но и ее критики. Ряд российских экспертов отвергает подобный путь развития, говоря порой даже о масштабной дезинформации со стороны США. Очень существенной уязвимостью сетецентрической концепции является применение противником средств радиоэлектронного противодействия (РЭП) для вывода из строя линий, сетей связи и передачи данных. Капитан Иванов: — Старлей, почему в шестую роту не подвозят боеприпасы. Патронов осталось на 2 часа боя. Старлей Петров: — У меня нет от 6–ой роты заявки на обеспечение. Вы не подсоединены к интерфейсу МТС127 сетецентрической системы. Капитан Иванов: — Бип-бип-бип …техник Сидоров сыграл в жмурки…бип-бип-бип лежит в кустах… бип-бип-бип Если через час не будет патронов, пойдешь под трибунал, бип-бип-бип Сукин сын. Тыловая крыса. Этот разговор гипотетический, но картинка может стать вполне реальной. Вывод: надо к каждой инновации подходить взвешено. Сетецентрическая система управления войсками может состоять из унифицированных типовых кустов. Интероперабельностью сетецентрической системы можно управлять по отработанной технологии программирования обработки данных в потоке. Вертикальные связи между кустами сетецентрической системы реализуют привычную командную вертикаль управления. Горизонтальные связи между кустами должны способствовать повышению инициативности младших командиров. 1.     Арзуманян Р. Теория и принципы сетецентричных войн и операций// 21-й век. — 2008. 2.     А.Е. Кондратьев, Будущее сетецентрических войн, https://topwar.ru/18666-buduschee-setecentricheskih-voyn.html 3.     С. В. Козлов, С. И. Макаренко, А. Я. Олейников, Д. В. Растягаев, Т. Е. Черницкая, ПРОБЛЕМА ИНТЕРОПЕРАБЕЛЬНОСТИ В СЕТЕЦЕНТРИЧЕСКИХ СИСТЕМАХ УПРАВЛЕНИЯ, ЖУРНАЛ РАДИОЭЛЕКТРОНИКИ, ISSN 1684-1719, N12, 2019, DOI 10.30898/1684-1719.2019.12.4 . 4.     Уэске Ф., Калаври В. Потоковая обработка данных с Apache Flink  – М.: ДМК Пресс, 2021. 5.     Бен Стопфорд, ПРОЕКТИРОВАНИЕ СОБЫТИЙНО–ОРИЕНТИРОВАННЫХ СИСТЕМ, Иркутск: ITSumma Press, 2019. 6.     Gunther M. J. Auftragstaktik: The Basis For Modern Military Command. — Pickle Partners Publishing, 2015. 7.     Бангей, Стивен Искусство действия. Как преодолеть разрыв между планами и их реализацией — М.: Манн, Иванов и Фербер, 2020",
    "22": "Небольшой дисклеймер: перед прочтением данной статьи ознакомьтесь с первой частью, дабы вникнуть в суть происходящего. Желаю вам приятного прочтения :) Сидел я тут на днях и думал, как можно улучшить мой эмулятор \"Intel 4004\" и перечитывая комментарии под первой частью, я осознал одну очень простую вещь - моё творение на 4004-ый не очень то и похоже.. Абсолютно рандомные опкоды, инструкции, которых в данном процессоре отродясь не было, например, инструкции HLT, AND и OR (HLT так вообще появилась только в Intel 4040). Я начал активно смотреть datasheet, стал рассматривать другие проекты по теме 4004-го, особенно мне понравился эмулятор пользователя markablov, написанный на языке JavaScript (именно оттуда впоследствии были взяты необходимые опкоды). В этот раз в эмуляторе используется всего 7 инструкций из 46 возможных (так что полноценным его назвать нельзя, скорее урезанным, в прошлый раз было также). NOP просто увеличивает значение счётчика команд (pc) на 1, что позволяет перейти к следующей инструкции в программе. INC увеличивает значение аккумулятора (acc) на 1, ограничивая его значением до 0-255, и затем увеличивает pc на 1. ISZ увеличивает значение в ячейке памяти с заданным адресом на 1, снова ограничивая его до 0-255. Если значение в ячейке становится равным 0, pc увеличивается на 2, иначе увеличивается на 1. ADD добавляет значение из ячейки памяти с заданным адресом к значению аккумулятора, ограничивает результат до 0-255 и увеличивает pc на 2. SUB вычитает значение из ячейки памяти с заданным адресом из значения аккумулятора, ограничивает результат до 0-255 и увеличивает pc на 2. LD загружает значение из ячейки памяти с заданным адресом в аккумулятор и увеличивает pc на 2. XCH обменивает значение аккумулятора и значение в ячейке памяти с заданным адресом, увеличивает pc на 2. Программу надо составлять прямо в коде, а конкретно в program.py. Вот пример программы, которая отнимает от числа 12 число 5 и прибавляет число 2: На этот раз я учёл ошибки прошлого эмулятора и в этой работе постарался сделать всё максимально верным. Добавить ещё больше инструкций 4004-го. Используя библиотеку tkinter, создать окно, где пользователь вводит программу и ему выводится результат (дабы не приходилось устанавливать сам Python, различные IDE к нему для запуска и теста эмулятора). Полный код можно посмотреть на моём GitHub. С вами был Yura_FX. Спасибо, что дочитали данную статью до конца. Не забывайте делиться своим мнением в комментариях :) Программист десктопных приложений.",
    "23": "Входные технические данные. Коротко. В качестве неплохого эмулятора, не нужно возиться с настройками, можно порекомендовать MAMEUIFX32\\64. Проект 8 лет как закрыт, но это не мешает запустить на нём большинство того, что может субъективно потребоваться. Внести монету - цифра 5, стартовать игру - цифра 1. По умолчанию подтягивается классический геймпад от Xbox 360 или клавиатура, нет нужды переназначать какие-либо клавиши. 80.7 MB file on MEGA - отсюда можно скачать эмулятор и биосы одним архивом (взято из этого видео). Файлы биосов нужно сложить в папку roms, туда же нужно складывать игры, после нажать кнопку \"Refresh\" в эмуляторе, выбрать игру. Исходно запускаться через mameuifx64.exe. В \"Options\" -> \"Default Game Options\" можно слегка подкрутить  графические настройки, например включив Direct3D режим и \"Use widescreen  stretch\". Дампы ROM'ов, т.е. игр, могут быть представлены великим множеством версий, поэтому, чтобы не потеряться, проще всего ориентироваться на те, у которых больше всего скачиваний или на ту, что порекомендует эмулятор как наиболее совместимую, если запустили что-то не то - если выбрана в целом \"нужная\" версия, то иконка игры будет зелёной, а если не совсем, то жёлтой / красной. Кнопки отвечающие за save / load state. По умолчанию Shift + F7 - сохранить, F7 - загрузить. После нажатия комбинации нужно выбрать номер слота, например первый, т.е. клавиша 1. Руководство - Эмулятор Arcade Video Games: MameUIFX v0175.1 | гайд по настройке. Герою необходимо последовательно преодолеть 50 этажей разной степени продолжительности, на каждом из которых нужно найти дверь. По дороге можно собирать ключи и освобождать пленников. Одновременно в рядах союзников может быть только один вызволенный из плена, как итог в процессе предполагается заменять таковых. Ростер \"помощников\" обширен - от войнов и магов до нинзя и гаргульи. Тоже можно сказать и о противниках - различные звери и странные существа, рыцари, маги, кого здесь только нет. Через каждые несколько этажей поджидает босс. Сложных комбинаций ударов нет, но есть возможность собирать временно действующие power up'ы.Упомянутое сопровождено красочной стилистикой, структурой уровней, явно отсылающей к \"Вавилонской башне\", а также моральным выбором в финале - оставить тёмную сферу себе, заняв тем самым место главного злодея, или же избавиться от таковой? Шаблонные happy end'ы мне успел несколько приестся, поэтому, подобно \"Hexen: Beyond Heretic\" (1995), я выбрал первый вариант. Впрочем в Hexen выбора, как такового, не давалось, и последствия нахождения \"сферы хаоса\" были совсем иными. Своеобразная вариация на тему \"Castelvania\" с некоторой примесью \"японской дичи\", выражающейся, помимо дизайна отдельных противников, в том, что весомая часть из них... говорит на немецком. По ходу прохождения можно подбирать временные магические апгрейды оружия, а также встретить колдуна, который может превратить противников в лёд или каменные статуи. В то время как первые две части этой серии считаются довольно сомнительными, третью оценивают как достаточно неплохую, поэтому выбор пал именно на неё. Характерный факт - исходно игра выходила только на территории Японии, с разделённым пополам экраном. Эксклюзивная версия для аркадных автоматов. Пожалуй самый красочный, в контексте графики, проект из всей линейки Golden Axe. В остальном же про него особо сказать нечего - пара ударов, пара новых персонажей, гном здесь обзавёлся помощником в стиле \"Безумный Макс 3: Под куполом грома\" (1985), пробегается одним заходом менее чем за 40 минут, чем-то действительно особенным на фоне аналогов с автоматов не выделяется. Четыре героя на выбор, стилистика представляющая собой что-то вроде комбинации \"Prince of Persia\" (1990) и \"Disney's Alladdin\" (1994), однокнопочный геймплей, комбо нет - я выбрал Синдбада и его атаки в прыжке оказались по существу бесполезными. Имеется незамысловатый сюжет - злой демон превратил султана в обезьяну. Приближённая гвардия оказывается рассержена таким положением вещей и отправляется в путь. Основных фишек у игры две. Первая - антураж восточной сказки, помешанный с необычными сегментами, как то возможность побывать внутри лампы, где по воздуху плывут рыбы, или полетать на ковре-самолёте. Вторая - по мере того как герои будут одолевать того или иного босса они будут переходить на сторону первых, ввиду чего появится возможность призывать их на помощь, подобно исходно доступному джину. Противники сыпятся пачками и если в кооперативе вопрос вероятно нивелировался количеством участников, то здесь в большей степени остаётся налегать на возможности союзников из лампы. Организация под названием \"SKULL\", преимущественно состоящая из злых нинзя, грезит порабощением мира. Пятерым добрым нинзя предстоит бросить им вызов, имея, среди прочего, личный мотив - организация похищает одного из них, видимо предвидя намерения героев. От прочих проектов этот тайтл отличается разнообразием локаций где происходит действо. Помимо типовых сражений в городских джунглях абстрактного японского города доведётся побывать на техногенной базе, под водой, в пещерах, покататься на лыжах в горах, а также поездить на вагонетках. Геймплейно также есть ощутимые отличия. Здесь это не столько beat 'em up, хотя атаки по средствам катаны на ближней дистанции никто не отменял, сколько, по сумме факторов, аналог \"Contra\". Помимо этого, в отличии от более \"типовых\" случаев, игра не столь явно налегает на поглощение жетонов. 5 этапов. 3 героя на выбор. Незамысловатый сюжет, непримечательные локации, отсутствие комбо или каких-либо бонусов. ~ 40 минут на прохождение. Что же этот тайтл в этом случае делает в моём списке? При всей общей невзрачности геймплей и антураж существенно схожи с первыми играми серии \"Double Dragon\", что и послужило его причиной включения в выборку. Мечтали поиграть за барышню в розовых стрингах, лихо раскидывающих врагов направо и налево? Тогда вам сюда. Вполне добротный, двухкнопочный геймплей с неплохо отрисованными задниками прилагается. Приквел довольно спорного рельсового шутера \"Cadillacs and Dinosaurs:  The Second Cataclysm\" (1995). В отличии от второй части является вполне  сносным битемапом. Основывается на одноимённом мультсериале. Очередные \"warriors\", на этот раз механические. Перипетии сюжета вкратце сводятся к следующему - жители Земли ведут войну с враждебно настроенными представителями расы с другой планеты. Дабы преуспеть в конфликте обе стороны сделали ставку на использование мехов. Предоставлен выбор из четырёх штук и такого же количества пилотов соответственно. Уникальной особенностью здесь становятся апгрейды трёх типов - шасси, основное орудие и установка на плечо меха. Так можно установить сочетание шасси, подобное тому, что используют танки, самонаводящиеся ракеты и, например дрель или лазерный меч. В зависимости от вида противников и боссов то или иное оружие может оказаться более или менее эффективным, а где-то будет лучше обойтись исходной конфигурацией. Сочетая одно с другим можно выстраивать простые комбо. Из интересных примеров - в случае упомянутых \"колёс\" мех может сворачиваться в что-то вроде шипастой шестерёнки и таким образом наносить урон в прыжке до тех пор, пока под ним есть \"основа\" в виде вражеских мехов. 10. Denjin Makai (1994). 6 персонажей, 5 этапов, враги и боссы опаснее чем в последующей игре, комбо выполнять сложнее, как итог жетоны тратятся только так. Появились полноценные сюжетные вставки, стиль стал ближе к тому, что игроки привыкли видеть на домашних консолях. 11. Guardians / Denjin Makai II (1995). 5 героев на выбор, 5 этапов, возможность выбора альтернативных развилок, пара видов подбираемого оружия, небольшой набор комбо, выполнить которые не то чтобы просто, поэтому я больше налегал на \"обычные\" удары, стилистика, в целом передающая дух аниме. Приквел Denjin Makai.",
    "24": "В процессе разработки часто приходится использовать словари для получения значения по ключу. Это отлично подходит для маппинга полей различных систем. Например, в одной системе тип документа \"Договор\", а в другой \"Contract\". Либо одна система принимает буквенный код валюты \"RUB\", а другая числовой \"643\". Для того чтобы они понимали друг друга, необходимо переводить значения в понятные для этой системы, и для этого прекрасно подходят словари. Внешне это выглядит просто, и обратный словарь можно собрать при помощи copy-paste из первого словаря. Это хорошо когда мало значений, но вот дошло дело до кодов валют и их словаря в 160 записей. Сразу пришла в голову идея: Был бы такой объект в python, в котором происходит маппинг не зависимо от передаваемого ключа. Передаешь RUB получаешь 643, передаешь 643 получаешь RUB Я подумал об этом и сразу начал искать в интернете что-то подобное. К сожалению, ничего не нашел, но везде рекомендовали просто создать обратный словарь с помощью кода (как я сразу об этом не догадался): И вот, после длительной работы, я представляю вашему вниманию мой класс SupperMapping. Этот класс позволяет осуществлять маппинг в обе стороны, независимо от того, какой ключ был передан. Я постарался подробно описать методы и их предназначение. Это начальный вариант, думаю потом прикрутить еще больше фишек. Буду рад замечаниям и советам. Если будет потребность в этом классе можно попробовать и библиотеку на PIP выложить)))",
    "25": "25 марта 2024 года в рамках программы Bug Bounty Extravaganza исследователь под ником 1337_wannabe обнаружил уязвимость типа SQL-injection в одном из популярных плагинов для WordPress - LayerSlider. Уязвимость получила идентификатор CVE-2024-2879 и балл CVSS равный 9,8 (критический). Данный недостаток хоть и является по большей части SQL-инъекцией типа blind основанной на времени, всё равно позволяет злоумышленнику легко получить конфиденциальную информацию из базы данных. Стоит выделить, что плагин LayerSlider достаточно популярен среди пользователей WordPress и насчитывает более 1 миллиона установок. Из чего можно сделать вывод, что на данный момент остается огромное количество уязвимых сайтов в сети, которые пока не обновились до последней версии, так как автоматические обновления данным плагином не поддерживаются. Для тестирования данной уязвимости мы подняли WordPress версии 6.5 и установили туда уязвимый плагин LayerSlider версии 7.9.11. Чтобы проверить работоспособность плагина, создали из имеющихся шаблонов простой Popup и разместили его на тестовой странице. В исходном коде плагина нас интересует функция ls_get_popup_markup(), находящаяся в файле assets/wp/actions.php. Она является обработчиком HTTP GET запроса /wp-admin/admin-ajax.php?action=ls_get_popup_markup. Из исходного кода видно, что плагин использует данную функцию для получения popup’a по id, где идентификатор может быть указан с помощью GET параметра. И если id не является числом, то он напрямую попадает в функцию find() класса LS_Sliders. В самой функции find(), аргумент проходит несколько проверок. Первая из них - это проверка на тип. Прочитав конструкцию условного ветвления, становится понятно, что поддерживаются несколько вариантов аргумента, но нам интересен только последний, когда аргументом является map. Начиная с 80 строки кода, мы можем наблюдать, как формируется запрос к базе данных. Примечательным местом тут является цикл со 102 строки по 107. Он написан с целью так называемого эскейпа спец символов SQL из пользовательского ввода. В результате такой процедуры все спецсимволы SQL синтаксиса просто экранируются. Но если обратить внимание на условие внутри данного цикла, становится ясно, что это не происходит с элементом стоящим под ключом 'where'. Скорее всего это было сделано с целью поддержки каких-то пользовательских фильтров на языке SQL, так как в коде ниже мы видим, как из этого элемента формируются дополнительные условия запроса. В конце все части аргументов собираются в один запрос и он отправляется базе данных. В итоге получается, что любые специальные символы и слова языка SQL, оставленные пользователем в параметре GET-запроса под ключом 'where', без изменений попадут напрямую в запрос к базе данных, а это означает, что в данном функционале присутствует уязвимость под названием SQL-инъекция. Остаётся отметить, что из-за специфики того, как формируется данный запрос и какой ответ доходит до пользователя, здесь не получится реализовать UNION-based инъекцию и получить данные напрямую. Но это не исключает возможность воспользоваться time-based методом слепой SQL-инъекции. Опираясь на информацию полученную из разбора выше, можно составить подобный HTTP-запрос, где в URI, в параметре id, будет находиться наш пэйлоад. Обратите внимание как задается параметр id. Таким образом PHP поймет, что это тип данных array, а точнее map. Для эксплуатации слепой SQL инъекции типа time-based мы будем использовать автоматизированный инструмент SQLmap. В качестве аргументов укажем сформированный URI, а также параметры --level=3 и --risk=2, чтобы SQLmap проверил большее количество пэйлоадов. Для демонстрации мы получали такие данные как имя текущего пользователя и имя хоста, для этого следует указать параметры --current-user и --hostname. Из результата работы инструмента, помимо данных, можно увидеть, что он определил тип SQL-инъекции как time-based blind и использовал SLEEP в своих пэйлоадах. К счастью, эта уязвимость уже устранена в версии 7.10.1 LayerSlider, и всем пользователям данного плагина настоятельно рекомендуется обновиться до последней версии. Если обновление невозможно, тогда есть шанс, что удастся снизить некоторые риски с помощью WAF, который должен быть настроен на блокирование попыток внедрения SQL-кода, хотя на это не следует полагаться. В данной статье мы детально разобрали уязвимость в плагине LayerSlider для WordPress, позволяющую выполнять произвольные запросы к базе данных и, как следствие, получать любую чувствительную информацию из нее.",
    "26": "Разработка игры началась в далёком 2014 году с изучением Unity. Чтобы выучить новую двиг или язык программирования я обычно делаю какой-нибудь проект на этой технологии, в моём случае это работает лучше чем зубрёжка. Таким проектом стал Monument - олдскульный шутер с видом от первого лица в стиле Doom, Quake и подобных игр (сказалась любовь к этому жанру и старым играм из детства). Т.к. опыта в написании таких больших проектов не было, за основу взял (купил, серьёзно) \"Fps Prefab\" в ассет-сторе и понеслась. Пару месяцев создавалась механика игры: подбор аптечек, снаряжения, открытие дверей ключами, загрузка/сохранение и т.д.. Всех монстров и оружие делал сам в Zbrush и 3dmax впервые, собственно всё видно по качеству. Анимациями занимался также самостоятельно, за которые мне по сей день стыдно. Хотя многие игроки даже сейчас говорят, что старые пушки были лучше. Тут в разработку пошли уровни. После создания первых двух локаций, 5 января 2015 года (нет чтобы отдыхать) была создана тема на gamedev.ru (https://gamedev.ru/projects/forum/?id=196847). Только сейчас нулевой пост изменён под новую версию. В теме была доступна демка с двумя первыми уровнями. Игра засветилась в самом первом обзоре инди игр у DUCAT'a (https://www.youtube.com/channel/UCMr6jfAPZnNORCRp__S-tLA), который обозревал игры исключительно с gamedev.ru. Также игра появилась в последнем обзоре, только уже в новой версии В этот момент было решено постить игру в Steam Greenlight, собственно что я и сделал. После публикации процесс набора голосов шёл очень медленно. Я пытался продвигать игру как мог, на сколько мне хватало \"знаний\" в маркетинге: заказывал рекламу в vk, писал посты на разных форумах, делал web демку для playgrounds и других мелких порталов. В итоге, ближе к лету 2015 года игра прошла Greenlight и началась активная стадия её допиливания, т.к. за несколько месяцев до этого уже опускались руки. Релиз игры состоялся 6 июня 2015 года. Чуда не случилось. Никаких миллионов, блекджеков и мадмуазелей. А ещё в это время летняя распродажа вроде была (рукалицо), но это не точно. Игру получалось продавать только на распродажах, с большими скидками. Покупали её, что понятно, только из-за карточек. После каждой такой распродажи негативные отзывы увеличивались в больших объёмах, т.к. игра была плохой, несмотря на то, что даже у этой кривой версии были свои фанаты. Monument был очень медленным, забагованным, неитересным и некрасивым. Игра лагала даже на мощных компьютерах. Было ужасно всё. Мучался я с распродажами ещё год, немного обновлял игру и после окончательно приуныл и забил на неё, перейдя с новым опытом уже к другим проектам. Отзывы к этому моменту были уже отрицательными. К началу лета 2019 года, поднабрав ещё опыта, было решено переписать игру с нуля, т.к. за первую версию было попросту стыдно. Отброшено было всё, кроме моделек монстров. Уровни стали интереснее, более похожими по геймплею на старые игры. Внешний вид оружий был изменён. Добавил новую \"Пушку-Телепорт\", с помощью которой можно телепортироваться в другие части уровня. Добавлена \"Донор-Станция\", в которой можно получить боезапас в обмен на своё здоровье. Перемещение по уровням стало очень быстрым, благодаря добавлению \"Стрейфджампа\" как в Quake. Все эти глобальные изменения были выложены в стим 24 января 2020г., спустя почти 5 лет после релиза. Многие были шокированы, т.к. мало кто из \"индюков\" так глобально обновляет свои проекты в Steam. После релиза обновления ситуация с обзорами стала меняться, игра стала нравиться многим, но и проблем не убавилось. Было куча багов, недоработок и прочих прелестей, из-за которых страдала статистика отзывов на странице. На игру чаще делали обзоры \"ютуберы\" и кол-во покупок за фулл прайс увеличилось. Я пытался в маркетинг, слил около 1000$ (я понимаю, это смешно), эффект чуть больше чем ничего. В ход шли рекламные сети и ютуберы. Спустя ещё год, 25 апреля 2021 года я выпускаю новый апдейт с изменениями, которые собрал из отзывов и тем на форуме игры. Все эти проблемы были решены окончательно. Также с этим обновлением добавил новый уровень в игру, где можно играть с \"пушкой-телепорт\". Каждому монстру была добавлена дополнительная атака, чтобы геймплей стал разнообразнее. Зелёные свечки теперь после смерти оставляют ядовитый дым, который дамажит игрока. Курицы-качки после смерти спавнят 2х таких же куриц, только маленьких. Хатчеры теперь могут периодически пускать огненный спрей. Большие курицы-качки стали бросать в игрока огромные фаерболы на определённом расстоянии, а пришелец может телепортироваться в разные места, но чаще за спину (да, подло, знаю). У всех противников изменились анимации (наконец-то). Это основные изменения, на самом деле их больше и все они перечислены на странице игры. Такое обновление, скорее всего, последнее, но это не точно:). В следующем посте я покажу ранние скетчи и наброски, очень много разного материала осталось.",
    "27": "Микросервис — это подход к разбиению большого монолитного приложения на отдельные приложения, специализирующиеся на конкретной услуге/функции. Этот подход часто называют сервис-ориентированной архитектурой или SOA. В монолитной архитектуре каждая бизнес-логика находится в одном приложении. Службы приложений, такие как управление пользователями, аутентификация и другие функции, используют одну и ту же базу данных. В микросервисной архитектуре приложение разбивается на несколько отдельных служб, которые выполняются в отдельных процессах. Существует другая база данных для разных функций приложения, и службы взаимодействуют друг с другом с использованием HTTP, AMQP или двоичного протокола, такого как TCP, в зависимости от характера каждой службы. Межсервисное взаимодействие также может осуществляться с использованием очередей сообщений, таких как RabbitMQ , Kafka или Redis . Микросервисная архитектура имеет множество преимуществ. Некоторые из этих преимуществ: Слабосвязанное приложение означает, что различные сервисы могут быть созданы с использованием технологий, которые им подходят лучше всего. Таким образом, команда разработчиков не ограничена выбором, сделанным при запуске проекта. Так как сервисы отвечают за конкретный функционал, что упрощает понимание и контроль над приложением. Масштабирование приложений также становится проще, поскольку если один из сервисов требует высокой загрузки графического процессора, то только сервер, на котором этот сервис должен иметь высокий графический процессор, а другие могут работать на обычном сервере. Микросервисная архитектура — это не панацея, которая решит все ваши проблемы. У нее есть и свои недостатки. Некоторые из этих недостатков: Поскольку разные службы используют разные базы данных, транзакции, включающие более одной службы, должны использовать итоговую согласованность. Идеального разделения услуг очень сложно добиться с первой попытки, и это необходимо повторить, прежде чем добиться наилучшего разделения услуг. Поскольку службы взаимодействуют друг с другом посредством сетевого взаимодействия, это замедляет работу приложения из-за задержки в сети и медленного обслуживания. Python — идеальный инструмент для создания микросервисов, поскольку у него отличное сообщество, простота обучения и множество библиотек. FastAPI — это современная высокопроизводительная веб-инфраструктура, которая обладает множеством интересных функций, таких как автоматическое документирование на основе OpenAPI и встроенная библиотека сериализации и проверки. Здесь вы найдете список всех интересных функций FastAPI. 100% тип аннотирован, поэтому автодополнение работает отлично. Перед установкой FastAPI создайте новый каталог movie_serviceи создайте новую виртуальную среду внутри вновь созданного каталога, используя virtualenv .Если вы еще не установили virtualenv: Теперь создайте новую виртуальную среду. Поскольку FastAPI не имеет встроенного сервиса, uvicornдля его запуска вам необходимо установить его. uvicorn— это сервер ASGI , который позволяет нам использовать функции async/await.Установить uvicornс помощью команды Прежде чем приступить к созданию микросервиса с использованием FastAPI, давайте изучим основы FastAPI. Создайте новый каталог appи новый файл main.pyвнутри вновь созданного каталога. Добавьте следующий код в main.py. Здесь вы сначала импортируете и создаете экземпляр FastAPI, а затем регистрируете корневую конечную точку /, которая затем возвращает файл JSON. Вы можете запустить сервер приложений, используя uvicorn app.main:app --reload. Здесь app.mainуказывается, что вы используете main.pyфайл внутри appкаталога, и :appуказывается имя нашего FastAPIэкземпляра. Вы можете получить доступ к приложению по адресу http://127.0.0.1:8000 . Чтобы получить доступ к интересной автоматической документации, перейдите по адресу http://127.0.0.1:8000/docs . Вы можете экспериментировать и взаимодействовать со своим API из самого браузера. Давайте добавим в наше приложение некоторые функции CRUD.Обновите свой файл main.py, чтобы он выглядел следующим образом: Как видите, вы создали новый класс Movie, который является продолжением BaseModelpydantic.Модель Movieсодержит название, фото, жанры и актерский состав. В состав Pydantic встроен FastAPI, что упрощает создание моделей и проверку запросов. Если вы перейдете на сайт документации, вы увидите, что поля нашей модели Movies уже упоминались в разделе примера ответа. Это возможно, потому что вы указали response_modelв нашем определении маршрута. Теперь давайте добавим конечную точку, чтобы добавить фильм в наш список фильмов. Добавьте новое определение конечной точки для обработки POSTзапроса. Теперь зайдите в браузер и протестируйте новый API. Попробуйте добавить фильм с недопустимым полем или без обязательных полей и убедитесь, что проверка автоматически выполняется FastAPI. Давайте добавим новую конечную точку для обновления фильма. Вот idиндекс нашего fake_movie_dbсписка. Примечание. Не забудьте импортировать HTTPExceptionизfastapi Теперь вы также можете добавить конечную точку для удаления фильма. Прежде чем двигаться дальше, давайте лучше структурируем наше приложение. apiСоздайте внутри новую папку appи создайте новый файл movies.pyвнутри недавно созданной папки. Переместите все коды, связанные с маршрутами, из main.pyв movies.py. Итак, movies.pyдолжно выглядеть следующим образом: Здесь вы зарегистрировали новый маршрут API, используя APIRouter из FastAPI. Кроме того, создайте новый файл , models.pyв apiкотором вы будете хранить наши модели Pydantic. Теперь зарегистрируйте этот новый файл маршрутов вmain.py Прежде чем двигаться дальше, убедитесь, что ваше приложение работает правильно. Раньше вы использовали поддельный список Python для добавления фильмов, но теперь вы, наконец, готовы использовать для этой цели реальную базу данных. Для этой цели вы собираетесь использовать PostgreSQL . Установите PostgreSQL, если вы еще этого не сделали. После установки PostgreSQl создайте новую базу данных, я назову свою movie_db. Вы собираетесь использовать кодировку/базы данных для подключения к базе данных asyncи awaitее поддержки. Узнайте больше о async/awaitPython здесь при этом будут установлены sqlalchemyи asyncpgнеобходимые для работы с PostgreSQL. Создайте внутри новый файл apiи назовите его db.py. Этот файл будет содержать фактическую модель базы данных для нашего REST API. Вот DATABASE_URIURL-адрес, используемый для подключения к базе данных PostgreSQL. Здесь movie_userуказано имя пользователя базы данных, movie_passwordпароль пользователя базы данных и movie_dbимя базы данных. Точно так же, как в SQLAlchemy, вы создали таблицу для базы данных фильмов. Обновите main.pyдля подключения к базе данных. main.pyдолжно выглядеть следующим образом: FastAPI предоставляет некоторые обработчики событий, которые вы можете использовать для подключения к нашей базе данных при запуске приложения и отключения при его завершении. Обновите movies.py, чтобы он использовал базу данных вместо поддельного списка Python. Давайте добавим db_manager.pyвозможность манипулировать нашей базой данных. Давайте обновим нашу систему models.py, чтобы вы могли использовать модель Pydantic с таблицей sqlalchemy. Вот MovieInбазовая модель, которую вы используете для добавления фильма в базу данных. Вам нужно добавить idк этой модели, получая ее из базы данных, следовательно, и модель MovieOut. MovieUpdateМодель позволяет нам сделать значения в модели необязательными, чтобы при обновлении фильма можно было отправлять только то поле, которое необходимо обновить. Теперь перейдите на сайт документации браузера и начните экспериментировать с API. Управление данными в микросервисе — один из наиболее сложных аспектов создания микросервиса. Поскольку разные функции приложения выполняются разными службами, использование базы данных может оказаться затруднительным. Вот несколько шаблонов, которые можно использовать для управления потоком данных в приложении. Использование базы данных для каждого сервиса отлично подходит, если вы хотите, чтобы ваши микросервисы были как можно более слабо связанными. Наличие отдельной базы данных для каждого сервиса позволяет нам независимо масштабировать разные сервисы. Транзакция с участием нескольких баз данных выполняется через четко определенные API. Это имеет свой недостаток, поскольку реализация бизнес-транзакций, включающих несколько сервисов, не является простой задачей. Кроме того, дополнительные сетевые издержки делают его менее эффективным в использовании. Если есть много транзакций с участием нескольких сервисов, лучше использовать общую базу данных. Это дает преимущества высокосогласованного приложения, но лишает большинства преимуществ микросервисной архитектуры. Разработчикам, работающим над одним сервисом, необходимо координировать изменения схемы в других сервисах. В транзакциях с участием нескольких баз данных композитор API действует как шлюз API и выполняет вызовы API к другим микросервисам в необходимом порядке. Наконец, результаты каждого микросервиса возвращаются клиентской службе после выполнения соединения в памяти. Недостатком этого подхода является неэффективное объединение больших наборов данных в памяти. Трудности развертывания микросервиса можно значительно уменьшить, используя Docker. Docker помогает инкапсулировать каждую службу и масштабировать ее независимо. Если вы еще не установили docker в свою систему. Убедитесь, что докер установлен, выполнив команду docker. После завершения установки Docker установите Docker Compose . Docker Compose используется для определения и запуска нескольких контейнеров Docker. Это также помогает облегчить взаимодействие между ними. Поскольку большая часть работы по созданию сервиса фильмов уже проделана при начале работы с FastAPI, вам придется повторно использовать уже написанный код. Создайте новую папку, я назову свою python-microservices. Переместите код, который вы написали ранее и который я назвал movie-service.Итак, структура папок будет выглядеть так: Прежде всего, давайте создадим requirements.txtфайл, в котором вы будете хранить все зависимости, которые вы собираетесь использовать в нашем movie-service. Создайте внутриновый файл и добавьте в него следующее:requirements.txtmovie-service Вы использовали все упомянутые там библиотеки, кроме httpx , который вы собираетесь использовать при выполнении вызова API между службами. Здесь сначала вы определяете, какую версию Python вы хотите использовать. Затем установите WORKDIRпапку appвнутри контейнера Docker. После этого gccустанавливается то, что требуется библиотекам, которые вы используете в приложении.Наконец, установите все зависимости requirements.txtи скопируйте все файлы внутри movie-service/app. Обновить db.pyи заменить Примечание. Не забудьте импортировать osв начало файла. Вам нужно сделать это, чтобы вы могли позже предоставить его DATABASE_URIв качестве переменной среды. Также обновите main.pyи замените Здесь вы добавили prefix /api/v1/moviesтак, что управление разными версиями API становится проще. Кроме того, теги упрощают поиск API-интерфейсов moviesв документации FastAPI. Кроме того, вам необходимо обновить наши модели, чтобы в них castsсохранялся идентификатор актера, а не фактическое имя. Итак, обновите файл, models.pyчтобы он выглядел так: Аналогично нужно обновить таблицы базы данных, давайте обновим db.py: Теперь обновите, movies.pyчтобы проверить, присутствует ли актерский состав с данным идентификатором в службе кастинга, прежде чем добавлять новый фильм или обновлять фильм. Вы делаете вызов API, чтобы получить приведение с заданным идентификатором, и возвращаете true, если приведение существует, и false в противном случае. Как и в случае с файлом movie-service, для создания casts-serviceвы будете использовать базу данных FastAPI и PostgreSQL. Добавьте следующее в requirements.txt: main.py Вы добавили префикс, /api/v1/castsчтобы управление API стало проще. Кроме того, добавление упрощает tagsпоиск документов, связанных с castsдокументами FastAPI. casts.py db_manager.py db.py models.py Чтобы запустить микросервисы, создайте docker-compose.ymlфайл и добавьте в него следующее: Здесь у вас есть 4 разных сервиса: movie_service, база данных для Movie_service, cast_service и база данных для сервиса Cast. Вы открыли movie_serviceпорт 8001аналогично cast_serviceпорту 8002. Для базы данных вы использовали тома, чтобы данные не уничтожались при выключении Docker-контейнера. Это создает образ докера, если он еще не существует, и запускает его. Перейдите по адресу http://localhost:8002/docs , чтобы добавить приведение в службе приведения. Аналогично, http://localhost:8001/docs , чтобы добавить фильм в службу фильмов. Вы развернули микросервисы с помощью Docker Compose, но есть одна небольшая проблема. Доступ к каждому из микросервисов должен осуществляться через отдельный порт. Вы можете решить эту проблему, используя обратный прокси-сервер Nginx. Используя Nginx, вы можете направить запрос, добавив промежуточное программное обеспечение, которое направляет наши запросы к различным службам на основе URL-адреса API. nginx_config.confДобавьте внутрь новый файл python-microservicesсо следующим содержимым. Теперь вам нужно добавить службу nginx в наш docker-compose-yml. Добавьте следующую услугу после cast_dbслужбы: Теперь вы можете получить доступ как к сервису фильмов, так и к сервису трансляции через порт 8080.Перейдите по адресу http://localhost:8080/api/v1/movies/, чтобы получить список фильмов. Теперь вам может быть интересно, как получить доступ к документации служб. Для этого обновите main.pyсервис фильмов и замените openapi.jsonЗдесь вы изменили конечную точку и откуда обслуживаются документы .",
    "28": "Обзор недооцененных UI компонентов, которые могут значительно улучшить взаимодействие пользователя с продуктом. В мире интерфейсного дизайна всегда есть место инновациям и нестандартным решениям. Некоторые UI элементы, хотя и не получают широкого распространения, могут значительно улучшить взаимодействие пользователя с продуктом. В этой статье мы рассмотрим 10 таких недооцененных элементов интерфейса. Микроинтеракции не только придают сайту живость, но и могут значительно улучшить пользовательский опыт, особенно в формах обратной связи. Например, анимация, подтверждающая успешный ввод данных или предупреждающая об ошибке, может сделать процесс заполнения формы более интуитивным. Тултипы, предлагающие контекстную информацию при наведении курсора или тапе, часто используются в интерфейсах. Но персонализация этих подсказок на основе поведения пользователя может сделать их еще более полезными и повысить взаимодействие. Для форм, требующих заполнения большого количества данных, индикаторы прогресса могут значительно улучшить пользовательский опыт, показывая, сколько осталось сделать и мотивируя пользователя завершить задачу. Гибкие фильтры позволяют пользователям настраивать отображаемый контент по своим предпочтениям, улучшая взаимодействие и удовлетворенность от использования продукта. Эти фильтры особенно полезны в приложениях с большим объемом контента. Ленивая загрузка (lazy loading) — это техника, при которой контент загружается по мере прокрутки страницы, что улучшает производительность. Добавление анимации к лениво загружаемым элементам может сделать ожидание более приятным для пользователя. Хотя плавающие кнопки действия уже используются в мобильном дизайне, их потенциал на десктопных версиях сайтов часто недооценивается. Они могут обеспечить быстрый доступ к основным функциям без необходимости прокрутки или перехода на другую страницу. Кастомные скроллбары могут не только улучшить внешний вид сайта, но и сделать прокрутку более интуитивной и удобной для пользователя, особенно на сайтах с большим объемом контента. Иллюстрации, реагирующие на действия пользователя, могут улучшить вовлеченность и сделать пользовательский опыт более запоминающимся. Например, анимация, активируемая при наведении или клике, добавляет элемент взаимодействия. Использование свайп-жестов для навигации по каруселям изображений или контента делает интерфейс более дружелюбным для пользователя, особенно на сенсорных устройствах. Коллапсируемые (скрывающиеся) секции контента позволяют сделать большие объемы информации более управляемыми, предоставляя пользователям возможность самостоятельно выбирать, какую часть информации отобразить. Интеграция этих элементов интерфейса, может существенно улучшить пользовательский опыт, делая ваш сайт или приложение не только более функциональным, но и более интерактивным и пользовательски ориентированным. Эти малоизвестные UI компоненты могут стать мощным инструментом в руках дизайнера, помогая выделить продукт на фоне конкурентов и повысить удовлетворенность пользователей.",
    "29": "На сегодняшний день даже система мониторинга в тепличных комплексах - это не дешёвое удовольствие. Мы протестировали относительно недорогой комплект, установили по 3шт промышленных датчика (modbus rtu) температуры +влажность в каждую 100метровую теплицу, которых было 9шт, собрали все данные по проводной шине (Utp5e RS485) около 900метров без каких-либо промежуточных коммутаторов и усложнения проекта. В промежутке поставили компьютер AntexGate. На главный экран выводится текущая температура и влажность по каждому прибору, в случае обрыва или отсутствия датчика выводится значение ERR, а для выбора страницы реализован всплывающий экран. За каждый день можно посмотреть данные на графике и проанализировать отношение температуры и влажности (синяя линяя - влажность, белая - температура), график строится как за отдельный датчик, так и средние значения в теплице. Для более подробного анализа возможно скачать архив в формате csv и построить аналитику внешними средствами, например, за целый сезон. Страница настроек для оповещения и привязки Teleram бота. Когда достигнут критический предел из всех устройств (телефоны, ПК) с открытым Web - интерфейсом, либо из подключенного монитора происходит голосовое оповещение о низкой, или высокой температуре определенной теплицы и конкретного датчика. Планируем расширять проект, т.к. в локальной wifi сети у Всех есть доступ к текущим значениям, но хочется посмотреть значения удаленно, тут мы планируем использовать Умный дом Яндекс, бесплатное приложение для мониторинга отлично подойдёт еще и строит график. Примерно такой дружелюбный интерфейс у Яндекса, вдобавок на производстве можно раскидать колонок и спрашивать о состоянии системы. В заключение хотелось бы сказать, что реализация данной системы подходит для конкретного заказчика и большое преимущество свободно программируемых систем AntexGate + Node-red в том, что ее можно доработать в дальнейшем. Можно добавить стыковку с другими системами, весами, поливом, приводами, климатическими системами и другим. Ниже наш прибор на котором реализован проект, система отработала более года. Подробнее об устройстве на котором реализован проект можно почитать в нашем Телеграмм-канале и задать вопросы. Железо можно брать на тестирование ЮР лицам и ИП. Для физ лиц есть скидка на железо до 50%",
    "30": "Статья будет разбита на несколько частей, чтобы более подробно описать все аспекты данной темы. В этой части я расскажу о проблеме и подходе к её решению. Когда я читал анонс Яндекс Станции Миди, обрадовался наличию внутри этой крохи Zigbee модуля и улучшению скорости обработки команд управления умным домом в несколько раз, однако подробнее узнал, что ускорение обработки касалось только zigbee устройств, которые у меня подключены уже к умному дому на базе Home Assistant. Идея пришла спонтанно, но получилось придумать, как заставить Алису подавать команды в Home Assistant и организовать связь уровня «координатор‑координатор». Zigbee в умном доме \"из коробки\" не подразумевает подключения формата \"координатор-координатор\", да и в целом это звучит не совсем логично, однако поскольку станция Миди и умный дом являлись координаторами, необходимо было сделать дополнительный узел в сети, который бы позволил одному из двух думать, что он общается с конечным устройством. Не смотря на простое решение в виду прослойки, у нас есть бревно в колесе нашей машины революции в управлении умным домом - конечное устройство может быть подключено только к одной Zigbee сети. Поскольку два устройства у нас могут только отдавать команды конечным устройствам, значит решение задачи стоит довольно просто — нужно сделать прослойку, которая будет говорить в обе стороны, что оно — конечное устройство и ретранслировать команды от одного координатора другому, принимая команду от одного и обновляя состояние для другого. Решением данной задачки является ESP32 контроллер с поддержкой Zigbee. Однако может возникнуть закономерный вопрос — как передавать и команду и изменение состояния двум координаторам, если конечное устройство может быть связано только с одной сетью? Ответ прост — никак. (занавес) На самом деле именно поэтому и используется ESP32 контроллер. Мы принимаем команду от станции через Zigbee, а уже далее для Home Assistant мы передаем информацию об изменении состояния через USB. Бинго, но как? Как вы могли заметить, в решении этой задачи я использовал плату ESP32-C6, которая имеет в себе поддержку Zigbee 3.0, что даст нам возможность подключаться к станции Миди. Переходим от теории к практике. Купили плату, разобрались что с ней делать, но как заставить Home Assistant принимать изменения состояния? И снова Бинго - прослойка в виде интеграции. К сожалению, в интернете довольно мало информации о том, как разрабатывать свои интеграции для Home Assistant, поэтому я хотел бы вынести этот блок в отдельную статью, чтобы показать на примере этой интеграции как разрабатывать, устанавливать и выкладывать в магазин интеграций свои разработки. Поэтому на данный момент обойдемся скриншотами. Я написал небольшую интеграцию с веб интерфейсом, в которой мы связываем канал Zigbee девайса с девайсом в Home Assistant. К сожалению на данный момент ссылки на репозиторий нет, поскольку интеграция еще не готова до стабильной версии. Ищем среди USB устройств нашу ESP32-C6, подключаемся по серийному порту и передаем список каналов, которые нужно слушать. При изменении состояния со стороны станции, ESP передает по USB JSON массив формата {'c':10,'st':0}, что означает 10 канал получил состояние выключен. Через API Home Assistant отправляем соответствующему каналу устройству on\\off событие в зависимости от полученного статуса. Интеграция читает с серийного порта платы информацию о состоянии устройств. Как именно это реализовано, давайте разбираться. Поскольку корневая задача состоит в том, чтобы станция могла изменять состояние чего‑либо, это самое «чего‑либо» должно быть поддерживаемым самой станцией. Что у нас умеет включаться и выключаться, иметь настраиваемые параметры (яркость, цвет) и так же хранить об этом информацию? Верно — лампочка! К тому же, согласно документации Яндекс, этот тип устройства самый «богатый» по возможным атрибутам. Но если у нас одна плата, как мы будем управлять несколькими устройствами? Здесь нам помогает реализация самого Zigbee протокола. Каждое физическое устройство может иметь в себе несколько составных устройств (например датчик влажности и температуры может содержать в себе и датчик освещения), и каждое такое устройство инициализируется отдельно, представляя в итоге несколько независимых устройств для координатора. Мы как раз и будем использовать данную «фичу» протокола, создавая выключатель на каждый из заданных каналов (по классификации Zigbee это называется Endpoint). Для реализации данного «обмана» станции я брал за основу прошивку на базе фреймворка ESP‑IDF, что даст нам возможность гибко сконфигурировать плату под наши задачи. Принцип работы прошивки выглядит следующим образом: Плата при включении сразу встает на ожидании ввода списка каналов с серийного порта, как только наша интеграция передала список, происходит инициализация устройства light на каждый из заданных каналов (эндпоинтов), после чего наша плата ESP подключается к сети станции. Теперь, если мы включим поиск устройств на станции, мы увидим столько «лампочек», сколько каналов мы задали в интеграции. Чтобы знать на какой канал что назначать в интеграции добавлено отображение статуса канала, поэтому мы просто по порядку включаем каждую из наших «ламп» и смотрим в интеграции, какой канал включился. В следующей части мы разберем, как создавать свою интеграцию для Home Assistant с нуля до установки в систему, а так же рассмотрим реализацию прошивки на ESP-IDF. Это моя первая публикация, поэтому просьба кидаться мягкими тапками, желательно не в лицо. Так же хотел бы рассказать, что у меня были планы о создании коммерческого «свистка» для реализации подхода Plug'n'Play под ту аудиторию, которой не хотелось бы «возиться», а просто подключить и пользоваться. Если вам будет интересно данное устройство, дайте знать в комментариях, расскажу о нём подробнее.",
    "31": "В этом разделе вы узнаете, как рисовать линии, дуги и диаграммы  с помощью Path и встроенных форм, таких как Circle и RoundedRectangle, в SwiftUI. Вот что мы изучим: Мы собираемся нарисовать множество различных фигур и таким образом научимся использовать ключевую в этом вопросе структуру Path и протокол Shape. В SwiftUI вы рисуете линии и формы с помощью Path. Если обратиться к документации Apple, Path - это структура, содержащая контур 2D-формы. Основная идея заключается в установке начальной точки, а затем рисовании линий от точки к точке. Давайте рассмотрим пример. Возьмем фигуру 2 и пошагово рассмотрим, как рисуется этот прямоугольник. В приведенном выше коде мы использовали Path для рисования прямоугольника и заполнили его зеленым цветом. Мы переместились в начальную точку (20, 20), нарисовали линии от точки к точке, чтобы создать форму прямоугольника, и закрыли контур с помощью path.closeSubpath(). Затем мы использовали модификатор .fill, чтобы задать зеленый цвет для заполнения области. Это простой пример использования Path для рисования векторных изображений в SwiftUI. Вы можете использовать Path для создания более сложных форм и заполнения их любым цветом или градиентом. В SwiftUI вы можете использовать Path для рисования линий, а не только заполнять области цветом. Для этого вы можете использовать модификатор .stroke, который позволяет установить ширину и цвет линии. Например, вы можете нарисовать контур прямоугольника, используя Path, и установить ширину и цвет линии с помощью модификатора .stroke. В приведенном выше коде мы использовали Path для рисования прямоугольника и заполнили его зеленым цветом. Если вы хотите просто нарисовать линии, вы можете удалить модификатор .fill и добавить модификатор .stroke, указав ширину и цвет линии. Например: В SwiftUI вы можете использовать Path для рисования сложных форм, таких как кривые и дуги. Для этого вы можете использовать методы addQuadCurve, addCurve и addArc, которые позволяют создавать кривые и дуги. Например, вы можете нарисовать купол на вершине прямоугольника, используя Path и метод addArc. В приведенном выше коде мы использовали Path для рисования прямоугольника и контура. Если вы хотите добавить купол на вершину прямоугольника, вы можете использовать метод addArc, чтобы создать дугу, соединяющую две верхние точки прямоугольника. Например: В SwiftUI вы можете использовать метод addQuadCurve для рисования кривых, определяя якорные и контрольные точки. Якорные точки определяют начало и конец кривой, а контрольная точка определяет форму кривой. Например, вы можете нарисовать угол оттягивающийся не с середины линии, используя Path и метод addQuadCurve, определив якорные и контрольные точки. И наконец метод addCurve позволяет рисовать более сложные кривые, определяя несколько контрольных точек. В отличие от метода addQuadCurve, который использует одну контрольную точку, метод addCurve использует три или более контрольных точек для создания кривой. Например, вы можете нарисовать волнообразную линию, используя Path и метод addCurve, определив несколько контрольных точек. В приведенном ниже коде мы использовали Path для рисования волнообразной линии, определив три контрольных точки: Что если вы хотите нарисовать границу фигуры и заполнить фигуру цветом одновременно? Модификаторы fill и stroke не могут быть использованы параллельно. Вы можете воспользоваться ZStack, чтобы добиться желаемого эффекта. Вот код: В SwiftUI вы можете использовать ZStack для наложения одного объекта Path на другой, чтобы рисовать границу фигуры и заполнять фигуру цветом одновременно. В приведенном выше коде мы создали два объекта Path с одинаковым путем и наложили один на другой с помощью ZStack. Нижний объект Path использует модификатор fill, чтобы заполнить прямоугольник с куполом фиолетовым цветом. Верхний объект Path использует модификатор stroke, чтобы нарисовать только границы черным цветом. Это простой пример использования ZStack для рисования границы фигуры и заполнения фигуры цветом одновременно в SwiftUI. Вы можете использовать Path для создания более сложных форм и использовать ZStack для наложения одного объекта Path на другой, чтобы рисовать границу фигуры и заполнять фигуру цветом одновременно. SwiftUI предоставляет удобный API для разработчиков, чтобы рисовать дуги. Этот API очень полезен для создания различных фигур и объектов, включая круговые диаграммы. Чтобы нарисовать дугу, вы пишете код следующим образом: В данном коде мы сначала перемещаемся в точку (200, 200) с помощью метода move(to:). Затем мы добавляем дугу с помощью метода addArc(center:radius:startAngle:endAngle:clockwise:). В качестве параметров мы передаем центр дуги, радиус, начальный и конечный углы, а также направление рисования дуги (по или против часовой стрелки). В данном случае мы рисуем дугу от 0 до 90 градусов по часовой стрелке и заполняем ее зеленым цветом с помощью модификатора .fill(.green). С помощью функции addArc вы можете легко создать диаграмму кругового графика с разноцветными сегментами. Для этого достаточно наложить друг на друга разные сегменты круговой диаграммы с помощью ZStack. Каждый сегмент имеет разные значения для startAngle и endAngle, чтобы составить диаграмму. Вот пример: Этот код создает круговую диаграмму из четырех сегментов. Каждый сегмент задается с помощью функции `Path`, которая создает путь, состоящий из одной дуги. Затем к каждому пути применяется функция `fill`, которая закрашивает путь определенным цветом. Центр круга задается с помощью `CGPoint(x: 187, y: 187)`, что соответствует центру экрана в этом случае. Каждый сегмент задается с помощью функции `addArc`, которая принимает параметры `center`, `radius`, `startAngle` и `endAngle`. Здесь радиус равен 187, а углы задаются в градусах. Первый сегмент закрашивается желтым цветом и занимает 190 градусов от 0 до 190. Второй сегмент закрашивается голубым цветом и занимает 80 градусов от 190 до 110. Третий сегмент закрашивается синим цветом и занимает 20 градусов от 110 до 90. Четвертый сегмент закрашивается фиолетовым цветом и занимает 270 градусов от 90 до 360. В итоге получается круговая диаграмма, в которой каждый сегмент занимает определенный процент от общего круга. В некоторых случаях вам может потребоваться выделить определенный сегмент, отделив его от круговой диаграммы. Например, чтобы выделить сегмент фиолетового цвета, вы можете применить модификатор offset, чтобы сместить сегмент: В этом примере сегмент фиолетового цвета смещается на 20 пикселей по оси X и на 20 пикселей по оси Y, чтобы выделить его из круговой диаграммы. Этот эффект может быть полезен, когда вы хотите обратить внимание на определенный сегмент или предоставить дополнительную информацию о нем. Опционально вы можете добавить границу, чтобы еще больше привлечь внимание людей. Если вы хотите добавить метку к выделенному сегменту, вы также можете наложить текстовое представление следующим образом: Прежде чем изучать протокол Shape, давайте начнем с простого упражнения. На основе того, что вы уже изучили, нарисуйте следующую фигуру с помощью Path. Не смотрите на решение сразу. Попробуйте сделать это самостоятельно. В этом коде создается путь, который начинается с точки (100, 100). Затем добавляется квадратичная кривая Безье, которая проходит через точку (200, 80) и заканчивается в точке (300, 100). Затем добавляется прямоугольник размером 200x40, расположенный внизу кривой. Затем путь заполняется зеленым цветом. Этот код создает фигуру, показанную на рисунке. Вы можете экспериментировать с параметрами кривой и прямоугольника, чтобы изменить форму фигуры. Давайте поговорим о протоколе Shape. Протокол очень прост и содержит только одно требование. Для его реализации необходимо имплементировать следующую функцию: Когда полезно реализовывать протокол Shape? Чтобы ответить на этот вопрос, предположим, что вы хотите создать кнопку с куполообразной формой, но с динамическим размером. Можно ли повторно использовать путь, который вы только что создали? Посмотрите еще раз на код выше. Вы создали путь с абсолютными координатами и размером. Чтобы создать ту же форму, но с переменным размером, вы можете создать структуру, реализующую протокол Shape, и имплементировать функцию path(in:). Когда функция path(in:) будет вызвана фреймворком, вам будет предоставлен размер прямоугольника. Затем вы можете нарисовать путь внутри этого прямоугольника. Мы применяем форму купола в качестве фона кнопки. Его ширина и высота основаны на указанном размере frame. Мы создали настраиваемую форму с помощью протокола Shape, но на самом деле SwiftUI поставляется с несколькими встроенными формами, включая Circle, Rectangle, RoundedRectangle, Ellipse и т.д. Если вам не требуется ничего сложного, эти формы достаточно хороши для создания обычных объектов. При помощи комбинирования и сочетания встроенных форм, вы можете создавать разнообразные типы векторных элементов управления пользовательским интерфейсом для ваших приложений. Рассмотрим еще один пример. На следующем скриншоте демонстрируется способ построения индикатора прогресса с использованием круга. Данный индикатор прогресса представляет собой комбинацию двух кругов. В основании расположен серый контур круга. Над ним находится открытый контур круга, символизирующий прогресс выполнения. В вашем проекте необходимо записать код в ContentView следующим образом: Для отображения контура серого круга применяется модификатор stroke. В зависимости от ваших предпочтений, вы можете корректировать значение параметра lineWidth, чтобы изменить толщину линий. Свойство purpleGradient задает фиолетовый градиент, который будет использован при рисовании открытого круга. Для создания открытого круга необходимо добавить модификатор trim, указав значения от и до, чтобы определить, какой сегмент круга должен быть отображен. В данном случае мы хотим продемонстрировать прогресс в 91%. Для этого значение from устанавливается равным 0, а значение to - 0,91. Для отображения процента выполнения мы расположили текстовое представление по центру круга. Последним примером, который я хотел бы вам показать, является диаграмма типа \"кольцо\" (donut chart). Если вы уже поняли, как работает модификатор trim, вы, вероятно, уже знаете, как мы собираемся реализовать диаграмму типа \"кольцо\". Изменяя значения модификатора trim, мы можем разделить круг на несколько сегментов. Эту технику мы используем для создания диаграммы типа \"кольцо\", и вот ее код: Этот код создает диаграмму типа \"кольцо\" (donut chart) с четырьмя сегментами разных цветов. Каждый сегмент представляет собой круг, к которому применен модификатор trim для разделения его на определенный сегмент. После этого каждый сегмент закрашивается определенным цветом с помощью модификатора stroke. Толщина линии каждого сегмента задается с помощью параметра lineWidth. В конце кода располагается текстовое представление, которое отображается поверх последнего сегмента и показывает процентное соотношение. Текст смещается с помощью модификатора offset. В итоге, этот код создает диаграмму типа \"кольцо\" с четырьмя сегментами, разделенными с помощью модификатора trim, и с текстовым представлением, отображающим процентное соотношение. Как и прежде подписывайтесь на мой телеграм канал - https://t.me/swiftexplorer В ближайшее время выйдут следующие части уроков по SwiftUI.",
    "32": "Технология VoWiFi позволяет совершать звонки по номеру телефона вне зоны действия мобильной сети, используя Wi-Fi. Сегодня мы решили обсудить актуальность такого сервиса и ключевые способы его настройки: от OTT и UMA до недоверенного доступа к ePDG. Также рассмотрим перспективы технологии и сложности, которые стоят перед операторами связи, предлагающими услугу звонков через интернет. В первую очередь технология VoWiFi привлекает операторов связи. Предлагая возможность совершать звонки по Wi-Fi, телекомы расширяют зону покрытия собственной сети и привлекают клиентов, которые работают в местах с проблемным доступом — например, в железобетонных офисных зданиях или подвальных помещениях. Звонки по Wi-Fi также удобны для жителей сельской местности и загородных домов. В то же время VoWiFi позволяет операторам снизить затраты на инфраструктуру. Им не приходится устанавливать новые базовые станции и расширять покрытие, чтобы работать в удалённых районах. В крупных городах операторы также получают возможность сократить нагрузку на свои сети. В мегаполисах пользователи находятся рядом с точками доступа не только дома. Как отмечают специалисты консалтинговой фирмы ACG Research, благодаря VoWiFi американские телекомы экономят цент на каждой минуте звонков клиентов, а один крупный оператор за пять лет сэкономил $2,2 млрд. Технология VoWiFi становится актуальной и в транспорте, где затруднительно обеспечить качественный уровень связи — например, в поездах. Специальное оборудование принимает сигнал от операторских вышек или спутника на внешнюю антенну. Затем Wi-Fi раздаётся по вагонам с помощью маршрутизаторов. Такой подход позволяет управлять доступом в интернет централизованно и отслеживать состояние соединения, при необходимости переключаясь между разными источниками сигнала. Благодаря этому звонки по Wi-Fi более стабильны, чем связь с использованием мобильной сети. Еще в 2020 году группа шведских инженеров провела исследование и сравнила качество связи на железнодорожном маршруте между двумя крупными городами Швеции — Стокгольмом и Гётеборгом. VoWiFi показала куда лучшие результаты, чем GSM или UMTS. Интерес к технологии звонков по Wi-Fi проявляют не только крупные операторы, но и частный бизнес. Им VoWiFi позволяет настроить связь между сотрудниками без привязки к мобильной сети. Так, её ещё с начала 2000-х применяют в американских медучреждениях для коммуникации врачей. Устройства связи работают в рамках виртуальной локальной сети, где происходит и обмен данными. По словам представителей Калифорнийского университета в Дэвисе, технология VoWiFi сокращает число инструментов связи, заменяя голосовую и электронную почту, а также факсы. Сегодня технология привлекает и обыкновенных энтузиастов, желающих поближе изучить нюансы работы сетей связи. Так, один инженер запустил частную мобильную сеть и подключил к ней смартфон. Он также предложил заинтересованным пользователям присоединиться к проекту, чтобы связываться и общаться с друзьями напрямую. В целом есть пять способов настройки звонков по Wi-Fi. UMA (Unlicensed Mobile Access). Переключение между мобильными сетями и Wi-Fi происходит с помощью так называемых контроллеров GAN. Они получают пакеты из интернета и передают их в телефонную сеть так, словно они поступили от антенны на сотовой вышке. Сегодня отдельные частоты, на которых работает UMA, несколько устарели, однако некоторые игроки рынка считают технологию крайне перспективной. Как коммерческую услугу UMA предлагают в Северной Америке и ряде стран Европы. Подключение через приложение оператора. В этом случае пользователю необходимо загрузить специальное мобильное приложение оператора, ввести логин и пароль. Для подключения даже не нужна SIM-карта, и этот способ связи можно использовать на старых устройствах, не поддерживающих VoWiFi по умолчанию. Доверенный доступ к EPC. Настройка происходит в рамках сети, построенной на Wi-Fi-хотспотах провайдера, отсюда и название — доверенный доступ. Такая доверенная сеть обеспечивает QoS и безопасное подключение пользователей за счет механизмов аутентификации и шифрования трафика. Клиентские терминалы взаимодействуют с мобильными сетями с помощью s2b-интерфейса. Но цена на абонентское оборудование достаточно высока, поэтому реализация доверенного доступа к EPC и IMS может быть не самым выгодным подходом для оператора. Недоверенный доступ к ePDG. Устройство устанавливает соединение с ядром поставщика услуг через ePDG и аутентифицируется с помощью SIM-карты. Поддерживать качество звонков и следить за QoS позволяет информация от 3GPP-серверов, отвечающих за авторизацию, аутентификацию и учёт пользователей. При желании оператор может ограничить выбор точек Wi-Fi. Технологию запускают на основе существующей инфраструктуры для VoLTE. На рынке есть вендоры, готовые помочь с реализацией. Например, наше решение предусматривает защищенное соединение от абонентского устройства до ядра. Телефон поднимает туннель IPSec до ePDG, который затем «перекладывается» в туннель GTP. OTT (Over the Top). По сути, этот вариант реализации VoWiFi представляет собой классические звонки через мессенджеры вроде Viber и Telegram. Можно сказать, что он не используется операторами связи. Несмотря на все преимущества VoWiFi, у технологии есть ряд особенностей. Одно из существенных ограничений связано с «железом». Звонки через Wi-Fi в большинстве случаев поддерживают модели телефонов среднего и высокого ценового диапазонов. В то же время операторы не способны контролировать качество интернет-подключения через сторонние точки доступа. Даже мощная Wi-Fi-сеть может быть перегружена большим потоком трафика, и если оператор не в состоянии определить это до переключения звонка, то качество передачи голоса может пострадать. Так, операторы уделяют отдельное внимание алгоритмам, отвечающим за переключение между сетями, оценке загруженности точек доступа и соответствия допустимым уровням задержки. Они стараются выделить достаточную пропускную способность на пограничных устройствах, а при плохом качестве Wi-Fi-подключения реализуют хэндовер на мобильную сеть. Ещё одним вопросом остаются возможные помехи от радиоустройств при звонке через общедоступные Wi-Fi-сети. Впрочем, для защиты от перебоев в связи уже используют RRM-алгоритмы. Они беспрерывно анализируют радиочастотные данные с точек доступа, чтобы автоматически оптимизировать пропускную способность канала, мощность передачи и не только — в том числе с помощью систем ИИ. Наконец, развивают технологии, призванные увеличить покрытие и качество сигнала. Так, группа испанских инженеров предложила использовать БПЛА в виде ретрансляторов для раздачи Wi-Fi. Такой подход может быть полезен, например, при ликвидации последствий аварий или других ЧС вдали от зоны покрытия операторов. В то же время стоит отметить, что технология VoWiFi потенциально уязвима для хакерских атак. Злоумышленники могут попробовать нарушить связь между пользователями с помощью DDoS-атак. В теории они также способны определить IP-адрес человека и статистику его звонков, если перехватят интернет-пакеты от уязвимой точки доступа. Хотя исследователи уже разрабатывают специальные антивирусные решения для операторов, предоставляющих услуги VoWiFi. Можно предположить, что распространение таких систем поспособствует дальнейшему развитию VoWiFi и расширит экосистему предложений вендоров. Как канадские власти стимулируют конкуренцию среди интернет-провайдеров. Местный регулятор обязал крупные телекоммуникационные компании сдавать инфраструктуру в аренду локальным операторам связи. Некоторые участники рынка выразили протест, пригрозив сократить инвестиции в оптоволоконные сети. Другие отметили, что закон был принят слишком поздно, чтобы оказать сколько-нибудь заметный эффект на конкурентную обстановку в индустрии, а всех независимых игроков уже выдавили с рынка. Новые рекорды: как изменились DDoS к концу года. В этом материале мы обсуждаем обстановку в сфере кибербезопасности. Рассказываем, насколько выросла мощность DDoS-атак за последний год и какие страны и отрасли хакеры выбирают в качестве целей. Спойлер: это финтех, транспортная промышленность и телекомы в США и России. Кто и зачем меняет стандарты широкополосной связи — разбираем ситуацию в США и других странах. Американский регулятор предложил пересмотреть broadband-стандарты, которые не менялись уже долгое время. Планку «скорости» хотят поднять с 25 до 100 Мбит/с. Инициатива встретила сопротивление со стороны крупных интернет-провайдеров. Их аргументы довольно стандартные: текущих скоростей хватает, а желающие и без того могут подключить себе высокоскоростной тариф за дополнительную плату. ЕС внедряет правительственные веб-сертификаты — почему инициатива вызывает опасения в ИТ-сообществе. Странам ЕС хотят позволить выдавать собственные QWAC-сертификаты для аутентификации сайтов, чтобы ограничить влияние разработчиков браузеров. В свою очередь, специалисты по ИБ говорят, что решение приведет к ухудшению информационной безопасности в интернете. В нашем материале обсуждаем аргументы за и против.",
    "33": "Любите детективы? Пройдите квест «В поисках пропавших ссылок»! Регистрируйтесь на сайте и попробуйте себя в роли сыщика: найдите на страницах Selectel спрятанные ссылки и первыми дойдите до финала. Выиграйте эксклюзивный мерч и промокод на сервисы Selectel.",
    "34": "В Отусе я прошла курс ML Advanced и открыла для себя интересные темы, связанные с анализом временных рядов, а именно, их сегментацию и кластеризацию. Я решила позаимствовать полученные знания для своей дипломной университетской работы по ивент-анализу социальных явлений и событий и описать часть этого исследования в данной статье. Шаг 1. Сбор данных В качестве источника данных я взяла информационно-новостной ресурс Лента.ру, так как с него легко парсить данные, новости разнообразны и пополняются в большом объеме ежедневно. Для теста я спарсила новости за последний год (март 2023 – март 2024) с помощью питоновских BeautifulSoup и requests. Шаг 2. Формирование датасета Немного отвлекусь на сам ивент-анализ. Как метод политической науки он зародился в 1960-х годах в научных трудах Чарльза Макклелланда. Ивент-анализ - это метод качественного исследования, который используется для описания и объяснения социального поведения и взаимодействий. Первый шаг из классической методологии я уже сделала – собрала данные. Вторым шагом идет определение системы так называемых классификаторов: проще говоря, категорий, по которым эти данные надо проклассифицировать для дальнейшего анализа. Сначала я брала в качестве классификаторов готовые рубрики новостей с Ленты, но, посмотрев на их список, можно увидеть, что они не особо репрезентативны для социальной сферы и вряд ли из них можно получить интересные результаты: Экономика, Наука и техника, Путешествия, Силовые структуры, Нацпроекты, Среда обитания, Забота о себе, Спорт, Интернет и СМИ, Россия, Бизнес, Бывший СССР, Ценности, Мир, Культура, Дом, Оружие. Поэтому я решила составить свои классификаторы. Сначала получила списки предварительных, более узких категорий для базовых рубрик, обучив Word2Vec на новостных заголовках. Это помогло определить те темы, которые освечиваются на данном новостном ресурсе, а не выбирать наугад. Фрагмент результатов определения топа слов с самыми близкими векторами: Далее сформировала из полученных слов список тематик таким образом, чтобы они имели выраженную эмоциональную окраску – так анализ выйдет более интересным и информативным. Затем несколько раз прогоняла и в полуавтоматическом режиме модифицировала списки новых категорий-классификаторов на готовой модели Zero-shot классификации mDeBERTa-v3-base-mnli-xnli, пока не получила средний скор модели для каждого классификатора > 0.8. Из этого можно сделать вывод, что я более или менее охватила базовый набор категорий, которые можно привязать к социальной сфере и их формулировки при этом были понятны модели, нет явных новостей-«изгоев», по которым не нашлось подходящего класса. Конечно, какие-то классификаторы получились более широкими и охваченными, а какие-то – более узконаправленными, но тут еще есть, куда расти) Как видно, проблемы в новостях все-таки любят освещать больше) Но тут следует делать поправки на то, что модель тематической классификации не всегда идеально определяет контекст, плюс некоторые сообщения имеют нейтральную эмоциональную окраску, но для получения общей картинки это хороший и быстрый вариант. Шаг 3. Сегментация временных рядов Переходим, собственно, к временным рядам, а именно, к их сегментации. Временные ряды я строила отдельно для каждого классификатора, где по оси x – дата, по оси y – количество новостей по данному классификатору за эту дату. Для сегментации был использован алгоритм PELT. Алгоритм ищет набор точек «перегиба» для заданного временного ряда таким образом, чтобы их количество и местоположение минимизировали заданную «стоимость» сегментации. Основные шаги алгоритма заключаются в определении функции «стоимости» для сегмента, затем итерации по всем возможным начальным и конечным точкам сегмента и проверке того, уменьшает ли разделение на новые сегменты значение функции стоимости по сравнению с сегментом без разделения. Здесь C – функция «стоимости» сегмента, t – точка «перегиба», m – общее количество точек «перегиба», βf(m) – регуляризатор для предотвращения переобучения. Было выявлено, что точки минимума и периоды сниженной новостной активности на временных рядах совпадают с праздничными днями и выходными. Поэтому данные по этим дням были удалены из выборки, чтобы временные ряды получились более сглаженными и не находилось ложных корреляций. Для решения задачи я использовала библиотеку ruptures. Для параметра «model» было выбрано значение «l1», так как результаты получились визуально лучше, другие варианты – это «l2» и «rbf». Параметр min_size – минимальная длина сегмента, в нашем случае – минимальное количество дней, которые можно объединить в один сегмент. Соответственно, чем он больше, тем меньше сегментов получится. В данном примере было выбрано объединение в сегменты размером не меньше недели. Параметр pen – это параметр регуляризации, который подбирается экспериментально, чтобы предотвратить переобучение алгоритма. Один из популярных подходов – брать регуляризацию как два логарифма от длины исходного ряда. Чем меньше значение параметра регуляризации, то есть меньше «штраф», тем больше сегментов выделяется. Здесь можно посмотреть, какие сегменты по каким категориям получились, подробный анализ получившихся данных с выводами – это уже больше для научной работы). В дальнейшем будет интересно повыделять ключевые слова для сегментов, точек «перегиба» и экстремальных значений. У данной задачи есть и свои подводные камни, которые могут вносить шум и приводить к ложным корреляциям. Все события можно поделить на две глобальных группы: Первая группа – это связанные друг с другом события из одной цепочки, имеющей жизненный цикл, как, например, события из ситуации со специальной военной операцией. Вторая группа – это, так сказать, «одноразовые» и не особо значимые для анализа события, которые слабо зависят от других и могут вносить характер случайности в частотное распределение по категориям. В идеале – научиться определять и удалять сообщения о таких событиях, а также о тех, которым невозможно дать ярко выраженную эмоциональную окраску. В общем и целом, метод показал интересные результаты, хотя тут следует еще поэкспериментировать с параметрами. Я посмотрела некоторые события, повлекшие за собой выбивающиеся сегменты. Например, резкий скачок в категории развития бизнеса и торговли после ноября 2023 года в основном произошел из-за начала продажи новых китайских автомобилей, что повлекло сообщения о развитии автомобильной отрасли в России. Чтобы посмотреть, есть ли корреляции этого события с сообщениями из других категорий, да и в целом, какие категории коррелируют друг с другом, нужно рассмотреть еще один, не менее интересный метод – кластеризацию временных рядов. Шаг 4. Кластеризация временных рядов Для проведения кластеризации лучше всего брать временные ряды не по дням, а по месяцам, так как корреляции будут видны лучше. Для начала преобразуем исходный датасет в вид, пригодный для анализа, а также нормализуем временные ряды: Для кластеризации я взяла классический k-means. Для данной задачи этот метод подходит лучше, так как нам важна именно близость в частотном распределении по месяцам. Более же продвинутый DTW ищет ряды с похожими шаблонами. По k-means считается евклидово расстояние между эмбеддингами несмещенных временных рядов, для них ищутся центроиды и наконец определяются кластеры в результате перемещения центроид по количеству итераций. Так как заранее не известно оптимальное количество кластеров, нужно определить это значение по методу «локтя» и по метрике силуэта. Метод локтя показывает оптимальное количество кластеров по следующему принципу: если после визуального «локтя» на графике идет резкое убывание общей ошибки, то такое количество считается оптимальным, но если кластеров много, то ошибка будет минимизироваться, но не будет смысла в кластеризации в принципе. Считается cумма квадратов расстояний от объектов до центра кластера (иначе говоря, ошибок). По методу силуэта оптимальное количество кластеров - пиковое значение на графике, после которого идет резкий спад. Метрика считает для каждого объекта среднее расстояние между ним и объектами внутри кластера (a) и между ним и объектами в ближайшем кластере (b). Чем больше нормализованное b-a, тем лучше. По обеим метрикам лучше всего подходит количество кластеров, равное шести. Обучаем модель кластеризации на наших временных рядах для их деления на 6 кластеров, а также строим усредненные временные ряды для каждого из кластеров. Далее визуализируем полученные кластеры и смотрим, какие категории оказались в одной группе. Большинство кластеров выглядят вполне логично, и взаимосвязи видны именно на временных рядах с периодом по месяцам, а не по дням. Вспомним наш пример с китайскими автомобилями, относящимся к категории «развитие бизнеса и торговли», и увидим, что резкий качок после ноября 2023 года произошел также в категориях «экономическое развитие» и «инновации и импортозамещение». Связаны ли эти события – это уже другая задача, но, определенно, есть корреляции во временных рядах этих тематик. Также интересны кластеры № 2, 3, 4: логически похожи ряды с «безработицей и бедностью» и «социальными проблемами», с «санкциями и русофобией», «преступлениями» и «политическими проблемами», с «катастрофами и катаклизмами» и «войной и оружием», с «пропагандой и агитацией» и «международными конфликтами и разногласиями». Есть, конечно, и выбивающиеся из общей логики временные ряды, но это, скорее всего, можно списать на обычные совпадения. В любом случае, для получения более интересных результатов нужно использовать дополнительные методы, в том числе анализ лингвистической составляющей новостей. Итак, я рассмотрела задачи сегментации и кластеризации временных рядов применительно к анализу новостей. Результаты вышли достаточно интересными и логичными, но для получения более информативных выводов в задаче анализа социальных явлений и процессов необходимо собрать больше данных, определить больше категорий, добавить дополнительные проверки и этапы, в частности, связанные с NLP.",
    "35": "Подборка полезных материалов и находок из мира Go за неделю. Проекты недели. ▪ Beego —  новая версия высокопроизводительного Go фреймворка для разработки RESTful API, веб-приложений и бэкенд-сервисов, вдохновленный Tornado, Sinatra и Flask. ▪LLocalSearch —  полностью локально работающий поисковый агрегатор с использованием агентов LLM. ▪ Freeze — полезный Go инструмент для генерации изображений кода и вывода терминала. ▪Go-cfg — Простой и удобный способ инициализировать конфигурацию в структуры, с помощью структурных тегов. ▪Hypert — это библиотека Go с открытым исходным кодом, которая упрощает тестирование клиентов HTTP API. ▪ Skopeo — это утилита командной строки, которая выполняет различные операции с образами контейнеров и их хранилищами. ▪ Tau — инструмент на Go, который переводит облачную инфраструктуру в режим автономной работы, плавно соединяя локальную разработку и облачные технологии. — Dataloader 3.0: Новый алгоритм для решения проблемы N+1 —  Выпущены версии 1.22.2 и 1.21.9 — Создание серверного Live Chat приложения с использованием Next.js, Fauna и WunderGraph для GraphQL Live Queries — Разбираемся в новом роутинге в Go 1.22",
    "36": "В предыдущей статье мы детально разобрали процесс установки Узла безопасности с Центром Управления Сетью (УБ с ЦУС), провели его начальную настройку через Менеджер Конфигурации (МК), выполнили настройку Системы мониторинга, подключили к ЦУС подчиненный Узел Безопасности (УБ) и обновили все библиотеки. Если Вы рассматриваете Континент 4 NGFW как решение, на которое планируете мигрировать с иностранного решения, настоятельно рекомендуем ознакомиться с заключением к данной статье. Там вы найдете перечень полезных материалов. Все правила на Континент 4 NGFW состоят из элементов, называемых Объектами ЦУС. Объекты ЦУС являются критериями отбора для транзитного трафика, который должен быть передан на обработку по правилам фильтрации. Континент 4 NGFW позволяет импортировать сетевые объекты из таблиц. Для этого должен быть сформирован файл в формате .csv с указанием сетевых объектов. Путь для импорта списка: ПКМ в «Сетевых объектах» — Импортировать. Для лабораторных работ нам потребуются различные объекты ЦУС. Давайте создадим их и параллельно ознакомимся с каждым объектом. 1. Сетевые объекты. Хост: единичный сетевой объект (например, 10.0.0.10); Сеть: подсеть (например, 10.0.0.0/24); Диапазон адресов: диапазон хостов (например, 10.0.0.1−10.0.0.10) Защищаемая подсеть в Центральном офисе 192.168.1.0/24; Защищаемая подсеть в филиале 10.0.0.0/24; Подсеть DMZ 172.16.20.0/24; Хост администратора 192.168.1.10; Хост MS AD 172.16.20.100; Хост пользователя в филиале 10.0.0.10 Внешний IP-адрес шлюза Центрального офиса 10.77.128.231; Внешний IP-адрес шлюза филиала 10.77.128.233 2. Сервисы По умолчанию в Континент 4 NGFW есть базовые сервисы (TCP, UDP и ICMP), которые можно дополнять. 3. Пользователи Учетные записи пользователей могут быть созданы в локальной базе данных ЦУС или импортированы из каталога Active Directory. 4. Приложения Контролируемые комплексом сигнатуры приложений для удобства пользователя разделены по категориям. В базовый контроль приложений входит около 100 приложений. Доступны такие категории как: бизнес-приложения, виртуализация, социальные сети, голосовая связь, облачные хранения, удаленный доступ и др. При использовании расширенного контроля приложений и протоколов в БД ЦУС загружаются дополнительные сигнатуры приложений. Активируется компонент в свойствах узла безопасности. В расширенный контроль приложений входит около 4000 приложений и протоколов. Приложения, входящие в расширенный контроль приложений и протоколов, можно модифицировать. Для этого выбирается определенный атрибут приложения. Например, запретим файловый трансфер в Яндекс Диске. В поисковой строке ищем нужное приложение и следуем по пути: ПКМ — Создать приложение — Отмечаем атрибут «file transfer». Также можно создавать группы приложений. 5. Страна В Континент 4 NGFW встроен модуль GeoProtection. Администратор может ограничивать доступ от и до ресурсов по их географическом расположению. 6. DNS-имя При помощи объектов «DNS-имя» можно разграничивать доступ по DNS-имени для пользователей. При этом поддерживается и кириллическое написание FQDN с автоматическим разрезолвом по Punycode. Пакеты проходят вниз до полного совпадения по всем критерии правила. Далее трафик данной сессии идет по совпавшему правилу; ЦО-Филиал. В данном блоке мы разрешим прохождение ICMP трафика между подсетями центрального офиса и филиала. Правила потребуются при рассмотрении L3VPN в статье про VPN; DMZ. В данном блоке мы разрешим хождение DNS трафика от локальных сетей до DNS-сервера, а DNS серверу разрешим ходить до публичных DNS-серверов по протоколу DNS. Также разрешим подключаться извне по порту 8080 и 80 до опубликованного ресурса IIS; Доступ локальной сети центрального офиса в Интернет. Запретим хождение трафика до двух ресурсов по DNS, запретим обращение к приложениям Messaging и P2P, запретим доступ до зарубежных ресурсов Везде также поставим логирование и выберем УБ, на которых эти правила будут устанавливаться. Дополнительно отметим, что существуют «невидимые» системные правила. Например, в самом низу находится классическое правило «CleanUP» или «drop any any». В случае необходимости администратор может указать его в явном виде. «Не транслировать». К трафику не применяются правила трансляции. Используется для правил-исключений при передаче части трафика без трансляции. «Скрыть». Маскарадинг (Hide NAT). Исходящим пакетам в качестве IP-адреса отправителя назначается IP-адрес, через который будет доступен получатель пакета. «Отправителя». Изменение IP-адреса отправителя (Source NAT) «Получателя». Изменение IP-адреса получателя (Destination NAT) «Отобразить». Трансляция IP-адреса отправителя в режиме «один к одному» Дополнительно отметим, что необходимо обращать внимание на занятые системные порты. Например, в УБ с ЦУС в версии 4.1.7.1395 и старше по умолчанию порты 80 и 443 заняты для служебных операций. Перед установкой политики необходимо активировать компоненты «Расширенный контроль протоколов и приложений» и «Модуль GeoProtection». Начнем проверку с модуля GeoProtection. Попробуем выполнить ping до зарубежного ресурса google.com и российского ресурса vk.com. Первый ресурс недоступен, нет разрешающего правила и происходит отброс пакетов. Второй корректно отвечает, так как есть правило, разрешающее доступ до российских ресурсов. Кроме команды ping, ресурс vk.com доступен в веб-браузере. Правило с гео-модулем работает корректно. Попробуем обратиться к ресурсу где-ударение.рф. Данный ресурс расположен в России. Сайт будет долго грузиться, но не откроется. Сработает блокировка по правилу с DNS-именем. Попробуем открыть официальный сайт Telegram, но он также не открывается, так как попадает в группу приложений Messaging и детектируется DPI системой. Все события мы можем увидеть и в Системе мониторинга. Как мы видим, подключение до ресурса где-ударение.рф не отличается от обращения к IP-адресу. В случае с Telegram мы видим сигнатуру. При открытии события можно получить расширенную информацию. Для обеспечения повышения качества передачи данных в комплексе Континент используется специальный механизм Quality of Service (QoS). Данная технология предоставляет различным классам трафика приоритеты в обслуживании. Маркировка IP-пакета определяется с помощью DSCP-метки в заголовке IP-пакета. Кодом DSCP называются шесть наиболее значимых бит поля DiffServ. DffServ — это модель, в которой трафик обрабатывается в промежуточных системах с учетом его относительной приоритетности, основанной на значении поля типа обслуживания (ToS). DSCP-метка группы EF имеет высший класс приоритета. Значение DSCP метки — 46. Это означает, что трафик будет передан самым лучший способом. Best Effort. DSCP значение 0. Означает, что трафик будет передан по возможности. Для обработки приоритетов используется метод HFSC, обеспечивающий распределение полосы пропускания очередей. Протестриуем данный функционал. Для этого активируем компонент «Приоритизация трафика» и создадим правила. Созданные правила устанавливают, что трафику из центрального офиса в филиал по протоколам DNS (udp/53 и tcp/53) будет установлен высокий приоритет и передача будет осуществляться лучшим образом. Трафик по протоколам ICMP определен как неприоритетный и будет передан по возможности. Профиль приоритизации трафика создается для определения полосы пропускания для каждого типа приоритета. Создается как для исходящего трафика, так и для входящего. Профиль применяется в настройках узла безопасности. Сохраним и установим политику с QoS. Выполним команды ping, сделаем несколько выходов в Интернет. После этого обратимся к Системе журналирования. Появились новые записи с действием «Приоритет» (работает механизм QoS). В комплексе имеется возможность сконфигурировать сеть при одновременном подключении УБ к нескольким внешним сетям. балансировка трафика между внешними интерфейсами УБ. При этом механизм Multi-WAN обладает некоторыми ограничениями в своей работе. Настройки Multi-WAN происходит в свойствах узла безопасности во вкладке Multi-WAN. Итак, в этой статье цикла мы с вами рассмотрели настройки межсетевого экрана, создали базовые политики межсетевого экранирования и NAT. А ткаже проверили работу межсетевого экрана и QoS. Объекты ЦУС можно импортировать в виде таблиц *.csv; В самом начале статьи мы упоминали, что в заключении поделимся полезной информацией для тех, кто мигрирует с зарубежных решений. Континент 4 NGFW имеет несколько конвертеров, позволяющих автоматизировать миграцию с решений Fortinet, CheckPoint и Cisco. В случае, если конвертера еще нет, можно выполнить промежуточную конвертацию через CheckPoint. Например: Palo Alto — CheckPoint — Континент 4 NGFW. До встречи в следующей статье, в которой мы детально рассмотрим работу с пользователями: локальными и доменными.",
    "37": "Среднегодовая доходность рынка акций США с 1930-x составила 9,5% годовых. Риск, который измеряется среднеквадратическим/стандартным отклонением (сигма), составлял 19,5%. То есть, по статистике, через год можно ожидать, что с вероятностью 68% (1 сигма) доходность рынка будет входить в диапазон от -10% до 29%. Или с вероятностью 99,7% (3 сигма) ожидаемая доходность останется в диапазоне 3-х сигм или от -49% до +68%.  А если посчитать то же самое, но для 10-летнего периода. Какие будут ожидаемые доходность и риск?Доходность рассчитывается просто. (1+0,095)^10=2,48 или 148%. Для риска хочется просто взять и умножить 19,5%*10 лет*3 сигма=585%. То есть для определения диапазона доходности через 10 лет нужно к 148% прибавить 585% (верхняя граница) и от 148% отнять 585% (нижняя граница). Но это крайне высокий риск и ошибочный подход к расчету! Верный метод на самом деле заключается в том, чтобы умножить 19,5% на квадратный корень из 10. Это приводит к правильному почти максимальному диапазону ожидаемой доходности через 10-и лет - от -38% до 333%. Сюда, согласно теории, индекс попадет с вероятностью 99,7%. Когда соединяются независимые распределения вместе, дисперсии каждого распределения можно успешно сложить, но их стандартные отклонения складывать нельзя, поскольку стандартное отклонение представляет собой квадратный корень из дисперсии. Таким образом, стандартное отклонение долгосрочного распределения вероятности возрастает пропорционально квадратному корню из времени. Если опустить технические детали расчета, на графике видно, нижняя граница диапазона ожидаемой доходности с увеличением длительности периода инвестиций растет. И к окончанию 13-и летнего периода подбирается к нулю. Доходность 0% за 13 лет (!), но это лишь нижняя граница диапазона, где средняя доходность выше 200%. (примечание: доходность 0% процентов за 13 лет была только один раз – при покупке рынка в 1929 году) Основной вывод следующий. Независимо от выбранного вами подхода при инвестировании – активного или пассивного - чем более длительный временной горизонт ваших вложений в акции, тем выше вероятность получить положительную доходность. Пост «Почему акции всегда растут» здесь:     https://t.me/TradPhronesis/128 Инвестирование (вложения в акции от года и более) – это игра с положительной суммой. И рано или поздно, какой бы кризис не случился, приток денег в систему под названием «Рынок акций» обеспечит положительную доходность. Заходите на телеграмм канал: https://t.me/TradPhronesis",
    "38": "Под словом «этих» мы подразумеваем результативных людей. Это не про проактивность, дисциплинированность и прочие клише из вакансий. А про личностный конструкт, благодаря которому человек способен создавать результаты для бизнеса. Ситуация: у вас функционирующий бизнес, вроде как все едет, но только вы отлучаетесь – начинаются проблемы. Вы пашете как проклятый, а выхлоп нулевой, результатов – ноль целых, фиг десятых. Зато косяков от команды - хоть отбавляй. А вообще-то хочется стратегическими делами заниматься, обгонять конкурентов, больше времени с семьей проводить, а не подтирать за своими же сотрудниками. Кончено, вы начинаете с поиска причин и решений. И всегда их находите. Но на самом деле мы умеем видеть решение проблем только в области своей компетенции. Для человека с молотком решение проблемы – это «забить гвозди». Для человека с гвоздодером – «вытащить гвозди». Поэтому ваши «решения» проблем ограничены вашими же знаниями и насмотренностью. Мало кто смотрит в глубину и видит истинные причины. Но мы с вами сейчас попробуем. Маркетолог и HR – это люди. Компания – это люди. Да, компания – это много чего еще: процессы, цели, офис, айдентика и тд – но все это создается людьми для организации своей же работы. Поэтому когда не хватает лидов – это означает, что кто-то из людей в компании не создает нужный результат. Не можете найти нужного человека уже полгода → ответственный за найм не создает нужный результат. Да, это одновременно просто осознать, и одновременно – сложно. Зачастую предприниматели считают, что (читайте подзаголовок ниже) Сильная команда для многих – «спящая потребность». Компании внутри нередко выглядят примерно как «я и три этажа исполнителей, которым совершенно неважно то, что действительно важно». И кажется, что это и есть — команда. Заниматься ей совершенно не хочется. Связано это, скорее всего, с низким уровнем нормы в отношении профессионализма сотрудников. Не имея твёрдых технологий в найме, мы обычно нанимаем классных людей, но слабых сотрудников. Отсюда такие семейные коллективы, где никто никого не обижает, но и больших результатов тоже не создает. Когда пытаемся с этим коллективом вырасти Х сколько-нибудь – начинаются проблемы. Но люди-то хорошие, на рынке лучше уже никого и не найти, будем растить. Берём в команду тех, кого пришлось, и воспитываем их столько, сколько оказывается сил. Соглашаемся на HR, которые замеряют уровень счастья, но не знают, приносят ли люди бизнесу прибыль. Соглашаемся на маркетологов, которые тестируют гипотезы, не влияющие на финансовые показатели, за большие деньги. Соглашаемся на финансистов, которые создают убытки. Постепенно посредственность результатов становится нашим уровнем нормы. И совершенно забываем, что если из двоих человек в комнате ни один не может создать нужный результат, дело не всегда в том, что предложенная задача по своей природе нерешаема. Гораздо чаще — дело в том, что в этой комнате просто не оказалось нужного профессионала. А еще – часто предприниматели недооценивают саму идею команды как группы, достигающей его целей. И при недостижении их – начинают задумываться об адекватности своих целей и о легитимности увольнять людей, которые этих целей достигать не помогают. Вместо того, чтобы задать себе вопрос «а точно ли эти люди приносят результат?» Хороший вопрос. Ответ на него есть, но давайте сначала раскрутим, как по-разному в работе ведут себя люди. Сразу обозначим, что «абсолютно результативных» людей не бывает. Также невозможно наводнить компанию «абсолютно НЕрезультативными» сотрудниками. Результативность – это всегда градиент. И вы сейчас его ощутите. Многие могут узнать своих подчиненных в следующей ситуации. Вы даете какую-то задачу, человек уходит делать, в процессе задает вопросы и в целом, создается ощущение, что он действительно старается. Много общается с заказчиком, решает с ним важные вопросы. А потом приходит и говорит: «Я очень старался, но вот заказчик…». И дальше следует либо перечисление объективных обстоятельств, либо тщательно замаскированные отмазки. Отличить одно от другого можно в моменте, уточнив детали обстоятельств, или в динамике, наблюдая за тем, как сотрудник будет действовать в похожих ситуациях. Подобное поведение регулярно – рэд флаг. Еще более четкий признак нерезультативности – когда все действия человека направлены на поиск причин почему что-то не сработает. Вы ставите задачу, а от него саботаж уже на старте. Вместо решения задачи уходит в рассуждения: «Да это не выполнить, потому что... Ведь если присмотреться, то...» И полчаса болтовни, чтобы отмазаться от работы еще до ее начала. И каких только умственных изворотов не наслушаешься. То расчеты им покажут полную «нереалистичность» задуманного. То сезонность спроса, то уровень инфляции и курс гривны. Сразу скажем, что подобные ситуации возможны и у результативных сотрудников, но никогда ни в одной компании не являются регулярностью. И делать это нужно, конечно, на этапе найма. В следующий раз на собеседовании вместо того, чтобы спрашивать какие последние книги прочитал кандидат, поинтересуйтесь, что он считал результатом своей работы на последнем месте. Прислушайтесь к ответам. Говорит про процессы – ред флаг. Говорит про KPI, деньги и результаты, которые можно измерить – грин флаг, человек работал не ради галочки, а для достижения определенных целей. Помимо упомянутого вопроса, есть еще несколько других, которые помогут проверить кандидата на способность приносить результат. Мы собрали их здесь, вместе с инструкциями о том, как интерпретировать ответы собеседуемого. Надеемся, эти вопросы помогут вам при отборе кандидатов. Если у вас есть свои способы отсеивать «своих» и «не своих» – пишите в комментариях, побуждаем :)",
    "39": "Статья тестировщика \"ITQ Group\" Леонида Галочкина. Привет! Меня зовут Леонид, я работаю тестировщиком в компании ITQ Group. В этой статье хочу кратко и доступно описать жизненный цикл Activity мобильных приложений на Android. Понимание принципов работы activity позволяет понять логику работы мобильных приложений, разрабатывать тест-кейсы разного уровня, готовить тестовую документацию. Принципы activity можно игнорировать, например, при тестировании UI приложения или при e2e-тестировании, но для полного тестирования МП знать их необходимо. Жизненный цикл Activity часто путают с жизненным циклом приложения вообще, что неверно. Activity — это основная часть любого Android приложения, с которой взаимодействует пользователь. То есть, это сам интерфейс или экран, на котором происходит взаимодействие в определенном состоянии. В Activity располагаются всевозможные элементы: кнопки, фрагменты, изображения и другие элементы View. View – это визуальный интерфейс приложения в том или ином статусе, с которым взаимодействует пользователь. Живым примером принципов активити является работа приложения от момента его вызова на устройство, прохождения определенного флоу, закрытия приложения и появления старта того же юзер-флоу в момент, когда вы заново открываете приложение. закрытым или уничтоженным системой. Activity берет начало в момент старта приложения. Система дает такому приложению высокий приоритет. Дальнейшее состояние activity помогает понять, что с ним делать, в какой момент освободить ресурсы, связанные с ним, и так далее. Эта приоритезации позволяет, например, в некоторых случаях не блокировать входящие звонки. Когда activity переходит из одного состояния в другое, система Android делает вызов методов (callback) обратного жизненного цикла activity, и параллельно генерирует уведомление о конкретной стадии после каждого действия. Например: OnCreate(). Вызывается при создании activity. В нем появляется интерфейс, код самого макета, кнопки и другие функциональные элементы формы. OnStart(). Вызывается при появлении activity на экране. OnResume(). Activity на экране, с ним можно взаимодействовать, оно в фокусе. OnPause(). Само приложение activity находится в состоянии паузы, не в фокусе. С ним нельзя взаимодействовать, но частично можно видеть на экране. Например, в виде диалогового окна, под другим activity. OnStop(). В этом методе activity переходит в состояние Stopped, то есть полностью невидима. В методе onStop следует освобождать используемые ресурсы, которые не нужны пользователю, когда он не взаимодействует с activity. При этом в памяти устройства остаются все элементы интерфейса, с которыми в дальнейшем можно взаимодействовать. Например, в случае, если в поле приложения был введен текст в момент когда activity было в статусе OnResume(), а затем активити было переведено в статус OnStop() и обратно в OnResume(), введенный текст сохранится. onDestroy(). Завершение работы activity происходит посредством вызова метода onDestroy, который возникает или при вызове finish(), или если система решит убить activity в силу конфигурационных причин (например, поворот экрана или при многооконном режиме). Покажу на примере последовательные действия на мобильном устройстве, связанные с созданием и изменениями activity. Для этого я использую стандартное приложение Google Chrome. На первом экране отсутствует какая-либо activity, связанная с этим  приложением. Нажимаем на иконку приложения Google Chrome  — инициируем отправку метода onCreate(). Появлении activity на экране вызываем метод onStart(). При появлении activity на экране и возможности с ним взаимодействовать вызываем метод onResume(). Для лучшего понимания здесь представлена таблица со схемами вызовов методов activity в зависимости от определенных действий. Есть также очередность, в соответствии с которой callback методы вызываются. Зависит она от того, что именно делает пользователь с приложением. Вот и всё. В статье мы разобрали основы принципов activity, и я надеюсь, это поможет начинающим специалистам познакомиться и разобраться с логикой работы приложения в любом из показанных статусов.",
    "40": "Сегодня я хочу поговорить про интересные моменты в Kotlin, связанные с вызовами конструкторов классов. Или не совсем конструкторов? Или же совсем не конструкторов? Давайте разбираться. Это неоднозначная техническая статья для любителей языковых интересностей, но не лишённая практического смысла. Мы можем с точностью сказать, что тут создаётся новый объект типа MyController, при этом как обычно вызывается конструктор этого класса с некоторыми аргументами. Что даёт нам гарантию того, что перед нами именно вызов конструктора? Ну, как минимум, ключевое слово new. И, что уж греха таить, идентификатор с большой буквы начинается. А теперь взглянем на аналогичный незамысловатый код на Kotlin. Разумно предположить, что мы просто создаём объект класса MyController, как и в примере на Java. Но можем ли мы быть так в этом уверены? Ну, конечно, в большинстве случаев такая догадка окажется верной. Но не всегда. Далее мы рассмотрим конструкции (no pun intended), в контексте которых листинг (1) мог бы компилироваться и работать примерно так, как мы ожидаем, не являясь при этом, строго говоря, вызовом конструктора. А почему мы вообще решили, что это в листинге (1) мы видим именно вызов конструктора? Наверное потому, что классы в Java/Kotlin в любом код-стайле принято называть в \"capitalized camel-case\" регистре. А ключевого слова new в Kotlin нет. В Kotlin разрешено объявлять и использовать функции верхнего уровня (top-level functions). Опять же, никакого ограничения на регистр их именования синтаксис языка не накладывает (это было бы совсем уж странно). Но если мы вдруг попробуем как-то нестандартно назвать функцию, например create_my_class() или CreateMyClass() то сообразительная IntelliJ сразу надменно подчеркнёт имя функции и укажет, что, мол, не по кодстайлу. Кончено, инспекции IDE можно выключить или \"подавить\" или писать на Kotlin не в IntelliJ и прочим образом быть сами себе злыми Буратино. Но давайте всё же попробуем следующий трюк: Вот так и получится, что мы объявили top-level функцию с именем MyController и возвращаемым типом MyController. Получилось как раз что-то вроде внешнего конструктора. И даже IntelliJ неожиданно перестанет ругаться на несоответствие код-стайлу, потому что, оказывается, это известный паттерн для Kotlin, который используют серьезные проекты, например Google в androidx. Чтобы пользоваться таким API из Java, для тех библиотек, которые не позиционируются как Kotlin-only, стоит применить аннотацию @JvmName. Тогда вместо странного добавив @JvmName(\"create\")на определение функции MyController. Это может быть разумно применять как раз в случае, если MyController - это интерфейс, а его реализаций несколько и/или они напрямую не должны быть видимы клиентам. Вот и получается, что можно подложить функцию вместо конструктора, но ведет она себя точно как конструктор. Конечно, можно написать функцию MyController, которая возвращает какой-то другой тип, но тогда ворнинг никуда не уйдёт да и клиенты такой функции будут неприятно удивлены. Вполне вероятно, вы могли сталкиваться с этим паттерном, так как он действительно применяется, например в Jetpack Compose. Так что давайте рассмотрим что-нибудь более экзотическое. При таком раскладе строка из листинга (1) опять же будет валидна. Чтобы разобраться, почему это работает, давайте попробуем не пользоваться сахарным синтаксисом оператора invoke и явно обратиться к компаньону. Тогда наш \"конструктор\" сразу раскрывается в следующее: Такую штуку можно применять в случае, когда мы хотим полностью контролировать создание экземпляров класса, например, применять глобальный пул объектов или кэш. Но при этом не хотим загрязнять места создания лишними деталями про это. Так же, если мы вдруг решим отказаться от такого делегированного создания совсем, то все места вызова можно будет оставить без изменений - в тексте ничего не поменяется, а компилятор станет резолвить обычный конструктор для класса или top-level функцию для интерфейса вместо Companion.invoke(). Хм, форма слегка другая, но ничего полезного в таком виде мы не выигрываем. Ещё и импорт функции invoke добавится в каждом месте вызова. Тем не менее тут важен факт, что оператор invoke не обязательно должен быть членом объекта-компаньона. Из чего следует ещё один занимательный вариант... Перед тем, как перейти к очередному извращённому способу прикинуться конструктором, скажу только, что тут важно добавить контекста к листингу (1). Дополним его так: При таком раскладе в контексте Context у нас будет резолвится оператор invoke сразу с двумя ресиверами - с MyController.Companion и c context: Context. Давайте избавимся от синтаксического сахара в листинге (1') чтобы понять, что на самом деле происходит: Смысл у этого трюка в том, что мы можем делегировать создание объектов в рамках некоторого локального (в противоположность глобальному из прошлых кейсов) контекста. И можем это сделать, просто вводя контекст как ресивер, сохраняя при этом само создание объекта выглядящим как обычный вызов конструктора. Возможную реализацию контекста и компаньона, в том числе возможность передавать параметры отличные от String, я здесь не привожу, опасаясь переусложнять статью техническими деталями сомнительного узкоспециализированного подхода. В прочем, если кому-то это станет интересно, реальный пример применения такой штуки можно увидеть в коде yatagan. Было бы намного проще, если оставить операторinvoke в самом компаньоне, как в случае 2, при этом добавив ему ресивер Context, а не наоборот, как сделали мы - уносить invoke в Context и давать ему ресивер Factory<T>,  который реализуется компаньоном. WTF?! Но, к сожалению, тогда Kotlin бы не дал нам это вызвать так, как мы хотим. Дело в том, что один ресивер должен быть явный, а второй - неявный, и их нельзя менять местами. Обидно, что нужная фича для решения этого в Kotlin уже есть, причем давно, но только в виде прототипа - Context receivers. С ней мы бы могли написать код: И тогда листинг (1') был бы валиден и мир пони печенье. Но, Context receiver застрял в прототипном состоянии, и непонятно, когда его доведут до ума. Стоит отметить, что такой паттерн можно заменить на простую top-level функцию fun Context.MyController(name: String): MyController. То есть на случай 1, но с ресивером. Но тогда эта штука не сможет получить доступ к приватному конструктору класса, и будет иметь смысл только для публичного API библиотеки. Что ж, мы с вами постарались разобраться, как прикидываться конструктором в Kotlin и как из этого извлекать какую-то пользу. Прошлись по случаям от менее упоротого до в меру упоротого и очень упоротого: Функции верхнего уровня с именем, совпадающим с именем класса/интерфейса. Полезно в API библиотек для создания экземпляров публичных интерфейсов, когда не хочешь открывать имена/детали реализаций. Используется на практике. Оператор Companion.invoke(). Может быть полезно для управления созданием объектов (пул, кэширование, ...) в статическом контексте. Оператор receiver(Context) Companion.invoke() (в синтаксисе context receivers, без них дело усложняется). Может быть полезно для управления созданием объектов (пул, кэширование, ...) в локальном Context. Следующая статья должна будет называться \"Как перестать прикидываться конструктором в Kotlin и начать жить\", но её я вряд ли напишу. Пишите в комментариях, что думаете про описанные способы, пользовались ли чем-то отсюда или только рядом стояли (я осуждать не буду). Или вдруг кто докинет чего от себя. Спасибо за внимание!",
    "41": "PGConf.Russia 2024 Стартует уже совсем скоро: 8 апреля, а завершится 9-го. Можно просмотреть расписание и список докладов. В этом году даты проведения конференции совпадают с завершением релизного цикла 17 версии. 8 апреля в 15:00 MSK прием изменений завершится. А мы сможем обсудить, что ожидать в осеннем релизе. Здесь и инкрементальное резервное копирование, изменения в логической репликации, триггер на подключение и наверняка появится что-то любопытное в начале апреля. Следующий доклад в конференц-зале - Про-Shardman, Алексей Борщев и П. Конотопов. Он первый среди \"продуктовых\" - это показывает, какое значение придают этой новой СУБД в компании, да и внешний интерес к ней действительно огромный. Докладчики расскажут о Шардмане с точки зрения SQL разработчиков и архитекторов БД: как адаптировать схему БД для работы на шардах. Конечно, будет рассказано и о других новинках компании: доклад Pooler, load balancer, proxy. Что их объединяет? Артём Галонский, pgpro_rp (приоритизация ресурсов, доложит Александр Попов) и других. Миграция с Oracle APEX/Forms на Эльбрус e2k. Проблемы. Производительность, Константин Ващенков (\"Ростех\" / НПФ ПАО \"Ростелеком\"). Вообще представителей компаний будет много: Тинькофф, Яндекс, Softpoint, Скала^р, PGMechanix, ЕДИНЫЙ ЦУПИС, ИнфоСофт, ВНИИЖТ, Sibedge, Конвертум, Всегда.Да, Сима-ленд, ГНИВЦ, Quillis, ФОРС, Хи-квадрат, ООО Кваллис. Их как раз больше в Чехове. Традиционно будет и астрономический доклад: Открытие Вселенной силой мысли, Алексей Семихатов. Вечеринка будет 8-го в 19:30. Напоминаем, что это лишь часть нового в PostgreSQL 17, а обо всём вместе можно будет узнать из доклада, с анонса которого начинается этот выпуск. Часть 4 начинается с \"козыря\": регистр динамической общей памяти. Часть 1. 2023-07 (ru / en), Часть 2. 2023-09 (ru / en), Часть 3. 2023-11 (ru / en). Выложены в открытый доступ записи докладов на PGConf.Russia 2023 Это 43 доклада. В том числе теперь доступны презентации иностранных спикеров и тех, кто выступал онлайн. Они собраны в два плейлиста на YouTube-канале компании. День первый и День второй. Предложения ещё принимаются. Это онлайновая конференция, раньше называлась Citus Con. Приятна тем, что доклады выкладывают в открытый доступ. Прошлогодних было 37, позапрошлогодних - 38. Доклады надо уложить в 25 минут. Конференция пройдёт 11-13 июня. Пройдёт тоже в июне - 27-28 в Восточно-Швейцарском Университете Прикладных Наук (OST Eastern Switzerland University of Applied Sciences), кампус в Рапперсвиллле. Доклады принимаются до 8 апреля. Дэвид Уилер (David E. Wheeler, основатель PGXN, перешедший в Tembo) в своём совсем не теоретическом блоге с названием Just a Theory делится впечатлениями от состоявшегося мини-саммита по расширениям. И выложил видео и слайды. Полноценный саммит будет называться Postgres Extension Ecosystem Mini-Summit и должен состояться 17 апреля. Иэн Стантон (Ian Stanton) - технический основатель Tembo (что-то это словосочетание founding engeneer стало часто попадаться в последнее время). На мини-саммите он сказал так: сначала мы смотрели на PGXN - мы его любим, его поддерживает сообщество с 2011, но ведь там только исходные коды. А нам нужны бинарники. Кроме того, с 2012 PGXN не шибко интенсивно развивался. Мы начали создавать Trunk, вдохновляясь PGXN, Apt и Yum. К нему прикрутили интерфейс командной строки. Теперь парочкой команд расширение создаётся и устанавливается. Trunk и CLI написаны на Rust и используют Docker. Как сообщество отнесётся к проекту - пока непонятно. Эта конференция состоится во Вьетнаме, в Ханое 9 апреля. Рабочий язык английский. Это целый набор мероприятий. Там в том числе будет PostgreSQL Day, посвященный эффективности и производительности. Объявлено, что выступать там будут: Крис Трейверс (Chris Travers, приезжал и к нам на PGConf.Russia 2023), но уже в другом качестве: теперь он главный инженер (principal engineer) в компании DeliveryHero (Сингапур), Жюльен Руо (Julien Rouhaud, тоже знаком нам по PGConf.Russia, но теперь он, оказывается, технический основатель (founding engineer) в интересной компании Nile). Знакомое имя в оргкомитете - Умейр Шахид (Umair Shahid, основатель Stormatics). Пройдёт 16 апреля в отеле \"Ренессанс Минск\". Язык мероприятия русский. На сайте PostgreSQL.org написано, что мероприятие проводится в соответствии с рекомендациями сообщества PostgreSQL и помогает его развитию. Есть англоязычная версия сайта. В программном комитете: опять мы видим Умейра Шахида; а также там хорошо нам знакомый Максим Милютин - контрибьютор PostgreSQL и openGauss; Михаил Гольдберг - организатор конференций PGDay lsrael (с 2017). В ДМК Пресс на днях выйдет обновлённое до 16-й версии издание книжки Егора Рогова! На нашем сайте доступна в PDF. В этом издании не только отражены изменения, произошедшие в версии PostgreSQL 16, но и учтены замечания читателей и исправлены опечатки. Это программа стажировок в Postgres Professional. Работать можно по 6 направлениям: разработчик C. На сайте сформулированы требования по каждому из направлений и навыки, которые стажёр приобретёт. 18-19 апреля в Университете ИТМО (СПб, ул. Гастелло, 12) пройдет профессиональная сертификация «Администрирование PostgreSQL». Тестирование позволяет подтвердить уровень знаний, получить преимущества при поиске работы и узнать зону роста и темы, которые стоит изучить. Тест можно пройти по PostgreSQL 10 (DBA2, DBA3, QPT) и 13 (DBA1, DBA2, DBA3, QPT). Специалисты с сертификатом уровня «Эксперт» по PostgreSQL 10 смогут сдать переходный тест Expert 10 → 13. Количество слотов ограничено. Записываться здесь. На сайте Redgate в их блоге Simple Talk Джо Селко (Joe Celko) предлагает ликбез по логике и \"логике\" в SQL. Джо несколько лет подряд выигрывал DBMS Magazine Reader's Choice Award, написал 8 книг по SQL. В этой статье он говорит о формальной логике. Конечно, SQL построена на трёхзначной логике, которая штука специфическая. Но SQL это вообще не логика - утверждает он. И цитирует пионера реляционных баз Дэйва МакГоверна (Dave McGoveran). По той причине, что если бы это была логическая система, там были бы правила вывода (inference rules). Дальше он говорит о странностях SQL. Например, DDL работают с UNKNOWN и FALSE  как со сходными, а для DML сходныеUNKNOWN и TRUE . Далее обсуждает конъюнкцию и дизъюнкцию, фразы с BETWEEN, подзапросы с [NOT] EXIST, поиск по выражению (pattern matching). И всё с историей вопроса. В конце поёт дифирамбы декларативным языкам и SQL в частности. Компания Stormatics не то, чтобы занимается образовательной деятельностью самой по себе, но ликбез, азы, жанр \"<Х> для чайников\" обильно представлены в их блогах. Случаются и deep dive, но реже. Вот примеры \"андерстендингов\": Меня заинтересовала статья по итогам вебинара Криса Трейверса (опять он! :)) NUMA policies and their impact on PostgreSQL. Там он представлен как главный (principal) консультант по PostgreSQL. Это, вроде и не ликбез, а дип дайв - о функционировании контроллеров в разных режимах, доступу к памяти, но совсем коротенький - выжимка из 5-минутного видео. Мы сами как-то публиковали на тему NUMA, но это был не ликбез, а глубокий анализ ситуации, которую разрулили Михаил Жилин и Пётр Петров. В названии нет нумы, но она там есть: Битвы на территории ZFS. Саймон покинул этот мир 26-го марта. Очень яркая личность, активнейший участник сообщества. Он основал 2ndQuadrant и уже только этим вошёл бы в историю Postgres. Он отвечал за конференции UK PostgreSQL пока не передал в прошлом году ответственность PostgreSQL Europe. Последнее выступление было в Праге - его открывающая речь на PostgreSQL Conference Europe 2023, она доступна на YouTube. Саймон с невероятной энергией пробивал в сообщество фичи, которые считал важными для сообщества и для собственной компании. Одному из эпизодов мы посвятили отдельную статью: Битва при MERGE. Хроника с выводами и моралью. Это был целый триллер. Артура я знаю и могу сказать о нём только хорошее. Но пусть уж он сам говорит: Я начинал с MS SQL Server. После 6 лет работы с ним я стал разработчиком в  Postgres Professional. Там я активно работал над Postgres Pro , в том числе над новыми фичами. Некоторые из них и из исправленных багов попали и в основную ветку PostgreSQL. Еще я был контрибьютором различных опенсорсных расширений, включая метод доступа RUM, pg_variables для сессионных переменных, работал над pg_probackup. Опыт и знания, полученные мною в компании, оказались очень ценными, я очень благодарен за возможность поработать в этой компании. Сейчас я поддерживаю pg_repack, которым мы активно пользуемся на моей новой работе. Одно из любимых расширений - postgres_fdw. Мы его используем для самых разных целей, оно облегчает бесшовный доступ к удалённым данным. Из приложений - Робер Хаас (Robert Haas) недавно закоммитил в PostgreSQL 17 поддержку инкрементального бэкапа (см. об этом, напр., в обзоре Лузанова - примечание редакции). Ещё очень хочется поддержки UNDO-логов. Жаль, что развитие zheap притормозилось. В OrioleDB реализованы UNDO-логи, но там надо патчить PostgreSQL, а это удобно не во всех окружениях. Сейчас я в Берлине, работаю в Adjust. А вырос я в посёлке в Башкортостане, окружённом зеленью, вдали от городской суеты. Тогда я и не думал переселяться в большие города. Теперь мне трудно представить себя живущим вне большого города. В Берлине я поселился 3 года назад, до того год работал в Токио. Кристоф Берг (Christoph Berd, Cybertec) пишет: делайте бэкапы, используйте транзакции. Ну а если всё же случилось непоправимое, его можно поправить - только бы (АВТО)VACUUM не успел бы пройтись по таблице! Поправить можно при помощи расширения, которое он сам и поддерживает. Называется pg_dirtyread. С ним можно читать таблицу так, как будто в ней нет строк, помеченных как удалённые. Но если это не помогло (мёртвые строки уже вычистили), то думать о суициде рано: можно попробовать извлечь информацию из WAL, прочитав FPW (Full Page Writes). Но это уже средствами утилиты pg_waldump. Сохранить данные в файл. Прикинув правильные LSN (Log Sequence Number - последовательный номер в журнале), убрать лишнее. И создать новую табличку. Название похожее, но речь о другом. На линуксовой конференции SCaLE 21x в Пасадене в марте этого года Джимми Эйнджелэкос (Jimmy Angelacos) рассказывает о реальной катастрофе, последствия которой он устранял. Тогда грохнулся жёсткий диск. Данные в основном сохранились, но компания, восстанавливавшая диск, вернула их в виде вороха файлов со случайными названиями. Нужно было собрать из них базу с осмысленной иерархией файлов. Видео его выступления выложено в youtube, и можно полистать PDF. Статье лет 5 и написана вовсе не ИТ-гуру. Меня заинтересовала в ней тема: проблемы крушения ZFS и LXD обсуждают в статьях и комментариях не так уж часто. Stan's blog ведёт, соответственно, некто Stanislas, поддерживающий некоторую соцсеть mstdn.io на базе мастодонта - довольно популярного, между прочим, среди постгресистов. Спасением данных он занимался самостоятельно и стратегию выбрал такую: убить часть данных, чтобы не потерять самое важное в базе. С этой программой-минимум он, похоже справился. А вообще-то не так давно - год назад - Евгений Бредня, возглавляющий службу поддержки в Postgres Professional, делал доклад на PGConf.Russia 2023: На этой странице есть видео, можно листать PDF. В предисловии Евгений предупреждает: ● Нет и не может быть готовых рецептов для восстановления  повреждённых данных, иначе они давно уже были бы реализованы в  виде утилит. ● Требует высокой квалификации: нужно знать как работает постгрес,  как работает MVCC, где и в какой форме хранятся данные на диске,  глубокие и широкие знания в предметной области. ● Долгий и кропотливый труд, который далеко не всегда приводит к  успеху. ● Мотивация: поделиться опытом, который можно будет использовать. ● Любое неверное движение приведёт к частичной или полной  потере данных. ● Не следует даже пытаться делать что-либо без полного  понимания последствий своих действий, лучше позовите на помощь. Автор представился как@Loxmatiymamont,ну Мамонт так Мамонт, почти Слон. Важная часть статьи: Мамонт сразу договаривается о терминах, об иерархии сущностей в DBaaS (снизу вверх): База данных -> Инстанс -> Кластер -> Проект. Последний и отличает разрабатываемый DBaaS от иерархий из учебников. Он создаётся для управления группами кластеров. У проекта есть владелец, который создаёт внутренних пользователей и наделяет их правами доступа. Сервис должен быть гибким. В этом сервисе: можно динамически менять выделенные инстансам мощности, включая размер дисков. Теоретически можно даже на ходу изменить количество процессоров, но это приведёт к перезагрузке ноды. Это особенность OpenStack. И, что удобно, конфигурировать можно как всех участников кластера разом, так и отдельные инстансы. Ориентация на OpenStack связана с ситуацией на российском облачном рынке. Репликация есть и физическая, и логическая. Все эти копии отправляются на S3. Есть 15-минутное демо-видео полугодовалой давности. Мамонт сразу говорит, что продукт в процессе разработки, поэтому хочется узнать мнение читателей и послушать пожелания. Security lessons from liblzma - такова тема ветки, которую стартовал Брюс Момджан в рассылке hackers. Андрес Фройнд (Andres Freund) проделал потрясающую работу по обнаружению дыры (backdoor), но надо и нам что-то делать, чтобы уже на 100% обезопасить и себя от аналогичных попыток  - примерно так говорят в сообществе. Мы следим с замиранием сердца за развитием событий и расскажем об этом в следующем выпуске. В Postgresso мы регулярно пишем об очередных пятнецах Райана Буза (Ryan Booz), при этом упоминая нередко, что сначала постгресовые пятнецы появились не у него, а у Шона Томаса (Shaun M. Thomas, сейчас в EDB) - для исторической справедливости. PG Phridays (в отличие от бузовских PGSQL Phridays) неизменно фигурировали в прошедшем времени. И время это 2017-й. Вдруг оказалось, что Шон (под влиянием успехов Райана или сам по себе) решил возобновить их на своём сайте BonesMoses.org. И вот они - за 15, 22 и 29 марта: Это любопытная статья. Во многом суждения его нестандартны. Направления переопределения высокой доступности такие: движемся дальше. Во 2-м пункте Шон формулирует проблему так: не случайно движок хранения невероятно надёжен и абсолютно не подготовлен к чему-либо другому (incredibly robust and entirely unequipped for anything else). Здесь он говорит об облачном хранении. Вообще пишет он довольно ярким языком. А что нужно-то? Есть Postgres pluggable storage API. Но Шону этот путь не кажется перспективным. Он говорит об ощутимом шоке в сообществе, когда появилась Aurora for Postgres (и советует посмотреть видео Deep Dive on Amazon Aurora). Потом подоспели NeonDB, AlloyDB, которые по-своему решают проблему хранения для кластера. И советует почитать концептуальную статью Хейкки Линнакангаса (Heikki Linnakangas, основывал Neon вместе со Стасом Кельвичем), где описаны архитектурные решения Neon по части хранения. Из которой, между прочим, можно узнать о новых бессерверных базах - DBaaS Socrates и PolarDB. В 3-м пункте Шон предлагает свою собственную концепцию, рассказывая, как соединить преимущества шардинга и модели MapReduce. Название - игра слов: как бы грязная постгресовая ветошь. Но RAG это и Retrieval Augmented Generation - способ работы с LLM. Но это не просто ещё одна статья о pg_vector. И играться он будет даже не с GPT. В качестве модели берётся некая  Mixtral 7B, умеренная в своих аппетитах. А поскольку RAG многоступенчатый процесс, то в статье немало кусков кода самого разного свойства. Статья Джонатана Каца (Jonathan Katz, Amazon RDS) непосредственно разрабатывавшего это расширение. Вообще pgvector приспособлен к секционированию, HNSW-индексы создаются на всех секциях одной командой. Но есть нюансы при исполнении запросов и важные настройки индексов. А для распределённых транзакций Джонатан использует postgres_fdw. В облаке он организует 3 узла, настраивает их, генерит при помощи PL/pgSQL-функции на двух рабочих нодах по 2.5 млн строк, потом запускает SELECT count(*) и смотрит, что происходит. Убеждается в асинхронном выполнении и дальше ищет 10 ближайших соседей некоторого вектора. И находит, причём на разных нодах. Но ещё и запускает бенчмарк ANN Benchmarks. Ссылается на свою статью о масштабируемости pgvector с рекордно длинным названием: Accelerate HNSW indexing and searching with pgvector on Amazon Aurora PostgreSQL-compatible edition and Amazon RDS for PostgreSQL. На этом пока всё. Собирайтесь на PGConf.Russia 2024 или следите за новостями с неё.",
    "42": "При реализации Cosmo Router, open-source замена Apollo Router, мы столкнулись с проблемой поддержания нашего кода для решения проблемы N+1. Реализация маршрутизатора для федеративных служб GraphQL в значительной степени зависит от возможности группировать вложенные запросы GraphQL для сокращения числа запросов к подграфам. Чтобы решить эту проблему, мы разработали новый алгоритм, который решает проблему N+1 более эффективно и проще для поддержания, чем наше предыдущее решение, которое было основано на шаблоне DataLoader, обычно используемом в сообществе GraphQL. Вместо разрешения сначала по глубине, мы загружаем данные сначала по ширине, что позволяет нам сократить параллелизм с O(N^2) до O(1) и улучшить производительность до 5 раз, сокращая сложность кода. Если вы заинтересованы в проверке кода, вы можете найти его на GitHub. Я также провел лекцию на эту тему на GraphQL Conf 2023, которую вы можете посмотреть здесь: https://youtu.be/vWQYI5fNytM Проблема N+1 - это общая проблема в GraphQL, которая возникает, когда у вас есть список элементов, и вам нужно получить дополнительные данные для каждого элемента в списке. Давайте рассмотрим пример, чтобы проиллюстрировать эту проблему: Откуда взялось название проблемы N+1? +1 относится к первому запросу на получение topProducts из подграфа Products. N относится к числу дополнительных запросов, которые требуются для получения связанных данных для каждого продукта. Давайте пройдемся по шагам разрешения этого запроса, чтобы увидеть, что это означает на практике. Допустим, поле topProducts возвращает 3 продукта. Мы должны сделать 3 дополнительных запроса, чтобы получить запасы для каждого продукта из подграфа Inventory. Мы также должны сделать 3 дополнительных запроса, чтобы получить обзоры для каждого продукта из подграфа Reviews. Мы просто предполагаем, что у каждого продукта есть 3 обзора. Для каждого обзора мы должны сделать 1 дополнительный запрос, чтобы получить автора из подграфа Accounts. Таким образом, в общей сложности мы должны сделать 1 + 3 + 3 + 3 * 3 = 16 запросов, чтобы разрешить этот запрос. Если вы внимательно посмотрите, вы увидите, что количество запросов растет экспоненциально с глубиной запроса и количеством элементов в списке, где вложенные списки усугубляют проблему, как в нашем примере, где у каждого продукта есть список обзоров. Что, если у каждого обзора был список комментариев, а у каждого комментария - список лайков? Вы видите, куда это ведет. Шаблон DataLoader - это общее решение для решения проблемы N+1 в GraphQL. Он основан на идее группировки запросов в списках для сокращения числа запросов. Что замечательно в GraphQL, так это то, что он позволяет нам группировать запросы по умолчанию. С REST API, вам нужно явно реализовать конечные точки группировки, но с GraphQL, вы можете легко объединить несколько запросов в один запрос. Кроме того, Федерация делает группировку еще проще, потому что хорошо известное поле _entities для извлечения \"Сущностей\" поддерживает список представлений в качестве входных данных. Это означает, что извлечение сущностей по их ключам позволяет вам группировать запросы по умолчанию. Все, что нам нужно сделать, это убедиться, что мы действительно группируем запросы в списках вместе, чтобы использовать эту функцию, что и делает шаблон DataLoader. Используя шаблон DataLoader, мы можем сократить количество запросов с 16 до 4, один запрос для загрузки topProducts, один для загрузки информации о запасах, один для загрузки обзоров и один для загрузки авторов. Первый шаг - это получить topProducts из подграфа Products. Вот как выглядит запрос: Теперь мы пройдемся по всем продуктам и загрузим связанные данные для каждого продукта. Давайте сделаем это для первого продукта. Теперь нам нужно получить запасы для этого продукта из подграфа Inventory. Хотя этот запрос работает, он не позволяет нам сгруппировать два других продукта-друга. Давайте изменим запрос, чтобы разрешить группировку, используя переменные вместо встраивания представлений непосредственно в запрос: Этот запрос будет точно таким же для всех продуктов. Единственное отличие будет в значении переменной $representations. Это позволяет нам реализовать упрощенную версию шаблона DataLoader. Вот как это может выглядеть: Это функция, которая принимает запрос и список представлений в качестве входных данных. Затем функция будет хешировать запрос и объединять все представления для одного и того же запроса в один запрос. Если есть дублирующиеся представления, мы удалим дубликаты перед созданием объекта переменных. Она будет делать запрос для каждого уникального запроса и возвращать \"негруппированные\" результаты каждому вызывающему. Отлично! Мы реализовали упрощенную версию шаблона DataLoader и использовали его для загрузки данных о запасах каждого товара. Теперь давайте также загрузим отзывы. Прогоните это через нашу функцию load, и мы получим отзывы для каждого товара. Предположим, что у каждого товара есть 3 отзыва. Вот как может выглядеть ответ: Далее мы заходим в каждый обзор и загружаем автора из подграфа Accounts. Теперь нам нужно найти автора этой рецензии из подграфа Accounts. И снова мы делаем это для всех друзей и объединяем запросы для одного и того же запроса. Наконец, мы закончили загрузку всех данных и можем вывести JSON-ответ в соответствии со спецификацией GraphQL и вернуть его клиенту. На первый взгляд, шаблон DataLoader кажется отличным ответом для решения проблемы N+1. В конце концов, он значительно сокращает количество запросов с 16 до 4, так в чем может быть проблема? Есть две основные проблемы с шаблоном DataLoader. Во-первых, он делает компромисс между производительностью и сложностью кода. Хотя мы сокращаем количество запросов, мы значительно увеличиваем сложность нашего кода. Хотя это не видно в однопоточной среде, такой как Node.js, многопоточные среды, такие как Go или Rust, будут испытывать трудности с эффективным использованием этого шаблона. Я вернусь к этому позже. Во-вторых, хотя шаблон DataLoader сокращает количество запросов, он экспоненциально увеличивает параллелизм нашего кода. С каждым уровнем вложенности и каждым элементом в списке мы экспоненциально увеличиваем количество одновременных запросов, которые выполняются в \"DataLoader\". Это сводится к фундаментальному недостатку в шаблоне DataLoader. Для того чтобы иметь возможность группировать запросы нескольких полей-друзей, все поля-друзья должны \"присоединиться\" к пакету одновременно. Это означает, что мы должны одновременно пройти через все поля-друзья и заставить их вызвать функцию load DataLoader в одно и то же время. Как только все поля-друзья присоединились к этому пакету, они блокируются, пока пакет не будет разрешен. Но это становится еще хуже с вложенными списками. Для нашего корневого списка мы должны разветвиться в 3 параллельные операции, чтобы загрузить данные для всех 3 продуктов, используя шаблон DataLoader. Так что у нас есть параллелизм на 4, если мы также учитываем корневое поле. Но поскольку мы загружаем не только информацию о запасах, но и обзоры для каждого продукта, мы должны добавить еще 3 параллельные операции для каждого продукта. С этим мы находимся на уровне параллелизма 7. Но мы еще не закончили. Как мы обсуждали, мы предполагаем, что получим обратно 3 обзора для каждого продукта. Чтобы загрузить автора для каждого обзора, нам, следовательно, нужно разветвиться в 9 параллельных операций для каждого обзора. С этим мы находимся на уровне параллелизма 16. Чтобы проиллюстрировать эту проблему, вот визуализация из доклада. Цифры немного отличаются, но проблема остается той же. Как вы можете видеть, группировка зависит от возможности одновременного вызова функции load для всех полей-друзей. Как я упоминал ранее, есть не только дополнительные затраты на параллелизм, но и для неоднопоточных сред, таких как Go, Rust, Java и т.д., необходимость одновременного разрешения полей-друзей представляет собой еще одну проблему. Федерация, но также и другие композитные стили GraphQL позволяют вам определять зависимости между полями. Это означает, что вы можете определить, что поле @requires другое поле, которое должно быть разрешено, прежде чем оно сможет быть разрешено само по себе. Например, у вас может быть объект пользователя, который содержит поле id. С помощью поля id вы можете получить улицу и номер дома пользователя. Как только у вас есть эти два, вы можете получить полный адрес пользователя, который зависит от других двух полей. В худшем случае может произойти следующее. У вас есть список объектов пользователей-друзей. Для каждого из них вы загружаете группой улицу и номер дома. Вы делаете это одновременно. Как только это сделано, вы должны объединить результаты с исходными объектами пользователя. Затем вы снова проходите через каждый объект пользователя, одновременно, и загружаете группой полный адрес. Добавляя параллелизм к этой операции, мы должны синхронизировать результаты, мы должны убедиться, что мы не читаем и не записываем одни и те же объекты одновременно, или мы должны скопировать все объекты, когда мы разветвляемся в параллельные операции, а затем убедиться, что когда мы объединяем результаты, мы не записываем в одни и те же объекты одновременно. В любом случае нам придется копировать множество объектов, синхронизировать параллельные операции или даже использовать блокировки и мьютексы, чтобы исключить одновременное чтение и запись одних и тех же объектов. Все это может не только усложнить наш код, но и негативно сказаться на производительности. Если у вас есть опыт работы с сильно параллельным кодом, вы можете знать, что отлаживать параллельный код очень сложно. Если отладчик перескакивает между потоками, трудно проследить за ходом выполнения кода. Именно здесь печать в консоль снова становится полезным инструментом отладки. Тем не менее, было бы здорово избавиться от всего этого параллелизма. Что, если бы мы могли писать простой синхронный код, свободный от блокировок, мьютексов, потоков и других примитивов параллелизма? И именно это мы и сделали! Большинство, если не все серверы GraphQL, разрешают поля в глубину. Это означает, что они разрешают поле и все его подполя, прежде чем разрешают следующее поле-друга. Таким образом, в списке элементов они исчерпывающе разрешают один элемент за другим. Если мы говорим о деревьях, они разрешают самую левую ветвь, пока не достигнут листового узла, затем они разрешают следующую ветвь и так далее, пока не достигнут самой правой ветви. Это кажется естественным, потому что так мы бы \"печатали\" JSON в буфер. Вы открываете первую фигурную скобку, печатаете первое поле, открываете другую фигурную скобку, чтобы начать объект, печатаете содержимое объекта, закрываете фигурную скобку и так далее. Так как же мы можем избавиться от необходимости разрешать поля-друзья одновременно для включения группировки? Решение состоит в том, чтобы разделить резолвер на две части. Сначала мы проходим через \"План запроса\" в ширину, загружаем все данные, которые нам нужны из подграфов, и объединяем результаты в один объект JSON. Во-вторых, мы проходим через объединенный объект JSON в глубину и печатаем ответ JSON в буфер в соответствии с запросом GraphQL от клиента. Поскольку это может быть немного абстрактно, давайте пройдемся по примеру шаг за шагом. Сначала мы загружаем topProducts из подграфа Products, как мы делали это раньше. Ответ все еще выглядит так: Затем мы проходим \"в ширину\" в список продуктов. Это дает нам то, что мы называем \"элементами\", список объектов. У нас теперь есть список продуктов, поэтому мы можем загрузить информацию о запасах для всех продуктов сразу. Давайте вспомним запрос, который нам нужен для загрузки информации о запасах: Мы можем просто вызвать эту операцию с нашим списком элементов в качестве входных данных, исключая, конечно, поле name. Кроме того, вы можете видеть, что нам больше не нужен шаблон DataLoader. Нам не нужен параллелизм для группировки запросов для полей-друзей. Все, что мы делаем, это вызываем подграф Inventory с списком представлений продуктов в качестве входных данных. Алгоритм объединения довольно прост. На основе позиции продукта в списке мы объединяем информацию о запасах из ответа _entities в объект продукта. Мы повторяем этот процесс для обзоров. Давайте предположим, что у нас есть 3 обзора для каждого продукта. Это вернет массив сущностей с 3 обзорами каждый. Давайте сделаем то же самое, что мы делали раньше, и объединим обзоры в каждый из элементов продукта. Отлично! Мы получили продукты, информацию о запасах и обзоры для каждого продукта. Как теперь мы разрешим поле author для каждого обзора? Первый шаг - это пройти \"в ширину\" в список обзоров. Наши \"элементы\" тогда будут выглядеть так: Это просто список обзоров, отлично. Затем мы переходим к authors, снова используя обход в ширину, что приводит нас к списку пользователей. Теперь мы можем загрузить поле name для всех пользователей сразу, без параллелизма, просто одним запросом к подграфу Accounts. Теперь все, что нам нужно сделать, это \"подняться\" по дереву и объединить разрешенные элементы в их родительские объекты. Здесь есть одна важная вещь, которую нужно отметить. Когда мы переходим в список элементов внутри элемента, нам нужно отметить, какие индексы дочерних элементов принадлежат какому родительскому элементу. Возможно, что у нас есть списки неравной длины внутри списка элементов, поэтому мы бы нарушили порядок, если бы не отслеживали индексы дочерних элементов. Всего девять обзоров с информацией об авторе в каждом обзоре. Наконец, мы проходим по дереву в последний раз и объединяем отзывы с товарами. Наш результирующий объект будет выглядеть следующим образом: И это все! Теперь мы загрузили все данные, которые нам нужны для формирования окончательного ответа JSON. Поскольку мы могли загрузить ненужные данные, например, обязательные поля, и мы не учитываем псевдонимы, это необходимый шаг, чтобы гарантировать, что наш ответ JSON имеет точно такую форму, которую запросил клиент. Это был взгляд на алгоритм с очень низкой высоты. Давайте немного отдалимся и посмотрим на этот алгоритм с более высокого уровня. Вы заметите две вещи на этой визуализации. Он показывает 3 потока вместо одного. Это потому, что мы параллелизируем получение информации о запасах и обзоры. Это возможно, потому что они находятся на одном уровне в дереве и зависят только от продуктов. Второе, что вы заметите, это то, что кроме параллелизации двух вышеупомянутых операций, мы вообще не используем параллелизм. Ключевое отличие разрешения полей в ширину, а не в глубину, заключается в том, что мы всегда можем автоматически группировать запросы для полей-друзей, так как у нас всегда есть список элементов (друзей), вместо того чтобы посещать каждый элемент (друга) ветка за веткой (в глубину) параллельно. Говоря о параллелизме и параллелизации, мы сделали еще одно важное наблюдение относительно параллельных выборок на одном уровне в дереве, как мы делали это для информации о запасах и обзорах. Во время нашего исследования мы изначально попытались параллелизовать два пакета и немедленно объединить результаты в элементы. Это оказалось плохой идеей, так как это потребовало бы от нас синхронизации объединения, например, с использованием мьютекса. Это сделало код более сложным и негативно повлияло на производительность. Вместо этого мы нашли лучшее решение. Когда мы рендерим входные данные для каждого из пакетов, мы только читаем из элементов, поэтому это безопасно делать параллельно. Вместо того чтобы немедленно объединять результаты в элементы, мы предварительно выделяем один набор результатов для каждого пакета, чтобы временно хранить результаты для каждого пакета. Затем мы ждем, пока все параллелизованные пакеты не завершатся, и объединяем результаты из наборов результатов в элементы в основном потоке. Таким образом, нам вообще не нужно синхронизироваться и мы можем полностью обойтись без блокировок, хотя мы используем параллелизм для получения пакетов. Теперь, когда мы подробно рассмотрели алгоритм, давайте поговорим о преимуществах загрузки данных в ширину. Мы снова можем легко отлаживать наш код, и вся реализация намного проще для понимания и поддержки. На самом деле, наша первоначальная реализация не была полностью завершена, но мы смогли быстро исправить и расширить ее, так как код был легким для понимания. Как побочный эффект, хотя это не было нашей основной целью, мы видели улучшения производительности до 5 раз по сравнению с нашей предыдущей реализацией, использующей шаблон DataLoader. На самом деле, разрыв в производительности увеличивается с увеличением количества элементов в списках и уровня вложенности, что отлично иллюстрирует проблему с шаблоном DataLoader. Шаблон DataLoader - отличное решение для решения проблемы N+1. Это отличное решение для улучшения проблемы N+1, но, как мы видели, у него есть свои ограничения и он может стать узким местом. Поскольку мы строим маршрутизатор для федеративного GraphQL, крайне важно сохранять наш код поддерживаемым и с низким накладным расходом на производительность. Возможность сокращения параллелизма нашего кода - огромное преимущество, особенно когда мы можем сократить параллелизм с O(N^2) до O(1). Я думаю, что мы все должны быть осведомлены о компромиссах, которые мы делаем, используя шаблон DataLoader. Существуют фреймворки, такие как Grafast, которые предлагают совершенно новые подходы к разрешению операций GraphQL. Это хорошее время, чтобы поставить под сомнение статус-кво и посмотреть, можем ли мы применить эти знания и применить их к нашему собственному серверу и фреймворкам GraphQL. Следует ли фреймворкам GraphQL рассмотреть возможность реализации загрузки данных в ширину? Стоит ли вам рассмотреть возможность реализации пакетной загрузки как гражданина первого класса на вашем сервере GraphQL? Я не знаю ответа, но я думаю, что правильно задать эти вопросы. Мне было бы интересно услышать ваши мысли по этому поводу. Еще раз, реализация является открытой и доступна на GitHub.",
    "43": "Самодельная механическая клавиатура, например моя, состоит из идей разных людей, описанных ими в статьях на Хабре и не только. Одна из таких идей оказалась абсолютно неожиданной, как  гром среди ясного неба. Дальше мысли, уточняющие углубляющие и расширяющие предыдущую. На эти мысли наводят статьи отцов-клавиатуростроителей. Клавиатура - кайф. Первая, собранная лично. Обошлась тысячи в 3 рублей. Некоторые идеи по раскладке и слоям использую до сих пор. Когда-то сделал детальную инструкцию полного цикла, файлы, модели, явки, пароли. В статье автора не только нюансы по проектированию, сборке и доводке, но и забойные комментарии, вагон увлекательного чтения. Так же у автора есть вдохновляющая подборка того, что может быть названо клавиатурой (актуальная на 2014 год). Из статьи Дениса Лещева по сборке взял важную мысль, что можно взять готовое с микросхемами и транзисторами, поменять ему форму. Автор возводит эту идею в \"абсолют\" и в статье описывает, куда это его приводит. Жёсткий техноэкшн. Для меня переход на прошивку QMK состоялся благодаря инструкции Сергея, в которой он прям по камешкам и полочкам все расписал. Так же есть интересная русскоязычная статья о возможностях прошивки QMK. Сервис Keyboard Firmware Builder немного \"подустал\" в части использования сложных составных кодов клавиш. Сервис Keyboard Layout Editor прекрасен. Однако заморочки с расположением клавиш вот-прям-как-на-твоей-клавиатуре кажутся избыточными.  Сергей в своей инструкции тоже акцентирует внимание на этом моменте. Поэтому для транзита из KLE в KFB приводит клавиатуру к прямоугольному упрощенному виду. Отличный приём. Классика. Некоторые решения в предлагаемой  технологии сборки кажутся переусложненными/спорными/дорогими, например медная фольга и куски текстолита на красной половинке. Однако навесной монтаж на серой половинке простой и аккуратный. Также стоит обратить внимание, что исходные файлы моделей корпусов требуют доработки в части размещения как платы, так гнезд под переходники для шлейфов соединения половинок. Когда собирал себе такую, пришлось дорабатывать уже напечатанный корпус. Туда же Limitium' dactyl manuform. Получится мышеклава... клавотрек ... клавомышь...  джойсти_клаво_треко_мышь В конструкцию клавиатуры действительно добавляют много чего интересного. Выглядит конечно аппетитно. От подобных проектов отпугивает ценник в 30 у.е. на оптический сенсор (PMW3389 Motion Sensor) для шара, его сложная доставка, да и сам шар по видимому от трекбола Logitech MX Ergo Graphite, который доступным тоже не назовешь. Так же есть весьма занимательный youtube-проект от Александра Смирнова @Onefabis, в котором он раскрывает множество интересных мыслей и нюансов. Шар видимо от трекбола Logitech Marble. Ко всему прочему Александр в одном из видео обозначил проблему некоторого подклинивания шарика в самодельном исполнении подшарового гнезда. Как понял, он решил/решает эту проблему увеличением как жёсткости гнезда, так и количества точек контакта с шариком. Благодарю энтузиастов за то, что поделились своими соображениями и опытом тогда. Теперь мне очередь делится наработками и мыслями по теме клавиатуростроения в виде то ли туториала, то ли дайджеста. Поехалите. Проектов самодельных клавиатур в интернете великое множество. Подбираем самый аппетитный вариант или скачиваем модель. Можно найти покупные варианты корпусов, kit-наборы с кнопками и крышками, но это может быть значительно дороже чем просто 3D-печать понравившегося корпуса. Из рекомендаций - специально не покупайте под эту задачу 3D-принтер... а то начнется... и закончится только когда уже и все моды поставишь и klipper накатишь. Поэтому 3D-печать понравившегося корпуса проще заказать с помощью сайта с объявлениями. Детальный материал о кнопках, колпачках и клавиатурах, в которых их можно встретить. Доступный вариант: идём на сайт с объявлениями, в поисковой строке вбиваем \"cherry mx' с сортировкой по стоимости. Когда клавиатура-донор за 500-3000р приедет, разбираем, распаиваем. Вопрос с кнопками решен. Для распайки может быть удобным такой набор. Делаю без ссылок на магазины, потому что приворот поиск по фотографии надежнее. Кейкапы клеим на кусок скотча малярного. А свитчи например можно втыкать в готовый корпус. Любая клавиатура это матрица и строк и столбцов. Нужно распределить клавиши по рядам и колонкам, сформировав матрицу. Подавая напряжение на колонку A, и сняв это напряжение с рядов 2 и 3, можно понять, что сейчас клавиши на пересечении этих рядов и этой колонки находятся в нажатом состоянии. Напряжение подаётся на колонки поочерёдно. Таким образом, до 72 точек пересечения (свитчей) можно получить, сформировав сетку  6x12=72, то есть 6 входов (строк) и 12 выходов (столбцов). Однако, у этого метода есть один серьёзный недостаток, проявляющийся в блокировании и пропадании нажатий. Свитчи будем соединять в строки и столбцы. Диодами свитчи соединяем в строки. Есть варианты с гибкими платами, с hotswap-модулями горячей замены свитчей, с подсветкой свитчей, экранчиками вместо кейкапов и т.д. и т.п. Отличный пример нанотехнологий - денег уходит все больше, а результатов все меньше. Доступный, простой и скоростной вариант - навесной монтаж. Диод располагаем черной риской (катодом) от ножки свитча к строке. Проводом соединяем свитчи в столбцы. Удобным оказался обмоточный эмалированный медный провод 0,2мм. Так и гуглить. Зажигалкой прожигаем конец провода. Накручиваем лишенный изоляции участок провода на ножку свитча, припаиваем. Тянем провод до ножки следующего свитча в столбце, хватаемся пинцетом за провод около ножки, ограничивая тем самым минимальную точку до которой может сгореть изолирующая эмаль, прожигаем небольшой участок провода около места хвата пинцета. Прожжённую оголенную часть провода накручиваем на ножку свитча пока оголенный участок не закончится. Повторяем. Монтаж ведем единым проводом, зигзагом, с большими зелеными петлями запаса на последние 3 столбца. Далее хорошо бы прозвонить. Если там где надо звенит, а где не надо не звенит, то можно разделить полученный зигзаг в обозначенных крестиками точках. Полученные в результате зеленые хвосты пойдут до контактных точек платы с микроконтроллером либо платы с переходником. Припаиваемся к строкам и также соединяемся с платой. Повторить для другой половины. В  коммерческих вариантах  и kit-наборах сплит клавиатур часто предлагается использование 2 макетных плат с микроконтроллерами, 2-х разъемов PJ-320A для синхронизирующего половинки кабеля, специальным витым бронированным ядерновзрывостойким лабороторномедным аудиофилокабелемь, экранчиками, энкодерами и прочими свистелками за которые можно хорошо накрутить. Все понимаю, бизнес. Тут мне ближе реализация Сергея Дронова с единственной макетной платой, 2-мя разъемами на плате и кабеля соединяющих половинки в целое. Просто, надежно и главное без лишних сложностей с прошивками/мастер-половинками и т.д. У себя делал на \"огромных платах\" с распаянными HDMI-разъемами, но сейчас доступны и аккуратные маленькие разъемчики. Есть и type-c интересные варианты. На МПМ обитает микроконтроллер, на микроконтроллере будет обитать прошивка. Полный список поддерживаемых микроконтроллеров из официальной документации QMK. Детальный список готовых к покупке МПМ. Наиболее ходовые: Nice!Nano на МК NRF52840 (беспроводной вариант). Когда МПМ в наличии, определяемся в какой половинке будет стоять МПМ+переходник, а в какой просто переходник. Далее припаиваем хвосты запаса от строк и столбцов к выводам МПМ. Здесь важно понять принцип, что порядок к каким контактам МПМ что припаивать - непринципиален. Принципиально записать, например в удобный специальный файлик, куда  и что припаяно, потом пригодится. Однако мало смысла припаиваться к контактам обозначенным GND,  RAW,  RESET,  VCC. C паяльником закончили упражнение. Здесь в жизни инструкции и во всей статье засада и разочарование, потому что появились значительно более доступные трекболы Jelly Comb MT50, ProtoArc EM04. Одна из фишек предлагаемой реализации в использовании части готового \"отполированного\" доступного трекбола, и интеграция его в клавиатуру без ковыряния в прошивке и без всех вопросов с заеданием шара/настройкой/ \"автоматическим слоем мыши\". В рассматриваемой реализации слой мыши включается зажиманием хитрой кнопки основанием основания большого пальца, а рука остается в домашней позиции. К тому же, в документации QMK говорится о возможности подключения одного датчика для pointing device (либо больше одного, но прошивку надо  дописывать и допиливать), и как раз хотел эту возможность использовать на другой половинке для аналогового джойстика. И все же, несмотря на то, что есть трекболы доступнее, продолжу.  Понадобится трекбол Logitech М570, доступный на сайте с объявлениями например. \"Берем готовое с микросхемами и меняем ему форму\". Разбираем и скидываем у трекбола верхнюю панель и колесико. А вместо верхний панельки в те же дырки отверстия тем же крепежом крепим правую самодельную половинку клавиатуры. З.Ы. Если у тебя дорогой читатель по случайному стечению обстоятельств есть время, желание, трекбол с шариком под большой палец и 3D-cканер, то буду рад помощи по адаптации идеи к другим трекболам. В CAD и реверс-инжиниринг умею сам, поэтому минимально достаточно просто облако точек с внутренностями трекбола. Logitech Marble тоже возможно зайдет. Есть несколько вариантов клавиатурных прошивок. Если в тему сильно не углубляться то актуальными наверно можно считать QMK, ZMK и VIA (надстройка над QMK). Делал на QMK. Еще разок сошлюсь на отличную статью о её богатых возможностях. Устанавливаем QMK MSYS по инструкции. Устанавливаем QMK Toolbox по инструкции. По адресу C:\\Users\\****username****\\qmk_firmware\\keyboards закидываем папочку с конфигом 6x12_сhimera. Папочка лежит в папке с наработками. Конфиг максимально простой для прямоугольной клавиатуры 6х12, структура и файлы с говорящими названиями на картинке ниже. В файлике keymap.json можно обнаружить/поправить раскладку, в файлик info.json прописывается конфигурация клавиатуры как то, загрузчик, особенности, на каких пинах МК колонки на каких столбцы, используемый МК. Если что-то чуть чуть не так, то заполнить/поменять в любом текстовом редакторе  нужные строки, или матрицу поменять согласно специального файлика. Посмотреть на какой МК какой загрузчик (bootloader) можно в интернете. Вообще, хорошей идеей кажется ознакомиться с документацией QMK, где подробно и плотно обо всем. Сейчас предпочитаю именно компилировать прошивку самостоятельно, по той причине. что у каждого из визуальных помощников в создании прошивки есть ограничения. KLE вспомогательный сервис для того, чтобы двораками и оптимизациями меряться. KFB то ли сейчас не может, то ли и не мог накинуть на клавиатуру кнопки мыши (инструкция по связке KLE-KFB). (инструкция как из json от KLE->KFB сформировать комплект для самостоятельной компиляции) VIA  кажется странным инструментом ежедневного изменения прошивки, сделанным с ограничения поверх QMK. QMK_configurator тоже доставляет дискомфорт в области ниже спины своим ошеломляющим выпадающим списком одного и того же. Поэтому запускаем QMK MSYS и компилируем прошивку. Прошивка лежит по адресу  C:\\Users\\***username***\\qmk_firmware\\.build\\6x12_chimera_default.hex Открываем QMK Toolbox прошиваем МК файлом 6x12_chimera_default.hex Если, дорогой читатель, ты дошел до сюда и ничего не получается, то напиши мне, постараюсь помочь. А дальше в повествовании пойдут прямо крамольные мысли и предложения. Предупрежден - вооружен. Количество статей на тему крутизны раскладок согласно частотности, мультиязычности, дворакости, типографскости, программируемости, русскоязычности, знакопрепинсовместимости, фонетичности на Хабре значительно. В этом празднике жизни не поучаствовать - грех. QWERTY хорошо, потому что везде. Инвестиции времени и труда на овладение многочисленными по тому или иному параметру оптимальными альтернативами окупятся, только если это сулит значительный профит. Оптимальность траекторий и минимизация времени подлета пальца к нужной кнопке на оптимальных раскладках оптимально для стандартных по формфактору клавиатур. Есть аккордовые клавиатуры, на которых вроде скорости такие что и в космос летать можно на английском языке. А что там насчёт любимых многими букв Ёжэхэбъ? Это веду к тому, что эти ёжэхэбъ-буквы должны быть доступны не на первом слое, а на нулевом и в одно нажатие. Когда пересаживаешься на нестандартную раскладку, то это история скорее про твой личный компьютер, не рабочий. Знаки препинания, скобочки и решётки s долларом должны быть совместимы на для en-ru, иначе больно. Переключение между языками одной клавишей Caps Lock как тут. Индикатор Caps Lock показывает текущий язык как тут. Глобальное состояние раскладки для всех приложений как тут. Невозможность «шибко умных» программ спонтанно менять раскладку тут. Чтобы эту раскладку можно было бы и на Linux-системе повторить. Чтобы учитывала некоторые статистические данные. В рассматриваемой реализации клавиши мыши тоже нужны. Диакритика и прочие спецсимволы нужны специалистам, которые с ними работают хотя бы чаще чем раз в год/жизнь. Простота в постоянном использовании за счет некоторых сложностей в разовой настройке. Любая цифра и математический знак, как и любая f-клавиша на левой руке. Потому что в CAD хочется размеры задавать одной левой, пока pointing device в правой. Самой лучшей и оптимальной раскладки \"для всех\" нет и не должно быть.  Самая лучшая и оптимальная раскладка для тебя - есть. Согласно этим соображениям представляю самую худшую и неоптимальную раскладку, которой совершенно невозможно пользоваться, чем бы ты, дорогой читатель, не был занят. Даже если в десять пальцев порхаешь на клавиатуре. Да, это ужасно, нажимая на цифры верхнего ряда нельзя получить ни одной цифры. Но можно получить абсолютно все знаки препинания со всей клавиатуры независимо от языка ввода (№ доступен и в английской, как и ~` в русской), сгруппированные по смыслу/частотности, с учетом того, что IDE  и разные текстовые редакторы добавляют закрывающую скобку. В гейминге и в других программах все работает ожидаемым образом, потому что когда \"просят\" нажать \"1\", то формально нажимаешь виртуальную клавишу  \"KC_1\", чего программа и ожидает. Даже GIMP спокойно обрабатывает комбинации пока клавиатура выдает кириллицу!!! Цифровой блок с правой части стандартной клавиатуры на левой руке...уиии...ради чего все и затевалось когда-то давно. Туда же все f-клавиши совмещенные с цифрами на  слое L3. Очень странная клавиша КС_EQL которая выдает ( ,  .  … ) и ее не парит, что там стоит в качестве децимального разделителя в системе. Правая часть клавиатуры содержит в себе и клавиатурную часть и многокнопочный трекбол. Здесь может быть интересным то, что не выходя из режима мыши можно исполнять любимые действия (оранжевые ячейки слоя L2: вырезать, копировать, вставить, Enter, delete, backspace ). Так же остались все английские скобочки  и точечки. Компромисс это буква \"Ъ\". На слое L1 расположены стрелочки, enter, delete (голубые ячейки). Все это реализовано как единственный файл австралийско-английской раскладки, у которого в CapsLock-слое задана кириллица. Посмотреть/сделатьнормально   можно программой MSKLC отредактировав файл. Компилируем, устанавливаем. Для Linux - аналогично через исправление существующей английской раскладки, расположенной по адресу /usr/share/X11/xkb. Поэтому правим самую ужасную раскладку до нормальную в файлике keymap.json, снова компилируем, прошиваем МК (клавиатурная часть). Правим раскладку логическую, компилируем устанавливаем в систему (часть операционной системы). Все, клавиатура готова. Мое почтение и поздравления, добравшимся до конца.",
    "44": "CRM, или Customer Relationship Management — это система управления взаимоотношениями с клиентом. CRM‑системы используются в любых продажах и подходят практически для любого бизнеса, где есть понятие «клиент» как таковое. При этом, несмотря на универсальность, видов CRM очень много, они сильно различаются, и важно подобрать для бизнеса такую систему, которая будет упрощать его работу, а не усложнять. Меня зовут Георгий Наврузбеков, я занимался внедрением CRM‑системы для «Фабрики кухни Zetta». Расскажу, как нам с коллегами удалось создать идеальную CRM‑систему для мебельного производства. До появления системы компания работала по старинке — всевозможные гугл‑таблицы, Excel, бумажный формат. Это было очень неудобно: постоянный ручной процесс, постоянно кто‑то что‑то забывает, это неизбежно приводит к проблемам — теряются заказы, теряются рекламации клиентов. Разумеется, уровень клиентского сервиса в результате катастрофически падает. Мы долго пытались подобрать подходящую CRM‑систему, проводили аналитику. Это был 2012 год, и выбор CRM‑систем был крайне невелик. Нам было необходимо, чтобы CRM‑система закрывала полный объем наших бизнес‑процессов — все процессы взаимодействия с клиентом, графики доставок, графики монтажей, автоматический расчет заработной платы сотрудников с учетом системы мотивации и масса всего другого. К сожалению, в других CRM‑системах этих функций либо не было, либо они были частично, и все равно системы требовали доработки. Мы выбрали систему на базе «1С.Управление торговлей», так как она достаточна популярна, для нее легко найти программистов. Внедрение системы заняло примерно год, но на протяжении всего времени использования CRM мы постоянно вносим доработки. В среднем 1–2 раза в месяц обязательно производится внедрение пакета обновлений, линейных корректировок. Это могут быть как декоративные корректировки вроде добавления новых полей, которые просто добавляют удобства в работе сотрудникам, так и корректировки сути бизнес‑процессов — изменение задач, каких‑то инструкций к задачам, функционала сотрудников и так далее. В итоге CRM-система, внедренная в 2012 году, не теряет актуальности - мы постоянно модернизируем и совершенствуем ее, вводим все новые алгоритмы, чтобы работать было максимально удобно. Важно то, что любой отзыв сотрудников о работе системы всегда будет учтен и при необходимости будет сделана задача на доработку. Да, интерфейс визуально оставляет желать лучшего, но по функционалу система сильно превосходит все CRM других кухонных компаний. Также очень важная фишка этой системы заключается в том, что работа каждого сотрудника любого отдела — розницы, службы сервиса, отдела закупок, производства или колл‑центра — построена по четким и понятным конкретным задачам, и у каждой задачи есть конкретные результаты. Например, если я сделал звонок клиенту, то у меня есть определенный набор 5–7 результатов того, чем завершился звонок. В зависимости от результата, который я выбрал, система выставит следующие задачи, причем не только на меня, но и на руководителя и на контрольные подразделения компании. У нас есть отдельное подразделение — отдел контроля выполнения стандартов, который на уровне компании‑франчайзера осуществляет мониторинг каждого салона и контролирует то, как выполняются стандарты по обслуживанию клиентов — как делаются звонки, как направляются коммерческие предложения, есть ли какие‑то ошибки или нарушения в бизнес‑процессе. Этот отдел дает независимую обратную связь и оценку по каждому сотруднику, по каждому клиенту, по директору салона. Даже если это салон франчайзинговый, мы таким же образом мониторим всех его сотрудников и всех клиентов и предоставляем ему высококлассную обратную связь о том, какие блоки у него хромают, что и как надо подтянуть у сотрудников. Например, есть конкретный сотрудник, у которого хромает вопрос коммуникации с клиентом, в частности, отработки возражений — мы это видим, прослушивая разговоры, которые зафиксированы CRM‑системой. Другой сотрудник допускает ошибки в коммерческом предложении, а третий не учитывает потребности клиентов, когда делает коммерческое предложение. Все это позволяет контролировать CRM‑система, потому что она автоматически выставляет задачи на контроль в тех ситуациях, где это требуется. Мы не проводим сплошной контроль всех клиентов. Мы контролируем те ситуации, когда есть риск потери клиента, риск того, что продажа не совершится. При этом даже процессы отдела контроля выполнения стандартов также завязаны на CRM‑систему: все действия сотрудников этого отдела мониторятся и регулируются задачами в CRM‑системе — это не хаотичный мониторинг, а строго системный, все автоматически учитывается и анализируется CRM‑системой. Мы с удовольствием бы показали скриншоты этого процесса, но по понятным причинам не можем это сделать, потому что там будут реальные данные компании, составляющие коммерческую тайну. Наша CRM работает на базе «1С:Управление торговлей» на серверном оборудовании, которое размещено в центре обработки данных. Все подключаются к серверам системы. Функционал системы отражает все бизнес‑процессы компании: Интегрированная телефония МТС — у нас корпоративная мобильная связь мобильная, все записи автоматически выгружаются в CRM‑систему, и можно легко и быстро найти любой разговор с клиентом. Система электронной очереди сотрудников. Система владеет информацией об эффективности работы каждого сотрудника — какие у него показатели конверсии, какая нагрузка по клиентам за последние два месяца. На основании этих данных CRM‑система дает рекомендации по развитию каждого сотрудника, а в работе выстраивает конкретную очередь: сначала клиента берет Вася, потом Петя, потом Маша и так далее. Это полностью решает вечную проблему кухонных компаний и вообще розницы — споры сотрудников о том, кому достанется вошедший клиент; Аналитика по маржинальности — сколько заработали с каждой проданной кухни, духовки, столешницы и так далее. Таким образом, в системе учитывается практически все, что заложено в бизнес‑процессах компании. У большинства кухонных компаний и мебельщиков CRM‑системы не дают четкого и конкретного процесса — они представляют собой скорее просто продвинутые записные книжки, где вы сами все учитываете и сами себе ставите задачи. Данная же CRM, в отличие от классических CRM-систем, построена на модулях искусственного интеллекта, способных к самообучению. Она подсказывает, помогает сделать продажу путем постановки конкретных задач на исполнение с разных точек входа в процесс. Это очень важное отличие, которое позволяет держать на высоте и качество кухонь, и качество клиентского сервиса. Благодаря тому, что CRM фиксирует каждого клиента за определенным сотрудником, у компании появилась возможность мгновенно получать статусы по клиентам, анализировать продажи и оценивать, кто из сотрудников более эффективен и какие пробелы необходимо восполнить тем, чья эффективность недостаточна. Автоматизированы система учета продаж, учета дебиторской задолженности и система учета зарплаты. Даже расчет зарплаты от и до выполняет CRM‑система — бухгалтерия только берет готовую цифру и вводит, сколько сотруднику надо отправить денег. Система управляет всеми сотрудниками и всеми бизнес‑процессами нашей компании в автоматическом режиме. Для сотрудников работа в CRM‑системе очень удобна. Алгоритмы работы CRM построены таким образом, что система, используя модули искусственного интеллекта, предлагает сотрудникам оптимальные решения в отношении каждого клиента. Она подсказывает сотрудникам, как довести клиента до продажи наиболее простым и эффективным путем. Сотрудник точно знает, что никакого клиента он не забудет, не пропустит никакой звонок и не просрочит никакую задачу. Он не тратит время на подсчет своих продаж — все считает система, чтобы люди могли сфокусироваться на своих рабочих задачах. По отзывам сотрудников Zetta, CRM‑система для них — электронный помощник, секретарь и наставник в одном лице. CRM‑система для мебельной фабрики — это инструмент, который дает компании существенное преимущество на рынке. Внедрение CRM на «Фабрике кухни Zetta» оказало комплексный эффект — упрощение бизнес‑процессов работы компании, улучшение качества сервиса и, как следствие, рост конверсии и объемов продаж более чем в 2 раза. Это стало возможно благодаря правильным настройкам системы, которые сделали ее мощнейшим инструментом по управлению сотрудниками и продажами.",
    "45": "Beego – это фреймворк для разработки веб-приложений на языке Go, ориентированный на быстрое развертывание и простоту использования. В его основе лежит идея создания полнофункциональных приложений с минимум усилиям на настройку и кодирование. Это достигается за счет широкого выбора инструментов, включая ORM, систему маршрутизации, интерфейс кмд и многое другое. Beego придерживается принципов RESTful и MVC. Команда загрузит Beego и добавит его в рабочую директорию GOPATH. После установки Beego и Bee можно уже приступать к работе. main.go — точка входа в приложение Так запустим веб-сервер на localhost с портом по умолчанию 8080 и можно увидеть результаты работы приложения, перейдя по адресу http://localhost:8080 в браузере. Маршрутизация в Beego определяется в файле routers/router.go. В этом файле можно указать какие контроллеры должны обрабатывать различные URL-пути. Beego поддерживает статическую и параметризованную маршрутизацию, а также группировку маршрутов. Например, пример статической маршрутизации: Для обработки запросов к корню сайта / используется MainController. Для обработки запросов к /user/123, где 123 — это динамический параметр id, используется UserController. Контроллеры в Beego наследуются от beego.Controller и содержат методы, соответствующие HTTP-методам Get, Post, Delete и т.д., которые вызываются при обращении к связанным с ними маршрутам. Пример контроллера: MainController обрабатывает GET-запросы. Данные, передаваемые в представление index.tpl, устанавливаются через c.Data. Beego позволяет настраивать маршруты гибче, используя функцииweb.NSRouter, web.Include, и web.NSNamespace для группировки маршрутов и в целом более удобной организации приложений. Создали пространство имен /v1, в котором группируются маршруты, относящиеся к пользователям. InterceptorFunc будет вызываться перед каждым выполнением методов контроллера, обрабатывающего пути, соответствующие шаблону /user/*. Так Beego будет автоматически маршрутизировать запросы типа /yourcontroller/methodname к соответствующим методам в YourController. Запросы к /user/123 будут обрабатываться UserController, при этом :id должен соответствовать регулярному выражению [0-9]+, что означает одну или более цифр. Beego суперски подходит для создания RESTful API благодаря своей системе маршрутизации. Можно определить маршруты для каждого из HTTP-методов (GET, POST, PUT, DELETE и т.д.) и связать их с соответствующими методами в контроллерах: Используя перехватчики, авто-роутинг, кастомные пути с регулярными выражениями и поддержку RESTful URL, можно идеально управлять потоками. ORM Beego позволяет отображать таблицы БД на структуры Go, превращая строки таблиц в объекты Go. В файле конфигурации приложения Beego conf/app.conf указываем параметры подключения к MySQL: В приложении (например, в main.go) юзаем информацию из конфига для инициализации подключения к БД: Прочие БД подключаются аналогично. Для взаимодействия с БД понадобится определить модели. В Beego модели — это структуры Go, которые отражают структуру таблиц в базе данных. Пример простой модели пользователя: Чтобы использовать эту модель в Beego ORM её нужно зарегистрировать в системе ORM. Это делается так: После настройки подключения к БД и определения моделей можно приступить к операциям с данными. Также есть транзакции, составные запросы, ленивая загрузка и т.д. Например, для выполнения транзакции: Beego использует систему шаблонов Go html/template для динамической генерации HTML. Шаблоны позволяют разделять логику приложения и его представление. Шаблоны обычно хранятся в каталоге views и имеют расширение .tpl или .html. В шаблонах можно использовать конструкции Go Template для вставки данных, выполнения условий и циклов: В контроллерах Beego можно передавать данные в шаблон, используя мапу Data. Beego автоматически компилирует шаблоны и рендерит их при вызове метода ServeHTTP. Имя шаблона указывается в свойстве TplName контроллера. Также есть возможность локализации с помощью поддержки i18n в пакете github.com/beego/beego/v2/server/web/i18n. Файлы локализации хранятся в формате INI или JSON и содержат пары ключ-значение для переводов. По дефолту они размещаются в каталоге conf/locale: В main.go или в начальной функции приложения можно инициализровать i18n и указать директорию с файлами локализации: Для использования локализации в шаблонах можно юзать функцию i18n.Tr. Язык для пользователя обычно определяется автоматически на основе заголовков HTTP-запроса, но его также можно задать явно, например, в зависимости от выбора пользователя на сайте. Тесты в Beego по дефолту размещаются в папке tests проекта. Допустим, у нас есть контроллер MainController с методом Get, который возвращает простое сообщение. Пример теста для этого метода: Создаем файл теста в папке tests, например, main_test.go: Юзаем пакет assert библиотеки testify для проверки, что ответ от сервера соответствует ожиданиям. Создаем HTTP-запрос к главной странице и проверяем, что статус-код ответа равен 200 и тело ответа содержит \"Hello, Beego!\". Команда рекурсивно найдет и выполнит все тесты в вашем проекте Beego. А вот для тестирования API можно использовать пакет httptest. Предположим, есть API для получения информации о пользователях. Создаем HTTP GET запрос к нашему API /user/1 и проверяем, что статус код ответа 200 OK и тело ответа соответствует ожидаемому JSON-объекту. При тестировании компонентов, взаимодействующих с БД, нужно изолировать тесты от реальной БД. Это можно сделать с помощью мокинга или настройки тестовой БД: Юзаем SQLite в памяти для создания изолированной тестовой среды. Вставляем нового пользователя в тестовую БД и проверяем, что операция вставки прошла успешно, а возвращаемый идентификатор пользователя не равен нулю. В Beego сессии активируются в файле конфигурации app.conf с использованием параметров sessionon, sessionprovider, и sessionname. Пример конфигурации для активации сессий: Настраивает логирование в файл с определенным уровнем логирования. Beego позволяет определять спец. методы обработки ошибок в контроллерах: Beego интегрирован с Swagger, позволяя автоматически генерировать документацию для API. Для этого необходимо использовать комментарии к коду и инструмент bee для генерации документации: Используя команду bee run -gendoc=true -downdoc=true, можно сгенерировать документацию Swagger, которая будет доступна по адресу /swagger приложения. Beego упрощает стандартные задачи в создание веб-приложений. Больше про языки программирования эксперты OTUS рассказывают в рамках практических онлайн-курсов. С полным каталогом курсов можно ознакомиться по ссылке.",
    "46": "4 года назад я решил для себя открыть новую сферу в виде разработки дополненной реальности (AR). Как и все новое, она будоражила мое сознание и я уже представлял как я быстренько переобуюсь из архитектора-проектировщика и перееду жить в Дубай на деньги от фриланса. На тот момент я имел: деньги на пару месяцев существования, ни одного человека в окружении связанного с IT и при этом непоколебимую веру в себя. Создание AR проектов для меня показалось схожим с архитектурным проектированием, где весь процесс проходит в программах по моделированию и на выходе мы просто собираем презентацию заказчику. И так было на самом деле, свой первый проект я смог создать просто посмотрев ролик на ютубе \"AR приложение на Unity за 5 минут\". Кто знаком с Unity, тоже понимает как быстро на нем можно что то \"сварганить\". Здесь кстати следует отметить, что по неимению опыта я создавал AR приложения для айфона и для этих целей даже купил новенький макбук, о чем потом зачастую жалел. Помню свой первый запуск AR приложения, я просто прыгал от счастья, настолько это было волшебно. Вот-вот и уже золотые горы!По итогу, весь первый год на фрилансе я создавал тестовые AR приложения для себя, от очень простых - к более сложным. Множество раз тестировал разные функции и сервисы, читал форумы и смотрел кучу роликов на ютубе. К концу года я уже видел что могу создавать проекты на заказ. Стоит отметить, что тогда я жил почти полностью без заработка, были пару небольших работ по архитектуре, но все остальное в долг у семьи и знакомых. Свой первый заказ я получил, как бы это типично не звучало, от родственника, с формулировкой \"Сделай, я кое-кому покажу, может оплатят\". Я так и сделал, все работало, записал ролики и выложил их на ютуб. В последующем ко мне обратился уже совершенно другой человек, с просьбой сделать подобное приложение для фестиваля.На середине второго года, летом я уже осознавал себя фрилансером. 2 первых заказа окрыляли воображение. И в то время мне еще подвернулась возможность участвовать в конкурсе на \"студенческий стартап\", для которого уже как раз зрела идея приложения-сервиса. Все было хорошо - лето, солнце, мечты и я беспечно растрачивающий последние деньги.После я создал еще несколько маленьких проектов, но денег в кармане становилось все меньше и в последствии их стало катастрофически не хватать. К тому же, в сентябре 22 года, начались события которые очень сильно подорвал все планы. А у меня уже на тот момент, ипотека и жена в декрете. И вот оно - денег нет, заказчиков нет, разлад в семье. Логичный исход не так ли? Уже зрели мысли \"а не туда ли я свернул? а как же мечты и надежды?\". Данная авантюра, которая длилась 2 года и могла бы закончится на первой же кочке. Благо на тот момент пришло сообщение о выигрыше гранта по конкурсу и свой первый кризис мне удалось пережить. Оправившись, я продолжил свой путь.Выводы по личному опыту: \"Не прыгайте из гнезда без парашюта\" - если вы уже где то работаете или учитесь, не стоит бросать все и выходить на рынок без накопленных средств и стабильных заказчиков. Поверьте, любая встряска может вас заставить вернуться на наемную работу и притом на ту где: \"лишь бы платили\". \"Поставьте дополнительную опору\" - на случай если вы уже решили выйти на фриланс без постоянных заказчиков, лучше найдите подработку или берите заказы с прошлой работы. \"Не покупай технику/сервисы если можно обойтись без них\" - я тогда купил макбук, которым сейчас в практике пользуюсь очень редко. Можно было обойтись без него. Лучше экономьте деньги. \"Берите ответственность\" - в одном из проектов я мог взять чуть больше ответственности, заявив об экспертности и тем самым реализовав проект еще круче, но побоялся. Это в последствии закрыло мне путь к другим заказам. В последующем вы сможете брать меньше ответственности, зная все подводные камни и свои цели. Но на первых парах лучше изучить как можно больше. \"Начните посещать IT сообщества, тусы стартапов и фрилансеров\" - Вам нужно окунуться в эту среду и обзавестись знакомыми, чтобы учиться на опыте других и начинать рекламировать себя. Тут я бы еще добавил \"не женись и не заводи детей\", но оказалось что наоборот, семья тот еще мотиватор. Сейчас я уже 4 года в IT, имею свой стартап и являюсь резидентом бизнес-инкубатор. На свою сферу я уже смотрю без розовых очков, но благо все еще люблю ей заниматься. Уже легче переживаю финансовые ямы и наконец определился со своим дальнейшим путем. Пожалуй главное что могу выделить для успешного фриланса сейчас это маркетинг и планирование. По пунктам: Рекламируйте себя всегда и везде. - раздавайте визитки, выступайте на мероприятиях, ведите блоги, записывайте ролики с вашими работами, пишите статьи - все для того чтобы о вас могли узнать. Знайте свои конечные цели. Чего вы хотите достичь по итогу своей деятельности? К чему прийти?  - это поможет вам не выгореть и фильтровать заказы,  вы сможете спокойно вставать утром каждый день и без сомнений продолжать свою работу. Сформируйте распорядок дня - у меня это получилось по ощущениям, усердная работа с короткими перерывами и обязательно 1 день в неделю без активной работы. Опять же, если нарушаешь распорядок, теряешь в эффективности в последствии. Планируйте вашу рекламную компанию в долгую. - Кажется что отправив письма всем возможным потенциальным заказчикам в один вечер, на утро принесет вам заказы? это не так. Нужно планомерно, путем рекламы по разным каналам притягивать к себе внимание заказчиков.",
    "47": "\"Что б они ни делали -- не идут дела. Видимо в прошивке багов дофига\". Как я напомнил в прошлой статье (где я подготовил утилиты для перепрошивки сенсоров) -- я рассказываю про платформу для VR игр, как с ней интегрироваться и как добраться до ее сенсоров напрямую. Её исходный ресивер обновляет сенсоры с частотой в 86Гц, тогда как технически возможно разогнать до 133 Гц, получив ощутимо ниже задержки, но связь была нестабильной. Давайте начнём погружение в сенсоры -- посмотрим, что за игра ghidra_11.0_PUBLIC установлена у меня в C:\\Games, заглянем одним глазком в саму прошивку и поковыряемся там грязными патчиками, да исправим race condition плюс выкинем немного отладочных глюков. В общем, готовимся к погружению. В этот раз -- всё серьёзно. Так как развлечения не позволяют покупать полноценную IDA с поддержкой ARM, а IDA Freeware только работает с x86, придётся пользоваться чем-нибудь другим. Этим чем-нибудь, разумеется, оказалась Ghidra -- нашумевший несколько лет назад выложенный в опенсорс мощный дизассемблер, с системой скриптования, поддержкой множества архитектур и так далее. Всё никак было не до него, а тут прекрасный повод подвернулся. Скачиваем гидру куда-нибудь (C:\\Games), распаковываем и запускаем через C:\\Games\\ghidra_11.0_PUBLIC\\ghidraRun.bat. Создаём проект, через \"File=>Import File\" импортируем наш Bin файл. Гидра умеет разные форматы, но вот HEX не умеет, хорошо, что мы уже превратили в bin. Еще не определяет сама содержимое, хорошо, что мы уже знаем что это ARM, Cortex M3, little endian: Идём в \"Windows => Memory Map\" и правим. Уже загруженный регион переименовываем во \"flash\" и убираем галочку \"W\" (это не записываемый регион). Через зеленый плюс в наборе инструментов в правом верхнем углу добавляем \"rom\" с адресом начинающимся с 10000000, длинною в 0x20000 (точнее в 0x1CC00 но это не важно), галочки ставим в R и X (убираем W). Еще добавляем регион \"ram\" с адреса 20000000, размером в 0x5000, R/W но не X. С новой информацией мы понимаем, что в самом начале у нас адрес на стек, в RAM, 0x20004000, как и положено согласно карты памяти. Затем есть вектор, с которого начнётся исполнение, затем обработчики прерываний -- все указывающие внутрь ROM, которого у нас нет, и в конце два каких-то кривых указателя. Не похожи на правду. Но с картой памяти мы еще не закончили, еще есть \"Peripherals\" регион, через чтение-запись адресов в котором идёт доступ и настройка железа. Самое удобное, скачать CMSIS-SVD пакет, в котором собраны в машиночитаемом виде описание железа для большинства ARM чипов и модулей. А чтобы воспользоваться им, скачиваем SVD-Loader-Ghidra, после чего идём в \"Window=>Script Manager\", жмём на третью справа иконку (три таких полосочки, левее крестика и красного плюсика, называется \"Manage Script Directories\"), где через зелёный плюсик добавляем путь до скачанного плагина ($USER_HOME/Documents/GitHub/SVD-Loader-Ghidra в моём случае).  Закрываем менеджер директорий, и в фильтр в менеджере скриптов вводим SVD, чтоб быстро найти его: Двойным кликом запускаем его, он попросит выбрать требуемый SVD файл. Выбираем ...GitHub\\cmsis-svd-data\\data\\TexasInstruments\\CC26x0.svd. Скрипт отработает и создаст требуемые регионы памяти в 0x40000000. Когда код представляет собой сырую портянку, каждый бит полезной информации на вес золота. А потому, скачиваем SDK, распаковываем, и начинаем копаться. Мы знаем, что отладка возможна. То есть какие-то отладочные символы должны лежать. Осталось их найти :) Проще всего начать с адреса прерываний: 1001c901, на который указывают все вектора. Возвращаемся в линукс, и: Отлично, выглядит интересно! Надо только прогрузить. В плагинах с гидрой уже есть ImportSymbolsScript.py, но игры с ним оказались неудобными. Под свой случай я подправил его до ArmImportSymbolsScript.py. Основная модификация: если символ это ссылка на функцию, то адрес округляется до четного и на этом месте создаётся dword, чтобы адрес+1 указывал прямо на этот символ. Это позволило импортировать символы более удобно для автоанализа и чтения когда потом. Впрочем, нам всё равно надо символы превратить в подходящий формат: <имя> <адрес> <тип f или нет>. А еще, карта хоть и совпадает для символов, то, что ниже 1xxxxxxx -- не всё правда. Например, по адресу 00001a25  main -- совсем не main. И теперь мы можем его подгрузить двойным кликом по ArmImportSymbolsScript.py в Scripts Manager, выберем файл из C:\\ti\\simplelink...\\scripts\\rom.txt: Импортируем ble_r2.symbols.txt и common_r2.symbols.txt. Как мы видели в карте памяти, с адреса 0x1000 начинается \"TI RTOS ROM Jump Table\". Если сходим туда, увидим кучу указателей. Но не джампов... Впрочем, в TI системе есть еще одна огромная табличка переходов -- \"ROM_Flash_JT\" -- она расположена совсем не по 0x1000, но найти её легко: достаточно взять функцию из тех, что мы уже нашли в символах -- я использую HCI_bm_alloc -- и найти ссылку на неё: Это и есть она. Переименовываем в \"ROM_Flash_JT\". Теперь мы можем сравнить табличку с содержимым в rom_init.c (полный путьC:\\ti\\simplelink_cc2640r2_sdk_5_30_00_03\\source\\ti\\blestack\\rom\\r2\\rom_init.c), чтобы расставить еще кучу имён. Чтобы не делать это вручную, Я накидал скриптик ROM_Flash_JT -- запускаем, выбираем rom_init.c, наслаждаемся: Еще один важный момент: при работе часто используется статическая инициализация разных структур в оперативной памяти. То есть когда в коде написано что-то типа то str можно ставить как указатель на строку в ROM, то str2 должна быть скопирована в RAM, так как мы можем её менять. То же самое с константами в статических структурах и так далее. Чтобы это работало, компилятор перед запуском пользовательского кода производит распаковку констант, сохранённых в ROM. Насколько понимаю, разные компиляторы делают это по разному, но в случае с GCC использованным в TI SDK (как часть XDC Tools), таблица перекидывания прописана в самом начале кода. Скачиваем скрипт arm-romtotram, затем идём по адресу указанному в Reset: Указатель на массив обработчиков использованный внутри цикла -- \"ROMtoRAM_Processors\". Таблица процессоров содержит три функции: функция распаковки чего-то напоминающего LZ (переносим байт как есть, или копируем N байт начиная с M байт в прошлое), memcpy и обнуления региона. Сама таблица rom2ram содержит просто пары адресов -- адрес в ROM аргумент для распаковщика и адрес в RAM для получателя. Теперь, когда мы подписали все три метки, можно запустить \"arm-romtoram\" скрипт, и бульк -- всё готово. Когда мы собрали всё, что может нам понять код, можем приступать. Для начала поправим непонятные ссылки, которые висят в SysTick и IRQ. Это не указатели, а уже начальный код прошивки, поэтому сбросим из через \"c\", переименуем SysTick в \"Begin\" и сделаем его кодом через F12, затем функцией через \"f\". Осталось понять чего. Для начала, сделаем поиск по константам (через Notepad++ или grep'ом): О, прекрасно, у нас же сенсор, значит из peripheral. Давайте подгрузим константы в гидру: File=>Parse C source=>зеленый плюс=>\"c:\\ti\\simplelink_cc2640r2_sdk_5_30_00_03\\source\\ti\\blestack\\profiles\\roles\\cc26xx\\peripheral.h\"=>Parse to program. Ругнётся на что-то, но константы добавится. Тыкаем в \"0x306\", жмём \"E\" видим GAPROLE_ADVERT_OFF_TIME -- двойным кликом применяем. Ниже видим 408/400/402... Можем повторить с ним, но лучше понять что за функции, и почему они не подписаны. Что у нас там в simple_peripheral.c: Ха! 1-в-1: GAPROLE_ADVERT_OFF_TIME, GAPROLE_SCAN_RSP_DATA, GAPROLE_ADVERT_DATA, GAPROLE_PARAM_UPDATE_ENABLE... Хмм... У нас следом за пачкой GAPRole_SetParameter идет просто четыре вызова, ничего напоминающего вызов GGS_SetParameter нет. Кажется, начинаю понимать, почему нет таблицы параметров -- её вырезали из примера. Но еще один непонятный момент -- GAP_SetParamValue должен иметь два аргумента, а у нас 4: Да, они.... А что такое GAP_SetParamValue, может, это макрос? Вот оно что! В зависимости от флагов компиляции, вызов либо прямой, либо косвенный через таблицу джампов (с индексом 152), либо косвенный по прямому адресу. Убедимся, что ICALL_SERVICE_CLASS_BLE == 0x10: переименуем FUN_00005bf4 в icall_directAPI, адрес 0xe165 в GAP_SetParamValue. Смотрим дальше пример: как раз 408, 400, ... переименовываем 0x3fb5 => GAPBondMgr_SetParameter. Процесс дальше, впрочем не похож на инициализацию таблицы. Таки либо пример неверный, либо таки отдельные сервисы типа GSS вырезаны. Заметки на полях: поиск полезных данных никогда не бывает лишним. Выше я показывал вывод strings, в котором были интересные вещи -- \"inputGyroRv\" и \"inputNormal\". Поиск по ним на github дал сходу интересную вещь, что позволило еще разметить часть функций и структур, которыми пользуется сенсор направления. Для ног однако подобной фкусности не обнаружилось. Тут можно бесконечно пытаться понять дальше, однако ж, давайте научимся менять код прошивки. Иначе зачем нам это всё? Для эксперимента, мы уже пропатчили прошивку, заменив \"KATVR\", которая используется для анонса данных, на \"KAT-F\" (типа \"Ноги\"). Но это не интересно. Каждый сенсор прошивается его типом -- левый или правый -- хорошо бы, чтобы он анонсировал себя \"KAT-R\" или \"KAT-L\"! Для этого надо найти где же хранится его тип (левый-правый). Мы знаем, что спаривание висит на USB, и пакеты 0x55/0xAA. Простым пролистыванием вниз натыкаемся на кусок: который явственно обрабатывает событие \"пакет по USB\" и реагирует на него. WriteDeviceId это команда 0x04: Вывод -- DAT_200011c1 это искомый ID сенсора, он же использован как аргумент в ReadDeviceId (команда 0x03). Кросс-реферес по ссылкам на него находит интересную простую функцию: Которая вызывается дважды из Begin() -- при инициализации (очевидно, после подгрузки параметров) и после WriteDeviceId. Идеальная точка врезки! Итак, функция грузит в r1 адрес объекта из неподалёку лежащей константы; затем грузит в r0 байт из объекта+смещение. Сравнивает этот байт с 0x3 (левая нога), и затем использует инструкцию \"ite ne\". Очень красивая система условного выполнения без переходов. В полноценном ARM режиме у каждой команды просто приписано выполняется ли она при флагах, в thumb режиме всё задаётся командой -- (I)f,(T)hen,(E)lse, которая может быть просто \"IT\" (if-then, выполнить инструкцию если совпадает условие), ITT (if-then-then, выполнить две). Управляется от 1 до 4 инструкций, и первая следующая всегда then. Затем два mov в R0, первая выполнится если R0 != 3, вторая если R0 == 3. Затем новое значение сохраняется в другой байт в структуре и идёт переход на адрес lr. Так как функция трогает только регистры r0 и r1 (они же -- параметры функций), их сохранять похоже не обязательно. Так как после возврата у нас есть еще два байта (выравнивание), мы можем заменить (bx lr + nop) на один длинный jmp куда-нибудь. Сразу по окончании ROMtoRAM таблицы у нас как раз пустое место, всё нули. Для удобства отмотаем до круглого числа (0x12e10) и придумаем что мы хотим вписать. Превратим \"KATVR\" в \"KAT-L\" или \"KAT-R\" в зависимости от настройки. Соответственно, надо просто превратить R0 в L или R, и записать куда-надо. А куда? Строка \"KATVR\" нас дважды: один раз в ПЗУ части, один раз в ОЗУ в пакете для анонса: К моменту выхода из функции в R0 у нас 4 или 5 в зависимости от ноги, а в R1 у нас указатель на 20001130. Расстояние от 20001130 до 200011A9 -- 0x79. В принципе, у нас еще и флаги уже стоят с прошлого сравнения. То есть можно сделать нечто вроде: Чтобы править код в гидре нужно очистить область от режима (если там уже есть инструкции), затем через ctrl+shift+g включить ассемблер для текущей строки, она ругнётся что процессор не тестирован. Вписываем инструкцию и аргументы, воюем с ней так как не всегда понимает, что мы хотим, но вроде получается. Правда на \"mov.ne r0,#'L'\" она откаывается реагировать! Когда такое начиналось, я пользовался online assembler где вводил инстуркции и потом переносил байтики. \"mov r0, #'L'\" => \"4f f0 4c 00\"... Не не, надо 2хбайтную, \"movs r0, #'L'\" => \"4c 20\" -- идеально. \"movs r0, #'R'\" => \"52 20\". И так далее... Эм.... Что-то гидре поплохело: Непонятно почему, но в дизассемблере залип отслеживатель флагов. Жаль, но, проигнорируем. Зальем патч... Вот только не сработало. :( Надо не только сменить структуру, но еще и вызвать GAPRole_SetParameter(GAPROLE_SCAN_RSP_DATA, 0x10, &scanRsp). Хорошо, тогда нам нужен указатель на scanRsp в R2. Поправим код: Всё хорошо, но тут гидре сорвало крышу, декомпиляция совсем сошла с ума. Ткнём в \"b.w\" инструкцию правой кнопкой мыши и через \"modify instruction flow\" сменим её на \"CALL_RETURN\". Уже лучше. Чтобы вылечить залипшие \".eq\", можно ткнуть на первую кривую инструкцию (adds.eq.w) правой, и выбрать \"Clear Flow and Repair\", после чего опять F12 -- код исправится (более-менее): Единственное, она потеряла аргумент к функции. Если нажать на ней правой кнопкой и сделать Edit Function, добавить три аргумента и выставить им простые типы: Прекрасно, опять экспортируем патч (File=>Export Program), \"fc.exe /b .\\katvr_foot_orig.bin .\\katvr_foot.bin\" и так далее. Загружаем в сенсор. Урра, работает! :) Видим \"KAT-R\" и \"KAT-L\" устройства плавающие вокруг. Красота. Поиграли - пора и делом заняться. Чтобы понять в чем проблема с сенсорами, надо думать как сенсоры. Итак, мы знаем, что сенсоры отдают данные через Notification. Пойдём искать, где же они используются. Слева в \"Symbol tree\" в поле поиска вводим Notifi и легко переходим на GATT_Noficiation. Ссылок не видно. Значит, используется косвенный вызов, делаем поиск по 0x10010045 (адрес+1, ибо код в Thumb режиме) и находим одну единственную функцию, где идёт подготовка, потом вызов: 0x2E -- это же наш handle, так что да, мы нашли то самое место. То есть, каждые 500 пакетов отправляется уровень заряда (плюс версия прошивки и ID сенсора), все остальные пакеты содержат данные. Причем если данные не готовы (или была ошибка связи с сенсором), то отсылаются нули. Еще, если стоит какой-то флаг, то начало пакета перетирается последовательностью 0-1-0-1-0-1. Теперь нужно понять, как часто этот GATT_Nofication вызывается. Ссылок на \"KatSendNotification\" (как я назвал эту функцию) две, и обе из основного потока, обе внутри цикла обработки событий: Событие №4 должно бы посылаться таймером, но таймер не запущен -- и даже если будет запущен, сразу будет остановлен. Никаких других ссылок на этот таймер я не нашел. Событие №6 генерируется по коллбэку на обработку события BLE, так что похоже на то, что события посылаются после того как два флага взведены (один из них -- соединение установлено, второй не совсем понял сходу), причем следующее событие формируется после отправки предыдущего. Окей, это совпадает с наблюдением, что поток начинается после изменения параметра соединения. Пока непонятно, но, вроде, сколько раз спросили -- столько раз мы должны ответить. Что ж, посмотрим, на когда данные появляются? Кросс-референс на DATA_OK приводит нас к функции: Функция занимается обновлением данных с сенсора до тех пор, пока SomeFlag не будет взведён. Как показало дальнейшее расследование -- этот флаг означает переход в режим сна. Семафор инициализируется на старте и сразу взводится, то есть датчик читается непрерывно пока мы не спим, с перерывами в 700+10+13байтSPI + обновление еще чего-то раз в 100 чтений. SPI настроен на 4 мегабод. Единицы сна в десятках микросекунд. То есть сенсор обновляется раз в ~7.11 миллисекунды, или около 140-141 Гц. Выглядит так, что не должно бы быть проблем с обновлением сенсора на 133Гц. Однако же они есть. Наблюдаемое поведение -- иногда мы видим нулевые пакеты (ведешь себе ровненько сенсор, а точка в гейтвее внезапно срывается в центр). Как ни странно, это поведение вполне объяснимо: при обновлении данных каждые ~140Гц и чтении 133Гц, мы двигаемся рядом, и вероятность того, что пакет был обновлён вот-только-что, а мы обновляем данные -- крайне высока. Мы наблюдаем состояние гонки, так как никакой синхронизации между отправкой, формированием и обновлением данных нет. Решение, как ни странно, очевидно. Нам нужны новые данные для отправки, поэтому читать сенсор нужно только когда есть связь. Пакеты запрашиваются регулярно. А что если... Перенести вызов Semaphore_Post из ReadSensorData в KatSendNotification? Тогда получится идеальная связка: ReadSensorData() подготовит данные, KatSendNotification их заберёт -- и запросит опять. Идеально. Единственное, что KatSendNotification вызывается после отправки прошлого пакета, то есть задержку изнутри ReadSensorData убирать нельзя. Я бы, пожалуй, чуток ее даже понизил, чтобы пакет точно был готов. Так как мы обновляем пакеты каждые 86..133Гц, момент замера скорости не так критичен, до тех пор, пока задержка одна и та же -- 5 миллисекунд погоды не сделают. Достанем блокнотик и спланируем наш патч. Во-1х снизим задержку: Во-2х, замкнём цикл до вызова SensorSemaphore. Способов замкнуть цикл множество: можно заменить в IF условный jmp (bne) на безусловный, можно просто заNOPитьвызов функции, а можно тупо вписать переход вместо всего IFа: И в-3их, поправить KatSendNotification. Действуем аналогично прошлому подходу с реакцией на лампочки -- уходим на свободный кусочек в конце (после прошлого патча, я взял 00012e40). Соответственно, в конце функции меняем return на jmp: И на новом месте формируем кусочек подобный вырезанному из ReadSensorData. Главная сложность, по сравнению с ReadSensorData, это спланировать сколько надо места перед хранением константы для загрузки указателя на семафор. Так же может понадобиться поправить переход, сбросить/вызвать Repair Flow после правок -- но после заполнения всего кода и сброса, всё получается: Заливка патча, на удивление, сработала с первого раза, ресивер на 133Гц получает пакеты стабильно. Если медленно вести, в гейтвее нет никаких проблем и не сбрасывается на ноль. Ура! Я уже начал радоваться, но вот Utopia Machine (да-да, еще раз спасибо за кучу тестирования) жаловался на то, что сенсор направления периодически залипает и требует перезагрузки. \"Залипает\" это когда направление меняется только на пару градусов, еще и перестаёт показывать уровень заряда. Еще жаловался на то, что иногда пропадают данные с одной из ног. При этом на оригинальном ресивере практически не воспроизводится. У меня не воспроизводилось... Но как-то выяснилось, что у меня не воспроизводится когда висит на зарядке, а у него без зарядки. Выкрутил сенсор, оставил на пару часов периодически пошевеливая чтоб убедиться что работает. И... Да! Воспроизвел! Когда произошло, я через Wireshark посмотрел на пакеты, и выяснилось, что в пакетах вместо направления и номера сенсора -- идёт \"0 1 0 1 0 1\". Хвост данных был как всегда, таким образом кусоче квартерниона направления таки менялся, потому и получалось что-то видеть. Это же объясняет, почему пропадала одна из ног -- когда этот код срабатывал, координаты-то не затирались (в ногах они в самом конце пакета), а вот данные о батарее и номер сенсора -- затирались. Прекрасно, мы знаем, что случилось -- но почему? К счастью, флаг \"somthing\" трогался только один раз, внутри обработчика таймера тикающего раз в секунду; и только если некий счетчик доходил до 1800, и не сбрасывался по дороге. Сбрасывался он при определённых условиях связанными с разрядом батареи. Проще говоря, это оказался отладочный код для настройки скорости разряда батареи и тюнинга перехода в режим сна. Я нашел в обоих сенсорах хвосты для настройки этого параметра по USB. В любом случае, это отладочный кусок который в релизной прошивке не нужен -- а значит, просто можно безопасно вырезать. Вырезать опять же можно несколькими споосбами -- через замыкание if'а, через затирание nop'ами... Я просто заNOPил строку, где ставилось \"something = 1\". Примечательно, что исправленные сенсоры остаются совместимы с оригинальными ресивером... Почти. Главная разница -- значения, которые читаются сенсорами, теперь читаются с частотой обновления (86Гц..133Гц) а не с фиксированными ~140Гц. Оптический сенсор на каждом чтении возвращает некое расстояние (в попугаях), которые он насчитал с прошлого чтения. Оригинальный сенсор читает последнее доступное значение 86 раз в секунду, получается, использует данные о расстоянии как скорость -- часть пройденного расстояния при этом теряется, но, с некоторой точностью, восстанавливается перемножая на время. Патченный сенсор начинает читать данные со скоростью опроса -- то есть данные о пройденном расстоянии почти не теряются (теряется каждый 500й пакет + часть пакетов просто теряется по радио), но при этом каждое значение получается больше, чем в оригинале: при обновлении 86 раз в секунду значения будут амплитудой до 163% по отношению к исходным, а на 133Гц всего 105%. Представляет ли это проблему? Зависит от того, как этими данными пользоваться. Если использовать данные для вычисления скорости напрямую (как, к сожалению, делает гейтвей) -- то и да и нет. Нет -- так как при использовании 133Гц ресивера и исправленные сенсоры практически не чувствуется разницы, зато задержки ощутимо ниже (реально ощущается, особенно при игре на 120fps). Да -- так как при использовании исправленного сенсора и исходного ресивера все скорости сильно выше и нужно исправлять настройки для каждой игры -- понижать скорость разбега. Можно ли это тоже исправить? Да, есть несколько способов исправления: патч гейтвея, патча исходного ресивера, более сложный патч сенсоров... Есть где разгуляться. Но это тема отдельного разговора. А дальше на самом деле -- избавление от гейтвея, как минимум для нативных игр -- сейчас, используя KAT SDK невозможно сделать Standalone игры, так как SDK жестко прибит гвоздями к винде и ресиверу подключенному к нему. А я раньше уже показал как связаться с ресивером напрямую, сделал маленький ресивер который прикидывается оригинальным... То есть есть всё, что нужно, для создания действительно standalone игры -- идеально вписывается в концепцию игры, которую разрабатывает Utopia Machine. Так что в следующей серии я покажу итог совместной наработки -- UE SDK с прямым доступом до платформы хоть под виндой хоть нативно на Quest 2/3 :) Не переключайтесь! Часть 1: \"Играем с платформой\" на [Habr], [Medium] и [LinkedIn]. Часть 2: \"Начинаем погружение\" на [Habr], [Medium] и [LinkedIn]. Часть 3: \"Отрезаем провод\" на [Habr], [Medium] и [LinkedIn]. Часть 4: \"Играемся с прошивкой\" на [Habr], [Medium] и [LinkedIn]. Часть 5: \"Оверклокинг и багфиксинг\" на [Habr], [Medium] и [LinkedIn]. Программист / сисадмин (Sr. SRE)",
    "48": "В данной статье бы хотел поделиться своим личным опытом успешного получения сертификаций CompTIA A+, Network+ и Security+  с первой попытки. Ссылки на подтверждение моих сертификаций прилагаются. На данный момент русскоязычный сегмент интернета испытывает недостаток в качественных ресурсах для подготовки к данным экзаменам, что побудило меня восполнить этот пробел. CompTIA (Computing Technology Industry Association) - это международная некоммерческая организация, которая разрабатывает и проводит сертификационные экзамены в области информационных технологий (IT). Сертификации CompTIA широко признаны в IT-индустрии и подтверждают наличие у специалиста базовых знаний и навыков в соответствующей области. Сертификация CompTIA A+ является отправной точкой для многих IT-профессионалов. Она охватывает широкий спектр тем, связанных с аппаратным и программным обеспечением компьютеров, операционными системами, сетями и информационной безопасностью. A+ подтверждает компетентность специалиста в области настройки, обслуживания, диагностики и устранения неполадок персональных компьютеров и ноутбуков. Экзамен состоит из двух частей (Core 1 и Core 2), которые нужно сдать для получения сертификата. Network+ - это сертификация, которая фокусируется на сетевых технологиях. Она подтверждает знания и навыки в области проектирования, настройки, управления и устранения неполадок в проводных и беспроводных сетях. Сертификация охватывает такие темы, как сетевые протоколы, топологии, оборудование, инструменты и безопасность. Network+ рекомендуется как для сетевых администраторов начального уровня, так и для специалистов смежных областей. Security+ - это сертификация в области информационной безопасности. Она подтверждает базовые знания и практические навыки, необходимые для выполнения функций безопасности и выявления угроз в IT-инфраструктуре. Экзамен охватывает такие темы, как управление рисками, криптография, сетевая безопасность, аутентификация и авторизация, применение политик и процедур. Security+ является хорошим стартом для тех, кто планирует развиваться в сфере кибербезопасности. Экзамены CompTIA проводятся на английском языке, их можно сдавать удаленно со своим компьютером дома. Они включают в себя вопросы с множественным выбором и практические задания (Performance-Based Questions). Стоимость каждого экзамена составляет около $370. Сертификаты CompTIA A+, Network+ и Security+ считаются начальным уровнем и служат основой для дальнейшего профессионального развития и получения более специализированных сертификаций. Я никогда не работал ни в сфере IT, ни в области кибербезопасности, и у меня нет формального образования по этим направлениям, поэтому ценность таких сертификатов на практике пока не могу оценить. Изучаю кибербезопасность из-за своего интереса к этой теме и, возможно, решусь когда-нибудь сменить свою профессию юриста на работу пентестера. Просматривая англоязычные источники по изучению кибербезопасности, я обнаружил, что многие рекомендуют начать с IT-основ и прохождения соответствующих сертификаций. Это объясняется тем, что в процессе подготовки к сертификационным экзаменам знания уже структурированы и систематизированы, а сам сертификат позволяет подтвердить приобретенные навыки и компетенции. Именно поэтому я и решил начать именно с этих сертификаций. Самое главное — это знание английского языка. Если можете читать и понимать техническую литературу на английском, то, скорее всего, вам будет по силам освоить материал. Для проверки своих знаний английского просмотрите это видео с субтитрами. Если вы понимаете, о чем идет речь, то, в целом, вы готовы. Также потребуется бюджет, в пределах 450 долларов на каждый сертификат. В эту стоимость входят расходы на обучение и оплата одного экзамена. Кроме того, необходимо выделить достаточно свободного времени для обучения и поддерживать мотивацию на протяжении всего процесса. На подготовку к каждому экзамену потратил около 3 часов ежедневно на протяжении 5–6 недель. Подход к изучению всех трех сертификатов одинаковый, меняется только содержание. Первым шагом прослушайте лекции профессора Мессера. Они полностью бесплатны и четко, без лишней \"воды\", объясняют основы. Мой подход к изучению был прост: слушайте лекции, не делая заметок. Главное — понять ключевые понятия, а процесс запоминания будет описан в следующих шагах. Важно учитывать, что коды сертификаций, такие как SY0-601 или SY0-701 для Security+, обновляются каждые три года. Соответственно, содержание экзаменов и подготовительные материалы меняются. Убедитесь, что используете актуальные ресурсы для подготовки. В самом начале обучения я не знал этого и месяц изучал старый материал =). Этот шаг является необязательным, на мой взгляд. Если вы сильно ограничены в бюджете, то можете пропустить его. Однако, если средства позволяют, я бы настоятельно рекомендовал не пропускать этот шаг. Прослушайте лекции Майка Майерса на платформе Udemy. Хотя это платные курсы, они часто (обычно раз в месяц) продаются со скидкой, и их цена составляет около 13–20 долларов. Лекции Майка Майерса более увлекательны и менее монотонны. Он умеет преподносить сложный материал интересно, а также освещает темы, которые не были затронуты в лекциях профессора Мессера. Как и в случае с лекциями Мессера, слушал курсы Майка Майерса, не делая заметок, что позволило информации естественным образом усвоиться. Курсы Майка Мейерса можно найти по этой ссылке. CompTIA выпускает для каждого экзамена соответствующий документ под названием Certification Objectives. В этом документе четко перечислены конкретные темы, навыки и области знаний, которые будут на экзамене. Вот пример одного из таких документов Certification Objectives. После завершения прослушивания лекций внимательно изучите CompTIA Exam Objectives, убедившись, что понимаете каждую тему и термин. Если есть незнакомые термины, используйте нейросети, такие как ChatGPT или Claude, для их объяснения. Затем создайте флеш-карточки для запоминания тем, терминов и аббревиатур, которые пока не знаете. Использовал приложение Anki, поскольку оно бесплатное. В итоге, для каждого экзамена у меня получилось примерно по 250 карточек. Регулярно просматривайте флеш-карточки, пока не сможете легко вспомнить все термины и понятия. Четвертый шаг включал в себя прохождение практических тренировочных экзаменов, предоставленных компанией Dion Training Solutions. Эти экзамены доступны на сайте Udemy, и их цена по скидке также варьируется от 13 до 20 долларов. Вопросы в этих тренировочных экзаменах очень похожи на те, которые встречаются на реальных сертификационных экзаменах. Решая их, приспособился быстро понимать и анализировать формулировки вопросов, что помогло мне сэкономить время на настоящем экзамене. Кроме того, к каждому ответу в практических экзаменах Dion Training Solutions прилагается подробное объяснение. Это помогает выявить пробелы в знаниях и понять, почему тот или иной вариант является правильным или неправильным. Если в процессе прохождения тренировочных экзаменов обнаруживал темы, в которых у меня были слабые места, то создавал дополнительные карточки Anki для их запоминания. Ссылку на курсы Dion Training Solutions можно найти здесь. Экзамены CompTIA состоят из двух частей: вопросов с несколькими вариантами ответов и практических заданий, так называемых Performance-based questions (PBQ). Вопросы PBQ требуют от кандидатов выполнения задач или решения проблем в смоделированной среде. Они предназначены для проверки практических навыков и применения знаний в реальных ситуациях. Например, может быть поручено настроить сеть или устранить неполадки в системе безопасности. Чтобы подготовиться к этому типу вопросов, использовал видеоролики канала Cyberkraft на YouTube, которые посвящены PBQ. Этот канал учит правильному подходу и мышлению для успешного решения практических задач. Рекомендую просмотреть каждое видео на канале не менее двух раз для каждого экзамена. Это поможет укрепить понимание и подготовиться к формату реальных экзаменов. Обратите внимание, что задания PBQ, представленные на канале Cyberkraft, зачастую сложнее тех, которые встречаются на реальном экзамене. Ссылку на канал Cyberkraft можно найти здесь. При сдаче экзамена CompTIA важно иметь продуманную стратегию прохождения вопросов, чтобы максимально эффективно использовать отведенное время. Первым делом рекомендую пропустить вопросы PBQ (Performance-based questions), которые появляются в первой части экзамена. Пометьте их для последующего просмотра, чтобы потом можно было легко к ним вернуться. Как правило, PBQ занимают больше времени, и их решение в первую очередь может существенно замедлить прогресс, оставив меньше времени на более простые вопросы. Вместо этого сразу приступайте к тестовым заданиям (вопросы с несколькими вариантами ответов). Внимательно читайте каждый вопрос и старайтесь исключить заведомо неверные варианты ответов. Если какие-то задания покажутся вам сложными, не тратьте на них много времени. Помечайте такие вопросы для последующего просмотра и двигайтесь дальше. После того как вы ответите на все простые вопросы, вернитесь к тем, которые вы отметили для повторного рассмотрения, начиная с PBQ. К этому времени, \"разогревшись\" на тестовых заданиях, практические вопросы могут стать легче. Для связи можете найти меня в LinkedIn.",
    "49": "Литературы никогда не бывает много — ни художественной, ни технической. Это касается и книг по языкам программирования, включая Python. Разработчикам, как начинающим, так и опытным, нужны надёжные руководства для того, чтобы повышать свой профессиональный уровень. В сегодняшней подборке — пять книг, которые могут быть полезны для любого Python-разработчика. В книге раскрываются базовые концепции, объясняется значение операторов, списков, функций. Автор рассказывает о том, как работать с файлами, обрабатывать исключения и GUI. Для закрепления материала в конце каждой главы есть практические задания. Их рекомендуется выполнять, поскольку именно так можно окончательно освоить теорию. Преимущество издания — достаточно простое изложение, так что начинающие специалисты не будут испытывать затруднений с пониманием. Книга хорошо структурирована, а основные моменты неплохо разбираются автором. В целом «Программирование» — отличный вариант для базового знакомства с базой разработки на Python. Для специалистов среднего уровня потребуется уже нечто более продвинутое. Структура этой книги немного необычна, но, по словам тех, кто с ней ознакомился, она позволяет быстро и эффективно осваивать материал. Автор рассказывает, как разработать веб-приложение, взаимодействовать с базами данных. Также раскрываются основы обработки исключений и применения различных полезных для работы инструментов Python. запись кода с использованием модулей. Достоинство книги — доступность, в ней автор очень понятно рассказывает о достаточно сложных вещах. Кроме того, теория закрепляется практикой, что крайне важно для освоения любого языка программирования. Среди недостатков читатели выделяют отсутствие новых изданий книги — с момента её написания Python неоднократно обновлялся. Но в любом случае «Изучаем программирование» можно советовать разработчикам начального уровня. Автор: Дауни Аллен Б. Эта книга может быть полезна уже не только новичкам, но и программистам среднего уровня, которые хотят вспомнить забытые моменты. Дело в том, что автор раскрывает темы разной сложности — от простых до более продвинутых. Кроме того, есть большое количество примеров и практических заданий, закрепляющих теоретическую подготовку. Среди плюсов книги — ясность изложения материала: автор доступно рассказывает о сложных темах. В издании вообще нет ничего лишнего, всё можно пустить «в дело». Кроме того, повествует автор и о главных моментах написания программ, хотя и использует уже не самые основы. Автор начинает с основ Python, рассказывает о стандартной библиотеке, поиске и установке сторонних пакетов. Далее он раскрывает более сложные темы, помогая осваивать различные практики тестирования, отладки, реутилизации кода и т. п. В новом издании, которое существенно доработано, более 100 новых страниц с полезной для разработчиков информацией. Среди того, что стоит упомянуть, — раздел о типах данных, переменных, сведения о сторонних библиотеках. Рассказывается о контейнерах, облаках, Data Science и машинном обучении. Положительные стороны книги — понятное изложение, продуманная структура, а также большое количество практических заданий. Это, как и у прочих изданий, важный момент, поскольку без освоения полученного материала не получится его закрепить. Подходит книга как для начинающих разработчиков, так и для уже освоившихся в мире программирования специалистов. В этом руководстве — множество примеров кода из сообщества Stack Overflow. Конечно, всё это можно обнаружить и самостоятельно. Но авторы постарались подавать материал дозированно и структурированно. Соответственно, можно найти большое количество полезных примеров кода, которые дают возможность улучшать навыки. Эта книга может пригодиться как относительно «зелёным» новичкам, так и более опытным специалистам. Среди достоинств — отсутствие воды, большое количество примеров кода, т. е. много полезных кейсов, которые, в общем-то, нужны всем. К слову, книга не обучает языку, но, скорее, служит вспомогательным инструментом, позволяющим быстрее освоить нужную грань Python.",
    "50": "Привет! Меня зовут Андрей Груненков, я iOS - разработчик в агентстве InstaDev. Делаем мобильные приложения, которые помогают бизнесу расти. В этой статье я расскажу о том, как разработать первое мобильное приложение для платформы Apple Vision Pro. Для начала надо сказать пару слов о самой платформе. Apple Vision Pro это компьютер Apple, который предоставляет пользователю новый опыт пространственного взаимодействия с интерфейсом. По сути представляет из себя гарнитуру смешанной реальности (AR/VR). Достаточно просто надеть устройство на голову, чтобы погрузиться в последние проекты или трёхмерный контент. Гарнитура отслеживает движение  глаз пользователя, передает изображение с внешних камер на дисплеи перед глазами и показывает 3D-контент. При нажатии на одну кнопку можно погрузиться в виртуальный мир, в остальное время окна и приложения перемещаются в воздухе перед пользователем и дают возможность с ними взаимодействовать. использование в качестве внешнего монитора для Mac. Wi‑Fi 6 (802.11ax) Bluetooth 5.3 В шлем смешанной реальности вшиты сразу два мощных процессора. Первый — М2, он уже показал себя в Mac и iPad последних поколений. Второй — VR-процессор R1, который собирает информацию со всех датчиков, микрофонов и камер, обрабатывает ее и выводит изображение на экран. Apple заявляет, что на этот процесс уходит не более 12 миллисекунд. Доступны шлемы со встроенной памятью на 256 или 512 гигабайт, а также на один терабайт. Оперативная память при этом составляет 16 гигабайт. За картинки и видео отвечают два Micro OLED дисплея с разрешением 23 мегапикселя. Они создают 3D-систему, обеспечивают плавность изображения, а также глубину и разнообразие цветов. Угол обзора в шлеме — 140 градусов. В шлем встроены шесть микрофонов и сложная аудиосистема. Она сканирует помещение, в котором находится пользователь, и адаптирует звучание. Гарнитура дополненной реальности совместима с наушниками AirPods Pro 2. Всего в устройстве 13 камер, в том числе две основные камеры с высоким разрешением. Во внутренней части действуют еще четыре — для отслеживания движения глаз. Еще шесть камер ориентированы на окружающий мир. Они делают фото и видео, которые нужны для создания 3D-системы. Также в гарнитуру встроены различные сенсоры и датчики: например, датчик освещенности, датчик мерцания и другие. Кроме того, Apple встроила в шлем новую систему аутентификации на основе радужной оболочки глаза — Optic ID. Она способна реагировать на движения глаз при открытии нужных окон, программ и даже при наборе текста. Она же защищает данные и шифрует информацию. Доступно управление гаджетом взглядом и жестами. Для активации экрана нужно дотронуться указательным пальцем до большого, то есть «щелкнуть» в воздухе. Также есть колесико Digital Crown и кнопки на корпусе. Можно вызвать виртуальную клавиатуру и печатать, нажимая на кнопки пальцами в воздухе. Vision Pro подключается к MacBook, работая в качестве виртуального монитора. Также гарнитура совместима с мышкой и клавиатурой от Apple. Интерфейс знаком всем пользователям продукции бренда. Устройство заряжается от съемного аккумулятора, который можно подключать с помощью кабеля. Сам аккумулятор компактный, легко помещается в карман. Заявленное время автономной работы гарнитуры — 2,5 часа. Гарнитура надевается на голову с помощью ремешков, причем можно выбрать один из двух вариантов, представленных производителем. Первый, более компактный и красивый, включает в себя один ремень Solo Knit Band с 3D-креплением. Второй — Dual Band, для более устойчивого крепления. Он снимает нагрузку с шеи. Сам шлем сделан из стекла и алюминия, вес устройства составляет порядка 650 граммов. Дизайн повторяет очертания человеческого лица, из-за чего выглядит эcтетичнее и более футуристично, чем его конкуренты. В боковых частях шлема расположен мягкий уплотнитель Light Seal, блокирующий свет из окружающего мира. Перед покупкой Vision Pro нужно отсканировать лицо с помощью технологии Face ID на iPhone, чтобы производитель подобрал внутреннюю мягкую подкладку нужной формы и размера. Далее я раскрою несколько терминов, понимание которых потребуется в дальнейшем. Виртуальная реальность - полностью цифровая среда. Окружение, звуки и происходящие вокруг пользователя действия смоделированы разработчиками. Дополненная реальность - это среда, в которой настоящий мир дополняется цифровыми элементами, такими как 3D-объекты, картинки, текст, звук, анимация и прочие. Размещать виртуальные объекты в пространстве не хаотично, а в определённых местах можно разными способами, вот 3 из них. 1. Привязка к маркеру Объекты накладывают с помощью специальных маркеров, например QR-кодов или картинок. При наведении камеры на такой маркер на его месте пользователь видит на экране виртуальный объект. 2. Привязка к плоскости Объект в дополненной реальности появляется в пространстве, привязанный к определённой точке, выбранной устройством в результате сканирования. Распознаются как горизонтальные, так и вертикальные плоскости. 3. Привязка к геолокации Виртуальные элементы размещают в пространстве на основе местоположения реальных объектов и времени взаимодействия пользователя с дополненной реальностью. Для этого используются данные с камеры, датчиков устройства, GPS и прочих источников и систем. Разберем создание первого приложения под VisionOS. Для разработки приложений под visionOS используется фреймворки ARKit и RealityKit. Когда мы создаем новый проект в Xcode, нам открывается новый помощник проекта. Он упорядочивает шаблоны проектов по платформам и типам проектов. Шаблон проекта приложения доступен в разделе «Приложение» на вкладке «Платформа». Новый помощник проекта представляет нам несколько опций, две из которых являются новыми для этой платформы. Давайте подробнее рассмотрим каждую из этих опций. Первая новая опция Initial Scene позволяет нам указать тип начальной сцены, которая автоматически включается в приложение. Новый помощник проекта всегда создает отправную точку с одной сцены выбранного вами здесь типа. Как разработчик, вы можете добавить дополнительные сцены позже. Они могут быть того же типа, что и исходная сцена, или  принадлежать к другому типу сцены. Шаблон предлагает два типа сцены: Window и Volume. Давайте посмотрим на различия между ними. Windows предназначена для представления содержимого, которое в основном является двухмерным. Их плоские размеры можно изменить, но их глубина фиксирована. Windows обычно отображается рядом с другими запущенными приложениями. Volume предназначен в первую очередь для представления 3D-контента. Их размеры во всех трех измерениях контролируются самим приложением, но не могут быть изменены пользователем, использующим приложение. Как и окна, объемы обычно отображаются рядом с другими запущенными приложениями. Вторая новая опция, Immersive Space, дает вам возможность добавить в ваше приложение отправную точку для иммерсивного контента. SwiftUI предлагает три различных стиля для вашей сцены: Mixed, Progressive и Full. Mixed позволяет вашему приложению размещать неограниченное виртуальное содержимое в полном пространстве, сохраняя при этом связь людей с окружением посредством сквозной связи. Progressive открывает портал, предлагающий более захватывающий опыт, который не полностью удаляет пользователей из их окружения. Когда портал открывается, пользователи получают обзор вашего иммерсивного контента примерно на 180 градусов и могут использовать Digital Crown для регулировки размера портала. Full полностью скрывает проход и окружает пользователей средой вашего приложения, перенося их в новое место. По умолчанию в ваше приложение добавляется тип Mixed. Apple рекомендует делать приложения всегда запускающимися в окне и обеспечивать четкие элементы управления входом и выходом, чтобы люди могли решить, когда им лучше погрузиться в ваш контент. Для создания UI и иммерсионного контента используется SwiftUI. Создаем проект и видим, что Xcode сгенерировал 3 структуры: myFirstApp, ContentView и ImmersiveView. Только что созданное приложение имеет основное окно с переключателем, добавляющим/удаляющим иммерсивное пространство Mixed типа. Добавим в проект возможность включать иммерсивные пространства разных типов. Для каждого типа пространства создадим свое View и добавим их в основную структуру приложения. Теперь запустим приложение и посмотрим, как оно будет себя вести при переключении пространств. После включения Full ImmersiveSpace появится предупреждение о погружении в виртуальные пространство, и необходимости принять меры предосторожности, и после тапа на ОК произойдет переключение в полное иммерсивное пространство. Все окружение исчезает и мы видим только виртуальный контент. Переключимся на Progressive ImmersiveSpace. Как и в прошлом случае, появится предупреждение. В этот раз мы увидим виртуальные пространство внутри некого портала. Переключимся на тип Mixed, который установлен по умолчанию и видим привычное окружение и виртуальный контент. Далее попробуем добавить контент в смешанное иммерсивное пространство методом привязки к плоскости, привяжем сферу из встроенного дефолтного пакета контена к поверхности стола. Запустим проект и включим Mixed ImmersiveSpace. После того как Apple Vision Pro отсканируют поверхность стола, на нем появится дополненная сфера: Таким образом, RealityKit предоставляет разработчикам довольно удобные инструменты для разработки приложений дополненной реальности и добавления в них иммерсивного контента. Apple Vision Pro перспективная VR/AR гарнитура, в которой безусловно присутствуют инновационные технологии.  Однако устройство требует доработок. Гарнитура может использоваться разработчиками для создания практически неограниченного рабочего пространства из множества окон, которое может заменить несколько мониторов. На сегодняшний день в AppStore опубликовано еще мало приложений под эту платформу. Однако, можно предположить, что в будущем приложений появится больше, благодаря использованию SwiftUI, позволяющему быстро и безболезненно переносить уже созданные на нем компоненты приложений для других платформ Apple в VisionOS, и простоте портирования уже существующих приложений других платформ на VisionOS. удобный SDK для разработки,  позволяющий быстро создавать приложения. новые технологии еще недостаточно эргономичны. Управление взглядом приводит к быстрому утомлению глазных мышц; в данный момент не доступно для российского региона.",
    "51": "Привет, я создатель известного в узких кругах приложения 15 Puzzle для Android. В статье я расскажу, как я генерирую стартовые позиции для своей игры, а также о том, как я добавлял новые конфигурации головоломки. Перемещать можно только те фишки, которые находятся рядом с пустой клеткой (по диагонали нельзя). Решение может выглядеть так: Каждый раз решать одну и ту же позицию будет скучно, поэтому неплохо было бы найти способ их генерации. Но сначала давайте разберемся, сколько всего существует уникальных стартовых позиций. Так как у нас 16 клеток, а каждая клетка может иметь одно из 16 состояний (число или пустое), всего существует 16! (20 922 789 888 000) вариантов. Однако, только половина из них решаемы. Таким образом, имеем 16! / 2 (примерно 1013) стартовых позиций, из которых мы можем достичь целевую. Значит, при генерации начального состояния нам нужно гарантировать его решаемость, иначе игрок не сможет решить пазл. Одно из решений может выглядеть так - взять те стартовые состояния, в которых мы уверены, что они решаемые, и выбирать случайное каждый раз. У такого решения есть проблема - нужно где-то хранить эти состояния. Если мы будем хранить 16! / 2 позиций в виде массива из 16 32-битных чисел, нам потребуется примерно 608 ТБ. А если мы еще захотим иметь разные размеры пазлов (3x3, 5x5 и т.д.), то места нужно будет еще больше. Есть и другая проблема - как-то нужно сгенерировать 1013 позиций (и каждую из них как-то проверить, что она решаема). Можно создать, например, 105 стартовых состояний, но они в какой-то момент начнут повторяться. Вместо подготовки стартовых позиций, мы можем генерировать их по запросу. Алгоритм может быть таким: Этот метод гарантирует, что получившаяся позиция будет решаемой. Остается только вопрос - сколько раз нужно повторить шаги 2-3? Средняя длина оптимального решения равна 52.59, то есть 150 итераций (повторений шагов 2-3 алгоритма) вполне достаточно. Проблема с этим методом в том, что на каждой итерации алгоритма (где мы перемещаем 0 в выбранную клетку) в среднем мы будем делать ~2.67 операций обмена в массиве состояния (всего ~400 за 150 итераций). И хотя это не будет заметно для современных компьютеров (и телефонов), есть вариант получше. Вариант получше - так же берем начальную позицию и перемешиваем. Для 4x4 перемешивание массива из 16 чисел можно совершить за 15 операций обмена, что намного лучше 400. За 15 операций обмена (дополнительные 0.5 пока проигнорируем) средняя длина решения 53 хода, что почти на 1 ход \"сложнее\", чем метод 150 случайных ходов. Но так как 50% состояний нерешаемы, в половине случаев мы будем генерировать тупиковую позицию. А что, если бы существовал способ проверить, решаема полученная конфигурация или нет? Берем финальную позицию, перемешиваем, проверяем, и, если все еще нерешаемо - повторяем процесс. При шансе успеха в 50% нужно будет повторить процесс всего несколько раз. И такой способ есть. Суть решаемости состоит в четности и инверсиях. По сути, мы считаем количество нарушений натурального порядка. Для финальной позиции количество инверсий будет равно 0, т.к. все числа в восходящем порядке. Давайте посмотрим, как меняется число инверсий, когда мы делаем ход. Например, в этой позиции 52 инверсии: Так как мы не считаем 0, порядок чисел остается таким же. Однако, ход по вертикали меняет количество инверсий. Переместив 4, получим 53: Обратите внимание, что мы не только меняем количество инверсий, но и четность: было 52, стало 53. Для пазлов с шириной в 4 клетки между 0 и перемещаемым числом будет всегда находиться 3 других числа. Так как 3 нечетное, никогда не возникнет ситуация, когда количество чисел \"до\" и количество чисел \"после\" будет одинаковым. Таким образом, возможны такие варианты: В финальной позиции 0 находится в первой строке снизу (считать будем всегда снизу), т. е. четность инверсий не будет совпадать с четностью номера строки пустой клетки. Что делать, если позиция нерешаемая? Как я уже говорил ранее, можно перемешивать массив чисел, пока не получим решаемую позицию. А можно воспользоваться небольшой хитростью: То есть, получая после перетасовки нерешаемую комбинацию, мы просто меняем местами два числа, делая ее решаемой. Вот откуда берутся дополнительные 0.5 на графике - в 50% случаев мы сразу получим правильную позицию, а в остальных нам нужно будет сделать одну дополнительную операцию (16-ю), что в результате дает нам 15.5 операций обмена в среднем. Возможно вы обратили внимание, что решаемость пятнашек привязана к ширине пазла, но о высоте не сказано ни слова. Ошибки нет: высота пазла может быть любой, а правила будут работать те же. До этого момента мы рассматривали только финальные позиции, где числа идут в возрастающем порядке, слева направо, сверху вниз. Но также мы можем расположить цифры в другом порядке. Например, \"змейка\": К сожалению, для такой конфигурации наш алгоритм работать не будет. Чтобы исправить эту проблему, давайте посмотрим, как именно мы считаем инверсии: Нам всего лишь нужно изменить порядок обхода для каждой второй строки. Как мы видим, четность инверсий не меняется. Алгоритм аналогичный. Также алгоритм будет работать для любой конфигурации, если мы можем нарисовать линию, не отрывая ручку от бумаги: Есть еще одна интересная конфигурация пятнашек: вместо удаления 16 с поля, удаляем любое другое случайное число. В этой позиции отсутствует число 2, 50 инверсий и пустая клетка стоит в третьей строке (помним, что считаем снизу), то есть, позиция решаемая. Однако, попробовав решить ее вы получите нечто такое (11 и 10 в обратном порядке): Такое будет происходить в случаях, когда номер строки пустой клетки в финальной позиции четный. Для пазлов с нечетной шириной мы проверяем, совпадает ли четность числа инверсий для стартовой и финальной позиции. Для пазлов с четной шириной номер строки пустой клетки может быть любым, поэтому мы просто возьмем разницу между номерами строк в начальной и финальной позиции. 1 Можно считать снизу, сверху, с 0 или 1 - не важно. Так как в следующем шаге мы вычитаем номера строк, нас интересует только разница позиций пустых клеток в стартовом и финальном состояниях На самом деле, нам не нужны никакие дополнительные алгоритмы. Можно просто делать отображение нужной нам позиции на позицию классических пятнашек. Универсальный алгоритм будет работать для любых конфигураций пазла, вне зависимости от позиции пустой клетки или отсутствующего числа. Спасибо, что дочитали статью до конца. Надеюсь, вы теперь знаете чуть больше о пятняшках. И, если хотите поиграть в саму игру, попробуйте мою реализацию на Android.",
    "52": "Сегодня в рубрике снова Роберт Мартин. Он же — «дядюшка Боб». Мы уже отзывались о нём тепло в материале о другой его книге на Хабре, так как при чтении от страниц буквально веет оптимизмом и заботой о людях и идеалах — в виде чистого кода и не только. Мы планируем перечитать всего его книги. На этот раз взяли с полки «Чистую архитектуру». Роберт Мартин — инженер и программист. Причем кодит он с 12 лет и с тех пор, когда писать код нужно было ещё на бумаге (это не шутка). Как автор он пишет отличные и понятные книги с чистой и лаконичной структурой: никакой воды, немного личных историй, всё по делу, по содержанию и иногда в табличках. Отнюдь не удивительно, что и с архитектурой программного обеспечения у него тоже всё хорошо. О дизайне и архитектуре. «Одна из целей этой книги — устранить весь этот беспорядок и определить раз и навсегда, что такое дизайн и архитектура . Прежде всего, я утверждаю, что между этими понятиями нет никакой разницы. Вообще никакой», — уточняет Роберт Мартин и раскрывает подробности на простом примере: «Возьмем для примера архитектора, спроектировавшего мой новый дом. Этот дом имеет архитектуру? Конечно! А в чем она выражается? Ну . . . это форма дома, внешний вид, уступы, а также расположение комнат и организация пространства внутри. Но когда я рассматривал чертежи, созданные архитектором, я увидел на них массу деталей. Я увидел расположение всех розеток, выключателей и светильников. Я увидел, какие выключатели будут управлять теми или иными светильниками. Я увидел, где будет находиться узел отопления, а также местоположение и размеры водонагревательного котла и насоса. Я увидел подробное описание, как должны конструироваться стены, крыша и фундамент. Проще говоря, я увидел все мелкие детали, поддерживающие все высокоуровневые решения. Я также увидел, что низкоуровневые детали и высокоуровневые решения вместе составляют дизайн дома. То же относится к архитектуре программного обеспечения. Низкоуровневые детали и высокоуровневая структура являются частями одного целого. Они образуют сплошное полотно, определяющее форму системы. Одно без другого невозможно; нет никакой четкой линии, которая разделяла бы их. Есть просто совокупность решений разного уровня детализации». Фокус или, иначе говоря, трюк Роберта Мартина заключается в том, что он умеет зрить в корень. Систему знаний и приемов ему удается объяснить через правила и принципы, которых раньше либо не существовало, либо их никто не оглашал. Даже если вы посмотрите оглавление, то заметите, что основы и закономерности он вынес в отдельные главы и блоки. Прямо как в учебнике или мануале. При этом Роберт Мартин может поспорить со стереотипами. Например, так: «Функциональность или архитектура? Что более ценно? Что важнее — правильная работа системы или простота ее изменения? Если задать этот вопрос руководителю предприятия, он наверняка ответит, что важнее правильная работа. Разработчики часто соглашаются с этим мнением. Но оно ошибочно. Я могу доказать ошибочность этого взгляда простым логическим инструментом исследования экстремумов. Если правильно работающая программа не допускает возможности ее изменения, она перестанет работать правильно, когда изменятся требования, и вы не сможете заставить ее работать правильно. То есть программа станет бесполезной. Если программа работает неправильно, но легко поддается изменению, вы сможете заставить работать ее правильно и поддерживать ее работоспособность по мере изменения требований. То есть программа постоянно будет оставаться полезной. Эти аргументы могут показаться вам неубедительными. В конце концов, нет таких программ, которые нельзя изменить. Однако есть системы, изменить которые практически невозможно, потому что стоимость изменений превысит получаемые выгоды. Многие системы достигают такого состояния в некоторых своих особенностях или конфигурациях». Здесь хотелось бы обратить внимание на то, что Роберт Мартин рассуждает и с учетом перспектив — окажется ли решение рациональным или нет. Как-то универсально даже получается. Можно применять и не только в IT. То есть это база. Как и книга про идеального программиста это чтение о том, как стать профессионалом в своем деле, но всё-таки в области программного обеспечения и конкретно архитектуры ПО. С одной стороны, всё написано лаконично и простым языком, выводы выделены, есть короткие заключения к каждой главе, примеры и иллюстрации — то есть для общего образования можно почитать, даже если тема просто вызывает у вас любопытство. С другой — именно специалистам в своей нише книга может помочь систематизировать и свой личный опыт, проанализировать его, а возможно, покритиковать автора и найти новые объяснения принципам своей работы. Совет читателю: не ожидайте от книги глубокого погружения в теорию, но рассчитывайте на то, что из фирменного легкого повествования Роберта Мартина вы сможете почерпнуть что-то свое. Руководство по товарным знакам в 2024 году.",
    "53": "Я хочу рассказать вам об одном инструменте для работы с соцсетями. Рабочее название его - «менеджер репутаций». Технически он довольно прост, а вот стоящая за ним идея требует пояснений. Не знаю, существуют ли аналогичные программы, мне ничего подобного обнаружить не удалось. Во-всяком случае, концепция ни откуда не заимствовалась, а рождалась постепенно, по мере накопления опыта. Она достаточно непривычна, попробую объяснить ее наглядно. Вообразите себе — крупного начальника или бизнесмена, и его помощника по информации. Допустим, они приезжают на какой-нибудь международный форум. Вокруг — тысячи людей из разных стран. Некоторых начальник знает в лицо, помнит о них какие-то факты, знает, с кем из них ему желательно встретится, а кого, напротив — лучше избегать.. Но их — тысячи, большинство он видит впервые, некоторых — знает только понаслышке.. А рядом с ним — помощник по информации. И он подает своему боссу незаметный для окружающих знак — вот это полезный человек, с ним стоит пообщаться. А вот это — наоборот, ходячая неприятность, лучше с ним в контакты не вступать. Про некоторых, особенно важных людей, помощник подает знак по собственной инициативе. Про других просто сигнализирует - «есть дополнительная информация». Про третьих — по собственной инициативе вообще внимания не обращает. Но и про первых, и про вторых, и про третьих — по первой просьбе своего шефа готов представить ему справку, краткую или развернутую — кто этот человек, откуда он, в каких действиях он и его организация были замечены и .т.п. Понятно, что такой помощник — это ценный специалист, такой есть далеко не у каждого начальника, не говоря уж о простых смертных. И надо учесть, что международный форум —это уже очень сильно отфильтрованный круг, там практически нет случайных людей. А теперь примерьте эту аналогию на любую дискуссионную площадку. В большинстве случаев, в дискуссии допускаются все подряд — например, на YouTube. Но даже там, где существует некоторый входной ценз — как здесь, на Хабре — количество пользователей сильно превышает возможности обычной человеческой памяти, а среди них есть как интересные, полезные, грамотные люди — так и демагоги, балаболы, откровенные тролли... А есть еще люди с разнообразными «изюминками», с сезонными обострениями, с разными другими особенностями... Кроме того, есть еще история личных взаимоотношений — с кем-то я поругался на какую-то тему, возможно несколько лет назад, с кем-то другим — напротив, очень хорошо и душевно пообщался. Наконец, есть аккаунты различных организаций — которые также сообщают различную информацию, выдают оценки, а порой и различные предложения. Разумеется, дискуссионные платформы позволяют посмотреть некоторую информацию о другом пользователе — где-то больше, где-то меньше, но обычно эта информация — это  какой-нибудь рейтинг, интегральный показатель не позволяющий понять, откуда он взялся. И это отражение чужого мнения об этом человеке, а не вашего. Даже если платформа позволяет просмотреть все сообщения пользователя — чтобы составить о нем какое-то определенное мнение, придется потратить массу времени, отфильтровывая ничего не значащие сиюминутные реплики в одних случаях, и вникая в контекст дискуссии — в других, а результат нигде не сохранится, и встретив того-же человека через год вы опять будете делать то-же самое.. Как это не печально, большинство людей решают эту проблему методом рыбки Дори — не помню, и хрен с ним, и каждый раз начинают общение с чистого листа. А ведь на самом деле, большинство людей сами сообщают огромное количество информации о себе — требуется только ее А) систематизировать Б) сохранить и В) обратиться к ней в нужный момент времени. Именно этим и занимается тот гипотетический «помощник по информации», с которого я начал эту заметку. Ему не требуется влезать ни в какие закрытые области, достаточно просто внимательно слушать и крепко запоминать. Решение для этой задачи складывалось постепенно. Несколько лет назад я написал очень простенькую программку для собственного использования — браузерное расширение, которое позволяло присвоить статус любому встречному пользователю - один из заданного списка. Расширение интегрировалось в сайт Континенталист, хранило данные о статусах в локальной базе браузера и автоматически подсвечивало выбранным цветом всех назначенных пользователей. Не смотря на крайнюю простоту оно уже умело помнить любое количество пользователей в течении неограниченного времени и своевременно напоминать при каждой встрече о назначенном статусе — непосредственно на странице. Очень быстро стало понятно, что этого совершенно не достаточно — самый главный вопрос, который возникал — это почему этому пользователю я назначил такой статус. Ожидаемо выяснилось, что человек настолько не одномерен, что одним-единственным статусом его характеризовать невозможно. Поэтому возникла логичная концепция, что человек характеризуется его собственными действиями. В рамках соцсетей, глобально таких возможных действий я выделил два — это публикация и комментарий, и была добавлена возможность сохранения событий. Помимо времени и точного адреса события, программа позволяет сохранить заголовок и текст события, выделив из него любую часть и снабдив собственными пояснениями — например, описывающими контекст, в котором то или иное высказывание было сделано. В заголовок можно записать краткую суть и оценку события. Каждому пользователю сопоставляется своего рода «досье» - список событий, отобранных лично вами, как характеризующие этого человека. Список этот доступен через меню, интегрированное непосредственно в страницы соцсети, про каждого встречного пользователя можно посмотреть список заголовков событий в один тык. Так как вся информация хранится в локальной базе, то в ней остаются в том числе и те события, которые могли быть позже удалены, отредактированы или скрыты. Третий концептуальный шаг — это возможность  снабдить каждое событие набором тегов. Теги,  в отличии от заголовков событий — присваиваются из некоторого относительно небольшого набора — что позволяет автоматически формировать «образ пользователя» из его собственных высказываний, а также суммарное облако тегов всех пользователей. Такой «образ» позволяет достаточно точно, но при этом очень оперативно увидеть, что человек собой представляет. Помимо концептуальных изменений, в программу вносились и чисто технические — например, добавлен дополнительный уровень абстракции, позволяющий легко адаптировать расширение для работы с другими соцсетями, достаточно просто описать несколько методов, при помощи которых находятся стандартные блоки событий. После этого наращивалось количество поддерживаемых соцсетей. Расширение для браузеров называется RepuTracker, оно работает в десктопных браузерах семейств Mozilla и Chrome. В текущей версии поддерживаются следующие дискуссионные площадки: После установки расширения, к каждому высказыванию на страницах поддерживаемых соцсетей, рядом с ником автора, добавляется треугольничек. Например, на Хабре это выглядит так: Нижний пункт этого меню добавляет данное событие к списку сохраненных. Сначала открывается диалог, в котором можно отредактировать свойства события — написать заголовок, добавить пояснения в текст, назначить теги. После этого надо нажать «Добавить» и событие будет сохранено. На страницах соцсети, сохраненное событие выделяется «мигающим» заголовком (для комментариев, у которых отдельного заголовка может не быть - «мигает» ник автора), плюс к каждому упоминанию автора, для которого сохранено хотя-бы одно событие цепляется полупрозрачный беджик, с количеством запомненных событий. Это все модификации, которые вносятся на страницу. Я старался, чтобы они были как можно менее навязчивыми и минимально бросались в глаза. Человек, который не знает об установленном расширении скорее всего даже не сразу заметит, что на странице присутствуют какие-то посторонние элементы. Разумеется, также есть отдельная страница с настройками расширения, на которой можно получить доступ ко всем сохраненным событиям, осуществлять выборку по статусам, соцсетям и тегам, выгружать и загружать данные и т. п. Интересующиеся могут найти подробные сведения в описании. Как видите, сам по себе инструмент достаточно простой, поэтому чрезвычайно важна методика его использования. Как видно из всего сказанного, сам по себе  RepuTracker не навязывает пользователю никакой системы оценок. Он прост, как молоток — когда и куда им ударить, полностью зависит от пользующегося. Какие именно критерии выбирать как значимые, какие именно аспекты чужих высказываний фиксировать в виде тегов, каков вообще будет общий подход к формированию репутаций — полностью остается на усмотрение пользователя. Единственное, что содержится в расширении «из коробки» - это некоторое количество предопределенных статусов, но их можно заменить на любые другие во встроенном редакторе в любой момент. Тем не менее, несколько вещей необходимо отметить. Первое — работа RepuTracker очень похожа на установку «плюсиков», «лайков», «кармы», написание комментариев, добавление «в друзья» и другие подобные способы одобрения/неодобрения другого человека в соцсетях. Я сам не ожидал, насколько мы, оказывается, привыкли к тому, что наши «плюсики» и «минусики» будут адресованы тому, кому мы их ставим. Важно помнить, что сохраняемые в расширении сведения не видны никому, кроме вас. Вы не взаимодействуете с другим человеком, а ведете записи для себя! Это чисто психологический момент, но опыт показывает, что люди в эту ловушку попадаются практически поголовно. Ведя записи, мы занимается тем, что из отдельных разрозненных кусочков формируем какой-то более целостный образ человека — это и есть его репутация.В обычной жизни наш социальный интеллект делает эту работу машинально — но при сетевом общении он дает сбой, так как а) людей слишком много и б) отсутствует невербальная компонента (жесты, мимика, интонации и пр). Тем не менее, у каждого из нас есть такой опыт. Надо только об этом помнить. Никакого смыслы в эмоциональности в этих записях нет — тот, на кого вы гневаетесь или, наоборот, восхищаетесь, их не прочтет. Вы их пишете не ему, а себе — но себе будущему, например через год. И тогда все эмоции уже схлынут, а нужна вам будет четкая и лаконичная информация. Кстати, отмечу еще один неожиданный эффект — сама попытка описать чье-то высказывание в формальном и лаконичном виде очень сильно снижает эффективность манипуляций. Само по себе это не новость — но использование  RepuTracker провоцирует пригасить эмоции и вдуматься в любую поступающую информацию с точки зрения логики, в том числе увидеть — на какую именно реакцию вас пытаются спровоцировать. Второе — хотя технически в RepuTracker можно заносить любую информацию, есть некоторая общая концепция работы с информацией, которая имелась в виду при создании инструмента и при следовании которой он оказывается наиболее эффективен. Для того, чтобы теги позволяли формировать непротиворечивый образ и адекватно сравнивать разных людей — они должны быть по-возможности инвариантны к обсуждаемой тематике. Лучший инвариант — это особенности логики. Проще говоря, стоит фиксировать в первую очередь логические ошибки и демагогические приемы в дискуссии — это гораздо лучше характеризует человека, чем конкретная обсуждаемая тема и конкретное мнение. Программа является, как это называется, «домашним проектом». То есть делается в свободное время, никакой коммерциализации в ней нет и не предполагается, весь исходный код доступен под открытой лицензией. Не смотря на то, что программа уже может использоваться практически, ее вряд-ли можно считать готовым продуктом. В промышленности это называется «демонстратор технологий» - так вот RepuTracker это демонстратор, только не технологий а идеи, подхода к анализу информации. Разумеется, в рамках этого подхода можно предложить огромное количество идей по улучшению и углублению анализа событий. Например, в текущей версии полностью игнорируются взаимосвязи между событиями, хронология учитывается самым простым способом — сортировкой по времени события, отсутствует сравнение и группировка собеседников… Не сомневаюсь, что вдумчивый читатель легко предложит еще массу возможных направлений для развития — поэтому, мне было бы очень любопытно получить отзывы от уважаемого сообщества относительно моего проекта. С интересом выслушаю как мысли общего характера, так и предложения по добавлению отдельных функций и сообщения о конкретных ошибках и неточностях. Для того, чтобы иметь более основательный повод к размещению этой статьи — в крайнюю версию расширения была добавлена поддержка Хабра. Правда, так как я не слишком знаю всякие здешние закоулки, не некоторых страницах разметка может не работать, я просто до них не добрался. Пока я включил разметку на страницы статей и в ленты. https://github.com/dmiandr/context/ Желающие могут запустить плагин прямо из исходников, но там же есть прямая ссылка на установку из каталогов расширений google и mozilla крайнего, на данный момент, релиза. Также, на заглавной странице расположено подробное описание всех функций.",
    "54": "«Я хочу творить, а не быть следствием чужого творчества. Я хочу принадлежать к тем, кто создает смыслы, а не быть плодом этого смысла». Представьте себе ситуацию. Скоро истекут сроки подачи документов в колледж. Очередная партия новичков изберет профессию, адепты которой постоянно задаются вопросом: «А нужны ли будут программисты через год... через пять лет... или через десять лет?». И хотя горячие дебаты на этот счет уже улеглись, в воздухе висит немой вопрос: не настанет ли час, когда крупные языковые модели, такие как ChatGPT, заменят нас на поприще написания кода. Неважно, кто ты — абитуриент, зависший в нерешительности перед выбором ВУЗа, или матерый кодер, я предлагаю тебе остановиться и кое-что переосмыслить. А поможет нам в этом деле... кукла Барби. В начале прошлого года в нашей индустрии наступил период эсхатологического переосмысления. На рынок хлынули инструменты, вызывающие озвученные мной выше опасения. Так, летом OpenAI опубликовала обновление, сделавшее генерацию кода еще более качественной. Система Vercel v0, как утверждается, позволяет создавать готовый к работе React-код по одному только описанию его функциональности. Однако по какой-то причине люди до сих пор с энтузиазмом изучают программирование. В начале прошлого года Code.org обнародовала предложение включить информатику в число обязательных предметов для получения аттестата о среднем образовании. В нескольких штатах это уже сделали. В поддержку этой инициативы существует немало удобных для ребенка инструментов изучения программирования: как минимум, это Scratch, Tynker и Hopscotch. В октябре специалисты лаборатории MIT Media Lab, занимающиеся программой Lifelong Kindergarten, запустили проект OctoStudio. С его помощью Scratch, в основе которого лежат готовые блоки кода, вместо браузера можно будет запустить на мобильном телефоне. Огромный шаг навстречу будущему поколению программистов. Фактически, идея о том, что «детям стоит научиться работать с кодом», наконец находит свое воплощение. Но сейчас, когда машины все лучше и лучше справляются с написанием кода, остается ли этот подход актуальным? Независимо от сложившейся ситуации, я хотел бы дать вам совет. Если вы учитесь, работаете в сфере образования или занимаетесь повышением квалификации, я хочу предложить вам сделать паузу и подумать. Что, если бы вы не учились писать код? Что, если бы вместо того, чтобы учиться кодить, вы научились бы создавать модели? Модель — это репрезентация отношений между двумя системами. А система — это набор элементов, которые связаны друг с другом. Имея на руках структуру модели, вы без труда найдете узлы и отношения в системе, которую она представляет. Модель как бы сообщает нам: «Вот эта штука происходит во-от отсюда». Или, как минимум, описывает ключевые компоненты систем. В детстве мы вместе с отцом собирали модельки автомобилей. Маленькие пластмассовые машинки поддерживали символическую связь со своими полноразмерными собратьями путем сохранения общих черт. Как правило, это были цвета, базовые формы и относительные размеры внешних элементов. Глядя на модель, можно представить, понять и предсказать наличие тех же особенностей и в моделируемой системе. Если модель красного цвета, то и автомобиль, который она имитирует, тоже должен быть красным. Это определение справедливо для любого контекста, в котором мы используем слово «модель». Даже супермодели символически представляют, как должен выглядеть обычный или идеальный человек. (Существует ли моделируемая система в этом случае на самом деле — это уже другая проблема... но об этом мы поговорим чуть позже). К счастью, чтобы освоить искусство моделирования чего-либо, не придется склеивать пальцы (как у Барби) или экспансивно худеть до эталонных параметров. Каждую секунду в течение каждого дня мы уже занимаемся моделированием. И с этим ничего нельзя поделать. В основе разума лежит память, а память сама по себе является моделью. Когда определенная комбинация стимулов воздействует на наш организм, это приводит к активации множества нейронов, связанных между собой в единую сеть. Эти нейроны могут сближаться или отдаляться друг от друга, делая более вероятным срабатывание определенного маршрута в следующий раз, когда эти стимулы (или что-то достаточно похожее на них) окажутся в поле зрения наших органов чувств. Это сближение или отдаление происходит, по сути, механически, за счет выработки определенных гормонов. Они, в свою очередь, образуются по относительно детерминированной схеме в ответ на другие стимулы, как правило, на обретение какого-то желаемого внутреннего состояния (пища, репродуктивный процесс и т. д.). В своей книге Grasp Санджай Сарма называет это «фундаментом» когнитивной системы, который служит основой для некоторых подходов в психологии, таких как оперантное обусловливание. Задумайтесь на мгновение, что это значит. Где-то во вселенной существует исходная система — начищенный до блеска автомобиль, идеальный человек или, например, медведь. И ежеминутно они подвергаются моделированию. Значит, есть и моделирующая система — набор нейронов внутри нашего черепа, которые связываются друг с другом и подают сигналы, напрямую коррелирующие с характеристиками и отношениями в исходной системе. Нет нужды учиться такому моделированию, мозг сам справляется с этой задачей. Это целая модельная фабрика. В один прекрасный день первобытный человек сделал кое-что очень странное. Он начал придумывать другие модели, описывающие сеть отношений между различными внешними системами. Доподлинно неизвестно, зачем ему это понадобилось. Возможно, в целях улучшения коммуникации с соплеменниками. Грубое изображение медведя или другого хищника, которое вызывает нечто вроде эффекта присутствия, напрямую способствует выживанию. К тому же это не требует особых усилий. Визуального сходства с объектом как правило достаточно, чтобы в сознании другого человека «всплыла» аналогичная модель — при условии, что у него было достаточно сенсорного опыта, чтобы построить модель, соответствующую этим стимулам. Если мы оба видели медведя, и я покажу вам его фотографию, то, скорее всего, у вас возникнет чувство узнавания, которое воскресит в памяти характерные особенности, присущие всем виденным вами медведям: покрытый шерстью, коричневый, милый, и так далее. Но иногда нейроны могут повести себя не совсем так, как ожидалось. Изучением этого специфического явления занимается семантика. В нашем мозге очень много нейронов. Потенциальное количество связей между ними применяется для оценки количества отдельных сетей или моделей, которые они могут поддерживать. Это грандиозная цифра. Поэтому рисовать картинку всякий раз, когда нужно показать кому-то свою модель, слишком утомительно. Так появилась письменность. (Да-да, это серьезное упрощение, мы вернемся к этому вопросу чуть позже). Возьмем, к примеру, следующий текст: Быстрая рыжая лиса перепрыгивает через ленивую собаку. Во время чтения этого фрагмента каждое из прочитанных вами слов, а также их порядок были достаточны, чтобы запустить головокружительную цепочку нейронных связей. Каждая из ваших моделей следующих существ должна была активироваться. Не знаю, что приходит вам на ум, когда вы читаете этот текст, но он должен был задействовать ряд моделей — то есть соответствующих нейронных связей, — относящихся к свойствам этих существ. Мы знаем, что существует определенная система. В ней есть два существа: собака и лиса. Лиса характеризуется следующими свойствами: она рыжая и быстрая, а собака, по-видимому, отличается ленью. Может показаться, что понятие «лень» выходит за рамки нашей модели, но на самом деле это не так. «Лень» — это символ, который мы присвоили опыту, изначально являющемуся сенсорным стимулом. Сюда входят ощущение раздражения, когда товарищ по команде проявляет недостаточно активности, представления о ленивых людях — обычно это синонимы слов «неряшливый» или «неопрятный», — и чувства, вызванные социальным гнетом и установкой, что «лень — это плохо». То, что мы только что построили, Дэниел Уиллингем называет контекстной моделью, и это один из самых мощных навыков, выработанных человечеством. Мы взяли модель (комбинация нейронов, образовавшая описанную текстом систему), преобразовали ее в другую модель (символы в тексте выше) и использовали ее для создания еще одной модели (комбинации нейронов, представляющей ситуацию, которая существует в вашем сознании). Фух! Сказать, что все это невероятно сложный процесс, — не сказать ничего. А чем запутаннее система, тем выше вероятность, что с ней возникнут какие-либо проблемы. Например, у вас может быть совершенно другая модель, отражающая ваш опыт взаимодействия с пушистыми зверями. И если я покажу вам фотографию медведя, она может не содержать часть атрибутов медведей, которые присутствуют в моей модели. Если в ваших моделях по какой-то причине отсутствуют атрибуты, подразумевающие потенциальную опасность, у нас возникнут неприятности. С этой проблемой относительно легко справиться, если в основе наших моделей лежат сенсорные стимулы, полученные из непосредственного опыта. Все, что мне нужно будет сделать, это показать вам (надеюсь, с помощью видео на YouTube) примеры того, что случается с людьми, которые слишком близко подходят к медведям. В конце концов, семантика не так уж и сложна. Больше всего проблем возникает там, где наши модели не основаны непосредственно на сенсорных стимулах, а построены как карточный домик — на основе внешнего представления моделей других людей. В этом заключается суть классической игры в испорченный телефон, когда цепочка людей шепчет друг другу на ухо разные фразы. Слова, произнесенные первым человеком в очереди, к концу цепочки могут исказиться до неузнаваемости из-за небольших расхождений между слуховыми стимулами и моделями, которые они вызывают у разных людей. Мы только что сыграли в эту игру. Вы прочли предложение «быстрая рыжая лиса перепрыгивает через ленивую собаку». Однако у вас нет сенсорных стимулов для этой конкретной ситуации. Все, что у вас есть, — это коллекция моделей, связанных с каждым элементом сцены, так что объекты (собака и лиса), их свойства (лень, быстрота, рыжесть) и их взаимоотношения (прыжок, скорость, один «перепрыгивает» другого). Все это в комплексе строит в вашем сознании определенную модель существующей в тексте системы. Но вот, в чем подвох: ни собаки, ни лисы никогда не существовало. Точно так же как нет корневых сенсорных стимулов, какой-то реальной, находящейся где-то во вселенной системы, которую описывает приведенная модель. Системы нет, а модель есть — и в вашем сознании, и в моем собственном. Видите, каков наш «испорченный телефон»? Мы способны смоделировать вещи, которых нет и не было никогда! Такие «фейковые» модели очень легко построить в среде, наполненной разнообразными словами и абстракциями. То есть, увы, в любой социальной среде. Люди уникальны, они уделяют огромное количество времени построению моделей несуществующих систем. В отличие от конкретной фотографии медведя, модели, которые мы только что описали, ничем не подкреплены. И, тем не менее, они вездесущи. Их можно встретить в сельском клубе, в религиозных или политических речах и даже в статьях на Medium (😉). Поскольку нам хочется, чтобы наши модели были как можно точнее (то есть соответствовали реальному миру и тому, что в нем можно испытать), мы изобрели способы создания устойчивых моделей, не подверженных «испорченному телефону». У нас даже есть целый язык, который включает в себя средства верификации — возможность взять модель, выйти в мир и убедиться, что она соответствует реальности. Этот язык — математика, и она издавна эффективно используется людьми для обмена поддающимися проверке моделями. Мы используем математику для описания отношений между элементами модели и утверждаем, что эти отношения сохранятся, если изучить фрагменты исходной системы. Однако у исходных систем все равно есть аспекты, которые сложно выразить средствами математики. Например, любая система существует в реальном времени. То есть некоторые свойства объектов исходной системы будут меняться в зависимости от того, в какой момент времени мы их исследуем. Классический пример из физики — объект в свободном падении, где расстояние объекта от земли непрерывно меняется. Мы видим эти изменения наглядно, когда наблюдаем систему воочию. Однако при математическом описании взаимосвязи между землей и камнем созданные нами модели (пусть даже строгие) не способны отразить изменения, происходящие со временем, в интуитивно понятной форме. Решение этой проблемы заключается в построении моделей, свойства которых меняются с течением времени. Мы проводим научные эксперименты с использованием репрезентативных образцов, испытываем автомобили и самолеты в аэродинамических трубах, строим масштабные модели зданий, двигателей и плотин. Все это делается при допущении, что поведение малых узлов, их свойства и взаимосвязи являются репрезентативными для целевой системы. Зачастую мы возводим подобные системы, чтобы донести до других людей информацию об устройстве Вселенной. Однако предполагаемое сходство этих систем с целевыми позволяет нам пойти еще дальше и совершать открытия в поведении целевой системы с помощью открытий, сделанных в поведении модели. В мире компьютеров можно легко найти модели такого рода: это симуляции. Компьютерное ПО является средством моделирования par excellence. Оно поддерживает строгие, проверяемые взаимосвязи, присущие языку математики, и в то же время допускает, что компоненты могут развиваться, изменяться со временем и даже влиять на окружающую среду. Более того, благодаря своей детерминированной природе, каждая из этих моделей (гипотетически) воспроизводима, что позволяет преодолеть проблему «испорченного телефона! Не всякая система представляет собой гвоздь. Поэтому не каждую систему стоит моделировать при помощи молотка. Если вы не играете в испорченный телефон, имеете возможность проверить достоверность информации или ставки попросту невысоки, вполне подойдет некий эмоциональный способ моделирования. Его крайне сложно реализовать на сухом и прямолинейном «компьютерном» языке. Меж тем, он может показать себя даже эффективнее. По этой причине мы прибегаем к использованию совершенно разных типов символов — лично мне больше всего нравятся музыкальные. Типы моделей, которые мы создаем, столь же разнообразны, сколь и системы, которые мы с их помощью изображаем. В своей книге The Model Thinker Скотт Э. Пейдж перечисляет целый ряд причин, по которым мы создаем модели. Как правило, тип создаваемой модели определяется результатом, которого мы стремимся достичь. Некоторые модели «хороши» только в узком диапазоне контекстов. В своей книге об алгоритмах Стив Скиенна приводит забавный, хотя и анекдотичный пример с теорией плоской Земли. Это не лучшая модель, если нужно предсказать движение звезд, ракет или спутников. Однако если речь идет о строительстве дома и анализе его будущей надежности... модель плоской Земли сработает просто на ура. На самом деле, сфера алгоритмов, находящаяся на стыке математики и информатики, особенно интересна с точки зрения моделирования. Она содержит целые подборки потрясающих решений как обычных, так и нестандартных проблем. Это достигается за счет концентрации усилий на очень небольшом наборе обобщенных структур данных. Пусть вас не отпугивают термины вроде «обобщенные структуры данных». Это некие совокупности «узлов» с очень специфическими отношениями. Они «обобщены» в том смысле, что отдельные узлы не наделены рядом особых свойств. Это позволяет легко и свободно использовать их при составлении моделей определенных систем. Цель ученого в области информатики — хорошо разбираться в методах моделирования и выбирать те, что подходят для решения конкретной задачи. Джордж Полиа в своей книге «Как решить проблему» называет этот процесс «проведением аналогии». Это не уникальная практика для компьютерных специалистов — Полиа был математиком. Говоря о теоретической физике, Ричард Фейнман однажды сказал, что «каждый хороший физик-теоретик знает шесть или семь различных теоретических представлений для одного и того же физического явления». Не знаю, как вы, но когда я услышал Фейнмана, говорящего о «теоретических представлениях», я понял, что фактически речь идет о «моделях». Это выходит далеко за рамки технических дисциплин. На выборах кандидаты и партийные объединения применяют различные модели, описывающие население, его потенциальное поведение и поведение противников. Успех или неудача будут определяться, в частности, способностью кандидата оценить текущее состояние моделируемого процесса. А также навыком манипулирования смоделированной социальной системой. Социальные системы, кстати, моделировать чрезвычайно сложно — именно этот тезис Дэн Рокмор приводит в своем эссе для The New Yorker, подчеркивая предполагаемую несостоятельность средств моделирования в нетехнических дисциплинах. «Сложно» не значит «невозможно». И уж тем более, что не стоит даже попытаться. Моделирование возможно и неизбежно, с разной степенью строгости, в любой дисциплине. Разница между превозносимой надежностью моделей в естественных науках и высмеиваемой неадекватностью математических моделей в политологии заключается, если воспользоваться фразой Скиенны, в «масштабе». Я не имею в виду масштаб моделируемой вещи, я имею в виду масштаб самой модели. Чем ближе наша модель к реальной вещи, тем достовернее мы сможем взаимодействовать с ней, предсказывать ее поведение или манипулировать ею. Таким образом, именно фундаментальные сходства между нашей моделью и целевой системой определяют, как ее следует использовать. В естественных науках может быть случайным совпадением, что вещь, которую мы выбрали для моделирования — математика — имеет нетривиальное сходство с вещью, которую мы моделируем — физикой — и это объясняет ее лаконичность и предсказательную силу. В социальных науках такое попросту невозможно. В человеческом мозге насчитывается около 86 миллиардов нейронов, поэтому наши ресурсы весьма обширны. Тем не менее, отсюда проистекает и ограничение для моделирования. В социальных науках изучаются системы, каждый элемент которых имеет в собственном мозге ровно столько же нейронов, сколько есть в распоряжении у исследователя. Это не оставляет возможности для полноценного моделирования человеческого поведения, не говоря уже о сложных взаимоотношениях и взаимодействиях людей друг с другом и окружающей средой. В любой модели человеческого поведения приходится поступаться астрономическим количеством деталей и нюансов. Просто-напросто невозможно создать идеально точную и при этом относительно компактную модель человеческого общества. Однако это не означает, что социальные науки лишены моделирования как инструмента. Сообщества социологов и вирусологов на протяжении пандемии COVID-19 работали сообща, спасая жизни людей, несмотря на то, что их модели были неполными или в чем-то ошибочными. Каждый день мы прибегаем к некоторым упрощениям — этаким моделям «плоской земли» — в таких дисциплинах, как социальные науки, трудовая динамика и медицина. При этом их использование обусловлено прагматической пользой — несмотря на все упущения и допущения. Как для строгого, так и для нестрогого моделирования справедливы слова Полиа об аналогии: «Было бы глупо не сомневаться в правдоподобности (вывода, сделанного на основе хорошей аналогии), но не менее глупо или даже еще глупее полное пренебрежение правдоподобными предположениями». Более простая цитата, приписываемая Джорджу Боксу, гласит: «Все модели ошибочны, но некоторые из них полезны». Это справедливо для всего разнообразия моделей, особенно тех, что порождает наш собственный разум. Мы постоянно ошибаемся. Однако порой придуманные нами модели оказываются и впрямь полезны. В своей книге о моделировании Скотт Э. Пейдж предлагает придерживаться в жизни концепции «множества моделей». Поскольку всякая модель может оказаться неточной, нельзя использовать для всех возможных жизненных ситуаций одну и ту же модель. Необходимо сформировать собственную «библиотеку» проверенных, хорошо проработанных моделей на все случаи жизни. И применять ту или иную модель следует, опираясь на проблему и предполагаемый подход к ее разрешению. В нашем арсенале метафорического моделирования должно быть как можно больше инструментов. Это касается любой сферы деятельности. Музыканты и композиторы оттачивают свое мастерство в работе со специфическими комбинациями символов, которые способны вызывать у людей общие эмоциональные воспоминания. Инженеры-программисты строят библиотеки моделей, опираясь на математические объекты и символы, характерные для компьютерного кода. Наша способность разрешать любые проблемы зависит от ширины и полноты арсенала моделей, подходящих для этой области. Поэтому важно время от времени пополнять этот арсенал. Целью образования де-факто является расширение глубины и широты наших навыков моделирования (впрочем, это не касается систем образования, заранее расположенных к какой-то конкретной идеологии). Проблемы, с которыми мы сталкиваемся как отдельные личности и как целые сообщества, становятся все более сложными. Это приводит к росту спроса на системы образования, которые позволяют обзавестись более глубокими и разносторонними моделями. Пандемия COVID-19 и повсеместное внедрение генеративных ИИ прекрасно демонстрируют, что традиционные системы образования не справляются с поставленной задачей. Когда я работал в Phenomena Learning, мы пытались решить эту проблему, создав систему моделирования, пригодную для применения в сфере образования. Основная цель Phenomena — помочь студентам «пополнить свой арсенал», создавая модели по различным темам STEM (комплекс отдельных, но при этом связанных друг с другом технических дисциплин в контексте описания образовательной политики учреждения либо учебной программы) и взаимодействуя с ними разными способами. Очень важно, что в число этих «способов» входит и код. Мы создали язык программирования на основе блоков, чтобы можно было разрабатывать строгие, императивные модели с минимальными ограничениями. Мы решили распространить этот подход за пределы кода и разработали отдельный язык математического моделирования, позволяющий будущим специалистам визуализировать взаимосвязи между объектами и их свойствами с помощью визуального веб-редактора. Особое внимание мы уделили наличию или отсутствию опыта у начинающих моделистов. Мы предпочли использовать блочный язык программирования вместо JavaScript, чтобы помочь неискушенным пользователям избежать синтаксических ошибок. Язык математического моделирования мы придумали для того, чтобы студенты могли брать формулы прямо со странички и в один клик приводить их в движение. Главное — сосредоточиться на модели, а не на средствах ее создания. Вовсе не обязательно владеть кодом в принципе. Наш софт поддерживает и более простые, статичные модели. Значительная часть нашей работы была связана с расширением библиотеки «компонентов», которые можно комбинировать как готовые строительные блоки. Несмотря на то, что я уже ушел из Phenomena, моя работа на поприще создания моделей и не думает прекращаться. На своем нынешнем посту в Coursemojo я работаю над созданием системы, которая улучшает контекстную модель, описанную мной выше. Программа принимает текстовые модели, наподобие тех, что можно встретить в классах языковых школ, и помогает студентам строить более обширные и глубокие ментальные модели, используя разговорный интерфейс. Помимо образовательных контекстов, я много писал о The Brain Attic — гипотетической системе, которая позволяет связать друг с другом внешние модели из какой-то конкретной области с тем, чтобы подтвердить или опровергнуть их состоятельность. Собирать такую библиотеку, применять представления, входящие в нее, и совершенствовать их в процессе итеративного использования — это приятное и полезное занятие. Я не могу придумать лучшего обрамления для этого вывода, чем фраза Марго Робби (несправедливо обделенной Оскаром) из фильма «Барби». Помните цитату из начала статьи? Если вчитаться в нее как следует, мы увидим, как Барби, модель, порожденная разумом дизайнера, выражает желание оказаться среди людей, которые создают вещи, а не быть самой вещью (моделью). Что же все это значит для будущих кодеров? Откровенно говоря, голое уметь писать код — это еще далеко не все. Код — это всего лишь инструмент, с помощью которого можно создавать мощные, динамичные модели. Поэтому совершенно неважно, что наше ремесло будет автоматизировано через какие-то 3-5 лет. Люди всегда будут нужны для создания обоснованных, строгих моделей, независимо от используемых инструментов и средств. Именно в этом контексте я часто рекомендую студентам, вчерашним выпускникам или людям, собирающимся войти в эту сферу, что им жизненно важно получить опыт за пределами сферы написания кода. Иными словами, неправильно хотеть быть только кодером — человеком, умеющим оперировать в специфической знаковой системе. Необходимо стремиться к тому, чтобы стать, например, «преподавателем, который умеет работать с кодом», или «специалистом по снабжению, который ориентируется в коде» — надеюсь, идею вы уловили. Фраза Барби о том, что она хочет «быть среди тех, кто создает смыслы» означает, что человеку, ступившему на путь технического образования, следует иметь намерение понимать моделируемые им системы. Именно об этом я говорю, когда призываю «не учиться кодить» — код не должен быть самоцелью. Учитесь моделировать. Накапливайте опыт в построении четких границ между системой, которую вы пытаетесь отобразить, и ее окружением. Осознайте, какой тип среды вы используете для создания такого представления. Постарайтесь как можно яснее определить свои цели в процессе моделирования, будь то предсказание, передача информации или проведение эксперимента. Самое главное — научитесь строить проверяемые модели, различая обоснованные и измеримые и не обоснованные. Всего этого вы можете добиться, используя мощь императивных языков программирования. А может, этот путь не для вас, и вам подойдет что-то еще. Но какой бы способ моделирования вы ни выбрали, создавайте как можно больше моделей. Если каждый из нас будет пополнять свои библиотеки моделей, все наше общество научится решать проблем гораздо быстрее и лучше. А если повезет, вы даже встанете рядом с людьми, которые «создают смыслы, а не являются их плодом».",
    "55": "Сегодня мы поделимся, как организовать межкомандную работу в трекере, какие трудности сопряжены с этим, и какие бывают способы организации такого взаимодействия. А для чего вообще нужна межкомандная работа? На определенном этапе развития компании появляется достаточно четкая структура команд. Очень часто нагрузка в командах не одинакова. В этих случаях многие прибегают к совместной работе разных команд над одним проектом. Вот здесь и начинается все то, что мы описываем термином межкомандное взаимодействие. Важно заметить. Если вы используете трекер без ролей и прав, или если ваши пользователи из разных команд видят задачи друг друга и могут с ними работать и вас это устраивает, то это статья не для вас. Статья будет полезна тем, кто разграничивает права доступа по командам, проектам и сталкивается с вопросами корректной организации совместной работы разных команд друг с другом. Временное добавление пользователей из разных команд в команды, с которыми есть взаимодействие. Создание общего совместного проекта и добавление взаимодействующих пользователей из разных команд в этот проект. Передача задач из проекта в проект. Давайте рассмотрим все достоинства и недостатки каждого из описанных подходов. В этом способе необходимо добавить пользователей одной команды в другую и наоборот - сделать перекрестное добавление. Этот способ можно реализовать не во всех трекерах. Здесь важно, чтобы трекер поддерживал добавление пользователей в несколько проектов. METEOR поддерживает добавление пользователей в разные проекты с указанием конкретных ролей, с которыми пользователь будет работать с проектом. Почему в этом способе мы заостряем внимание на слове “Временное”? Этот способ самый быстрый. Он фактически сразу может дать все необходимые инструменты взаимодействия. Нарушается принцип “единственного отношения пользователя к команде”. Т.е. для таких пользователей очень усложняется получение ответа на вопрос: “К какой конкретно команде относится этот пользователь”. Следствие - нарушение отчетности, регламентных процедур (например, расчет премий команды). Может потребоваться дополнительная настройка учетных инструментов, чтобы не “поплыла” статистика. Этот способ дает пользователю из другой команды видеть и участвовать в задачах, которые не относятся к межкомандным взаимодействиям. Это может повлечь за собой ошибки в учете (например, пользователей из других команд ошибочно будут выбирать в качестве исполнителей в локальных задачах команды). А также в целом нарушается изоляция перечня задач команды. Еще большим недостатком является то, что необходимо помнить, что по завершению межкомандого взаимодействия нужно удалить пользователей из “чужих” команд. Малое количество ограничений для взаимодействующих пользователей. Требуется администрирование как на старте такого взаимодействия, так и по завершению. Этот способ гораздо более хорош во всех отношениях по сравнению с предыдущим. На схеме это можно изобразить так: В этом способе автоматически появляется аналитический разрез “Проект” для отслеживания такого взаимодействия в статистике и отчетности. У вас всегда будет информация о том, на какие проекты участник команды был отвлечен и насколько. Вторым большим достоинством является полная изоляция такого взаимодействия от скопов внутренних задач. У вас в “домашнем” проекте команды не возникает никаких изменений, которые могли бы привести к нарушению безопасности или ошибкам. Создаваемые для такого взаимодействия проекты можно помечать особым образом чтобы они не разрушали вашу статистику и регламенты, а наоборот давали полезные данные о подобных взаимодействиях. Отметка проектов строиться различными способами в зависимости от возможностей трекера. У нас в METEOR вы можете помещать проекты в единую иерархию или задавать значения кастомных полей для фильтрации. Также важным считаем, что этот способ не требует “уборки” после окончания взаимодействия. Малое количество ограничений для взаимодействующих пользователей. Минус мелкий и только один: требуется поддержка в трекере (как минимум возможность пользователю быть участником нескольких проектов). Этот способ мы считаем альтернативным предыдущему. У него есть ряд преимуществ, но, к сожалению, и недостатки. Выглядеть это может так: В тех случаях когда задача в процессе своего жизненного цикла рождается в одной команде, а завершается в другой этот способ будет самым простым в реализации. Он не требует настройки прав и ролей, создания дополнительных сущностей. Можно легко отслеживать все показатели работы в разрезе команд/проектов. Вторым несомненным плюсом является естественность этого процесса. Давайте посмотрим на пример: Пришло обращение в команду поддержки. Поддержка квалифицировала это обращение, сделала техническое описание, определила маршрут для этой задачи и передала ее в работу конкретной команде. Это очень просто и понятно. Из недостатков можно отметить то, что не все трекеры поддерживают изменение команд/проектов в задаче. Вторым недостатком можно назвать то, что если задача может “путешествовать” по разным командам и даже возвращаться в ту, где она была создана, сильно усложняется сбор аналитики по таким взаимодействиям, поиск “узких мест”, синхронизация по уровням обслуживания. Итогом можно отметить, что в ряде конкретных сценариев этот подход хорош, а в других, может быть неуместен. Очень хорошо подходит для задач, которые повторно не возвращаются в команды (поддержка, администрирование, маркетинг). Не подходит для “постоянно путешествующих задач”, например, взаимодействия архитекторов и разработчиков, если они участники разных команд. Примеры, которые мы приводили в этой статье взяты из жизни. Конечно компании бывают разные и уровень зрелости процессов в них также отличается. Но “средняя температура по больнице” примерна такова. Если у вас есть другие находки по организации межкомандной работы, поделитесь, пожалуйста, в комментариях. Спасибо за внимание! Желаем вам простого, качественного, прозрачного учета и высокой эффективности. Тратьте больше времени на созидание!",
    "56": "Новый «Технотекст» стартовал в конце прошлого года. Посмотрев список номинаций, некоторые авторы-инженеры YADRO, которые занимаются разработкой и программированием серверов, СХД, микроэлектроники, не поняли, какую из них выбрать. Раньше же была отдельная «железная» номинация… Куда пропала? Не долго думая, написали организаторам конкурса и спросили, как нам вернуть важную, на наш взгляд, тему в фокус обсуждения. Так и запустили номинацию «Железо: проектирование и технологии производства» — свои тексты на эту тему вы можете отправлять до 15 апреля включительно. А чтобы немного подогреть к ней интерес, собрали «хардверные» призы для лучших текстов. В тексте рассказали больше о призах и условиях участия. Как производители IT-инфраструктуры, телеком- и клиентского оборудования мы в YADRO видим, что интерес к «железным» темам не ушел вместе с иностранными вендорами. Не ушли эксперты и энтузиасты, которые делятся опытом, реализованными проектами, знаниями существующих технологий, инструментов, методов работы. Тексты от них мы как раз ждем в номинации «Железо: проектирование и технологии производства». При этом, где получен опыт — в DIY-проекте на одноплатнике или в enterprise, совершенно не важно. Статьи по теме, написанные с 1 января 2023 года по 14 апреля 2024 года. В номинации не участвуют переводы, рерайты чужих статей, дайджесты и другие вторичные, неуникальные тексты. Мы подобрали полезные призы, которые, на наш взгляд, помогут участникам продолжать реализовывать свой потенциал и интересы. Помимо призов для участников от Хабра, мы собрали три набора призов, которые хотим распределить по каждому грейду. Набор инструментов для электроники от KNIPEX. Книга Харрисов. Возможность принять участие в программе стажировок YADRO Impulse. Мы сможем отправить вам приз только в том случае, если вы живете на территории РФ. Для нас важно подарить именно физический приз, поэтому замену на денежный эквивалент мы не предусматриваем. Мы готовы предложить стажировку в YADRO нескольким лучшим участникам категории Junior, а не одному (если у нас будет несколько классных текстов от ребят). Тексты от инженеров YADRO, опубликованные в блоге компании, не участвуют в розыгрыше призов. Если по числу баллов от членов жюри победит текст, написанный сотрудником компании, подарок получит следующий по баллам участник не из компании. Мы хотим открыть и отметить новых авторов в этой теме! Если вы подали текст в другую номинацию, но поняли, что она лучше подходит для нашей, перенести текст можно, написав на почту neo@habr.team с темой «Железо». Интересуют другие вопросы? Задавайте в комментариях или в личке.",
    "57": "В ближайшие годы AI/ML будет развиваться намного быстрее, чем ожидают люди. Это позволит масштабировать модели и улучшить их производительность. Примером этого является случай обучения, когда модель, обученная на предсказание следующего символа в отзывах на Amazon, автоматически приобрела навыки анализа настроения. Такой результат может показаться невероятным, но он демонстрирует, как модели могут неожиданно приспосабливаться к задачам. Однако, с увеличением вычислительной мощности и размеров моделей появляются новые вопросы. Исследователи пока не знают, как изменится эффективность моделей при увеличении их размеров. Это означает, что сейчас активно обсуждаются перспективы исследований в области искусственного интеллекта. В то время как уже есть значительные достижения в таких областях, как классификация, планирование и обучение с подкреплением, недостаточно исследований, посвященных реальному пониманию существующих методов и их ограничений. Например, ранее считалось, что для оптимальной параллелизации вычислений необходимо использовать как можно меньше данных. Реальные эксперименты показывают, что иногда простые изменения в архитектуре могут сделать вычисления более эффективными. Это позволяет сделать вывод о том, что современные исследования в области искусственного интеллекта должны уделять больше внимания не только методам, но и основам уже существующих алгоритмов. Таким образом, в ближайшем будущем можно ожидать значительных инноваций в архитектуре ПО и методах обучения моделей искусственного интеллекта. Развитие специализированного оборудования, близкого к архитектуре мозга, может привести к еще более значительному ускорению работы моделей искусственного интеллекта. Интерес к вопросам входа в индустрию искусственного интеллекта нарастает, и одним из наиболее обсуждаемых вопросов является, с чего начать для студентов, изучающих компьютерные науки. Ответ зависит от характера проекта, над которым они хотели бы работать. Проект по разработке крупномасштабного обучения с подкреплением для игры Dota 2 требует инженерной работы. Важно понимать, что для старта в индустрии искусственного интеллекта не всегда необходима долгая подготовка. Например, хороший инженер может быть продуктивным с первого дня работы в компании, такой как OpenAI, даже без предыдущего опыта работы в области искусственного интеллекта. Основная задача заключается в масштабировании уже реализованных алгоритмов для улучшения производительности. Поэтому для таких проектов ценными являются инженерные навыки. Когда речь идет о реальном машинном обучении, важно учитывать разницу между наукой и инженерией. В повседневной работе они часто переплетаются. Например, для проекта по игре Dota 2, где требуется масштабирование алгоритмов, инженерная работа преобладает над наукой. Структурированный подход к решению задачи и понимание, что делать, а также что не делать, являются ключевыми аспектами успешного выполнения проекта. Во время нашего проекта по разработке бота для игры Dota 2 наша команда состояла из десяти человек, что создавало высокую плотность разработчиков. Мы активно искали пути расширения возможностей искусственного интеллекта через тестирование алгоритмов. В качестве первого шага мы решили выбрать игру для разработки ботов. Мы обратили внимание на Twitch и выяснили, что Dota 2 обладает всеми необходимыми свойствами для наших целей. Valve, компания за игрой Dota 2, предоставила API, специально ориентированное на ботов, что было идеальным решением для наших задач. В начале проекта нашей команде нужно было тщательно изучить игровое API, чтобы понять его семантику и различные угловые случаи. Одновременно мы начали разработку скриптовых ботов. Мы разбирались в основах игры, изучая все детали. Смотрели повторы матчей Последующие шаги включали разработку инфраструктуры для работы с API и ботами. Нам пришлось столкнуться с техническими ограничениями, такими как размеры файлов игры и ограничения контейнера Docker. Несмотря на сложности, мы разработали систему, позволяющую эффективно работать с игровым API и интегрировать наши боты. Перенос скриптов с Lua на Python также потребовал определенных усилий. Мы переименовали файлы и переписали код, учитывая специфику двух языков программирования. Мы стремились максимально механизировать этот процесс, чтобы обеспечить быструю и эффективную адаптацию скриптов. Перенос скриптов на Python позволил нам использовать более продвинутые инструменты для машинного обучения и ускорить процесс разработки итераций. Для обучения бота мы использовали метод поведенческого клонирования. Этот метод заключается в том, что бот наблюдает за текущим состоянием игрового окружения и выполняет определенные действия на основе этого состояния. Затем мы оцениваем эффективность действий бота и даем ему обратную связь. Наблюдение за состоянием среды: Бот наблюдает за текущим состоянием игрового мира, например, расположение игровых объектов, здоровье персонажей, наличие определенных ресурсов и т.д. Выбор действия: На основе наблюдаемого состояния бот принимает решение о следующем действии, которое ему следует выполнить в игре. Выполнение действия: Бот выполняет выбранное действие в игровом мире. Оценка эффективности: После выполнения действия оценивается его эффективность. Например, если бот атакует вражеского героя и успешно наносит урон, это может быть считаться положительным результатом. Обратная связь и обновление стратегии: В зависимости от оценки эффективности действия бота, мы даем ему обратную связь. Если действие было успешным, мы поощряем бота продолжать использовать такие действия в будущем. В противном случае, если действие было неэффективным, мы корректируем стратегию бота, чтобы он выбирал более эффективные действия в будущем. Таким образом, процесс обучения бота состоит из цикла наблюдения, выбора действия, выполнения действия и обратной связи, который позволяет боту постепенно улучшать свою стратегию и достигать более высокого уровня мастерства в игре. Когда бот достигал определенного уровня мастерства, мы проводили турниры, чтобы проверить его эффективность. Кроме того, у нас было табло, отображающее метрику успеха бота, которая была определена как процент побед в играх. Это позволяло нам видеть прогресс бота и определять, насколько хорошо он справляется с поставленной задачей. В процессе разработки мы столкнулись с вызовами, связанными с необходимостью постоянного обновления стратегии и поиска новых идей для улучшения бота. Благодаря целенаправленному подходу к обратной связи и постоянному изучению результатов, мы смогли добиться значительного прогресса и достичь высокого уровня производительности бота в игре. За две недели до начала турнира The International, мы уже понимали, что наша подготовка подходит к завершению. Мы собирались провести наш самый крупный эксперимент, объединив все наши вычисления в одну стратегию и посмотреть, что из этого выйдет. Иногда нам приходилось консультироваться с профессионалами, чтобы проверить наши тесты, но не всегда это было возможно. Для нас 8 июля стал важной датой. В тот день мы одержали первую победу над нашим профессиональным тестером. После этого мы стали более последовательными, хотя наш тестер отправился в отпуск и использовал не очень мощный ноутбук. Это происходило за неделю до The International, и мы не знали, насколько хорошо мы подготовились. Нам было известно только, что мы шли вверх, судя по последним результатам. Перед турниром мы оценивали свои шансы как 50/50. Мы понимали, что вероятность всегда подвержена изменениям, и мы не могли полностью доверять статистике. Наши фанаты каждую ночь говорили, что у нас есть все шансы выиграть каждую игру, но мы знали, что это не так просто. Мы были готовы к тому, что всякое может произойти. Мы прибыли в Сиэтл за неделю до начала события. Нам предоставили раздевалку под Key Arena. Там мы проводили большую часть времени, готовясь к турниру. Мы привлекали профессиональных игроков для тренировок и проводили многочисленные сессии. Настроение было напряженным, мы не знали, что нас ждет, но мы были готовы к любым испытаниям. Наши оппоненты привели с собой полупрофессиональго игрока с высоким рейтингом. Нам было неизвестно, что ожидать, но мы сумели победить со счётом 3:0. Это было волнующим для нас и всех наших болельщиков. Мы продолжили показывать хорошие результаты в последующих играх. Наши оппоненты принесли второго профессионального игрока. Мы смогли выиграть у него один раз, затем ещё один, но потом он смог победить нас. Мы анализировали каждую игру и были уверены в своих действиях. Один из ключевых моментов в игре был связан с выбором предметов. Противник выбрал стратегию , которая была нам неизвестна. Наши боты не были готовы к такому ходу, и это стало проблемой. Наш бот не умел использовать стики и накапливать их заряды, что существенно повлияло на исход битвы (информация исключительно для тру дотеров). Мы потерпели поражение в третьей игре против другого профессионала со счётом 3:0. Мы интересовались реакцией профессионалов на этот опыт, и получили разные мнения. Некоторые нашли это интересным и обсуждали его с увлечением, в то время как другие считали это неэффективным. Реакция профессиональных игроков была очень эмоциональной. Они никогда раньше не сталкивались с поражением от компьютера, поэтому это было невероятным опытом. Один из игроков, которого я лично победил, сначала был разочарован, но затем признал, что это хороший урок. Он даже позвонил мне через пять или десять минут и сказал, что это будет полезным для его практики. После того, как профессиональные игроки ушли, мы провели четыре часа обсуждая наших ботов и возможные улучшения... Мы начали обсуждать контринтуитивные стратегии, которые помогли победить. Профессиональные игроки интересуются тем, что может улучшить их игровую практику. Однако после мероприятия мы организовали большую LAN-вечеринку, на которой около 50 человек стали тестировать все возможные \"трюки\" в игре. Мы понимали, что боты могут учиться только в определенных условиях, и новые \"трюки\" могут быть для них полезны. Мы волновались о том, как применить наши знания в 5 на 5 играх, где можно использовать новые стратегии. Мы поняли, что большая часть нашей работы заключалась в поиске и исправлении алгоритмов поведения. Теперь я задумываюсь о том, как системы машинного обучения могут помочь улучшить наши боты. Эти системы дают нам возможность увеличить эффективность нашего программирования и быстрее адаптироваться к новым стратегиям. Это похоже на обучение человека, который изучает математику. Сначала он может быть запутан, но постепенно начинает понимать. Так же и наши боты учатся использовать новые стратегии и предметы в игре. Мы стараемся сделать все возможное, чтобы уменьшить количество работы, которую бот должен выполнять. Это позволяет ему лучше сосредоточиться на стратегии игры. Большая часть нашей работы заключается в том, чтобы обеспечить модели больше возможностей для изучения всех тонкостей игры. Наша работа похожа на производственный процесс, где каждый пытается сделать улучшения. От дня к дню наш бот постепенно улучшался, но был достаточно хорош, чтобы победить следующего профессионала. Мы поняли, что нам нужно загрузить новые параметры сети, чтобы улучшить его производительность. Но мы поняли это только за день до следующего мероприятия. Какое совпадение... Мы организовали специальное мероприятие, чтобы сыграть против Денди, одного из лучших игроков на то время. Мы также решили проверить нашего бота против других профессионалов, которые были физически здесь. После начал эксперимента мы провели несколько игр и получили обратную связь о проблемах, с которыми столкнулся бот. Мы поняли, что у нас был баг, который не позволял боту эффективно играть. Мы устранили эту проблему и запустили бота снова, чтобы проверить его исправность Он играл пять игр, и каждый раз проигрывал, пока наконец не понял, как использовать стратегию \"приманки\". Это стало примером того, как иногда отсутствие явного стимула приводит к тому, что модели находят неожиданные решения. И это также вызвало интересный психологический эффект на людей, которые реагировали на такую стратегию. Нам предстояло соревнование с лучшими игроками, и у нас был сломанный бот. Мы знали, что наш бот не справится. Поэтому мы решили объединить его с другой версией бота, который был сильным на первой волне. Мы смогли сшить двух ботов вместе за три часа, и это сработало так, что каждый бот играл определенное время, а затем переключался на другого. Мы завершили эту операцию за 20 минут до прихода лучших игроков. Было здорово, что мы смогли сделать это в такие краткие сроки. Сначала мы проверили его против T'II, который уже играл с нашим ботом, и он обыграл его три раза. Потом мы попробовали его против другого бота, и он тоже выиграл. Он просто веселился. В конечном итоге он сыграл десять игр, и это было круто. <Пропущенный кусок с описанием итераций игр против профессиональных игроков.> Иногда это было вызовом. Мы вынуждены были проявлять творчество, когда что-то неожиданное происходило, но это было частью процесса. Нам приходилось понимать систему на более глубоком уровне, чем обычно, потому что машинное обучение требовало такого подхода. Некоторые дни были трудными, но мы продолжали работать вместе с профессионалами. Это был хороший пример того, как боты могут обнаружить неочевидные стратегии, которые могут быть использованы против людей. В очередной игре с профессионалом мы выиграли 5-0. Мы также играли против группы профессионалов, и это было удивительно видеть, как они борются с ботом. Я думаю, что это интересно для анализа статистики. Есть ли игроки, которые последовательно побеждают ботов? Да, есть один, который имеет около 20% побед. Он играл много игр и нашел стратегии, чтобы победить. Это доказывает, что боты могут стать чрезвычайно сложными противниками, и игроки должны применять серьезные стратегии, чтобы с ними справиться. Игра профессионального игрока в видеоигры - это довольно высокая планка. Каждый хочет стать профессиональным игроком в видеоигры, но количество профессионалов очень мало. Концентрация: Повторное выполнение одних и тех же действий много раз требует высокой концентрации. Написание чистого кода: Предпочтение короткому, но чистому коду для минимизации ошибок. Чтение кода для отладки: Навык, позволяющий эффективно находить и исправлять ошибки. Это важно, потому что каждая ошибка в ML имеет очень высокую стоимость Минимизация ошибок - важный аспект при разработке программного обеспечения. Хотя невозможно избежать ошибок полностью, стремление к написанию чистого и надежного кода может существенно снизить их количество и улучшить качество продукта. Наш проект требует знания о распределенных системах и умения писать эффективный код. Кроме того, навыки в линейной алгебре и базовой статистике важны для проведения экспериментов. Наши проекты также требуют умения в области оптимизации. Смирение: Важное качество для работы инженера, особенно когда экспертность в области ослабевает. Коммуникация и обратная связь: Навыки, необходимые для эффективного взаимодействия с коллегами и принятия обратной связи. Вы правильно заметили, что использование инженерной дисциплины может значительно улучшить рабочий процесс и привести к созданию ценных инструментов. Это особенно важно в контексте машинного обучения, где техническая экспертиза часто требуется для создания и оптимизации алгоритмов. Образование и разъяснение вопросов, связанных с искусственным интеллектом и его этическими аспектами, могут помочь людям лучше понимать его влияние и принимать обоснованные решения в этой области. Игры действительно представляют собой замечательную среду для тестирования и обучения алгоритмов машинного обучения. Игровые сценарии могут представлять сложные ситуации, которые помогают алгоритмам развивать и улучшать навыки. Поэтому игры являются очень удобным испытательным полем, и, я думаю, вы увидите, что много работы будет сделано в области игр. Но, конечно, целью является вывод этой работы из мира игр и использование её для решения реальных проблем, для взаимодействия с людьми и выполнения полезных задач. Мне нравится проект Dota потому, что он позволяет нам понять, как будущее будет связано с продвинутыми системами искусственного интеллекта. Сейчас у нас мало представлений о том, как эти системы работают, где возникают проблемы и как с ними взаимодействовать. Игра дает возможность начать это понимание и взаимодействие с передовой технологией искусственного интеллекта. Раз вы дочитали до конца, буду рад видеть вас в своём тг канале. Aктивно выжимки из эссе Пола Грэма (находятся по #пг) Ежедневно по 1-2. Так же пишу про мой EdTech стартап и разные технические материалы, которые нахожу в процессе работы. Силой никого не тяну :) Software Engineer. Founder hikemyskill.com",
    "58": "Пошаговое руководство по созданию масштабируемого, чат-приложения реального времени с использованием серверных технологий... с небольшой помощью от NextAuth.js для входа через GitHub. Кому нужны WebSockets, когда у вас есть Live Queries? Не нам! Если вы создаете приложения, которые работают с данными в реальном времени, вы, вероятно, используете WebSockets. Они позволяют веб-браузеру и веб-серверу общаться в реальном времени, поддерживая постоянное соединение между ними - данные отправляются клиентам, как только они становятся доступными, а не когда клиент постоянно опрашивает сервер на предмет новых данных. Для обеспечения высокой эластичности и отказоустойчивости эти среды разработаны так, чтобы быть без состояния и временными по своей природе, что означает, что нет гарантии, что ваш код будет работать на одном и том же физическом сервере от одного запроса к другому - и, следовательно, нет постоянного соединения между клиентом и сервером. Итак, какое решение для создания приложений в реальном времени на серверных архитектурах? Давайте выясним! Давайте создадим этот чат в реальном времени в стиле Slack/Discord, используя Next.js в качестве нашего JS-фреймворка, Fauna (с использованием GraphQL) в качестве нашей базы данных, и WunderGraph в качестве Backend-for-Frontend (BFF), который обеспечивает связь между ними. Наше приложение также будет использовать вход через GitHub, и мы будем использовать знаменитый NextAuth (теперь Auth.js!) для наших нужд в области аутентификации. Прежде чем мы начнем, давайте обсудим, как именно мы планируем решить проблему данных в реальном времени, если мы не можем использовать WebSockets. Спецификация GraphQL определяет подписки - вы устанавливаете состояние WebSocket-соединения между клиентом и сервером, а затем подписываетесь на событие на сервере. Когда сервер видит событие, которое соответствует определенной подписке, он отправляет запрошенные данные по WebSocket-соединению клиенту - и вуаля, у нас есть наши данные. 💡 Это объяснение немного неоднозначное, но потерпите со мной. Обсуждение различий между транспортами graphql-ws (GraphQL через WebSocket), graphql-helix (GraphQL через SSE) и @n1ru4l/socket-io-graphql-server (GraphQL через Socket.io) немного выходит за рамки учебника. Когда вы не можете использовать WebSockets (на серверных платформах, как упоминалось ранее), вы не можете использовать подписки... но здесь на помощь приходят Live Queries. Они не являются частью спецификации GraphQL, поэтому точные определения отличаются в зависимости от клиентской библиотеки, но в сущности, в отличие от подписок, Live Queries стремятся подписаться на текущее состояние данных сервера - не события (новые полезные нагрузки всякий раз, когда запрос даст другие данные)... и в отличие от подписок, когда соединения WebSocket недоступны, они могут использовать старый добрый клиентский HTTP-опрос с интервалом. Я вижу, как читатели берут в руки вилы, как раз вовремя. И вы были бы правы! Но... здесь на помощь приходит WunderGraph. Видите ли, мы не собираемся опрашивать эти данные на стороне клиента. Вместо этого мы перекладываем эти обязанности по Live Querying на Wundergraph, наш Backend-for-Frontend, или API Gateway, как вы хотите его назвать. WunderGraph - это инструмент для разработчиков, который позволяет вам определить ваши зависимости данных - GraphQL, REST API, базы данных Postgres и MySQL, Apollo Federations и все, что вы можете придумать - в виде конфигурации как код, а затем он интроспектирует их, превращая все это в виртуальный граф, который вы затем можете запросить и изменить через GraphQL, а затем представить в виде JSON-over-RPC для вашего клиента. Когда вы делаете Live Queries из слоя WunderGraph, независимо от того, сколько пользователей подписаны на ваш чат в реальном времени на фронтенде, у вас всегда будет активен только один экземпляр опроса в любой момент времени, для всего вашего приложения. Гораздо лучше, чем сырой клиентский опрос; но это идеально? Нет - интервал опроса все равно добавляет задержку и не идеален для приложений, которым нужна действительно мгновенная обратная связь, и без JSON-патча, весь результат запроса будет отправлен по проводу клиенту каждый раз, даже неизменные данные - но для нашего случая это вполне подходит. Прежде всего, давайте поговорим о потоке этого приложения. Наши пользователи входят в систему со своим аккаунтом GitHub (через OAuth) для чат-комнаты. Мы создаем чат для рабочей группы/внутренней команды, и использование GitHub в качестве провайдера для этого имеет смысл. Использование NextAuth значительно упрощает нашу историю с аутентификацией - и у него даже есть официальная интеграция с Fauna - серверной базой данных! Это делает последнюю действительно хорошим выбором в качестве базы данных как для аутентификации, так и для бизнес-логики (хранение сообщений чата). Наш фронтенд Next.js относительно прост благодаря WunderGraph - он предоставит нам автоматически генерируемые (и типобезопасные!) хуки для запросов и мутаций, построенные поверх SWR от Vercel, и именно их наши компоненты будут использовать для получения (сообщения чата и список онлайн-пользователей) и записи (новые сообщения чата) данных. CLI create-wundergraph-app от WunderGraph - это лучший способ настроить как наш сервер BFF, так и фронтенд Next.js за один раз, так что давайте сделаем именно это. Просто убедитесь, что у вас установлена последняя LTS версия Node.js. Настройка NextAuth требует немного рутинной работы. Давайте сначала разберемся с базовым пакетом. Затем получите адаптер NextAuth для FaunaDB. \"Адаптер\" в NextAuth.js подключает ваше приложение к любой базе данных или системе бэкенда, которую вы хотите использовать для хранения данных пользователей, их учетных записей, сессий и т.д. Адаптеры технически необязательны, но поскольку мы хотим сохранять сессии пользователей для нашего приложения, нам понадобится один. Наконец, нам понадобится \"провайдер\" - доверенные сервисы, которые можно использовать для входа пользователя в систему. Вы могли бы определить свой собственный провайдер OAuth, если бы хотели, но здесь мы можем просто использовать встроенный провайдер GitHub. Прочтите документацию по OAuth GitHub, чтобы узнать, как зарегистрировать свое приложение, настроить его и получить URL и секретный ключ от вашего аккаунта GitHub (НЕ опционально). Для URL обратного вызова в настройках GitHub используйте http://localhost:3000/api/auth/callback/github Наконец, когда у вас есть идентификатор клиента GitHub и секретный ключ, поместите их в файл ENV вашего Next.js (как GITHUB_ID и GITHUB_SECRET) соответственно. Fauna - это географически распределенная (идеально подходит для серверной/Edge эры Vercel и Netlify) документно-реляционная база данных, которая стремится предложить лучшее из обоих миров SQL (моделирование на основе схемы) и NoSQL (гибкость, скорость). Она предлагает GraphQL из коробки, но самое интересное в Fauna - это то, что она делает тривиально простым определение хранимых процедур и их представление в виде запросов GraphQL через схему. (Они называют это User Defined Functions или UDFs). Очень крутые вещи! Мы будем использовать это щедро. Запишите ваш URL и Secret в вашем Next.js ENV (как FAUNADB_GRAPHQL_URL и FAUNADB_TOKEN соответственно) и переходите к следующему шагу. Чтобы использовать Fauna в качестве нашей базы данных для аутентификации, нам нужно определить ее коллекции (подумайте о таблицах) и индексы (все поиски в Fauna выполняются с их использованием) определенным образом. NextAuth проводит нас через эту процедуру здесь, так что это просто вопрос копирования и вставки этих команд в вашу оболочку Fauna. Сначала коллекции... ...затем индексы. Теперь, когда у нас есть Fauna, настроенная для удовлетворения наших потребностей в аутентификации, вернитесь в каталог вашего проекта и создайте файл ./pages/api/auth/[...nextauth].ts с этим содержимым: Вот и все, у нас есть некоторая базовая настройка аутентификации! Мы протестируем это через минуту. Но сначала… Как только мы закончим с аутентификацией, пришло время определить схему GraphQL, необходимую для нашей бизнес-логики, т.е. пользователей, чатов и сессий (с небольшими изменениями для учета существующей схемы NextAuth). Перейдите на вкладку GraphQL в вашей панели управления Fauna и импортируйте эту схему. Почему у чата есть надлежащее отношение к пользователям, но не к сессиям? Это ограничение того, как NextAuth в настоящее время взаимодействует с Fauna для хранения сессий пользователей - но мы обойдем это, как вы скоро увидите. Видите последний запрос, помеченный директивой @resolver? Это хранимая процедура Fauna (User Defined Function), которую мы будем создавать! Перейдите на вкладку Functions и добавьте ее. Знание синтаксиса FQL помогает, но эти функции должны быть самоочевидными - они делают именно то, что предлагают их названия - принимают аргумент и ищут значение(я), которые соответствуют ему, используя ранее определенные индексы. Когда они используются с директивой @resolver в нашей схеме, они теперь представлены в виде запросов GraphQL - невероятно полезно. Вы можете делать почти все, что захотите, с UDFs Fauna, и возвращать любые данные, которые захотите. WunderGraph работает, интроспектируя все источники данных, которые вы определяете в массиве зависимостей, и создавая для них модели данных. Сначала убедитесь, что ваш файл ENV настроен правильно... Замените на свои собственные значения, очевидно. Также дважды проверьте, чтобы URL Fauna был установлен в регионе, где вы размещаете свой экземпляр! ...а затем добавьте нашу базу данных Fauna в качестве зависимости в WunderGraph, и позвольте ему сделать свое дело. Затем вы можете написать запросы/мутации GraphQL для определения операций над этими данными (они идут в каталог .wundergraph/operations), и WunderGraph сгенерирует типобезопасные хуки клиента Next.js для доступа к ним. AllMessages.graphql Получение всех сообщений, вместе с их связанными пользователями. AllSessions.graphql Способ работы NextAuth с GitHub заключается в том, что он хранит текущие активные сессии в базе данных, с полем expires. Вы можете прочитать userId из активной таблицы сессий, и тот, кому принадлежит этот userId, может считаться текущим пользователем в сети. Таким образом, этот запрос GraphQL извлекает всех пользователей, у которых в настоящее время есть активная сессия - мы будем использовать это в нашем пользовательском интерфейсе, чтобы указать на тех, кто онлайн. WunderGraph делает тривиальным выполнение JOIN с использованием нескольких запросов - и используя его поле _join (и директиву @transform для уменьшения ненужного вложения), мы можем преодолеть то, что NextAuth не добавляет надлежащие отношения в Fauna - видно здесь при извлечении пользователя, связанного с сессией по их userId. С достаточно коротким временем жизни для токенов OAuth GitHub, вам не придется беспокоиться об устаревших данных, когда речь идет об онлайн-пользователях, и NextAuth достаточно умный, чтобы аннулировать устаревшие токены в любом случае, когда эти пользователи пытаются войти с истекшим токеном. UserByEmail.graphql CreateMessage.graphql И, наконец, это наша единственная мутация, срабатывающая, когда в настоящее время вошедший в систему пользователь отправляет новое сообщение. Здесь вы видите, как Fauna обрабатывает отношения в мутациях - поле connect (подробнее здесь) используется для соединения текущего создаваемого документа (сообщение чата) с существующим документом (пользователь). _app.tsx Вам придется обернуть ваше приложение в <SessionProvider>, чтобы иметь возможность использовать хуки useSession от NextAuth, предоставляя контекст сессии на верхнем уровне вашего приложения. Также я использую TailwindCSS для стилизации - инструкции по его настройке с Next.js здесь. index.tsx Хуки NextAuth делают реализацию авторизации тривиально простой - вторую часть аутентификации. Используйте useSession, чтобы проверить, вошел ли кто-то в систему (это возвращает объект пользователя с именем пользователя GitHub, электронной почтой и URL аватара - удобно!), и хуки signIn и signOut для автоматического перенаправления пользователей на эти страницы. Вы можете стилизовать собственные страницы входа/выхода NextAuth, если хотите (инструкции здесь), но стандартные, не брендированные стили вполне подходят для наших нужд. ./components/OnlineUsers.tsx Здесь нечего особо смотреть; наша стратегия определения онлайн-пользователей уже упоминалась в запросе GraphQL для этого. ./components/ChatWindow.tsx Здесь мы видим, как WunderGraph может превратить любой стандартный запрос в Live Query с добавлением всего одного параметра - liveQuery: true. Чтобы настроить интервалы опроса для Live Queries, проверьте ./wundergraph/wundergraph.operations.ts и отрегулируйте это значение в секундах. Для временной метки мы будем использовать простую вспомогательную функцию, чтобы получить текущее время в эпохе - количество секунд, прошедших с 1 января 1970 года (полночь по UTC/GMT) - и преобразовать его в строку, понятную человеку, для хранения в нашей базе данных. ./utils/epochToTimestampString.ts ./components/Navbar.tsx Компонент NavBar снова использует хуки useSession от NextAuth - сначала, чтобы получить текущее имя пользователя GitHub и URL аватара и отобразить их, а затем signOut для... ну, выхода из системы. Совмещение NextAuth, Fauna и WunderGraph для GraphQL Live Queries - это мощное сочетание, которое будет служить вам хорошо при создании интерактивных чатов в реальном времени - будь то простое создание для небольшого сообщества или сложные платформы уровня предприятия. Для серверных приложений использование WunderGraph в качестве backend-for-frontend с GraphQL Live Queries даст вам гораздо лучшую задержку, чем клиентский опрос - только один экземпляр опроса для всех клиентов, подписанных на приложение для чата, равно уменьшенной нагрузке на сервер и сетевому трафику. Оставьте liveQuery: true закомментированным, пока вы строите пользовательский интерфейс, и включите его только при тестировании функций чата. Вы не хотите нагружать сервер Fauna вызовами - особенно если вы на ограниченном бесплатном тарифе! Если вы вносите изменения в схему GraphQL Fauna во время разработки, вы, вероятно, обнаружите, что интроспекция WunderGraph не улавливает новые изменения. Это намеренно - он строит кэш после первой интроспекции и работает на основе этого, потому что вы не хотите, чтобы WunderGraph (который вы бы развернули в продакшене на Fly.io или WunderGraph Cloud) тратил ресурсы на полные интроспекции каждый раз. Чтобы обойти это, запустите npx wunderctl generate –-clear-cache, если вы внесли изменения в схему базы данных во время разработки и хотите, чтобы модели и хуки были сгенерированы заново. Развертывание этой комбинации довольно простое; WunderGraph - это сервер Go и может быть развернут на Fly.io или управляемой облачной платформе WunderGraph Cloud, Fauna уже имеет ваши данные в управляемых экземплярах, а ваш фронтенд Next.js может быть развернут на Vercel или Netlify. Кстати говоря - установите переменную среды NEXTAUTH_URL в канонический URL веб-сайта при его развертывании. Если вам нужно что-то прояснить по поводу WunderGraph в качестве BFF/API Gateway, загляните в их сообщество Discord, здесь. Удачного кодирования!",
    "59": "Настал тот момент когда необходимо уходить от всех зарубежных программ удаленного подключения, во всяком случае у нас в компании. Посмотрев отечественные аналоги мы пришли в ужас от стоимости и качества работы ПО. Поразмыслив какой функционал нам необходим для подключения 1-линии к пользователям поняли: А значит нам будет достаточно простого powershell'a. Powershell это очень хорошо, но запускать его жутко не удобно. Есть прекрасная возможность конвертировать полученный скрипт в exe файл",
    "60": "В 2023 году мы выпустили цифровое рабочее пространство Squadus. Продукт быстро приобрел популярность и стал востребованным: это полноценная замена решениям иностранных вендоров, ушедших с российского рынка. В частности, функциональность Squadus покрывает потребности пользователей сервиса Microsoft Teams, доступ к которому был ограничен в нашей стране. Приложение позволяет общаться в чатах, совместно работать над документами, проводить конференции и автоматизировать типовые действия с помощью Bot SDK. При этом все данные защищены от утечек. Мы продолжаем активно совершенствовать Squadus; свежий релиз 1.4 — уже второе обновление в этом году. В нём мы сосредоточились на функциях, улучшающих взаимодействие участников команд и повышающих скорость совместной работы. Теперь детально разберём важные обновления в релизе. Важный элемент работы коллектива — успешные видеоконференции. Чтобы при встрече или демонстрации проекта всё прошло гладко, необходимо учитывать много факторов: понимать, демонстрируется экран или отдельное приложение, включены ли камера и микрофон. Иногда бывает нужно скрыть от участников встречи внутренние документы компании или всплывающие окна почты с конфиденциальными письмами. Проконтролировать это поможет функция просмотра границы демонстрируемого экрана или конкретного окна, реализованная в Squadus 1.4. С ней вы будете понимать, какой именно файл или приложение показываете в данный момент (появляется зелёная рамка). Это особенно важно, если на рабочем столе открыто сразу несколько приложений или экранов. Опция доступна в настольной версии на платформе Windows. Просмотр файлов, документов и комментариев, которыми обменивались пользователи во время видеоконференции — также значимый элемент командной работы. В обновлённом Squadus все эти файлы, а также будущие диалоги, сохраняются в общем чате после конференции. Это обеспечивает удобство и бесшовность взаимодействия сотрудников. Новая функция особенно актуальна для компаний, которые продолжают работу в едином пространстве для встреч, общения и обмена документами после видеозвонков. Если нужно срочно обсудить важный вопрос или назначить серию встреч, достаточно нажать на одну кнопку. При этом необходимые документы и комментарии будут всегда под рукой. Улучшения также коснулись общения в чатах — мы реализовали возможности WYSIWYG-редактора. Теперь при вводе в окне сообщения текста, ссылки или кода, можно до отправки увидеть и отредактировать результат. Изменяйте внешний вид текста (жирный, курсив или зачёркнутый), форматируйте однострочные и многострочные блоки кода, вставляйте или редактируйте гиперссылки — привычное меню форматирования появится в сообщении автоматически. Редактор позволяет увидеть предварительный формат сообщения, чтобы сразу же внести правки. Возможность посмотреть и скорректировать символы кода особенно пригодится в работе ИТ-компаниям. «МойОфис стремится быстро отвечать на запросы клиентов и улучшать опыт использования наших продуктов. Так, в обновленном Squadus 1.4 мы реализовали продвинутые опции редактора сообщений и улучшенные функции чата видеоконференций. Эти изменения помогут нашим клиентам повысить эффективность командной работы и ускорить бизнес-коммуникацию. Кроме того, Squadus предлагает проработанные сценарии легкого и бесшовного перехода с популярных иностранных средств коммуникации на российские альтернативы без остановки бизнес-процессов», — заявила Елена Бардина, руководитель отдела продуктового маркетинга МойОфис. Сейчас российские пользователи не могут оплатить полную версию программы Microsoft Teams. Кроме того, компания приостановила все новые продажи продуктов и услуг в Беларуси и России. В этих условиях обращение к безопасному отечественному ПО — гарантия независимости от геополитических условий и санкционных рисков. В Squadus есть аналогичные Slack и Microsoft Teams группы коммуникаций, такие как команды и каналы, а также возможность их гибкого структурирования. Цифровое рабочее пространство от МойОфис совместимо с платформами на базе всех популярных операционных систем и подойдет компаниям любого масштаба — от небольших фирм до корпораций. Развёртывание Squadus на серверных мощностях клиента осуществляется по тщательно проработанной методологии МойОфис, проходит при всесторонней поддержке и консалтинговом сопровождении со стороны высококвалифицированных специалистов и инженеров. Если вы ищите замену иностранному ВКС, то можете оставить заявку на нашем сайте и протестировать Squadus. Узнать подробнее о том, как он устроен, можно из этих хабр-материалов: МойОфис выпустил Squadus — единое цифровое рабочее пространство. Рассказываем о новинке Как мы создаём Squadus. Реализуем «прыжок к сообщению» в мобильной версии",
    "61": "Автоматизация стала неотъемлемой частью профессиональной деятельности инженеров-проектировщиков. Главное, что стоит отметить: она существенно упрощает и ускоряет процесс проектирования, обеспечивает высокую точность и надежность создаваемых чертежей. Кроме того, современные инструменты проектирования позволяют вести совместную работу над проектом, обмениваться информацией и вносить изменения в режиме реального времени. А это, в свою очередь, помогает специалистам сосредоточиться на творческих и инновационных аспектах проектирования, способствует развитию и совершенствованию профессиональных навыков. 🔹оформление проектно-конструкторской документации по СПДС: быстро, просто, эффективно, на уровне всей организации. 🔹автоматическая векторизация и оцифровка сканов чертежей. Для начала хотелось бы отметить опыт, полученный специалистами компании «Нормасофт» при выполнении проектов с применением отечественного программного обеспечения. Командой нашего проектного подразделения выполнены крупные разработки: 1.Объект: Реконструкция здания цеха обжига и помещений сушильно-помольного отделения, прессового отделения (рис. 1). Заказчик: ООО «КиперШтат». САПР: Платформа nanoCAD с модулем «СПДС», программа nanoCAD Металлоконструкции. 2.Объект: реконструкция плотины Айского водохранилища города Златоуст (рис. 2). Заказчик: Инжиниринговая компания «ТОР», ФГАОУ ВО «ЮУрГУ (НИУ)». САПР: Платформа nanoCAD с модулем «Растр», программа nanoCAD Стройплощадка. Теперь о том, что в первую очередь стоит за внедрением САПР. Решение об автоматизации – это, как правило, способ преодоления стагнации в основных рабочих процессах. Исследуя различные предприятия, мы выявили следующий комплекс проблем (рис. 3): ❗затруднения, связанные с неотъемлемой частью проектирования – уточнениями и корректировками ключевых проектных решений. Решению этих и многих других сложностей служат инструменты, которые мы сегодня представим. Прежде чем перейти к рассмотрению модулей и приложений, отвечающих за автоматизацию 2D-проектирования, нужно понять, на чем они базируются. Основа этих решений, Платформа nanoCAD (рис. 4), включена за № 8814 в Единый реестр российских программ для ЭВМ и БД в информационно-телекоммуникационной сети Интернет. Платформа разработана компанией «Нанософт» с учетом российских стандартов, совместима с другими САПР, имеет удобный интерфейс и проста в освоении, является базовым программным инструментом 2D-проектирования, 3D-моделирования, создания цифровых двойников и реализации ТИМ (BIM). Оформление проектно-конструкторской документации по СПДС. Быстро, просто, эффективно, на уровне всей организации Модуль «СПДС» Платформы nanoCAD (рис. 5) позволяет быстро и качественно оформить конструкторскую документацию. Он одинаково надежен и в процессе классического 2D-проектирования, и как инструмент при работах по технологиям информационного моделирования (ТИМ). Модуль «умеет» создавать архитектурно-строительные чертежи зданий и сооружений, редактировать блоки архитектурных элементов и деталей, а также оформлять строительные чертежи (рис. 6). Одно из преимуществ этого продукта – наличие утилит оформления (рис. 7), которые позволяют объединять отдельные отрезки и тексты в таблицу nanoCAD, автоматически нумеровать объекты чертежа, изменять масштаб выделенной графики, а также создавать собственные объекты с вычисляемыми свойствами. Еще один серьезный плюс – поддержка формата BCF (рис. 8), который обеспечивает обмен информацией и совместную работу над проектами в формате BIM. Это позволяет фиксировать изменения в проекте и отслеживать их историю. Также следует отметить библиотеку параметрических объектов (рис. 9), включающую широкий набор наиболее часто используемых металлических профилей, сборных железобетонных изделий и деталей креплений. Все библиотечные элементы соответствуют стандартам и сериям. В число ключевых возможностей ПО входит создание координационных осей и отметок уровня (рис. 10). Поддерживается автоматическая генерация отчетов. Есть Мастер объектов, который позволяет создавать и редактировать параметрические объекты, а также архитектурные элементы. Кроме того, в модуле «СПДС» объекты можно масштабировать. Перейдем к автоматизации создания основных комплектов рабочих чертежей марок КМ, КМД, КЖ, КЖИ при помощи специализированных вертикальных приложений nanoCAD Металлоконструкции и nanoCAD Конструкции PS (рис. 11). Основой обоих приложений служит Платформа nanoCAD, на которую они устанавливаются. Рассмотрим эти решения более подробно. Программа nanoCAD Металлоконструкции предлагает весь функционал, который необходим при проектировании металлоконструкций, элементов из сборного и монолитного железобетона. Требуемые параметры объектов задаются в удобных диалоговых окнах. Работа организована в единой среде nanoCAD, благодаря чему поддерживается использование всех инструментов модуля «СПДС» и базы параметрических строительных объектов (металлопрокат, сборный железобетон, крепеж и т.д.) – (рис. 12). Основным функциональным элементом программы является Менеджер проекта. Он играет ключевую роль в структурировании будущего проекта, и от уровня владения этим инструментом зависит удобство всей дальнейшей работы. Чем более детально вы проработаете структуру в Менеджере, тем комфортнее вам будет управлять проектом в будущем. Менеджер проекта позволяет изменять параметры элементов, выделять выбранные элементы на чертеже, копировать и перемещать их между разделами, вести учет в спецификациях и ведомостях (рис. 13). Инструменты работы с комплектами рабочих чертежей КМ включают в себя постоянно обновляемую базу сортамента металлопроката (рис. 14).  Поддерживаются автоматизация графического оформления, создание узлов с автоматическими маркировками, выполнение подрезок и учет видимости линий элементов Также существует возможность разработки марок основных комплектов рабочих чертежей КМД. Программа автоматически генерирует специализированные ассоциативные ведомости и спецификации на основе выполненных чертежей проекта. Все это доступно и в разделе проектирования конструкций из железобетона. Что касается проектирования марок основных комплектов рабочих чертежей КЖ и КЖИ, то оно осуществляется с помощью специально разработанных программных средств и «умных» объектов (детали арматуры, раскладка арматуры, фоновое армирование, а также специальные объекты для создания свай) – (рис. 15). Такие объекты можно использовать как для детального усиления элементов конструкции, так и при усилении больших объемов – например, стенных плит. Следующий программный продукт –nanoCAD Конструкции PS – нацелен исключительно на автоматизацию разработки марок основных комплектов рабочих чертежей КЖ и КЖИ (рис. 16). Этот продукт будет полезен строительным компаниям, выполняющим соответствующие разделы проектирования. Проектным компаниям и девелоперам, работающим в сфере 2D-проектирования и не планирующим переходить в сферу BIM. Заводам ЖБИ. Среди ключевых преимуществ программы можно выделить ускоренную разработку проектной документации, наличие библиотеки типовых информационно-параметрических 2D-элементов, возможность настроить создание документации в соответствии со стандартами любого предприятия. Кроме того, благодаря встроенной базе элементов арматуры и сборного железобетона (рис. 17), nanoCAD Конструкции PS уменьшает риск появления ошибок. Кстати, эта база может быть редактироваться и пополняться. Первый модуль приложения – «КЖ» – отвечает за разработку марок основных комплектов рабочих чертежей КЖ и КЖИ в полном соответствии с российскими стандартами. Второй – «Фундаменты» – предназначен для подготовки схем расположения и чертежей столбчатых фундаментов на свайном и естественном основании. Также он поддерживает расчеты параметров основания для фундаментов колонн, свайного куста. Рассчитывает прочность по несущей способности сваи монолитных и сборных ленточных фундаментов. Модуль «Оформление» позволяет ускорить процесс оформления чертежей (рис. 18), выполненных с применением первых двух модулей. Для автоматизации процессов разработки проектов организации строительства (ПОС), организации демонтажа (ПОД) и производства работ (ППР) предназначен программный продукт nanoCAD Стройплощадка (рис. 19). С применением программы выполняется оформление стройгенплана, проектируются временные дороги, определяются потребности в кадрах и ресурсах, а также планируется строительство, подбирается строительная техника и рассчитываются временные коммуникации (рис. 20). Расчеты производятся на основе соответствующих методических рекомендаций и ГОСТов (рис. 21), среди которых: 📃Постановление Правительства РФ № 87 от 16.02.2008 г. «О составе разделов проектной документации и требованиях к их содержанию»; 📃МДС 12-46.2008 Методические рекомендации по разработке и оформлению проекта организации строительства, проекта организации работ по сносу (демонтажу), проекта производства работ; 📃МДС 12-81.2007 Методические рекомендации по разработке и оформлению проекта организации строительства и проекта производства работ; 📃ГОСТ 21.204-2020 Условные графические обозначения и изображения элементов генеральных планов и сооружений транспорта; 📃ОДМ 218.6.019-2016 Рекомендации по организации движения и ограждению мест производства дорожных работ; 📃ГОСТ 12.4.026-2015 Цвета сигнальные, знаки безопасности и разметка сигнальная Основным инструментом программы nanoCAD Стройплощадка является уже знакомый нам Менеджер проекта. С его помощью можно настроить все необходимые элементы. В первую очередь настраиваются объемы выполняемых работ с формированием перечня этих работ и расценки на них с привязкой к работам из перечня. Данные из Менеджера можно напрямую экспортировать в сметные программы (рис. 22). Далее создается список техники, которая будет использоваться на стройплощадке. Программа предоставляет обширную базу машин и механизмов со всеми необходимыми характеристиками, что позволяет подбирать технику по заданным параметрам (рис. 23). Объем различных материалов, взятых из базы или обозначенных в проекте вручную, результаты автоматического расчета площади складирования, схемы складирования материалов, полученные из готовой библиотеки, отображение складирования на строительном генеральном плане – все это учитывается в сгенерированных спецификациях проекта. Кроме того, в nanoCAD Стройплощадка есть функция календарного планирования, которая поддерживает построение диаграммы Ганта по списку работ и создание календарных графиков. Продолжительность работ рассчитывается автоматически или устанавливается пользователем (рис. 24). При оформлении стройгенплана в первую очередь настраиваются объекты площадки: здания, складские площади, ограждения. Далее наносятся схемы инженерных сетей, выбирается строительная техника. Обозначаются опасные зоны, откосы, выемки, насыпи, намечаются места установки информационных щитов и т.д. Еще один важный плюс: программа позволяет в автоматическом режиме формировать пояснительную записку в соответствии с Постановлением Правительства РФ № 87 от 16.02.2008 г. – в формате DOC и с параметрами, ранее указанными в Менеджере проекта (рис. 25). Результаты всех расчетов и обоснование выбора той или иной техники приводятся в соответствующих разделах записки. Также нужно отметить, что записка формируется по шаблону, который можно корректировать в соответствии с требованиями заказчика или внутреннего документооборота организации. Еще один модуль, о котором хотелось бы рассказать, – «Растр». Он входит в состав Платформы nanoCAD и предназначен для автоматической векторизации. Позволяет создавать различные векторные объекты из исходных растровых изображений, таких как точки, отрезки, дуги, окружности, контуры и др., которые затем могут быть отредактированы. После векторизации средствами модуля можно распознать типы линий, окружностей, дуг и полилиний, а также направления на отрезках и дугах (рис. 26). В модуле также представлено множество инструментов фильтрации монохромных изображений (рис. 27). Существует возможность сглаживания, утоньшения и утолщения линий. Доступны функции удаления «мусора», заполнения разрывов в линиях и сортировки объектов по типу и размеру. Применение этих команд не только повышает качество растровых файлов, но и уменьшает их размер. Отдельно отметим функцию бинаризации и разделения цветов (рис. 28). Она позволяет преобразовать цветное изображение в монохромный растр, где объекты (дороги, реки и т.д.) будут размещены на отдельных слоях в зависимости от их цвета на исходной картинке. Векторизация такого растра оказывается более эффективной, чем аналогичный процесс, выполняемый при черно-белом сканировании цветного оригинала. Документы, которые мы перевели в векторный формат, можно использовать в проектах и подгружать в программы и модули, представленные в этой статье. Подведем итог. Внедрение систем автоматизированного проектирования приносит предприятию множество преимуществ, включая повышение эффективности работы, сокращение временных и финансовых затрат, улучшение качества проектирования и упрощение процедур согласования. Программные комплексы являются неотъемлемой частью современной инженерной практики и помогают компаниям повысить конкурентоспособность. По материалам доклада на прошедшей в Екатеринбурге всероссийской инженерной конференции «ТИМ-практика: от внедрения до экспертизы».",
    "62": "Из первой части вы узнали (или узнаете, если еще не читали), почему дефицит в 20% от поддержки — стандартная частая рекомендация и что будет, если есть меньше, например, следуя очень низкокалорийным диетам и неожиданный положительный эффект таких диет. Из второй части вы узнали про побочные эффекты очень низкокалорийных диет и то, как на самом деле работает метаболическая адаптация. А в этой части поговорим про математику похудения и чем отличается дефицит, созданный снижением поступающих калорий от дефицита, созданного дополнительной активностью. Есть такая популярная формула: 1 кг человеческого жира = 8000 кк. Если мы хотим похудеть на 1 кг, нужно создать дефицит в 8000 кк. А если мы знаем, что наш дефицит 500 кк в сутки, значит за условные 2 месяца можно похудеть на 3.75 кг (500 кк * 60 дней / 8000 кк). Будет ли это верным предположением? Прежде чем ответить на этот вопрос, нужно сказать, что можно узнать точный уровень дефицита энергии, наблюдая за изменением состава тела. Если у вас есть два достоверных измерения состава тела (не биоимпеданс и не умные весы) как минимум в двух временных точках, то можно узнать уровень дефицит энергии. Почему необходим достоверный метод определения? Потому что жировая и сухая массы тела могут меняться независимо друг от друга. Та самая рекомпозиция (снижение % жира на фоне растущих мышц), это как раз демонстрирует. Еще важно отметить, что плотность метаболизируемой энергии в тканях (ккал/кг) отличается от привычных нам БЖУ 4-9-4. Это значит, что человек, набравший 4 кг сухой массы и потерявший 1 кг жира, при этом став на 3 кг тяжелее в общем, все равно будет в дефиците энергии, потому что 4*1816 - 9441 = -2177. Но для того чтобы потерять один 1 кг жировой ткани, которую большинство из нас обычно называют килограммом жира, нам уже не нужно жечь 9441 кк, а только 8260 кк. Жировая ткань на 87% состоит из чистых тригрицеридов, а остальное — вода и немного белков. Исследователи считают, что как раз столько необходимо “надефицитить” (8260 кк), чтобы потерять 1 кг жировой ткани. Так что да, формула вполне рабочая и надежная, а в исследованиях она служит ориентиром, на основании которого строятся предположения и ожидания. Если участники отклоняются от прогнозируемого темпа похудения, это может послужить основанием для недостоверности результатов исследования, потому что прогнозируемый темп — штука надежная. Но бесполезность использования этого расчета в бытовых условиях заключается в том, что мы не знаем точный уровень созданного дефицита энергии из-за чего могут возникать неточности в прогнозировании из разряда “два месяца держу дефицит в 1000 кк, а похудел только на 3 кг”. Случается это, например, потому, что сам метод подсчета не на 100% точный и даже при детальном подсчете будут погрешности. Частенько люди халатно относятся к регистрации еды, т.к. сам по себе метод достаточно утомителен. В попытке сэкономить время получается сэкономить только наетые килограммы. Но еще большим костылем является то, что в формуле энергетического баланса есть две переменные, и многие, делая расчет и прогноз, забывают о второй после потребления — расходе энергии. Люди очень сильно отличаются друг от друга в том, сколько они тратят энергии. Это мы сейчас не рассматриваем тренировки, где уровень подготовки определяет, сколько калорий каждый может потратить, а обычную бытовую активность. Например, в одном исследовании, где люди “просто сидели” в метаболических камерах (в которых особо нечего делать), разница к энергорасходе составляла 100-800 кк в сутки. В заключении исследовательское использовали термин «fidgeting», что можно перевести как «суетливость». В таких комнатах живут испытуемые. Заняться там реально нечем. Откуда тогда разница в расходе калорий? Вот такие суетологи могут сильно пострадать от дефицита калорий, потому что неосознанная и спонтанная активность в первую очередь снижается на минимум, когда жировая ткань начинает покидать тело. Мало активные люди меньше этому подвержены, потому что их бытовая активность и так в основном целенаправленна и снижать ее уже некуда. Это подтверждают и исследования. Компенсация энергозатрат более выражена у людей, ведущих активный образ жизни, и менее выражена или отсутствует у тех, кто ранее вел сидячий образ жизни. Так что дефицит калорий на бумаге может быть и 2000 кк, а фактически не превышать и 300 кк. Напомню, что многие называют метаболическую адаптацию “замедлением жиросжигания”, что вводит людей в заблуждение и склоняет к мысли о том, что организм теряет способность эффективно сжигать жир. Но это не так, ничего он не теряет. Просто при адаптации дефицит энергии становится меньше, потребности в энергии снижаются, как и необходимость в сжигании накопленного жира для компенсации дефицита. Это и приводит к замедлению похудения. Увеличив дефицит, ожидаемый темп снижения веса вернется. Несмотря на достоверную формулу расчета темпа похудения, как правило, на практике, реальный темп похудения ниже, чем расчетный. С точки зрения стерильных и неправдоподобных условий, нет никакой разницы в потере веса. Создаете вы 500 кк дефицита тренировкой или пропуском завтрака, худеть вы будете одинаково. Однако, если от стерильных условий перейти к реальным, где есть настроение, изменчивость поведения и т.д., все резко приобретает индивидуальный окрас и чаще мы стремимся к тому, чтобы контролировать дефицита за счет срезанного потребления. Вот почему. Обычно, создавать дефицит за счет увеличения активности не рекомендуют, потому что у нас нет точных способов определения расхода энергии. Умные браслеты, часы и прочая техника с этим отвратительно справляются их даже не будем рассматривать. Кроме всего мы склонны переоценивать то количество энергии, которое потратили на тренировке. Уровень воспринимаемых напряжения субъективен и не может служить объективной оценкой затраченных усилий. Начинающие тренирующиеся, которые чувствуют себя очень уставшими после тренировки, в действительности не могут потратить много энергии из-за низкой работоспособности и быстрой утомляемости. Впрочем, даже опытные атлеты подвержены не адекватной оценке. Еще увеличение осознанной активности (любые тренировки, любая целенаправленная активность, например, с целью дополнительной траты калорий) может влиять на всю оставшуюся активность (бытовую). \"компенсация энергии у типичного человека составляет в среднем 28% за счет снижения базальных энергозатрат; это предполагает, что только 72% дополнительных калорий, которые мы сжигаем в результате дополнительной активности, превращаются в дополнительные калории, сожженных в этот день\". И еще один установил среднее на 18%. А у некоторых людей гораздо более адаптивный метаболизм, чем у других. Оба анализа показали, что у некоторых людей компенсация аэробной тренировки составляет более 80 %. Хотя, как было обозначено выше, люди с сидячим образом жизни меньше подвержены такой адаптации, поэтому им увеличение активности точно поможет похудеть. Тем не менее тренировки лучше использовать как во время похудения, так и после. Силовые тренировки помогут поддерживать мышечную массу, которую организм тоже может терять в процессе снижения веса. С тренировками же проще поддерживать вес тела в дальнейшем после похудения. Так же тренировки многим помогают контролировать аппетит. Как заметили в одном исследовании, дефицит в 400 кк за счет тренировки на велоэргометре не приводил к повышению аппетита у участников, в то время как дефицит в 400 кк за счет снижения калорийности рациона приводил. Хотя, это может объясняться не способом создания дефицита как таковым, а тем, что тренировки снижают аппетит через активацию симпатической нервной системы — та, которая отвечает за реакцию бей или беги. Таким образом, физическая активность активирует реакцию, которая не располагает к приему пищи, тем самым улучшая контроль голода и аппетита во время диеты. Подводя итог, можно утверждать, что увеличение тренировочной активности можно полностью или частично компенсировать снижением бытовой активности, что не приведет к желаемому темпу похудения. А отсутствие точных методов определения энергорасхода приводит нас к тому, что использование повышения активности, как единственную тактику по созданию дефицита, будет крайне не эффективным методом стабильного и долгосрочного снижения веса (хотя все еще очень полезным для здоровья в целом). Тем не менее, лучше использовать тренировки в процессе снижения веса и после (на всю жизнь), так как они могут облегчать контроль диеты подавлением голода, поддерживают мышцы и защищают их от разрушения, и все таки могут создать дополнительный энергорасход, если получится его не компенсировать и еще раз — это очень полезно для здоровья. Я веду отличный телеграм канал, который часто называют «самым адекватным». Рекомендую подписаться. Там еще больше мудростей и полезностей про силовые тренировки, контроль веса, ЗОЖ и рациональный подход к изменениям. Фитнес тренер, профессиональный спортсмен.",
    "63": "Меня зовут Александр Денисов. Я работаю в компании Naumen и отвечаю за документирование и локализацию программного продукта Naumen Contact Center (NCC). В этой статье я расскажу о том, как у нас был автоматизирован процесс перевода документации с помощью нейродвижков без использования CAT-систем и каких-либо других инструментов перевода. 30 000 – 40 000 строк интерфейса. 3 000 – 4 000 страниц документации. Локализуем и переводим документацию на английский и немецкий языки. Обращается в бюро переводов, связывается с менеджером. Если компания уже более или менее опытная, то она готовит глоссарий. Отправляет менеджеру по E-mail глоссарий и тексты, которые нужно перевести. Практически любое бюро переводов использует CAT-систему, в которую менеджер загружает тексты и глоссарий. Дальше, либо переводчик сразу переводит тексты по предложениям, либо предварительно тексты переводятся привязанным нейродвижком. Далее, в зависимости от требований к качеству, может выполняться вычитка. В завершение, менеджер выгружает результаты перевода и отправляет компании-заказчику вместе с актом и счетом, и компания оплачивает работу. На схеме этот процесс можно представить следующим образом. В левой части можно увидеть, как все тексты в CAT-системе разбиваются по предложениям, в левой колонке – исходный текст, в правой – переведенный текст. В правой части можно увидеть, что для выделенной строки в глоссарии у нас есть, например, термин «Оператор», который должен переводиться как «Agent», а также есть термин «Проект», который должен переводиться как «Campaign». А еще есть такой проблемный термин, как «Обзвон», для которого в английском языке отсутствует нормальный перевод. В документации таких разработчиков решений для контактных центров, как Cisco или Avaya везде фигурирует сочетание «Outbound Dialing Campaign». И что интересно, что это сочетание «Outbound Dialing Campaign» содержит в себе слово «Campaign», что еще больше все осложняет. Т. о. у переводчика стоит непростая задача по выправлению всей этой терминологии в ручном режиме. Сначала мы переводили документацию с помощью бюро переводов на английский, частично на немецкий, потратили много денег. Но потом несколько лет не обновляли переводы, т. к. продажи не пошли. В итоге документация сильно устарела. Но приходит партнер и говорит, что документация устарела, а продавать то ему как-то хочется. И вот в чем собственно проблема: Средняя стоимость перевода – 2-3 рубля за слово. 250 слов на страницу. 3 000 – 4 000 страниц сложной технической документации. 2 языка – английский, немецкий. Надо 3-6 миллионов рублей. Сроки до полугода и выше. Качество – еще надо проверять. Но надо то все вчера, а денег уже никто не даст, клиентов то еще нет. Тем более, что один раз уже денег дали, а результата не было. В данной ситуации выхода уже никакого не было, кроме как поставить вопрос, а нельзя ли все просто перевести нейродвижком? И забегая вперед скажу, что да – можно! И ниже я постараюсь объяснить почему для нас это стало возможным, и что у нас уже получилось. Во-первых, качество нейродвижков с каждым годом становится все лучше, и на данный момент оно сопоставимо с качеством перевода среднего переводчика, да, где-то хуже, но где-то даже лучше. А нам и не надо для наших целей супер-качество. Нам надо переводить техническую документацию, нам главное, чтобы с помощью нее можно было решить задачу. Мелкие шероховатости нас мало волнуют. Да и вспомните Microsoft, они уже давным-давно переводят свою документацию (качество, надо сказать у них ужасное, но тем не менее), если уж в Microsoft себе такое позволяют, то почему мы не можем. Да и на текущем этапе мы даже не знаем, будет ли нашу документацию кто-то читать или она отправится в стол. А как переводят в бюро переводов? Все больше и больше бюро переводов также переводит нейродвижками. И чтобы это проверить, нужна независимая вычитка. Дак почему нам не сделать эту первоначальную работу самим, т. о. мы можем точно знать какое у нас качество и дальше уже принимать решения что с ним делать дальше. Как я рассказывал выше, нам нужен глоссарий, который загружается в CAT-систему. С недавних пор движки перевода научились применять глоссарий налету и выдавать перевод с учетом глоссария, при этом CAT-системы еще не умеют передавать глоссарий в нейродвижок или еще только учатся делать это. Т. о. было решено, что надо просто пробовать! Конечно, сначала было интересно понять, какой выбрать нейродвижок, какой из них лучше. И я приступил к тестированию. Изначально было известно, что лучшим считается DeepL, но хотелось убедиться лично какой движок переводит лучше и какой лучше работает с глоссарием. Сначала я просто протестировал движки, которые удалось подключить в CAT-системе и стало понятно, что там использовать глоссарий на стороне нейродвижка невозможно. После этого я стал тестировать движки, которые умеют работать с глоссарием, выбрал 3 дивижка: PROMPT, Yandex и DeepL. Ниже привожу небольшую таблицу сравнения. Сложность выбора еще была в том, что на момент начала тестирования DeepL не имел возможности применять глоссарий при переводе с русского на немецкий, поэтому изначально было принято решение делать двойной перевод с русского на английский Yandex-ом, а затем переводить с английского на немецкий с помощью DeepL. Но вскоре в DeepL появилась возможность применения глоссария с русского как на английский, так и на немецкий, и тестирование пришлось начинать сначала. В первую очередь интересовал глоссарий, т. к. как я сказал выше, у нас к нему есть специфические требования и поэтому для нас это очень важно. На изображении ниже можно увидеть пример применения глоссария нейродвижками. В верхней части пример перевода в CAT-системе без применения глоссария, а ниже перевод напрямую через API Yandex и DeepL. Т. о. можно увидеть, что наше сочетание «проект обзвона» переводится следующим образом: Без глоссария – «calling project». Yandex с глоссарием – «outgoing outbound dialing campaign». DeepL с глоссарием – «outbound dialing campaign». Как можно заметить, Yandex не плохо умеет применять глоссарий, но в таких сложных случаях как у нас он по факту дублирует слова, что, конечно, необходимо выправлять вручную. Но, как видно, DeepL решает задачу на отлично! Такой же результат мы увидели и с таким сочетанием как «исходящий обзвон», DeepL справился лучше и с ним. Т. о. мы поняли, что DeepL для нас лучше. Вся документация переводится за ночь. Еженедельные обновления занимают несколько минут. Перевод нейродвижком – это очень дешево. 2-3 рубля за слово превращаются в 0,003-0,004 рубля за слово: Полный перевод: 2-3 миллиона рублей (с помощью бюро перевода) превращаются в 2 000 рублей нейродвижком. Двухнедельное обновление – пара рублей. Тогда нам нужно сохранить все переводы и обеспечить себе возможность загрузки этих переводов в CAT-систему, чтобы отдать их на вычитку переводчику. Для этого переводы можно сохранить в XLIFF-файле. Качество можно улучшать постепенно, например, по мере поступления замечаний. Для этого необходимо организовать обратную связь. В моем случае я обрабатываю замечания от партнера. Изначально можно посмотреть, а какие из переведенных материалов должны быть качественными. Можно построить статистику по просмотру документации на исходном языке и сделать качественными только страницы с наибольшим просмотром. Вернемся к той схеме, что я приводил сначала. В итоге нам не нужно все взаимодействие с бюро переводов и CAT-системой. А нужно лишь отправить текст через API в нейродвижок, получить перевод и сохранить его. Т. о., в качестве R&D был написан скрипт на Python, который выполняет следующие действия: Выкачивает GIT-репозиторий в котором лежит проект с исходными текстами документации. Выкачивает из GIT-репозитория XLIFF-файл с двуязычными текстами и загружает его в память (если это первый перевод, то файл пустой). Выгружает по API из WebLate (инструмент, в котором мы делаем локализацию) XLIFF файлы, которые также могут быть полезны при переводе. Весь исходный текст разбивается на параграфы. Для каждого параграфа:- Проверяет есть ли перевод в XLIFF из WebLate. Если есть, подставляет.- Проверяет есть ли перевод в XLIFF от нейродвижка. Если есть, подставляет.- Если в сохраненных переводах ничего не найдено, то отправляет текст и глоссарий на перевод в нейродвижок. Полученные результаты подставляет в переводимый файл и сохраняет в XLIFF от нейродвижка. Сохраняет переводы в GIT-репозиторий. На схеме это можно представить следующим образом. После этого в одно и то же время на всех языках в автоматическом режиме может происходить сборка документации. Документация разрабатывается в MadCap Flare и структуру проекта примерно можно представить как на изображении ниже. Target – файл, который отвечает за сборку документации. Для каждого формата свой Target-файл, но к нему может быть привязан один и тот же TOC-файл. TOC (Table of Content) – оглавление. Может иметь древовидную структуру и включать другие TOC-файлы, к оглавлению как листики к дереву крепятся HTML-файлы. HTML – топики, которые содержат текст в формате HTML. HTML могут содержать сниппеты (Snippets), изображения (PNG-файлы) и переменные (Variables). Snippets – маленькие кусочки текста в формате HTML. Могут встраиваться в большие HTML-файлы, а также содержать в себе другие сниппеты, переменные или изображения. PNG – изображения (скриншоты или диаграммы). Variables – XML-файлы со строками (переменными). Переменные могут содержать только не форматированный текст, а также содержать другую переменную. Предполагается, что переменная может динамически изменяться. Скрипту перевода в качестве параметра указывается TOC-файл, он по цепочке выдергивает все тексты из всех файлов и переводит их. Нейродвижок не может угадать правильный перевод для элементов интерфейса. Как видно на изображении ниже, нейродвижок перевел название формы не так, как на скриншоте. Для решения этой проблемы мы вместо текстов интерфейса используем переменные, которые получаем путем конвертации файлов ресурсов из репозитория разработки в формат переменных MadCap Flare. Т. о. тексты из интерфейса всегда соответствуют интерфейсу и скриншотам (если они, конечно, не устарели). Влиять на контекст. Например, два одинаковых предложения, но содержащие разное название переменной могли переводиться по-разному, что не красиво, особенно тогда, когда предложения идут друг за другом, например в списке или таблице. Сами эти английские названия могут портиться и становиться не валидными. Например, у названия переменной после перевода может поменяться регистр. Для решения этой проблемы было решено все, что не должно переводиться, с помощью регулярных выражений, заменять на плейсхолдеры. Т. о. на перевод в нейродвижок отдается текст с плейсхолдерами, возвращается перевод с плейсхолдерами, а затем плейсхолдеры заменяются обратно: Теперь, когда перевод выполняет скрипт, встает вопрос, а что можно еще автоматизировать? Да все что угодно! Можно делать любые проверки и преобразования. Например, как известно, в английском есть особые правила написания заголовков. Первая буква каждого слова должна быть большой. Не составило большого труда найти библиотеку и после перевода конвертировать тексты в тегах h1-h6 в формат заголовка. На изображении ниже видно, как перевод выглядит в CAT-системе и какой перевод получается автоматически после перевода скриптом ниже. Непонятный текст в переводе может быть просто по причине того, что он плохо написан в исходнике. В данном случае мы сразу делаем задачу на правку исходного текста и не трогаем перевод. После изменения исходного текста просто выполняем перевод повторно. Неверный текст в переводе может быть по причине выбора неверных терминов или неверно переведенных терминов. В данном случае дорабатываем и правим терминологию в исходных текстах и пополняем глоссарий. Иногда при разборе структуры проекта скрипт находит различные другие проблемы, которые мы также исправляем, например:- Битые ссылки на топики, изображения, сниппеты и переменные.- Использование «запрещенных» нами тегов в HTML. В качестве выводов можно сказать, что на 99% удалось автоматизировать перевод текста. Да, иногда возникают некоторые проблемы, которые приходится устранять в ручном режиме, но, в любом случае, все эти проблемы незначительны по сравнению с тем, какой объем работы выполняется автоматически. На данный момент переведена подавляющая часть документации и ей уже пользуются партнеры. Есть еще много идей что можно автоматизировать и что мы уже пытаемся делать, постараюсь рассказать об этом в следующих статьях.",
    "64": "Как часто вам приходится сталкиваться с конструкцией sizeof(array)/sizeof(array[0]) для определения размера массива? Очень надеюсь, что не часто, ведь на дворе уже 2024 год. В заметке поговорим о недостатках конструкции, откуда она берётся в современном коде и как от неё наконец избавиться. Не так давно я бороздил просторы интернета в поисках интересного проекта для проверки. Глаз зацепился за OpenTTD — Open Source симулятор, вдохновлённый Transport Tycoon Deluxe (aka симулятор транспортной компании). \"Хороший, зрелый проект\", — изначально подумал я. Тем более и повод имеется — недавно ему исполнилось целых 20 лет! Даже PVS-Studio и то моложе :) Примерно здесь уже было бы хорошо переходить к ошибкам, которые нашёл анализатор, но не тут-то было. Хочется похвалить разработчиков — несмотря на то, что проект существует более 20 лет, их кодовая база выглядит прекрасно: CMake, работа с современными стандартами C++ и относительно небольшое количество ошибок в коде. Всем бы так. Однако, как вы понимаете, если бы совсем ничего не нашлось, то и не было бы этой заметки. Предлагаю вам посмотреть на следующий код (GitHub): С виду ничего интересного, но анализатор смутило вычисление размера контейнера _settings_client.network.default_company_pass. При более детальном рассмотрении оказалось, что lengthof — это макрос, и в реальности код выглядит так (чуть-чуть отформатировал для удобства): V1055 [CWE-131] The 'sizeof (_settings_client.network.default_company_pass)' expression returns the size of the container type, not the number of elements. Consider using the 'size()' function. network_gui.cpp 2259 В этом случае за _settings_client.network.default_company_pass скрывается std::string. Чаще всего размер объекта контейнера, полученный через sizeof, ничего не говорит о его истинных размерах. Попытка таким образом получить размер строки практически всегда является ошибкой. Всё дело в особенностях реализации современных контейнеров стандартной библиотеки и std::string в частности. Чаще всего они реализуются с помощью двух указателей (начало и конец буфера), а также переменной, содержащей реальное количество элементов. Именно поэтому при попытке вычислить размер* std::string* c помощью sizeof вы будете получать одно и то же значение вне зависимости от реальных размеров буфера. Убедиться в этом можно, взглянув на небольшой пример, который я уже приготовил для вас. Конечно же, реализация и конечный размер контейнера зависят от используемой стандартной библиотеки, а также от различных оптимизаций (см. Small String Optimization), поэтому результат у вас может отличаться. Интересное исследование на тему внутренностей std::string можно прочитать здесь. Итак, в проблеме разобрались и выяснили, что так делать не надо. Но ведь интересно, как к этому пришли? В случае OpenTTD всё достаточно просто. Судя по blame, почти четыре года назад тип поля default_company_pass изменили с char[NETWORK_PASSWORD_LENGTH] на std::string. Любопытно, что текущее значение, возвращаемое макросом lenghtof, отличается от прошлого ожидаемого: 32 против 33. Каюсь, не стал сильнее вникать в код проекта, но надеюсь, что разработчики учли этот нюанс. Судя по комментарию, после поля default_company_pass 33 символ отвечал за нуль-терминал. Legacy и небольшая невнимательность при рефакторинге — казалось бы, вот она, причина. Но, как ни странно, такой способ вычисления размера массива встречается даже в новом коде. Если с языком C все понятно — иначе никак, то что не так с С++? За ответом я пошёл в Google Поиск и не сказать, чтобы удивился... Прямо в самом начале, даже до основных результатов поиска, выдаётся вот это :( Здесь стоит сделать ремарку, что для поиска использовался приватный режим, чистый компьютер и прочие нюансы, которые отметают подозрения в том, что это поиск на основе моих прошлых запросов. Прим. автора: стало даже немного интересно. Напишите в комментариях, что показывает вам в топе выдачи по такому же запросу. Печально. Надеюсь, что ИИ, обучающиеся на текущем коде, не будут совершать подобных ошибок. Было бы некрасиво обозначить проблему и не предложить хороших путей решения. Осталось только понять, что с этим делать. Предлагаю начать по порядку и постепенно дойти до наилучшего на текущий момент решения. Итак, sizeof((expr)) / sizeof((expr)[0]) — это настоящий магнит для ошибок. Посудите сами: Если builtin-массив передали в функцию по копии, то sizeof на нём тоже вернёт не то, что надо. Раз уж мы тут пишем на С++, то давайте воспользуемся мощью шаблонов! Тут мы приходим к легендарным ArraySizeHelper'ам (aka \"безопасный sizeof\" в некоторых статьях), которые рано или поздно пишутся почти в каждом проекте. В стародавние времена — до C++11 — можно было встретить таких монстров: ArraySizeHelper — это шаблон функции, который принимает массив типа T и размера N по ссылке. При этом функция возвращает ссылку на массив типа char размера N. При вызове ArraySizeHelper компилятор должен будет вывести шаблонные параметры из шаблонных аргументов. В нашем случае T будет выведен как int, а N как 10. Возвращаемым типом функции при этом будет тип char (&)[10]. В итоге sizeof вернёт размер этого массива, который и будет равен количеству элементов. Как можно заметить, у функции отсутствует тело. Сделано это для того, чтобы такую функцию можно было использовать ТОЛЬКО в невычисляемом контексте. Например, когда вызов функции находится в sizeof. Отдельно замечу, что в сигнатуре функции явно указано, что она принимает именно массив, а не что угодно. Благодаря этому и работает защита от указателей. Если всё же попытаться передать указатель в такой ArraySizeHelper, то получим ошибку компиляции: Насчёт стародавних времён я не преувеличиваю. Мой коллега ещё в 2011 году разбирался, как работает эта магия в проекте Chromium. С приходом в нашу жизнь C++11 и C++14 писать такие вспомогательные функции стало намного проще: Скорее всего, далее вы столкнётесь с тем, что захотите считать размер контейнеров: std::vector, std::string, QList, — не важно. В таких контейнерах уже есть нужная нам функция — size. Её-то нам и нужно позвать. Добавим перегрузку для функции выше: Здесь мы просто определили функцию, которая будет принимать любой объект и возвращать результат вызова его функции size. Теперь наша функция имеет защиту от указателей, умеет работать как с builtin-массивами, так и с контейнерами, да ещё и на этапе компиляции. Ииии я вас поздравляю, мы успешно переизобрели std::size. Его-то я и предлагаю использовать, начиная с C++17, вместо устаревших sizeof-костылей и ArraySizeHelper'ов. Ещё и не нужно каждый раз писать заново: он становится доступен после включения заголовочного файла практически любого контейнера :) Ниже я также предлагаю рассмотреть пару распространённых сценариев для тех, кто вдруг попал сюда из поиска. Далее я буду подразумевать, что std::size доступен в стандартной библиотеке. В ином случае можно скопировать описанные выше функции и использовать их как аналоги. В большинстве случаев лучше использовать функцию-член класса size. Например: std::string::size, std::vector::size, QList::size и т.п. Начиная с C++17, рекомендую перейти на std::size, описанный выше. Также используйте свободную функцию std::size. Как мы уже выяснили выше, она может вернуть количество элементов не только в контейнерах, но в обычных массивах. Очевидным плюсом этой функции является то, что при попытке подсунуть ей неподходящий тип или указатель, мы получим ошибку компиляции. Также используйте свободную функцию std::size. В дополнение к неприхотливости в плане типа объекта она ещё и работает на этапе компиляции. Здесь возможны два варианта в зависимости от ваших потребностей. Если нужно только узнать размер, то достаточно воспользоваться std::distance: Если нужно что-то интереснее простого получения размера, то можно использовать read-only классы-обёртки: std::string_view для строк, std::span в общем случае и т.д. Например: Опытные читатели также могут добавить вариант с адресной арифметикой, но, пожалуй, я оставлю его за скобками, т.к. целевой аудиторией заметки являются начинающие программисты. Не будем учить их плохому :) В большинстве случаев придётся немного переписать программу и добавить передачу размера массива. Увы. Если же вы работаете именно со строками (const char *, const wchar_t * и т.п.) и точно знаете, что строка содержит нуль-терминал, то ситуация немного лучше. В таком случае можно воспользоваться std::basic_string_view: Как и в примере выше, получаем все достоинства view-классов, имея изначально только один указатель. std::char_traits — это настоящий швейцарский нож для работы со строками. С его помощью можно писать обобщённые алгоритмы вне зависимости от используемого типа символов в строке (char, wchar_t, char8_t, char16_t, char32_t). Это позволяет не думать о том, какую функцию требуется использовать в тот или иной момент: std::strlen или std::wsclen. Обратите внимание, что я не просто так уточнил про обязательное наличие в строке нуль-терминала. В противном случае получите неопределённое поведение (undefined behavior). Надеюсь, мне удалось показать хорошие альтернативы для замены такой простой, но опасной конструкции как sizeof(array) / sizeof(array[0]). Если вам кажется, что я что-то незаслуженно забыл или умолчал — добро пожаловать в комментарии :) Если хотите поделиться этой статьей с англоязычной аудиторией, то прошу использовать ссылку на перевод: Mikhail Gelvikh. How not to check array size in C++.",
    "65": "Всем привет! 7 апреля будет день рождения Рунета. Именно в этот день в 1994 году появился первый сайт в домене .RU. 30 лет назад! Как же давно это было. Такие даты навевают множество воспоминаний, в том числе о том как мы подключались к интернету через таксофон на улице. Предлагаю вам сегодня погрузиться в ностальгию вместе со мной. Мое первое знакомство с Интернетом произошло в 1992 году, когда я заканчивал школу. В Йошкар-Оле сложно было найти доступ к интернету, а точнее даже невозможно. Каких-то компьютеров и ЭВМ, которые можно было подключить к сети, тоже не было. В основном это были компьютеры РК-86, ZX Spectrum, которые мы собирали сами, у них не было интерфейсов, позволяющих сделать такое. У меня было два приятеля старше меня на 4 года. Они уже учились на втором курсе московского вуза. И вот один из них привез персональный компьютер на основе процессора 8086 из Москвы с «горбушки» (ДК Горбуново), а второй принес модем US Robotics внешний на 2400. И мы его подключили к обычной телефонной городской линии. Я тогда вообще не понимал, что такое интернет, зачем он нужен, что дает. То есть цель этого приключения мне в очень общих чертах была вполне понятна. Но после того, как друг всё подключил, настроил и начал заниматься студенческой работой через интернет, видение процесса вызвало крайне скептические настроения. У друга была лазейка для доступа через наш Мартелком (в настоящее время Ростелеком). Друг запустил терминал, подключился к IRC-чату и начал переписываться с американцами, но у меня это не вызвало никакого восторга. Вроде с кем-то общаешься, но не видишь человека и не понимаешь, кто он. Там были какие-то комнаты, в которых можно было найти себе группу людей по определенным знаниям или интересам и пообщаться с ними. В общем, я посмотрел на это все и не оценил. Остались непонятные чувства, что вроде что-то произошло, но ничего интересного. Интернет показался мне странным, но окей. Прошла пара лет, я уже учился в Марийском политехническом институте, в котором тоже интернета как такового не было. Первые два курса были общеобразовательные предметы, и они никаким образом не касались ИТ-сферы. А вот на третьем курсе я устроился работать к одному из проректоров заниматься документальной работой, в том числе мне доверили вопросы, связанные с администрированием серверов и настройкой различных сетевых служб и сетей внутри института. В тот момент в 1997 году в институте появился первый интернет. Тогда-то и произошло мое более плотное знакомство с ним. Институт выиграл грант Джорджа Сороса. И вроде бы все обрадовались: есть грант и выделены деньги на подключение к интернету, но технической возможности для реализации и прокладывания линий передачи данных, подключения, закупки каких-то модемов и так далее не было. Помог местечковый провайдер, который занимался телефонией. Сейчас это Ростелеком, а до этого была компания Мартелком. Наш ВУЗ скооперировался с Мартелком, чтобы они завели телефонные линии хорошего качества для подключения ADSL-модемов, которые бы позволили передавать данные на достаточно хороших скоростях (64 Кбит/с считалось лучшей скоростью в то время). Всего таких четыре модема, суммарно на весь институт 256 килобит. С современными скоростями, конечно, это вообще никак не сравнить. Нужно понимать, что это очень большой институт на три корпуса. Поэтому интернет был в институте очень «прикольный» — его практически не было днем, потому что все туда заходили массово, и пользоваться им нормально можно было уже только после того, как все уходили. Мы договаривались с охраной и оставались в корпусе до 2 ночи без зависающего интернета. Тогда я познакомился с первыми поисковыми системами. Был такой сервис, если я верно помню, назывался Yellow Pages. В нем был список доступных для посещения сайтов, в том числе несколько поисковых систем, которые можно было использовать. Мы в основном пользовались Yahoo, AltaLaVista, позже появился Rambler. Но тогда работа в интернете конечно сильно отличалась от нынешней. Нужно было забить в поисковик запрос и подождать минут 10-15, занимаясь пока своей работой. Загружаться могло достаточно долго, и все это отнимало время. Я скачивал достаточно много книг и другой документации, распечатывал документы, благо доступ был и к принтеру, и к сканеру, и бумагу купить было легко даже на студенческую стипендию. Позже появились первые MP3, и кроме как через рабочий интернет мне было неоткуда их получить. Еще из интересного. Помню, появился загрузчик бинарных файлов через интернет, который поддерживал докачку — ReGet. Мы им тоже начали активно пользоваться. Перед уходом домой я запускал ReGet, создавал в нем большое количество заданий на скачивание контента. И за ночь он это все выгружал. С утра по приходу на работу у меня уде был контент. Около 7 лет я работал в Эр-Телеком, где был ведущим администратором серверов и биллинга. Тут мы уже занимались именно «созданием», постройкой интернета в нашем городе. Нужно понимать, что в то время основные технологии доступа в Интернет, которые предоставлял тот же Мартелеком, представляли собой ADSL и DialUp модемы. На DialUp была максимальная скорость 56 Кбит/с в секунду, на ADSL 64 Кбит/с. Это максимум, наверное, который можно было получить по нашим телефонным городским линиям. И вот в 2006 году к нам пришел провайдер Диван-ТВ (в настоящий момент Эр-Телеком), и я начал работать там. Небольшая ремарка. Звучит смешно, но, когда я устраивался в эту компанию, не понимал до конца, что это за компания и что я там буду делать. Но там я познакомился с интернетом не через университетский компьютер, а через крутое, мощное оборудование крупного оператора связи. У нас на головной станции, где размещались все сервера и сетевое оборудование, канал составлял всего 10 Мегабит/с на весь город численностью более 200 тыс. человек!!! Это была оптика от ТрансТелекома – компании, которая предоставляет связь для железной дороги. То есть это был основной провайдер интернета в нашем городе. Ростелеком ничего нам продать не мог, потому что мы были его конкурентом. Но хоть у Ростелекома и были на тот момент большие мощности, технологии были старше наших. Мы в то время строили уже оптические каналы связи, соответственно, настраивали сервера, маршрутизаторы и коммутаторы. Я работал над созданием такой вещи, как безлимитный интернет. Это были тарифы 64 Кбит/с в секунду, 128 Кбит/с в секунду и 256 Кбит/с в секунду, как сейчас помню. То есть это были специальные сервера, их называли шейперами, потому что они делили между клиентами канал на небольшие полоски, которые мы раздавали дальше. Поделюсь с вами еще парой примечательных историй из прошлого. Отчасти, можно сказать, они помогли мне осознать важность ИБ в жизни еще когда самого термина ИБ не было. Когда я уже окончил институт, спутниковый интернет стал моим новым увлечением. Он был недоступным, дорогим и нестабильным из-за погодных условий, и единственное, что я смог купить, это б/у спутниковую тарелку марки SUPRAL. Она была 2,4 метра в диаметре. Сварил сам стойку для нее, залез на крышу пятиэтажного дома, где мы жили тогда, все это прикрутил. Мы с приятелем, с которым работали в Эр-Телеком, с помощью спутникового анализатора Promax эту тарелку настроили. Она еще у меня была с подвесом DiSEqC, что позволяло тарелку поворачивать на нужный спутник в автоматическом режиме и, соответственно, получать с него контент. Сам ресивер у меня был DreamBox 500s на Linux, он очень хорошо перепрошивался, и на него устанавливалась куча софта. То есть я занимался в тот момент так называемой «рыбалкой». В 2005-2006 году было достаточно плохо с шифрованием трафика в интернете. Все друг другу доверяли, весь контент был в любом канале в открытом доступе, будь то .mp3, какие-то веб-странички, почта и не только. Нудно было настроиться на определенный спутник, поймать нужный канал и спокойно «прослушивать» весь трафик. Это помогло мне собрать огромную кучу информации: песни, книги, софт и не только. Тогда данные не шифровали и не могли подумать, что их можно перехватить. Примерно в 1994 году у меня появился первый личный DialUp модем с шиной ISA. Это была моя первая железка, но я не мог её использовать. То есть она у меня физически была, но в квартире у родителей телефонная линия с частотным уплотнением не позволяла использовать стандартные модемы. Я договорился с другом и его родителями, которые жили этажом выше, что буду через их телефон выходить в интернет и отдавать им свои карманные деньги. Я отдавал, по-моему, 2 рубля в минуту. Да-да, раньше интернет был поминутный, не как сейчас. Для этого провел параллельную телефонную пару от их щитовой в свою квартиру.Это было очень дорого и позже я нашел очень интересный вариант решения проблемы. У нас на торце дома висел телефонный аппарат, тот самый что монетки принимает. Мы нашли в нем прикольный «баг». Если на этом телефонном аппарате резко нажать на рычаг от телефонной трубки и резко его отпустить, то с этого телефона можно звонить без монеток. Мы этим багом пользовались периодически, чтобы звонить «дворовым» друзьям и звать их играть на улицу. Я подумал: если этот телефон так умеет, то может с него можно выйти в интернет. Ну и действительно — сработало! Какое-то время мы были, скажем так, хакерами, пользовались интернетом через этот телефон, который висел на улице. Линии проходили через крышу нашего дома. Взобраться на чердак и подключиться к нему параллельно было несложно. Мы протянули витую пару из «полевки» к себе домой через балкон и подключили модем. Примечательно то, что ни у кого никогда не возникало вопросов по этой нашей самодеятельности, и даже проверки по дому не выявляли каких-то нарушений. Так мы и пользовались этим некоторое время. Надеюсь, вам было интересно почитать о моем опыте. Сейчас дети рождаются и растут с интернетом как неотъемлемой частью их повседневной жизни. А для нас с вами интернет когда-то был фантастикой, потом непозволительной роскошью, редкостью и лишь многим после обыденностью. С удовольствием обсужу с вами ваш опыт и разделю чувство ностальгии.",
    "66": "Итак, одним прекрасным утром вы проснулись и решили, что вам нужно попробовать Arch. Вам нравиться всё настраивать под себя, избавляясь от лишнего мусора и вообще вы хотите досконально разобраться в Linux. Но главной проблемой подобных дистрибутивов является то, что нужно уметь с ними работать. Хотя бы установить. И если изучение самого Arch ложиться сугубо на плечи читателя, то с установкой мы сейчас и разберёмся. Главное, что нужно знать - писать придётся много. Причём в этот страшный чёрно-белый терминал, в который вы хоть раз в жизни да заходили. С другой стороны, если всё делать по-инструкции и хоть немного разбираться в Linux - серьёзных проблем возникнуть не должно. Если же они всё-таки возникнут: Сам богоподобный и великий Arch Wiki, а именно статья по установке Arch. Видео, которое очень помогло мне разобраться в установке Arch Linux. Именно оно вдохновило меня на создание данной статьи, поэтому практически вся информация (за исключением некоторых редактирования и дополнений) взята именно оттуда. Пусть данная статья и не рассказывает обо всём в мельчайших деталях, но всю основную установку от А до Я мы здесь разберём. Заваривайте чай, читать придётся много. archinstall - псевдографическая утилита для упрощённой установки. Если не хочется повторять всё нижеизложенное, то можно попробовать данный способ. Команды для Vim. Скачивание образа. Делаем интернет. Разметка диска. Установка ядра. Точки монтирования для системы. Смена корневого каталога. Настройка времени. Локализация. Имя компьютера. Настройка хоста. Настройка аккаунтов. Настройка Vim. GRUB. Финальная настройка системы. :q - выйти.:q! - выйти без сохранения.:w - сохранить.i - режим ввода.Esc - обычный режим.x - удалить символ./ - поиск. Первое испытание для того, чтобы скачать Arch - найти образ системы. Благо это очень простая задача: заходим сюда, качаем и делаем загрузочную флешку (Rufus для Windows, Ventoy для Linux - да хоть dd в терминале Linux - это тема для отдельной статьи и рассматривать мы её здесь не будем). Для начала давайте посмотрим, как интернет доходит до компьютера. Если через кабель Ethernet, то всё должно подхватиться автоматически. А вот с Wi-Fi всё интереснее. Запускаем утилитку iwctl: Но будем надеяться, что вам хватит той информации, что есть здесь. Итак, для начала посмотрим, чем вообще мы можем ловить Wi-Fi и как это устройство назвала система: Тут же мы можем увидеть его состояние (вкл\\выкл). Если оно выключено, то исправляем это безобразие: Итак, всё включено, всё работает (по крайней мере надеемся на это). Теперь сканируем, выводим результаты и подключаемся к той сети, которая понравилась (и к которой у нас есть пароль). Ах да, SSID - это имя сети: Итак, всё готово? А вот сейчас и узнаем. Вводим команду обнаружения сетевых устройств, а затем проверяем соединение с любым сайтом (я предпочитаю linux.org): ip link ping linux.org 64 bytes from 104.26.15.72: icmp_seq=1 ttl=54 time=67.0 ms 64 bytes from 104.26.15.72: icmp_seq=2 ttl=54 time=67.6 ms 64 bytes from 104.26.15.72: icmp_seq=3 ttl=54 time=68.3 ms Начало положено. Теперь время сжечь мосты с предыдущей ОС на компе и сделать новую разметку диска. Для начала посмотрим, с чем мы вообще имеем дело: То, что мы видим - это наши диски. А теперь про то, как это вообще читать. Мы видим наши диски в формате '/dev/sdxY', где 'x' - это буква диска, а 'Y' - номер раздела. Нам понадобиться эта информация, чтобы не действовать наощупь. А увидим мы её в следующем формате: Это, кстати говоря, разметка моего Arch, поэтому можете ориентироваться на неё (Но учтите, что у меня BIOS. Если у вас UEFI, то к разметке добавиться раздел EFI, но об этом чуть позже). А теперь сама разметка. Переходим к диску, который мы хотим размечать (раздел не указываем): Мы перешли в саму утилиту fdisk, поэтому команды теперь представляют из себя лишь одну букву. Чтобы увидеть их все, вводим m. Затем выбираем таблицу разделов и на этом моменте мы немного остановимся. Таблица разделов - это то, где храниться информация о наших разделах и том, как мы разметили наш диск. Есть 2 вида - MBR (более старый) и GPT (более новый). Разница между ними есть и вот по каким критериям их выбрать: Современные материнские платы с UEFI заточены именно на GPT, поэтому для них предпочтительно использовать именно его. Некоторые UEFI вообще не поддерживают MBR. На старых системах с BIOS GPT может просто не поддерживаться. Если вы хотите раздел ёмкостью >= 2,2 ТБ, то ваш выбор GPT. Но если вам нужна более подробная информация - тогда вам сюда. Саму же таблицу разделов можно выбрать командами g (GPT) или o (MBR). Итак, после долгих душевных терзаний вы всё-же решили, какая таблица разделов вам больше всего подходит. Замечательно. Теперь по-порядку: Создаём новый раздел с помощью команды n. Выбираем его номер. Указываем начальный сектор (место на диске, где начинается раздел. Просто выбирайте тот, что даётся по-умолчанию). Выбираем конечный раздел. По-умолчанию даётся конечный сектор (до конца диска), но приберегите этот простор для домашнего раздела, где будут все ваши сокровища. Для увеличения (или уменьшения) раздела совсем необязательно высчитывать номер сектора, а можно просто прибавить ГБ, например вот так: +40G. Так можно химичить с МБ, КБ - да хоть с ПБ, лишь бы места хватило. Размечаем по следующей схеме, которая зависит от того, что у нас стоит на материнской плате: BIOS - swap-подкачка, корневой каталог /, домашний каталог /home. UEFI - efi, swap, корневой каталог /, домашний каталог /home. swap - как ОЗУ, только на жёстком диске. Рассчитываем размер по-формуле: ёмкость ОЗУ + 1 или 2 ГБ. Если ОЗУ =< 4 ГБ, то swap-раздел просто необходим efi - раздел, на котором храниться сам EFI (необходим для запуска системы на UEFI). Ёмкость где-то 550 МБ минимум или 1 ГБ для большей уверенности. Устанавливаем типы для наших разделов по команде t, затем номер нашего раздела и тип, который мы хотим ему присвоить. Список всех типов выводится по команде L. Будет он огромным, но нужны нам лишь следующие типы разделов: Linux swap - наш раздел подкачки. Linux filesystem - сама файловая система формата ext4 (по-умолчанию все разделы имеют именно этот формат). EFI System - раздел для EFI загрузчика. Где какой раздел? Команда 'p' любезно покажет вашу новую разметку и ёмкость каждого раздела, поэтому ориентируйтесь на них. Всё готово? Тогда вводим 'w'. Эта команда запишет изменения и выйдет из утилиты. А снова набрав 'fdisk -l' вы увидите новую разметку и её типы. Всё устраивает? Тогда приступаем к их форматированию. mkfs.fat -F32 /dev/sdxY Тут мы не только форматируем, но и активируем раздел подкачки:  mk.swap /dev/sdxY swapon /dev/sdxY mkfs.ext4 /dev/sdxY Когда всё отформатировано - время всё это примонтировать. Для начала примонтируем корневой раздел: base - основной инструментарий (cat, ls, cd и прочее). linux - само ядро. Их есть несколько видов, можно установить любое другое, но будем честны - нам главное, чтобы система просто работала, а значит стандартного ядра нам за глаза хватит. linux-firmware - основной пакет драйверов. Есть ещё и дополнительный, но этого вам должно хватить. Если у вас, конечно, не имеется какого-то непонятного самописного устройства сумрачного китайского гения, о существовании которого знают 3 китайца и вы сами. В таком случае поиск драйверов вообще может не иметь успеха. Следующий пункт нам нужен, чтобы система знала, какие разделы есть в её распоряжении и откуда её извлекать информацию о запуске. Делается всё это одной командой: В этот файл должны записаться все разделы, что мы делали. Но нужно проверить, всё ли записалось. Для этого вводим следующее: Если нам нужны часы (а нам нужны часы), то нам необходимо настроить время. Для начала делаем синхронизацию с сервером NTP: Теперь устанавливаем часовой пояс и проверяем. Регион и город берём свой: А затем делаем установку значения аппаратных часов на основе значения системных. Кстати говоря, при использовании DualBoot с Windows этот параметр перезапишется самой Виндой, имейте это ввиду: Тут нам уже придётся копаться в конфигах, но не переживайте - вы будете часто это делать в Arch, поэтому привыкните. Сначала заходим в файл локализации: vim /etc/locale.gen Затем ищем и раскомментируем (убираем # в начале строки) те языки, которые мы хотим. В нашем случае это en_US.UTF-8 UTF_8 и ru_RU.UTF-8 UTF-8. Потом сохраняем файл и выходим. Осталось лишь сгенерировать эти раскомментированные локализации и вписать язык системы в конфигурационный файл: locale-genecho \"LANG=en_US.UTF-8\" > /etc/locale.conf Один из самых простых пунктов. Заходим с помощью Vim по адресу /etc/hostname, вписываем имя своего компьютера (1 слово латинницей) и выходим. Всё делаем в Vim. Заходим по адресу /etc/hosts и вписываем следующее: Для начала сделаем пароль для пользователя root: вводим passwd и затем задаём пароль. Использовать его мы будем редко, поэтому лучше сделать его посложнее. Потом мы создаём аккаунт обычного пользователя, с которого и будем сидеть 99% времени. Для этого создаём аккаунт, задаём ему пароль, даём права для нормального пользования системой и смотрим, какие права мы ему дали: Затем мы устанавливаем sudo. Да-да, по-умолчанию его здесь нет: Потом мы химичим с Vim. Сначала мы делаем его редактором по-умолчанию (да, это вводим просто в терминал): Затем вводим visudo и в открывшемся файле для того. чтобы мы могли с обычного аккаунта запускать команды, раскомментируем следующую строчку: В качестве загрузчика будем использовать GRUB. Для этого его сначала нужно установить: Для UEFI также необходимо установить efibootmgr, подробности здесь и здесь. --target - версия загрузчика: i386-pc для BIOS, x86-64-efi для UEFI. --bootloader-id - имя для загрузочной записи GRUB, актуально для UEFI. --recheck - проверка установки. dhcpcd - для интернета в целом. iwd - для беспроводного соединения (именно с помощью этой утилиты мы подключались к интернету в начале). После перезагрузки логинимся как обычный (не root) пользователь и начинаем финальную настройку нашего Arch. Для того, чтобы нам сделать интернет вводим следующее (первая строчка нужна лишь при Wi-Fi подключении): sudo vim /etc/pacman/pacman.conf [multilib] include = /etc.pacman.d/mirrorlist Если же вам выбило ошибку - просто перезагрузите систему ещё раз и заново подключитесь к интернету. После нам нужно установить мелочи - графическую оболочку. Этот пункт необязателен, если вам нужно не более, чем работа в терминале без графических приложениздесьй, но всё же 99% графическая оболочка понадобиться. Я же буду использовать X11 как основу и i3 как оконный менеджер. Вы же вольны выбирать любые другие альтернативы, ибо данный пункт максимально свободен в своей реализации. Для начала установим сам Xorg и i3: Затем установим эмулятор терминала на свой вкус, лично я предпочитаю Alacritty. Ах-да, и красивые шрифты тоже не помешают, хотя их вы вообще можете не устанавливать, это чистая вкусовщина: echo \"exec i3\" >> .xinitrc Всё. Вводим startx, настраиваем клавишу Mod, терминал открываем на Mod+Enter. Поздравляю, вы установили Arch Linux. Теперь у вас есть система, которую вы контролируете от и до (ну не совсем, но до этого вам ещё нужно дойти). Если же эта статья найдёт своего читателя, то я напишу про настройку i3, Alacritty, добавление приложений в автозапуск и в целом как сделать из системы минималистичную конфетку. Удачи в использовании)",
    "67": "Всем привет! В этой статье я поделюсь нашим первым выпуском второго сезона рубрики \"Открытый микрофон\", в котором Николай Мухранов, старший системный аналитик в \"Спортмастер Лаб\" рассказал о менторстве в IT. Публикую расшифровку доклада и саму запись трансляции. Также по теме мы подобрали экспертов, которые поделились своей экспертизой и мнением, их комментарии - ниже по тексту. Алексей Сможенков, ведущий трансляции: Приветствуем вас на нашей рубрике \"Открытый микрофон\", где мы даём площадку для выступлений нашим коллегам и подписчикам. Сегодня мы поговорим об увлекательной теме, которая рано или поздно становится актуальной для любого специалиста, когда он дорастает до определённого уровня экспертности в своей области. Это - делиться своими знаниями с другими и помогать им в развитии и решении своих задач. Речь пойдет о менторстве в IT, а разбираться в нём мы будем вместе с Николаем Мухрановым, старшим системным аналитиком из SM Lab. почему важно мотивировать участников с саморазвитию и взаимопомощи в рамках профессионального сообщества. Менторство - это процесс, при котором более опытный профессионал, ментор, бадди помогает и передаёт свои знания менее опытному специалисту, ученику, помогая ему развивать свои профессиональные и личностные качества. Отличие менторства от простого обучения заключается в создании доверительных отношений между ментором и менти (учеником). Обычно, когда мы где-то учимся, не всегда у нас есть обратная связь высокого качества от преподавателя. Для ментора это шанс не только делиться своим опытом, но и перенимать опыт, ментор обучается сам в ходе обучения своего менти. Это помогает развивать свои коммуникативные навыки и поддерживать своего менти в развитии. Менторство - это процесс, куда входит и тренировка, и мотивация, и советы, достигаем общую цель, стремимся к результату, мы поддерживаем своего менти, наставляем, направляем. Для всего этого мы используем инструменты, подробнее поговорим о них чуть позже. Эмпатия и способность к пониманию. Очень важно \"видеть\", кто перед вами. К каждому человеку необходимо подобрать свой подход, который поможет раскрыть вашего менти. Глубокое знание своей области. Когда мы достигаем определённого потолка в своей карьере или определённого уровня экспертности, можно задуматься о том, чтобы эту экспертность передавать, тем самым помогая развитию сообщества. Умение обучать. Я хотел бы подчеркнуть, что это отдельный навык, который можно оттачивать и постепенно прокачивать его. Коммуникативные навыки. Способность вдохновлять. Важно не только критиковать и наставлять своего менти, но ещё и поддерживать его, чтобы менти не выгорел. Гибкость и адаптивность. Способность подстраиваться под менти, но также доверять своему менти и перенимать это доверие. Личностное развитие. Постоянная обратная связь от ментора позволяет менти осознавать свои сильные стороны и области для улучшения, способствует самоосознанию и личностному росту. Постоянное общение и передача знаний вашему менти помогает расти вам обоим. Целевое менторство. Важно установить конкретные, измеримые, достижимые, релевантные и временные (SMART) цели с вашим менти, чтобы направлять его процессу обучения и развития. Мы с менти проговариваем те цели, которые он хочет достичь, но прежде чем их фиксировать, мы определяем, на каком этапе развития находится наш менти. Это позволяет построить качественное обучение, выявить точки роста. Обратная связь 360 градусов. Важно вовлекать коллег менти, а также его руководителей или подчинённых. Это позволит предоставить более комплексную оценку и отметить те точки, от которых можно отталкиваться при наставлении своего менти. Обучение на основе проектов. Вовлекайте своего менти в реальные проекты, где он сможет применить на практике полученные знания и навыки, что способствует глубокому обучению. Улучшение навыков. Конструктивная обратная связь направлена на улучшение профессиональных навыков менти, помогает в корректировке и совершенствовании его технических и межличностных навыков. Для аналитика soft skills, помимо технического бэкграунда, играют очень важную роль. Важно учитывать, что все люди воспринимают информацию по-разному, и, адаптируя подачу материала под каждого конкретного менти, мы достигаем наибольшей эффективности. Miro-доски. Удобны в построении road map, создании общего с менти пространства, где можно фиксировать цели, результаты, позволяет легко отслеживать прогресс выполнения вашим менти задания. Онлайн-встречи с камерой. Зрительный контакт, который мы поддерживаем со своим менти, позволяет нам, находясь на расстоянии друг от друга, детальнее погрузиться в то, что вы делаете. Я призываю всегда при общении с менти включать камеру. Общее пространство. Кроме досок Miro можно использовать Google docs, хабы, файлообменники, телеграм-каналы, это позволяет отсматривать информацию и возвращаться к ней как самому менти, так и ментору, чтобы можно было её улучшать. Шаг 1. Самоопределение и постановка целей. Определите свои сильные стороны и интересы в IT. Это поможет выбрать направление менторства. Здесь также важно учесть более опытных менторов, к которым можно обратиться за советом и развитием. Это позволяет со стороны посмотреть на свои скиллы, где-то подтянуть их. Шаг 2. Построение сети контактов. Присоединяйтесь к IT-сообществам, конференциям, митапам, чтобы наладить связи с потенциальными менти и другими менторами и прокачать свои софт-скиллы через общение на широкую аудиторию. Шаг 3. Создайте личный бренд. Будьте активны в соцсетях, блогах и платформах. Таким образом мы можем транслировать свою деятельность, вовлекаться в обсуждения, обмениваться опытом, улучшить информацию. Найдите менти. Это можно делать через соцсети, профессиональные сообщества или реферальные программы в вашей компании, открытые площадки. Определите потребности менти. Проведите начальный скрининг по резюме/профилю вашего менти. Оцените свои силы, насколько вы действительно сможете помочь вашему менти. Ваши методы и подход должны соответствовать уровню и потребностям вашего менти. Если вы понимаете, что не сможете быть полезным вашему менти, лучше прямо об этом сказать и передать его другому ментору. Планируйте и следите за прогрессом. Разработайте индивидуальный план обучения для менти и регулярно отслеживайте прогресс Всегда совершенствуйтесь. Тут важно собирать обратную связь. Просите менти оценивать вашу работу. Это поможет выявить области для улучшения. Рефлексируйте и адаптируйтесь. Анализируйте свой опыт менторства, чтобы постоянно совершенствовать свои навыки и методы. Расширяйте своё влияние. Делитесь знаниями, создавайте контент, участвуйте в публичных выступлениях и обучающих сессиях, чтобы расширить своё влияние в IT-сообществе. Это также может помочь найти вам ваших первых менти. Наставляйте других менторов. Делитесь своим опытом с коллегами, которые также хотят стать менторами, чтобы вместе вносить значимый вклад в развитие IT-сферы.\\ Когда вы становитесь ментором, вы не только помогаете другим расти профессионально, но и сами получаете ценный опыт, новые знания и удовольствие от совместной работы и достижения успеха. Обратная связь от учеников сильно мотивирует, позволяет оценить себя. Этот опыт поможет продемонстрировать другим, что вы можете брать ответственность не только за себя, но и за других людей, своих учеников. Здесь всё очень индивидуально, зависит от задачи, с которой пришёл менти. В среднем у меня выходит 2-3 часа в день 3-4 дня в неделю на несколько менти. Количество сессий и их продолжительность важно проговорить с менти на берегу. Я лично столкнулся с проблемой взаимоотношений, так как все люди разные. Я для себя понял, что важно на берегу выстроить границы, правила, которым мы с моим менти будем следовать. К ним можно периодически возвращаться. Важно обсуждать с менти все возникающие проблемы. К договорённостям можно вернуться, обозначить, что происходит их несоблюдение, и, если продолжить их не соблюдать, то, вероятно, лучше в таком случае расстаться. Я, например, благодаря менторству могу осваивать новые инструменты, посмотреть на свои процессы под другим углом. Так, например, я стал использовать PlantUML, хотя раньше больше вместо BPMN. Я был знаком и раньше с этим инструментом, но именно благодаря менти я снова окунулся и понял, как это можно использовать в работе. Мне лично это позволяет раскрыть свои хард- и софт-скиллы. Конечно, обратная связь от учеников очень вдохновляет, когда, например, он сообщает о том, что получил желанный оффер. Ты развиваешь своего менти и в то же время сам подтягиваешься. Руководитель системных аналитиков SM Lab. Ведущий тренинга по менторству. Стал ментором 6 лет назад, последние два года является наставником для аналитиков в его командах. \"В докладе Николай, основываясь на своём опыте, сделал на упор на менторстве людей, с которыми, как правило, не приходится вместе работать над рабочими задачами. Мой же опыт по большей части связан с корпоративным онбординговым менторством и дальнейшим наставничеством аналитиков в компании. Применяемые подходы в этих случаях немного отличаются, поэтому точка зрения Николая для меня была интересна. Как верно отметил Николай, важнейшим аспектом в любом менторстве является наличие доверительных отношений с менти. Правда, на мой взгляд, в докладе тема выстраивания и удержания этих доверительных отношений не была раскрыта достаточно глубоко. По своему опыту скажу, что особенно сложно их выстраивать в корпоративном менторстве, когда ментор выступает не просто другом и помощником, а принимает решение об успешности прохождения испытательного срока. Мне понравилось, что Николай подчеркнул важность пребывания ментора в профессиональных сообществах для постоянного развития. Тут прямо в точку. При обучении и наставничестве очень важно самому быть в тренде современных практик. Считаю ценным, что Николай коснулся темы дифференцирования подходов к обучению в зависимости от особенностей менти по восприятию информации. Рекомендую развить эту тему и стараться смотреть ещё в целом на психотип человека. Одной из таких типологий является модель DISC. Изучение этой темы в своё время мне помогло неплохо расширить кругозор в плане разнообразия мышления людей, хоть и применять подходы следует с осторожностью без навешивания ярлыков. \"Понятие менторинга очень широкое, замечаю, что есть разные интерпретации. От того, что это взаимное развитие двух людей, как в вашем случае, до того, что менторинг - это вид наставничества, где ответственность лежит не только на менторе, а в большей степени на менти. Думаю, что справедливо сказать: каждая ситуация индивидуальна и зависит от тех договоренностей, к которым придут двое людей. Очень сложно найти и связаться с ментором. Так как это сотрудник со своими задачи и проблемами, не всегда готов инвестировать время в развитие. Также грустная ситуация, когда ментора заставляют быть ментором, этот процесс всегда должен быть добровольным, чего, к сожалению, не всегда получается добиться. Я бы рекомендовал посмотреть на менторинг в разных сторон. Углубиться в тему рапорта между ментором и менти, методов мотивации обоих и создания между ними доверительных взаимовыгодных отношений. Также всегда интересен вопрос: а как сделать ментором того, кто не хочет, но обладает важной экспертизой?\" Руководитель направления аналитической поддержки проектов организационного развития в \"SM Lab\", опыт наставничества и менторинга с 10+ менти в разных компаниях, лидирует и преподаёт в образовательных проектах внутри компании в направлении бизнез-анализа и процессного управления. \"Я бы сравнила менторство со своего рода трамплином, который позволяет прийти из точки А в точку Б гораздо быстрее и увлекательнее, чем это получится, используя многие другие способы освоения новых знаний. И главное здесь - четко и правильно сформулированная цель. И, конечно, важна более субъективная составляющая - это некий мэтч, который обязательно должен произойти между ментором и менти, чтобы результат их совместной работы выглядел как формула 1+1=11. Спикер очень ёмко раскрыл в своем докладе критически важные качества ментора, аспекты менторства и эффективные стратегии, а также обоюдную пользу этого процесса. На мой взгляд, к потребителям пользы здесь можно отнести и сами компании, чьи представители - с одной стороны - созрели до готовности разделять свой опыт с другими коллегами своей сферы, а с другой - вовлечены в свою деятельность настолько, что готовы выделять дополнительные ресурсы на более глубокое погружение в текущую (или новую) профессиональную область. Это прекрасный опыт обмена знаниями и их последующей трансформации и развития. Я бы добавила, что ключевое отличие и преимущество менторства над другими форматами обучения - это адресность подхода. Ментор упаковывает свои знания под запрос менти, менти получает выжимку опыта и инструменты, которые помогают конкретно ему на его текущем отрезке пути. Хочется добавить, что менторство - это очень трудоемкая и ресурсозатратная деятельность. И чтобы разобраться, действительно ли я могу и хочу идти в это, либо поддержать себя, как практикующего ментора на пути к нарастающему успеху - важно найти в этом процессе зону вдохновения и свой профит. Как и в случае с менти - точно понять, какие цели я ставлю перед собой и какой результат будет меня вхохновлять продолжать свою просветительскую деятельность.\" \"Информация, представленная в выступлении, полезная и применимая на практике. Я не являюсь ментором в IT, я, скорее, наставник для новичков, приходящих в подразделение. Но в моей практике также применимы и эффективны описанные методики, такие как: постановка целей по SMART, обучение на основе конкретных проектов и задач, обратная связь. Составление дорожной карты и оценка промежуточных итогов также являются важными моментами для успешного обучения. Мне показалась не достаточно раскрытой тема мотивации для ментора делиться своим опытом. Упор был сделан на эмоциональном подкреплении после обратной связи. На мой взгляд, также важно, что менторство позволяет систематизировать свои знания. Компетенция ментора из категории неосознанной компетенции преобразуется в осознанную. Когда ментор не только сам владеет информацией или функцией, но и может системно и доходчиво передавать знания другим. При стремлении к постоянному самосовершенствованию и развитию у ментора появляется важная миссия, которая придает жизни смысл и заряжает позитивом, особенно при положительной обратной связи.",
    "68": "Я Илья Пухов, Старший менеджер продукта в Дм-техе, где занимаюсь маркетплейсом Детского мира. До этого PM в Авито и основатель маркетплейса Гильдия Квестов. Статья основана на воркшопе по метрикам маркетплейсов CPO Outdoorsy и ветерана индустрии Колина Гардинера (1). Я дополнил материал основываясь на моем профессиональном опыте, и знаниях из лекций значимых авторов в нашей сфере. Например, Елены Серегиной из Яндекса и Ильи Красинского. Постарался заполнить пробелы и создать полноценное руководство. Кое-где с комментариями из российской практики. Цель публикации конструктивная обратная связь и развитие знаний в предметной области. Предлагаю вам ознакомиться и прокомментировать. Поехали! Это процессы создания инструментов для сбора данных о продукте, изучения и интерпретирования данных, получения инсайтов, и коммуникации полезных сведений заинтересованным лицам. Проще говоря — это процесс получения полезной информации из данных. «Что невозможно измерить тем невозможно управлять» — Питер Друкер, самый известный теоретик менеджмента. «Сделай свои данные сам» «Единственная проблема с метриками — эта кроличья нора не имеет конца. Всегда можно копнуть глубже, пока не потеряешься совсем» — Колин Гардинер, автор воркшопа на котором основана статья. В традиционном бизнесе вы сами оказываете услугу или продаете товар клиенту. В маркетплейсе ваша задача свести продавца и покупателя, фасилитировать сделку. Отсюда важные отличия в продуктовой аналитике и метриках. В обычном бизнесе вам надо справиться с одной стороной сделки — Покупателем. Желательно, чтобы доход за время жизни клиента (CLTV) покрывал все расходы, связанные с привлечением (CAC) и оказанием услуги (COGS) и другие расходы, покрывал риски, финансировал развитие, и возможно оставлял немного денег акционерам. В маркетплейсе вы работаете с двумя сторонами — продавцом и покупателем. Желательно обеспечить большое число сделок, то есть высокую Ликвидность. Для этого вам нужно много покупателей и поставщиков, здоровое соотношение между ними (Byer-to-Seller Ratio), высокая Плотность профессиональных продавцов, и высокая Эффективность транзакции. Крупные маркетплейсы имеют и другой источник прибыли — ритейл медиа. Судя по открытой отчетности, доходы от рекламы на Озон сопоставимы с доходами от комиссионного бизнеса, а доходы от рекламы в Амазон сопоставимы с общим доходом группы компаний Мета (запрещенной в России). Тему рекламы на маркетплейсах оставлю для другой статьи. Задача не простая. В маркетплейсе очень много данных и источников. Программные решения тяжеловесные, требуют дорогой инфраструктуры и высококлассных специалистов. Аналитика включает в себя разные сферы. Маркетинг и продажи, платежи и финансы, менеджмент категорий товара и листингов, поиск, поведение пользователей, показатели продавцов, безопасность, ИТ инфраструктура и так далее. Задача собрать данные из разнородных систем, нормализовать и поместить в одно место данные. Чтобы потом проанализировать и отдать в инструменты отчетности. В следующих шести разделах мы рассмотрим основные метрики маркетплейса. Не обязательно использовать все из них. По моему опыту разные команды могут трактовать метрики по-своему, отходя от предлагаемых здесь определений. Считаю это нормальным, при условии, что внутри команды метрики формализованы, и все говорят на одном языке. Gross Merchandise Value (Валовая стоимость товаров) — Общая стоимость товара или услуг, проданных через маркетплейс в период времени в денежном выражении. Исключая возвраты, обмены и скидки разумеется. В мире маркетплейсов это основной показатель. GMV не является выручкой маркетплейса, но позволяет оценить объем продаж и динамику бизнеса. Например, GMV Озон в 2023 включая услуги составил 1,7 трлн. рублей, а чистый денежный поток от операционной деятельности ~ 80 млрд. рублей. Average Order Volume (AOV) — Средняя стоимость заказа на торговой площадке за определенный период. Позволяет прогнозировать потоки доходов и управлять бюджетом. Take rate — Средний процент комиссии, взимаемого маркетплейсом с каждой транзакции. За вычетом отмен, возвратов и скидок. Может отличаться по категориям товаров и состоять из разных компонентов. Например, комиссия за хранение на складе и возвраты от покупателя и логистику может быть переменной. Вознаграждение за продвижение товара в Take rate обычно не входит. Net Revenue (Чистый доход)  = GMV * Take rate. Произведение валовой стоимости товаров на Take rate показывает фактический доход маркетплейса от комиссий за транзакции на платформе. Gross margin (Валовая маржа) = (Net Revenue - COGS) / Net Revenue. Грубо говоря это процент прибыли, который остается после вычета стоимости проданных товаров. «Валовая прибыль» и «Валовая маржа» — это разные понятия: «Валовая прибыль» это сумма в деньгах, в то время как «Валовая маржа» — это процент или коэффициент. Contribution margin (CM) = Gross margin — Переменные расходы. Может рассчитываться не только в денежном выражении, но и как процент от Net Revenue. Отличие от предыдущего пункта в том, что Gross margin учитывает только прямые затраты фасилитацию транзакции, а Contribution margin учитывает все переменные затраты бизнеса. Зачем различать Gross margin и Contribution margin? На ранней стадии бизнеса у вас может быть слишком маленькая база покупателей, чтобы оплатить все переменные расходы. На этом этапе для LTV анализа лучше использовать Gross margin. У зрелого бизнеса покупателей достаточно много, и даже большие расходы на поддержку и продажи могут быть покрыты доходами. В этом случае лучше использовать Contribution margin Юнит экономика маркетплейса может выглядеть хорошо на уровне Gross margin, но на уровне Contribution margin быть убыточной. Колин предлагает смотреть CM в двух разрезах — CM на одного покупателя и CM на одного продавца. Первое поможет определить какой CAC вы можете себе позволить. COGS (стоимость проданных товаров) — в COGS маркетплейса НЕ входит закупочная стоимость товара, так как маркетплейс обычно не закупает товар для продажи. Вместо этого в COGS могут входить операционные расходы, связанные с обеспечением транзакций на платформе, такие как расходы на обработку платежей, обслуживание клиентов, маркетинг и развитие технологий. Variable costs — затраты, которые изменяются пропорционально объему производства, но не связанные с каждой единичной транзакцией.  К ним можно отнести расходы на маркетинг требующие. Постоянных инвестиций, например бренд медиа. Так же переменные части вознаграждения продавцов, тарифные планы ПО и другие расходы. Fixed costs — расходы, которые остаются постоянными независимо от объема продаж бизнеса. Арендная плата, ФОТ сотрудников, ИТ инфраструктура, лицензии ПО и так далее. Ликвидность — Доля объявлений, которые завершаются сделками за определенный период времени. Основной показатель способности маркетплейса генерировать сделки, и ключевой фактор успеха. Вот некоторые прокси метрики ликвидности: Конверсия (из визита)— Отношение числа транзакций к числу посетителей. Конверсия (из поиска) — Иногда маркетплейсом пользуются и покупатели и продавцы, и сложно считать их визиты отдельно. В этом случае можно считать как отношение числа транзакций к числу поисков. *Лид (от англ. Lead — зацепка) — контактные данные покупателя, изъявившего намерение совершить сделку. Конверсия в лид — Отношение числа лидов к числу посетителей (или поисков). В случае, если сделка не завершается на маркетплейсе, как бывает в модели классифайд или агрегатор, площадка «отгружает» продавцу лид. Маркетплейс достоверно не знает, совершилась ли сделка после. В случае комиссии за факт продажи нужны дополнительные усилия для проверки факта сделки. Например обзвон покупатеелй. Dated search — число поисков с указанием даты. В сфере путешествий такой поиск может показывать более явное намерение к покупке. Хотя я с трудом представляю себе сайт в категории путешествий, на котором могут быть поиск без указания даты. Bid-to-ask spread — Отношение цены товара к стоимости, которую готов заплатить покупатель.  Выяснить бывает непросто, понадобятся опросы. Если спред большой вы просите больше денег чем покупатель готов платить. Транзакция не произойдет. Ликвидность на площадке будет низкая. Buyer-to-Seller Ratio (Marketplace ratio) — Отношение числа активных покупателей к числу активных продацов. Вам еще понадобится определить, кого считать активным. Коеффициент показывает сколько покупателей может обслужить продавец, или сколько покупок может сделать покупатель. Считается в определенный период времени, в определенной категории. В частотных видах товаров и услуг коэффициент обычно выше, так как покупатель чаще приходит совершить покупку. Сравнивать коэффициент имеет смысл между похожими категориями/магазинами/территориями, используя наиболее развитые из них как бенчмарк. Концентрация показывает долю выручки приходящуюся на когорту (группу, сегмент) продавцов. Обычно меньшее число продавцов делает большее число продаж — правило Парето. Эти продавцы ваш профессиональный сегмент, самые выгодные партнеры. Их нужно развивать. Один из важных вопросов для вас будет — как перевести больше покупателей в профессиональный сегмент. А концентрация покажет, насколько вы преуспели. Обычно сюда относят Число активных пользователей, Среднее время сессии, CTR (Click Through Rate), Bounce rate, Число транзакций, NPS (Net Promotion Score), Retention rate, Churn Rate и так далее. Остановимся на особенностях этих метрик в маркетплейсе. *Когорта — группа пользователей объединенных по одному признаку. Например, все покупатели, совершившие первую покупку в таком-то месяце. Метрики юнит экономики имеет смысл считать по когортам, чтобы избежать ошибки среднего. Рекомендую материалы по когортному анализу Ильи Красинского. Retention cohorts (сторона продавца) — метрика показывает, насколько долго и в каком объеме удерживаются продавцы на площадке. Когортой в этом случае называют группу продавцов, которые начали торговать на площадке в один период времени. Retention рассчитывается для каждой когорты отдельно. Потом его можно сравнивать между когортами. Здоровая динамика, когда продавцы задерживаются на площадке достаточно долго, чтобы общее число продавцов и листингов было оптимальным, при разумных инвестициях в привлечение. Retention можно считать по-разному. Простой способ — число месяцев со времени последней поставки товара. Такой метод работает пли логистической схеме FBO, когда продавец завозит товар на распределительный центр маркетплейса разом. При логистических схемах с разовыми поставками, например FBS будет сложнее. Более продвинутый Retention может выглядеть как отношение GMV поставщика на 12й месяц работы к GMV поставщика в первый месяц работы. Показатель показывает, насколько сократился бизнес поставщика на площадке. Реальность такова, что большинство поставщиков не получат хорошие продажи на маркетплейсе. Например, три четверти товаров на складе Вайдлбериз за пол года ни разу не были проданы. Это плохо для поставщиков, но не для маркетплейса. Маркетплейс все равно заинтересован привлекать как можно больше поставщиков, чтобы увеличить номенклатуру товаров на витрине и поднять конверсию в покупку. То есть какой-то отток поставщиков это нормально. Важно, что он не должен быть больше чем приток новых. Другой вариант Retention — Доля опубликованных товаров спустя n-месяцев работы. Если ваш листинг слишком быстро раскупают это проблема, ведь вам нужно время и деньги, чтобы «завести» новый товар и поставщиков. Repeat cohorts (сторона покупателя) — показывает, насколько долго удерживается покупатель на площадке. Можно считать как доля покупателей совершивших вторую или n-ую покупку. Или среднее число транзакций на покупателя. *Life time (Срок жизни клиента) — Среднее время активности покупателя. Под активностью чаще всего понимают время, когда покупатель продолжает совершать покупки. Lifetime Value (LTV) — Средний gross margin (или contribution margin) с клиента (или когорты клиентов). Метрика показывает какую выручку вы можете ожидать получить с клиента за время его жизни. LTV — важная область оптимизации для многих компаний — как получить устойчивый CAC, который позволяет иметь хороший LTV. LTV юнита может быть положительным, но вы можете быть в минусе, из-за больших расходов, например, на поддержку. Подробнее про LTV анализ читайте ниже. Customer Acquisition Cost (CAC) — Стоимость привлечения покупателя, включая все расходы на маркетинг (performance и бренд медиа), маркетинговую платформу, комиссии и зарплаты продавцов, их инструментов. Имеет смысл считать по когортам. Для бенчмаркинга иногда удобно различать Loaded CAC, включая зарплаты маркетинга и продавцов и Unloaded CAC, исключая зарплаты. CACD — Стоимость привлечения покупателя. Если сравнить с LTV позволяет понять является ли текущая модель маркетинга устойчивой, или бизнес держится только за счет инвестиций. CACS — Стоимость привлечения продавца. Вместе с оценкой необходимости в продавцах (сторона предложения) позволяет прогнозировать динамику роста маркетплейса. LTV анализ помогает бизнесу понять потенциал прибыли в долгосрочной перспективе, найти и сфокусироваться на самых выгодных сегментах покупателей. Оптимизировать стратегии привлечения и удержания. Оценить эффективность маркетинга и продаж, и сфокусироваться на лучших каналах привлечения. Найти точки роста выручки, через улучшения Retention и числа повторных покупок. Колин дает довольно краткое описание LTV анализа. Более подробно с темой можно познакомиться в двух статьях на Медиуме: (2) \"Diligence at Social Capital Part 3: Cohorts and (revenue) LTV\" и (3) \"Spaghetti graphs — a better solution for measuring customer engagement\". Ссылки в конце статьи. Планирую написать об этом в другой публикации. Для компаний ранних стадий удобнее использовать Gross margin. Клиентская база еще слишком мала, чтобы оплачивать все переменные и фиксированные расходы. На этом этапе компания еще может быть операционно убыточной и зависеть от инвестиций. Для зрелых компаний честнее использовать Contribution margin. На этой стадии мы ожидаем что бизнес будет покрывать все свои расходы из выручки, и даже быть прибыльным. Прибыль нужно считать по когортам. Чтобы разделить группы клиентов по времени их появления и избежать ошибки усреднения. Обычно когорты считают по месяцам. Моментом, когда пользователь попадает в когорту удобно считать событие конверсии. Например первая покупка. Для анализа других эффектов можно считать началом когорты и иные события, например регистрацию. Для LTV анализа лучше подходит первая покупка. Когорты следует рассчитывать для покупателей и продавцов отдельно. Так же полезно разделять сегменты рынка и территории. Колин Гардинер приводит график Эндрю Чена из Uber просто по тому, что график ему нравится. Это НЕ LTV анализ, но в общем график относится к теме и хорошо иллюстрирует различия темпов роста бизнеса в разных обстоятельствах. Видно, что число поездок в месяц от момента открытия бизнеса в Китае растет гораздо быстрее, чем в других территориях. Uber запустился в Китае позже, и к тому времени они уже умели хорошо делать продукт. Возможно локальные особенности рынка и талант команды тоже сыграли на руку. На следующем графике приведен уже действительно LTV анализ. По оси абсцисс отложены месяцы с момента первой транзакции. Кривые раскрашены по годам. Не все спагетти одинаково полезны. Некоторые растут круче, что говорит что ценность производимая когортой для бизнеса растет с ускорением. Чем выше коэффициент наклона тем лучше. Как правило, именно такие спагетти вы хотите видеть в своих когортах. Высота начальной точки графика тоже важна. Она показывает сколько прибыли приносит когорта в первый месяц. Чем выше этот показатель, тем лучше общий результат когорты. Скачки показывают сезонные всплески покупок. Строительство таких графиков требует времени и большого числа данных. Чем раньше вы начнете отслеживать эти параметры тем лучше. Популярный эвристик такой — LTV/CAC = 3:1 в первые три года. Возьмите данные по прибыли за последние 36 месяцев и разделите на три. Если исторических данных не хватает — экстраполируйте. Переводя на простой язык Колин говорит, что если клиент окупается за первый год, это хорошо. На мой взгляд этот показатель сильно зависит от ниши и фазе экономического цикла в стране. В спокойные периоды инвесторы готовы подождать прибыли чуть дольше, во времена спада инвестиции в проекты с долгим сроком окупаемости снижаются. В России этот срок все еще ниже, чем в США и Европе. Большинству маркетплейсов приходится стимулировать продажи скидками, что снижает и комиссию. При низкой комиссии маркетплейс зарабатывает мало, и не может себе позволить высокую стоимость привлечения. Для маркетплейсов это обычно означает, что надо получать существенную долю трафика из органики. По низкой цене. Ситуация может быть несколько лучше в нишевом маркетплейсе, открытом на базе действующего омниканального бизнеса. Например, Летуаль, Мвидео, Спортмастер, Детский мир и так далее. Эти маркетплейсы сразу «питаются» клиентской базой сильного бренда родителя. В этой конструкции есть и проблемы, о которых я планирую написать в другой публикации. На представленном графике отмечен CAC. Это показывает инвесторам и стейкхолдерам момент, когда покупатель (или когорта) окупается. LTV/CAC = 3:1 — это хорошая начальная точка. Как дальше оптимизировать? Для этого нужно сбалансировать множество показателей: Трафик, конверсию, стоимость первой сессии, средний чек, число покупок на клиента и т.д. Колин не дает подробного ответа на этот вопрос, по этому я советую обратиться к курсу по юнит экономике Ильи Красинского. Там в табличках google sheets вы моделируете разные точки роста и чувствуете на конкретных примерах, какая точка роста дает больше эффекта. Особенностью маркетплейса при LTV/CAC оптимизации является его двусторонняя природа. Нужно учитывать стоимости привлечения и покупателя и поставщика. Для этого оценивать каждую сторону отдельно. Общий CAC маркетплейса можно рассчитать из годового прогноза продаж. Допустим, вы планируете обеспечить 20 млрд. рублей GMV в этом году. Сколько вы заработаете на комиссии и рекламе, и сколько из этого вы готовы потратить на привлечение? Разделить общий CAC между покупателями и поставщиками сложнее. Можно использовать вот такую формулу. Общая стоимость привлечения  складывается из CAC продавца и CAC покупателя умноженного на отношение числа покупателей к числу продавцов. Не забывайте, что каждый маркетплейс уникален как снежинка. Вам нужно наилучший способ считать юнит экономику для себя. Осмысление этого воркшопа стало для меня увлекательным и не простым путешествием. Я вижу два направления развития этой темы: изучение статей других авторов, и проверка теоретических выкладок на практике. В моей ежедневной работе. Буду рад делиться наблюдениями в дальнейших публикациях. Если вы дочитали до этого момента, должно быть вам и правда интересна эта тема. Напишите мне в телеграм и я постараюсь ответить на ваш вопрос. Меня легко найти по имени и фамилии. Воркшоп Колина Гардинера по метрикам маркетплейса 1 час — https://www.youtube.com/@EverythingMarketplaces Статья Алекса Тассига Spaghetti graphs — a better solution for measuring customer engagement на Медиум (копия в google doc)— https://docs.google.com/document/d/1zNeJigCy1zM3-kOBiQnAeTyVngmigCSSiPxbkyMPi_0/edit?usp=sharing Статья \"Diligence at Social Capital Part 3: Cohorts and (revenue) LTV\" на Медиуме — https://medium.com/swlh/diligence-at-social-capital-part-3-cohorts-and-revenue-ltv-ab65a07464e1",
    "69": "Системный аналитик играет важное значение в успешной реализации проектов в области информационных технологий. Эффективность и успешность разработки программных продуктов прямо зависят от качественной постановки задач и ясного определения требований. Вопрос заключается в том, как формулировать требования правильно, чтобы обеспечить эффективное взаимодействие между заказчиком, аналитиком и разработчиками.В данной статье мы рассмотрим основные принципы, практики и рекомендации, которые помогут системному аналитику сделать процесс формулировки требований более ясным, понятным и успешным. Мы детально рассмотрим ключевые моменты, аспекты и советы, которые помогут не только правильно поставить задачу, но и достичь желаемого результата в процессе разработки программного продукта. Для успешной реализации проектов в IT-сфере критическое значение имеет профессионализм системного аналитика и его умение точно определять требования. Важно, чтобы системный аналитик выделил для себя четыре ключевых аспекта, которые помогут ему эффективно управлять процессом и добиться успеха: Конкретизация и четкость формулировок задач. Разделение проекта на более мелкие и управляемые части. Детальное прорабатывание требований с четкой структурой. Учет безопасности данных и производительности системы. Разработчики ожидают от системного аналитика четко и полно описанной задачи, где определены цель, входные данные, ожидаемые результаты, возможные ошибки и обработка исключений. Причина или корень проблемы: В описании задачи необходимо описать вводные, которые послужили инициативой для доработки. Например, \"У пользователя платформы нет возможности направить на согласование документацию\". Описанный сценарий: Пользователь платформы не имеет возможности направить документацию на согласование. Потенциальная проблема: Отсутствие функционала направления документов на согласование может привести к задержкам в рабочих процессах и неэффективной документообороту. Важные аспекты: В описании бизнес проблемы также следует учитывать важные детали, такие как частота возникновения данной проблемы, важность доработки для пользователей и бизнеса. Сценарий воспроизведения проблемы: Описание сценария возобновления ошибки или проблемы, поможет не только разработчику и другим системным аналитиком, но и автору постановки. Цель задачи: В описании необходимо четко указать цель задачи - что требуется сделать, какой конкретный результат ожидается в итоге. Например, \"Разработать новую функциональность для платформы, позволяющую пользователям загружать и просматривать фотографии в высоком качестве.\" Входные данные: Спецификация входных данных является ключевым элементом описания задачи. Разработчики должны точно знать, какие данные им нужно использовать или получить для выполнения задачи. Например, \"Информация о формате загружаемых фотографий, максимальный размер файлов, доступные форматы изображений.\" Ожидаемые результаты: Описывать желаемые результаты работы системы или функциональности. Например, \"Пользователи должны иметь возможность загружать фотографии в формате JPEG, просматривать и увеличивать изображения без потери качества.\" Возможные ошибки и обработка исключений: Важно предусмотреть возможные сценарии неудач и предложить механизмы их обработки. Например, \"Если загружаемый файл не соответствует допустимым форматам, система должна выдать сообщение об ошибке и предложить повторить загрузку в другом формате.\" Как итог, четкое и подробное описание задачи от системного аналитика предоставляет разработчикам понятный и прозрачный набор требований, что способствует эффективной реализации проекта и улучшает взаимодействие в команде разработки. Для системного аналитика крайне важно организовать постановку задач для бэкенд- и фронтенд-разработчиков таким образом, чтобы оба специалиста могли работать параллельно и не ждать друг друга. Четкое разделение задач: Разделите процесс разработки на конкретные задачи для бэкенд- и фронтенд-разработчиков. Например, бэкенд-специалист может начать работу над созданием логики отправки и хранения документов в базе данных, пока фронтенд-разработчик занимается созданием пользовательского интерфейса для загрузки и просмотра документов. Для этого системный аналитик сформирует метод отправки извещения, который он приложит к обеим постановкам. Для бэкенд-разработчика аналитик опишет что должен делать метод, а для фронтенд разработчика при каких сценариях он должен вызываться. Коммуникация: Ключевым элементом успешной доработки системы является эффективная коммуникация между аналитиком, бэкенд- и фронтенд-разработчиками. Регулярные обсуждения, четкое выражение требований и обратная связь помогут устранить возможные недоразумения, улучшить координацию и синхронизировать усилия команды для достижения общей цели. Детализация и структурирование информации играют важную роль в процессе разработки, так как уточненные указания помогают разработчикам лучше понять требования и как именно нужно выполнить задачу. Вот несколько ключевых моментов, которые могут быть использованы для структурирования задачи: Описание функциональности: Подробно опишите как новая функциональность должна работать, какие действия пользователей она должна поддерживать, какие результаты ожидаются при ее использовании. Интерфейс: Если задача связана с разработкой пользовательского интерфейса, укажите требования к дизайну, расположению элементов, их внешнему виду и поведению. Если имеются макеты, обязательно приложите их к постановке. Технические аспекты: Если задача требует изменений на уровне базы данных, логики приложения или других технических аспектов, укажите соответствующие детали и требования. Например, для фильтрации поисковой функции, будет уместно приложить SQL запросы с применением фильтров. На этапе анализа это поможет исключить ошибки и конфликты данных. Тестирование: Опишите какие тесты должны быть проведены для проверки новой функциональности или внесенных изменений, укажите критерии успешной проверки. Взаимодействие с другими компонентами: Если новая функциональность должна быть интегрирована с другими системами или компонентами, укажите определенные требования к этому процессу. Структурирование информации в соответствии с вышеперечисленными пунктами поможет разработчикам четко понять задачу, предотвратить недопонимания и ускорить процесс ее выполнения. Учет безопасности и производительности является критически важным аспектом при разработке программного обеспечения. Для того чтобы задача была успешно реализована, необходимо уделить должное внимание требованиям к безопасности, производительности и другим аспектам. Разберем более подробно, какие моменты стоит учесть: Необходимо предусмотреть механизмы аутентификации и авторизации пользователей, чтобы гарантировать только авторизованный доступ к системе. Защита от уязвимостей, таких как инъекции кода, CSRF, XSS и других атак. Шифрование данных для предотвращения утечек конфиденциальной информации. Оптимизация запросов к базе данных и работы с данными для увеличения скорости отклика системы. Кэширование данных для ускорения доступа к повторно используемым ресурсам. Мониторинг производительности приложения для выявления узких мест и возможных улучшений. Планирование роста системы и способы масштабирования, чтобы обеспечить бесперебойную работу при увеличении нагрузки. Горизонтальное и вертикальное масштабирование ресурсов для эффективного увеличения производительности. Примеры: Безопасность: Разработка мультифакторной аутентификации для защиты учетных записей пользователей от несанкционированного доступа. Производительность: Оптимизация алгоритмов обработки данных для уменьшения времени ответа приложения на запросы пользователей. Масштабируемость: Разработка архитектуры микросервисов для возможности легкого добавления новых сервисов и масштабирования горизонтально при необходимости. Учет безопасности и производительности помогает создать надежное и эффективное программное обеспечение, способное работать стабильно и безопасно даже при значительной нагрузке. Как заключение, следует отметить, что правильная постановка задач - залог успешного проекта. Системный аналитик играет важную роль в этом процессе, и его работа должна быть максимально четкой, подробной и структурированной. Эффективная коммуникация, учет безопасности и производительности, а также разделение задач между специалистами помогут достичь желаемого результата. Подробно изучите требования и ожидания разработчиков перед постановкой задачи. Организуйте четкое разделение задач между бэкенд- и фронтенд-разработчиками для параллельной работы. Придайте особое внимание детализации и структурированию информации при составлении задачи. Учитывайте аспекты безопасности, производительности и масштабируемости при разработке. Ведите активное взаимодействие с командой разработки, поддерживайте открытость и прозрачность в процессе работы. Надеюсь, что внесенные изменения помогут улучшить статью и облегчат понимание процесса постановки задач в системе документооборота.",
    "70": "Давно хотел посмотреть что-нибудь содержательное по китайской ядерной космической энергетике. Наконец, что-то такое попалось. Мировые СМИ разнесли новость, опубликованную гонконгским изданием South China Morning Post, об успешных испытаниях наземной модели 1,5 МВт ядерной энергетической установки. Из содержательного в новости. В работе принимало участие более 10 институтов. Прототип системы ядерного реактора с литиевым охлаждением прошел некоторые начальные наземные испытания. Рабочая температура реактора -  1276 С. На фото ниже - установка, на которой проводились испытания. Надпись сверху: \"Внедряй инновации или погибни. Никаких оправданий\". Перейдем к первоисточнику - статье \"Проектирование мегаваттного космического ядерного реактора с литиевым охлаждением\" китайского журнала \"Ядерная наука и технологии\" (использовался машинный перевод). «Разработана техническая схема малого космического реактора мегаваттного класса с литиевым охлаждением, используемого в сочетании с системой преобразования энергии на цикле Брайтона, которая является легкой и долговечной. Рассматриваются ключевые технологии, задействованные в проектировании». Обоснование необходимости проведения работ. «По мере того как ситуация космической конкуренции становится все более серьезной, требования к функциональности и эксплуатационным характеристикам космических аппаратов и космического оборудования постепенно возрастают. Запрос на мощное и надежное энергоснабжение постепенно возрастает.» «Приводится проект мегаваттного космического реактора с литиевым охлаждением. Основное внимание уделяется проектным идеям и концепциям. Приводится план проектирования некоторых ключевых компонентов и рассказывается о ходе разработки и экспериментальных испытаний проверочных образцов и прототипов». «Метод охлаждения реакторной системы жидкометаллическим контуром имеет значительное преимущество в весе», потому выбран именно он. «В системе машинного преобразования тепла в электричество используется гелий‑ксеноновая генераторная установка, которая вырабатывает мощность 1,55 МВТ. Чистая электрическая мощность составляет 1,5 МВТ. Расчетный срок службы системы не менее 10 лет». «Чтобы уменьшить общий вес системы, учитывая, что холодильника излучателя пропорциональна четвертой степени температуры, температура рассеивания излучаемого тепла искусственно повышается. Вес системы рассеивания излучаемого тепла уменьшается, но это также приводит к определенной потере эффективности выработки электроэнергии. После комплексной оптимизации температура на входе компрессора выбрана 560 К. Температура на входе турбины составляет 1500 К. Эффективность цикла системы выработки электроэнергии составляет 25,79%. Система радиационного охлаждения использует ртутно‑калиевые тепловые трубы с ребрами радиационного охлаждения.» (Тепловая схема на рисунке ниже.) Эксперименты проводились с целью «проверки ключевого оборудования системы космического реактора с литиевым охлаждением, проверки совместной работы главного контура с литиевым охлаждением и выработки электроэнергии в цикле Брайтона, а также испытаний высокотемпературных материалов на коррозионную стойкость». «Фактические потребности в применении космических реакторов мегаваттного класса в нашей стране ожидаются примерно в 2035–2050 годах». Объектом для «списывания» стал американский проект «Прометей» 20-летней давности. В тексте гигантское количество отсылок к нему; повторен выбор всех ключевых технических решений. Что поражает — китайцы всерьез воспринимают многие американские рекламные проспекты 90-х годов и закладывают обещанные в них показатели в свой проект. В тексте очень мало ссылок на российские работы. Подчеркивают лишь отличия. Причем подчеркиваются не таким образом: «Русские делают так, а мы так». Отличия обсуждаются в следующем стиле: «Весь цивилизованный мир, и мы в том числе делаем так то, и только эти русские делают иначе». Буквально такое, конечно же, не говорится. И может быть дело вообще в машинном переводе. Но впечатление именно такое. Ну и, наконец, сравнение с российскими работами. Пример аналогичной российской статьи от 2020 года. По уровню российскую и китайскую работы и сравнивать не имеет смысла: настолько мы сейчас впереди. Такие российские научные статьи проходит без какого‑либо внимания СМИ, как российских, так и мировых. А китайские скромные успехи обсуждает и приветствует весь мир. С чего бы это? Первоисточник.",
    "71": "Королевство Саудовская Аравия в последние годы активно заявляет о себе как о мощном инвестиционном и деловом центре на Ближнем Востоке, стремясь занять лидирующие позиции среди стран региона в этих сферах. Государственная политика Королевства направлена на масштабное вложение средств в улучшение инфраструктуры, развитие передовых технологий и повышение качества образовательной системы. Эти действия способствуют формированию привлекательной среды для роста как национального, так и глобального предпринимательства. Создание специализированных экономических районов с уникальными условиями, предлагающими значительные налоговые преимущества и привилегии, стало одной из ключевых инициатив правительства для привлечения зарубежных капиталовложений. Более того, упрощение процессов регистрации и управления компаниями также играет важную роль в укреплении репутации Саудовской Аравии как привлекательного делового узла. В рамках данной статьи предлагается всесторонний анализ процедуры регистрации компании в Саудовской Аравии. Мы подробно рассмотрим все аспекты данного процесса, начиная от важности осознания особенностей местного корпоративного права до выполнения конкретных действий, необходимых для успешного начала и деятельности вашего предприятия в рамках Королевства. Мы обсудим выбор наиболее подходящей правовой формы для вашей будущей компании, организацию и подачу необходимого набора документации, этапы регистрационного процесса и ключевые моменты получения всех требуемых лицензионных разрешений. Находясь в ключевом географическом положении на Аравийском полуострове, Королевство Саудовская Аравия утвердило свое положение как лидера экономического развития Ближнего Востока благодаря обилию нефтяных запасов. Однако, осознавая необходимость сокращения зависимости от одного экспортного ресурса, правительство Королевства в последние десятилетия активно работает над экономической диверсификацией своей экономики. Это стремление находит свое выражение в амбициозной программе развития \"Видение 2030\", целью которой является переориентация экономической структуры страны, сокращение ее нефтяной зависимости и капиталовложения в развитие других ключевых секторов, включая туризм, медицина, образование и альтернативная энергетика. Эти изменения создают новые перспективы для зарубежных бизнесменов и инвесторов, открывая перед ними привлекательные варианты для бизнеса в Саудовской Аравии. Главные цели, определенные инициативой \"Vision 2030\" и сопутствующим ей Национальным планом преобразований, заключаются в достижении Королевством статуса одной из 15 ведущих мировых экономик, увеличении доли экспорта не сырьевых товаров в ВВП с текущих 16% до амбициозных 50%, а также в росте вклада частного сектора в ВВП с 40% до 65%. Кроме того, планируется значительно повысить уровень прямых капиталовложений из-за рубежа с 3,8% до 5,7% ВВП и нарастить объемы ресурсов паевых инвестфондов. Эти меры свидетельствуют о серьезности намерений Королевства преобразовать свою экономику и укрепить ее позиции на мировой арене. Данное государство также активно участвует в международной торговле и экономическом сотрудничестве, являясь членом Совета сотрудничества арабских государств Персидского залива (ССАГПЗ). Взаимодействие в пределах этого союза предусматривает множество  договоренностей о взаимной торговле, направленных на укрепление экономических связей между странами-членами. В 2003 году был утвержден Общий таможенный закон, унифицирующий таможенные процедуры во всех государствах союза и предоставляющий преимущества по импортным тарифам, что способствует упрощению международной торговли и способствует экономической интеграции данных государств. Таким образом, Королевство Саудовская Аравия представляет собой регион с высоким потенциалом для иностранных бизнесменов и капиталовкладчиков, стремящихся развивать свой бизнес в динамично развивающейся и в то же время диверсифицированной экономике. Стратегическое видение развития, обширные реформы и активное международное сотрудничество создают уникальные шансы для участия в экономике одной из ведущих стран на Ближнем Востоке. Ключевое местоположение. Располагаясь на стыке Европы, Азии и Африки, Саудовская Аравия служит важным узлом для фирм, стремящихся расширить свое влияние на глобальных рынках. Ее выгодное местоположение обеспечивает легкий доступ к основным маршрутам международной торговли и способствует международному коммерческому сотрудничеству, делая ее прекрасным местом для глобального предпринимательства. Финансовая устойчивость. Импрессивная экономическая стабильность Саудовской Аравии, подкрепленная значительными запасами нефти и активной инвестиционной стратегией в инфраструктуру и ведущие экономические отрасли, привлекает иностранные инвестиции и способствует надежному бизнес-окружению. Развитая инфраструктура. Существенные инвестиции государства в инфраструктурные проекты привели к созданию современной и эффективной транспортной сети, включая аэропорты мирового класса, морские порты, дороги и передовые телекоммуникационные системы. Такая развитая инфраструктура лежит в основе успеха бизнеса, предоставляя надежные логистические возможности. Привилегии для инвесторов. Саудовская Аравия предлагает привлекательные условия для зарубежных инвесторов, включая налоговые преференции, административную поддержку для оформления виз и разрешений на работу, а также право на полное владение компанией иностранцами без необходимости наличия местного совладельца, что значительно упрощает процесс создания и ведения бизнеса в Королевстве. Простота регистрации предприятия. Цифровизация государственных сервисов и минимизация бюрократии упрощают процесс входа на рынок Саудовской Аравии и снижают административные препятствия для иностранных капиталовложений. Поддержка инноваций. Саудовская Аравия акцентирует внимание на инновациях и развитии технологий, особенно в ключевых сферах, таких как здравоохранение, образование и IT. Создание бизнес-инкубаторов и технопарков стимулирует инновационные проекты и стартапы, делая страну привлекательной для технологических предпринимателей. Доступ к экспансивному рынку. Высокий уровень покупательской способности населения Саудовской Аравии, а также обширные государственные вложения в социальную сферу и инфраструктуру, способствуют росту внутреннего спроса на разнообразные продукты и услуги. Значительным шагом на пути к упрощению и либерализации экономической среды стало изменение в 2016 году, когда зарубежным организациям  в секторе оптовой и розничной торговли стало возможно быть полностью принадлежащими иностранным инвесторам, избавившись от предыдущего требования иметь саудовского резидента в числе учредителей. В последующем году были внесены нововведения для международных инженерных и консультационных компаний, дав их владельцам право на полное владение бизнесом в Королевстве без необходимости включения местного учредителя. Законодательство Саудовской Аравии о компаниях регулируется несколькими ключевыми документами, которые определяют правовую основу для функционирования как местных, так и иностранных предприятий. Основополагающим документом выступает \"Закон о компаниях\" 2015 года, который пришел на смену предшествующей версии закона, датированной 1965 годом. Новый закон предоставил обновленную и более адаптированную к современным реалиям правовую основу для функционирования и регулирования компаний. Категории правовых субъектов. Нормативные акты разрешают учреждение и эксплуатацию разнообразных правовых форм, включая акционерные компании (Joint Stock Companies), общества с ограниченной ответственностью (LLCs) и разные виды ассоциаций. Для каждой категории правового лица предусмотрены уникальные критерии, связанные с размером основного капитала, организационной структурой и обязательностями по отчетности. Минимальный основной капитал. Размер начального основного капитала варьируется в зависимости от типа предприятия: для акционерных компаний он обычно равен 500,000 SAR (саудовских риалов), в то время как для обществ с ограниченной ответственностью этот показатель ниже и может колебаться с учетом конкретной сферы деятельности компании. Управление компанией. Законодательство определяет строгие требования к управлению предприятиями в юрисдикции Королевства, требуя от акционерных компаний формирования совета директоров, а от обществ с ограниченной ответственностью ‒ назначения директоров. Финансовая отчетность и аудит. Важной обязанностью всех фирм является ведение бухгалтерского учета и представление финансовой отчетности в соответствии с международными нормами, а также проведение обязательного годового аудита. Министерство инвестиций Саудовской Аравии (MISA) занимает центральное место в привлечении зарубежных инвестиций и оказании поддержки зарубежным предприятиям при регистрации и ведении бизнеса в Королевстве. MISA предлагает широкий спектр консультационных услуг, помогает в оформлении необходимых разрешений и лицензий, тем самым способствуя упрощению процесса вхождения на саудовский рынок для зарубежных инвесторов. \"Закон о компаниях\" 2015 года и регулирующие органы, такие как Министерство инвестиций, закладывает надежную платформу для стабильного и продуктивного бизнес-процесса в Саудовской Аравии, подчеркивая стремление государства к созданию благоприятного инвестиционного климата и открытости экономической системы для международного сотрудничества. Иностранным предпринимателям, стремящимся вести коммерческую деятельность в Саудовской Аравии, необходимо ознакомиться не только с \"Законом о компаниях\", но и комплекс других ключевых нормативных документов и правил, устанавливающих рамки для их операций в пределах Королевства. Особое внимание стоит уделить следующим документам: Данный норматив позволяет зарубежным организациям владеть до ста процентов долей в саудовских компаниях в большинстве экономических секторов, за исключением определенных ключевых областей, как нефтегазовый и телекоммуникационный сектора, где действуют специализированные лицензионные требования. Этот закон устанавливает правила для корпоративной активности в контексте соблюдения принципов конкуренции, придавая особую значимость для операций, связанных с объединениями и поглощениями, с целью препятствования монопольному доминированию и поддержания справедливой конкурентной среды. Дополнительно, для запуска зарубежного предприятия в Саудовской Аравии требуется лицензирование со стороны Министерства инвестиций Саудовской Аравии (MISA), ранее известного как Saudi Arabian General Investment Authority (SAGIA). Процесс получения лицензии включает предоставление подробного бизнес-плана и подтверждения финансовой устойчивости компании. Все эти законы и регуляции формируют комплексную правовую базу, на которой иностранные инвесторы могут строить свою коммерческую деятельность в Саудовской Аравии, обеспечивая легальность, прозрачность и защищенность их инвестиций. Важно тщательно изучить эти и другие соответствующие законодательные акты, чтобы гарантировать успешное и эффективное ведение бизнес-процессов в Королевстве. Королевство Саудовская Аравия предлагает разнообразные юридические структуры для организации дела, предоставляя возможность выбора оптимальной компании в соответствии с задачами и масштабом деятельности. Каждая форма предприятия подразумевает специфичные условия по основному капиталу, управленческой структуре и прочим важным параметрам. Давайте подробнее ознакомимся с ключевыми видами предприятий, доступными для регистрации в Саудовской Аравии: Эти общества позволяют привлекать финансирование через эмиссию акций. Они чаще всего выбираются большими фирмами с обширным количеством участников. Размер уставного фонда. Минимальный уставный капитал акционерной организации может различаться зависимо от сферы деятельности, при этом стандартное требование обычно составляет около 500,000 SAR (примерно 133,333 USD). Особенности. Обязательно формирование директорского совета и проведение годовых собраний акционеров. Существует разделение на публичные и частные акционерные общества, где публичные могут предлагать акции широкому кругу лиц, а частные – ограниченному числу инвесторов, что позволяет большую секретность и управленческую гибкость. Уставной капитал частных фирм обычно аналогичен публичным, но может быть скорректирован для специфических потребностей. Такие компании являются предпочтительным выбором для мелкого и среднего бизнеса из-за простоты управления и ограниченной ответственности участников. Минимальный основной капитал. Не установлен фиксированный размер основного капитала и он может изменяться в зависимости от отрасли. Например, для торговых предприятий минимальный фонд может составлять около 100,000 SAR (приблизительно 26,666 USD), в то время как для производственных и сельскохозяйственных предприятий минимумы могут достигать 5,000,000 SAR и 25,000,000 SAR соответственно. Особенности. Управление может осуществляться как непосредственно учредителями, так и назначенными ими менеджерами. Не требуется проведение открытых собраний акционеров, что упрощает административный процесс. Помимо уже упомянутых юридических форм компаний, Саудовская Аравия предлагает и другие структуры бизнеса, которые могут соответствовать различным потребностям и стратегиям иностранных инвесторов. Эти альтернативные формы предполагают коммандитные товарищества, производственные кооперативы и филиалы зарубежных фирм. Организации с ограниченной и полной ответственностью (партнёрства). В данном государстве существуют два вида партнёрств: с ограниченной и полной ответственностью. Участники партнерства с ограниченной ответственностью рискуют только размером своих инвестиций, что делает этот вид собственности привлекательным для инвесторов, желающих снизить финансовые риски. Филиалы зарубежных фирм. Филиалы международных организаций позволяют международным фирмам осуществлять деятельность на территории Саудовской Аравии напрямую, избегая необходимости учреждения отдельной правовой единицы в Королевстве. Этот вариант может быть оптимальным для предприятий, стремящихся поддерживать централизованное руководство и контроль над своими международными операциями. При выборе ОПФ для коммерческой деятельности в Саудовской Аравии важно принимать во внимание не только критерии к основному капиталу и управленческой структуре, но также налоговые условия, правила ведения учета и отчетности, а также особенности получения лицензий в зависимости от отрасли деятельности. Разнообразие форм собственности предлагает различные возможности для бизнеса разного уровня и специализации, а также разные степени прозрачности и конфиденциальности. Выбор формы собственности ‒ это только начало пути. Затем необходимо тщательно проработать процесс регистрации и учесть все юридические и экономические нюансы ведения бизнеса на саудовском рынке. Создание предприятия в Саудовской Аравии является многоэтапной процедурой, охватывающей действия от подбора и проверки наименования компании до ее официальной регистрации в правительственных учреждениях. Этот процесс демонстрирует необходимость детальной подготовки и соответствия всем локальным законодательным требованиям. Давайте взглянем на ключевые этапы этого процесса. Первоначальным этапом становится подбор уникального имени для вашего бизнеса, соответствующего законодательству Королевства и отображающего деятельность организации. Критически важно избегать наименований, которые могут быть интерпретированы как неприемлемые или несерьезные. Для подтверждения и бронирования выбранного имени необходимо воспользоваться государственным порталом Саудовской Аравии, предназначенным для бизнес-сообщества. Выбирая ОПФ для предприятия в Саудовской Аравии, следует учитывать несколько ключевых факторов, включая масштаб бизнеса, стратегические цели, количество учредителей и уровень риска, который они готовы принять на себя.  Так, общество с ограниченной ответственностью является оптимальным выбором для малых и средних предприятий из-за ограничения ответственности участников. Между тем, акционерные общества выбирают для осуществления крупномасштабных проектов, требующих привлечения внешних инвестиций. Заявка о регистрации предприятия. Доказательство резервирования названия фирмы. Копии паспортов основателей и руководителей. Учредительная документация и устав организации, подписанные всеми учредителями. Доказательство юридического адреса организации. Список участников и их долей в уставном капитале. Для иностранных инвесторов требуется получение инвестиционной лицензии от Саудовского органа по инвестициям (SAGIA). Все бумаги, подаваемые предприятиями из-за рубежа, должны быть переведены на арабский язык и легализованы в консульстве Саудовской Аравии. До подачи документов на регистрацию важным шагом является получение одобрения инвестиций от соответствующих ведомств, что гарантирует соответствие бизнес-плана требованиям локального законодательства и экономической политики государства. Для инкорпорации компании следует подать соответствующую форму заявки и весь подготовленный пакет документации в Министерство торговли и инвестиций Саудовской Аравии. Процесс можно выполнить в цифровом формате через интернет-портал департамента. В итоге успешного завершения регистрации предприятие получает сертификат инкорпорации предприятия, который подтверждает легальность и правомочность компании вести бизнес в пределах королевства. Период оформления коммерческой деятельности в Королевстве может колебаться. Продолжительность зависит от сферы деятельности, комплектности и корректности поданных бумаг, а также специфики лицензионных и разрешительных требований. Обычно процесс занимает до трех месяцев, тогда как бронирование наименования компании обычно осуществляется за несколько дней. Стремясь стимулировать экономическое развитие и привлекать зарубежные капиталовложения, Королевство уделяет особое внимание формированию и развитию зон свободной торговли, которые выступают ключевыми локомотивами экономического прогресса, улучшения ключевых отраслей и усиления конкурентных преимуществ национальной экономики. СЭЗ предлагают привлекательные условия для ведения бизнеса, льготы по налогообложению, минимизацию административных барьеров и доступ к современной инфраструктуре. Ниже представлен анализ ключевых фризон Саудовской Аравии и выделены их уникальные особенности. Раскинувшийся вдоль живописного побережья Красного моря, Экономический город короля Абдаллы представляет собой один из самых масштабных инициатив в Королевстве. Эта территория открывает обширные перспективы для вложений в быстро растущие отрасли, такие как морское и логистическое направление, легкая промышленность, банковские и финансовые сервисы, а также сектор отдыха и туризма. В KAEC акцентируется внимание на создании оптимальных условий для предпринимателей с целью привлечения глобальных инвестиций, что способствует его становлению ключевым местом для экономического инновационного развития. Специализированная на развитии энергетического сектора, SPARK ориентирована на привлечение инвестиций в исследования и разработки возобновляемых источников энергии, нефтегазовую промышленность и производство соответствующего оборудования. Эта зона играет ключевую роль в реализации стратегических направлений Саудовской Аравии в отношении экономической диверсификации и отказа от столь значимой зависимости от нефтяной отрасли, стимулируя инновации и развитие высокотехнологичных сфер. Фризона Джазан представляет собой стратегически значимый промышленный и коммерческий узел, обеспечивающий мощную связь между быстрорастущими рынками Азии и Африки. Благодаря своему географическому положению и прямому доступу к крупнейшему морскому порту региона, Джазан открывает уникальные возможности для экспорта и импорта товаров. Эта зона привлекает инвестиции в проекты инфраструктуры и промышленного производства, выигрывая от упрощенного доступа к необходимым ресурсам и продвигая международную торговлю. ОЭЗ Рас-Аль-Хайр выступает как ключевой участник в индустрии морских услуг Персидского залива, предоставляя широкомасштабную интеграцию морской экосистемы. С 40% земельных участков, выделенных для действующих и потенциальных инвесторов, зона открывает обширные перспективы в секторах судостроения, офшорных разработок и морских логистических цепей. Особая экономическая зона (ОЭЗ) Облачных вычислений, внедренная в рамках Королевского научно-технологического центра (KACST), представляет собой один из самых прогрессивных проектов в Саудовской Аравии, направленный на стимулирование инноваций и ускорение роста технологического сектора страны. Этот проект отражает стремление Королевства к созданию современной экономики, основанной на знаниях и инновациях, и демонстрирует важность цифровых технологий в современном мире. СЭЗ предлагает уникальную гибкую модель, которая позволяет инвесторам разрабатывать как физические, так и виртуальные вычислительные центры, расширяя инфраструктуру облачных вычислений по всему Королевству. Фискальные привилегии. Инвесторы могут воспользоваться сниженными налоговыми ставками или полным изъятием из налогообложения на доходы на установленный период, что значительно уменьшает экономическое давление на стартапы и растущие фирмы. Облегченные таможенные регламенты. Специальные условия для внешнеэкономической деятельности, в том числе освобождение от таможенных сборов, создают лучшие условия для глобальной коммерции. Регуляторные льготы. Сокращение бюрократических препятствий и упрощение процессов организации и функционирования компаний делают процедуру более прямолинейной и доступной. Доступ к передовым технологиям. Возможность использовать последние технологические достижения и сотрудничество с ведущими научно-исследовательскими центрами и университетами открывает новые перспективы для инноваций и прогресса. Возможности для международного сотрудничества. Благодаря стратегическому местоположению зон свободной торговли и развитой логистической инфраструктуре упрощается интеграция на мировые рынки, расширяя горизонты для экспорта и импорта товаров. Инфраструктурная поддержка. Готовая к использованию инфраструктура для бизнеса, включая офисные пространства, производственные и складские помещения, а также доступ к необходимым энергоресурсам и коммуникационным сетям, создает оптимальные условия для старта и развития проектов. Квалифицированные кадры. Привлекательные условия для иностранных специалистов способствуют привлечению высококвалифицированных работников из разных уголков мира, обогащая локальный рынок труда и способствуя обмену знаниями и опытом. Чтобы в полной мере реализовать потенциал спецзон Саудовской Аравии, организациям следует глубоко анализировать особенности и требования каждой зоны, стратегически планировать свою деятельность и адаптировать свои бизнес-модели под местные и глобальные рыночные тренды. Успех в этих условиях зависит от способности предприятий быстро адаптироваться к меняющимся условиям и эффективно использовать предоставляемые выгоды для достижения своих бизнес-целей. Заведение корпоративного счета в банке Саудовской Аравии становится ключевым шагом для эффективной работы и финансового управления предприятия в Королевстве, обеспечивая возможность совершения финансовых операций и доступ к разнообразию банковских сервисов и продуктов. Прежде всего, предприятию в Саудовской Аравии нужно подготовить и представить набор бумаг и выполнить ряд требований, чтобы банковское учреждение активировало финансовый счет для фирмы. Сертификат о регистрации фирмы (Commercial Registration), подтверждающий законную регистрацию вашего бизнеса в Королевстве и содержащий ключевую информацию о компании. Устав фирмы и договор о ее основании, устанавливающие структуру и правила управления организацией, а также определяющие права и обязанности основателей. Решение о назначении руководства и документы, удостоверяющие их полномочия, обязательны для предоставления и описывают роли и полномочия директоров в управлении фирмой. Перечень конечных бенефициарных владельцев компании, включающий информацию о всех лицах с бенефициарными интересами в фирме, их долю в уставном капитале и прочие релевантные данные. Краткое описание бизнеса. Необходимо подробно описать предмет деятельности компании, указать основные источники дохода и основных клиентов. План денежных потоков и прогнозируемый оборот. Важно предоставить детализированный финансовый план работы организации, включая прогнозируемые доходы и расходы. В Саудовской Аравии существует ряд банков, которые выделяются своей открытостью к сотрудничеству с иностранными инвесторами и предлагают разнообразные банковские продукты и услуги, специально разработанные для удовлетворения потребностей международного бизнеса. Эти финансовые учреждения играют ключевую роль в поддержке и развитии финансовых инициатив Королевства, способствуя привлечению капиталовложений из других стран и обеспечивая необходимую инфраструктуру для ведения предпренимательства в регионе. National Commercial Bank (NCB). NCB занимает лидирующие позиции среди банков Саудовской Аравии, предлагая комплексные финансовые решения для корпоративных клиентов. Благодаря глубоким традициям и обширному опыту, NCB стал одним из основных выборов для многих международных компаний, стремящихся расширить свое присутствие в регионе. Riyad Bank. Riyad Bank выделяется своей способностью предоставлять масштабируемые банковские решения для бизнеса любого размера ‒ от стартапов до крупных корпораций. Сильная корпоративная культура и инновационный подход к финансовым услугам делают Riyad Bank надежным партнером для предпринимателей. SABB (The Saudi British Bank). SABB, являясь результатом сотрудничества между местными и международными финансовыми организациями, специализируется на предоставлении банковских услуг, которые отвечают специфическим требованиям иностранных инвесторов. Благодаря глубокому пониманию как местных, так и глобальных бизнес-процессов, SABB является ведущим выбором для глобального бизнеса. Al Rajhi Bank. Al Rajhi Bank, крупнейший банк в мире, работающий по принципам исламского банкинга, предлагает уникальный набор финансовых сервисов и продуктов, соответствующих шариатскому праву. Это делает его предпочтительным выбором для тех фирм, которые стремятся вести свой бизнес в рамках исламских финансовых принципов, предлагая при этом конкурентоспособные корпоративные счета, финансирование проектов и услуги в инвестиционном поле. Saudi Investment Bank: специализируется на оказании финансовых решений для корпоративных клиентов, включая корпоративное финансирование, инвестиционные банковские услуги и управление капиталом. Эти банки имеют развитые отделения для обслуживания корпоративных клиентов и предлагают различные услуги, адаптированные под нужды международного бизнеса. Важно отметить, что условия и требования к открытию счетов могут варьироваться в зависимости от конкретного банка, поэтому рекомендуется заранее связаться с банком для уточнения всех деталей. Большинство из финучреждений предлагают услуги интернет-банкинга, позволяющие управлять корпоративным счетом дистанционно, что критически важно для международных фирм, которые базируются в Саудовской Аравии. Предварительное консультирование. Перед началом процесса многие банки предлагают услугу предварительного консультирования для клиентов из-за рубежа. На этих консультациях обсуждаются ключевые аспекты открытия счета, включая перечень требуемой документации, специфические требования банка к клиентам и особенности ведения коммерческой деятельности в пределах в Саудовской Аравии. Сбор и подача документации. На следующей стадии следует собрать и предоставить в избранное банковское учреждение полный комплект документов, который обычно состоит из организационных бумаг компании, подтверждений полномочий управляющих, сведений об акционерах, а также паспортов и прочих личных данных основателей и руководителей. Анализ документации и дополнительная информация. После получения документов банк проводит их детальную проверку согласно собственным правилам и стандартам. В этот период может возникнуть необходимость в предоставлении дополнительных сведений или уточнениях по уже предоставленной информации. Одобрение заявки. Когда все документы удовлетворяют критериям и процессам банка, запрос на открытие счета получает одобрение. Стоит подчеркнуть, что банки в Саудовской Аравии придерживаются строгих правил относительно \"Знай своего клиента\" (KYC) и противодействия отмыванию денег (AML), что делает тщательную проверку неизбежной частью процесса. Подписание банковских документов. Сразу после того, как запрос получит подтверждение, делегатам организации необходимо в индивидуальном порядке явиться в финансовое учреждение для оформления надлежащих контрактов и банковских записей. Этот этап официально заканчивает процедуру открытия счета для компании в Королевстве Саудовская Аравия. Активация счета. Как только все записи оформлены и все процедуры выполнены, счет становится активным, что дает зеленый свет для начала экономической деятельности и выполнения финансовых операций. Временные рамки для открытия корпоративного счета могут колебаться от нескольких дней до нескольких недель и зависят от полноты сдачи документов и уникальных условий данной финансовой организации. Система налогообложения и бухгалтерский учет в Саудовской Аравии строго регулируются с целью обеспечения прозрачности финансовых операций предприятий и справедливого исчисления налогов. Эти требования являются фундаментальными для устойчивости экономического прогресса и привлекательности для зарубежных капиталовложений. Рассмотрим основные моменты налогового обложения, бухгалтерского учета и таможенного регулирования, с которыми должны быть знакомы предприятия, работающие на территории Саудовской Аравии. Зарубежные предприятия, занимающиеся бизнесом на территории Саудовской Аравии, подлежат налогообложению по ставке 20% с их чистого дохода. Отдельно для организаций, действующих в нефтегазовой отрасли, установлены специальные ставки, варьирующиеся от 50% до 85%, что отражает стратегическую значимость этих секторов для национальной экономики. С июля 2020 года ставка НДС была увеличена до 15%, что является значительным изменением по сравнению с предыдущей ставкой в 5%. Компании, чей годовой оборот превышает установленный порог, обязаны регистрироваться в качестве плательщиков НДС, собирать налог с покупателей и перечислять его в бюджет страны. Для саудовских компаний и компаний с саудовским капиталом, помимо налога на прибыль, может быть введён закят – исламский налог со ставкой 2,5% от чистых активов компании. Этот налог, являющийся элементом исламской финансовой системы, предназначен для поддержки социальной справедливости. Однако он не универсален и применяется не ко всем компаниям, а дополнительно к основному налогообложению. Компании обязаны вести бухгалтерский учет на арабском языке и подготавливать финансовую отчетность в соответствии с Международными стандартами финансовой отчетности (МСФО) или другими признанными стандартами. Финансовая отчетность должна подвергаться ежегодному аудиту независимыми аудиторами, что способствует укреплению доверия инвесторов и партнеров к финансовому состоянию компании. Налоговая декларация подается в течение 120 дней после окончания налогового периода, обычно совпадающего с календарным годом. Все налоговые декларации и связанные с ними документы должны быть поданы в электронном виде через официальный портал Генеральной дирекции по налогам и пошлинам. С 4 декабря 2021 года в Саудовской Аравии введена система электронного счет-фактурирования (E-invoicing), требующая от всех зарегистрированных налогоплательщиков НДС использовать электронные счета-фактуры для всех внутренних операций. Налоговые органы имеют право проводить проверки налоговых деклараций и бухгалтерских книг предприятий, а за несоблюдение налоговых и бухгалтерских требований предусмотрены штрафы и пени. Более того, предприятия в Саудовской Аравии, занятые в области международной торговли, сталкиваются с различными таможенными процедурами при ввозе и вывозе товаров. Эти процедуры охватывают подачу деклараций, уплату таможенных сборов (если они применяются) и соблюдение правил маркировки и стандартов качества. Чтобы избежать задержек в доставке и лишних затрат, импортерам и экспортерам важно тщательно следовать этим процессам. В Саудовской Аравии действует система электронного декларирования, которая помогает упростить и ускорить таможенное оформление, облегчая компаниям управление международной торговлей. При этом, глубокий анализ налогообложения и бухгалтерии, включая использование электронных систем учета и выставления счетов, дает возможность компаниям улучшить внутренние процессы, уменьшить угрозу наложения штрафов и повысить общую производительность. Немаловажный нюанс ‒ поддержание актуальности документации в финансовых и налоговых вопросах, что предполагает систематическое обновление сведений и готовность к изменениям в законодательстве. Для облегчения процесса ведения бухгалтерии и удовлетворения требований по таможенному регулированию, многие организации в Королевстве Саудовская Аравия прибегают к услугам экспертов в области консультаций и аудита. Такие специалисты предлагают важные рекомендации для уменьшения налоговых обязательств, усовершенствования бухгалтерских процессов и уменьшения издержек, связанных с таможенной декларацией, что представляет особую ценность для зарубежных инвесторов и предприятий, занимающихся деятельностью на международном уровне. Саудовская Аравия активизировала привлечение иностранных инвесторов, открывая широкие перспективы для бизнес-деятельности в разнообразных отраслях экономики. Это часть обширных экономических и социальных реформ под общим названием \"Видение 2030\". Однако для успешного ведения бизнеса в королевстве критически важно глубоко понимать местные законодательные, культурные особенности и нормы бизнес-этикета. Компаниям, желающим войти на саудовский рынок, рекомендуется начать с разработки реалистичного бизнес-плана, охватывающего все стороны деятельности в стране. Обращение за квалифицированной помощью к юридическим и консультационным службам, имеющим опыт в Саудовской Аравии, станет ключом к безопасному и эффективному старту, минимизируя риски и препятствия.",
    "72": "Привет! Я — Алексей Бондаренко, работаю в команде Платформа Банки.ру. Сегодня хочу рассказать о semantic-release и его практическом применении на примере упрощения разработки и внедрения библиотеки в проект. разберу, как работает инструмент, в чем его особенности и ограничения. В конце статьи будет ссылка на репозиторий. Его можно использовать в качестве стартовой точки для работы с semantic-release. Хотя с этим понятием большинство знакомо, напомнить считаю нелишним. С версионированием мы встречаемся часто. Будь-то модель автомобиля в разных поколениях и модификациях или литературное произведение в разных изданиях. С кодом похожая история. Пишем код, начинаем его использовать — одна версия. Делаем доработки, включаем новый функционал — появляется другая.Встает вопрос — а как называть эти версии? Словами, которые передают смысл изменений, нумеровать по порядку или как-то еще? В разработке используют разные варианты присваивания версий коду. Например, Calendar Versioning (CalVer), zer0ver. Подробнее о них можно почитать в этой статье. Но самый популярный, пожалуй, это семантическое версионирование или semver. Семантическое версионирование —  это набор правил и требований, которые определяют, как назначаются и увеличиваются номера версий. Версия в семантическом версионировании — это три числа, разделенные точкой. Z — патч-версия, когда меняется что-то внутри и публичное API остается прежним. Например, поменялся метод сортировки в одном из элементов, библиотека стала быстрее работать. Y — минорная версия, когда добавлена новая функциональность и не нарушена обратная совместимость API. К публичному методу добавился дополнительный необязательный параметр. X — мажорная версия, когда нарушена обратная совместимость API . Например, переименовали публичный метод. Приведу аналогию из жизни автомобилиста.Z — поменяли лампочки в фарах автомобиля: свет как был, так и остался, просто стали ярче гореть фары. Y —  появился новый режим у кондиционера: добавили обдув заднего стекла. Z —  поменяли органы управления: кнопку перенесли из одного места в другое. Тут без прочтения инструкции не обойтись. Корректная схема управления версиями помогает разработчикам легко идентифицировать текущий выпуск, внесенные изменения, а также совместимость версии с предыдущими. Конкретно семантическое версионирование имеет смысл только при разработке библиотек: кодовая база одна, а потребителей много. Как писал мои коллега в одной из статей, мы прошли долгий путь от монолита с сабмодулями, до микрофронтендов с большим количеством общих библиотек. Наша команда разрабатывает и поддерживает внутренний UI-KIT компонентов, которые используют в проектах всех продуктовых вертикалей компании. С этой библиотекой внутри компании мы работаем как с open-source проектом: каждая команда может внести изменения и прислать их на ревью. Раньше библиотека компонентов подключалась к основному монолитному проекту сабмодулем (git submodule). В такой схеме были риски: можно было установить указатель не на тот коммит и сломать код. Чтобы этого избежать, приходилось использовать скрипт, который проверял, что в релизе хеш сабмодуля указывает на коммит из мастера. В нашем случае использование семантического версионирования помогает понять и определить масштаб «бедствия» при установке обновления пакета. Дает возможность планировать работу и делает ее более предсказуемой. Экономит время, силы и нервы. Допустим, мы поняли, что семантическое версионирование это здорово, и решили его использовать. Что для этого нужно?Начнем с минимума — ручного управления версиями и изменениями без деплоя.Представим, что у нас есть репозиторий, где мы храним код. Присвоить тег можно и через веб-интерфейс git репозитория. С этим минимумом работать можно, но неудобно. Не хватает одного важного момента. Как было написано выше, семантическое версионирование помогает понять, какие изменения были сделаны. А значит, где-то нужно хранить информацию об этих изменениях. Обычно их фиксируют в файле changelog.md. Как это всё синхронизировать? И тег проставить, и написать, какие изменения были сделаны конкретно в этой версии. Устанавливать пакет напрямую из git репозитория — хорошее решение для начала, но для production разработки не очень подходит. Так как исходный код пакета, как правило, отличается от поставляемого кода Всем хочется простоты: устанавливать уже готовые к работе библиотеки, используя их имена. Обычно для этого используют реестр пакетов npm. Код готовят и публикуют в этом реестре. Здесь важно следить за правильностью версии в package.json. Если нужно, присваивать тэг. Задача опять усложняется: нужно синхронизировать git репозиторий, информацию о внесенных изменениях (changelog.md) и реестр пакетов. Делать это руками можно, но сложно. Человеческий фактор часто приводит к ошибкам. Semantic-release позволяет всю эту рутину автоматизировать и ошибок избежать. Semantic-release — это npm пакет, который полностью автоматизирует большой объем работы. Самое важное для корректной работы semantic-release — писать коммит-сообщения в определенном формате. Для этого есть правила, описанные в cоглашениях о коммитах. По умолчанию используются Angular коммит-сообщение. Когда мы пишем коммиты в соответствии с соглашениями, мы тем самым решаем проблему «добавления информации о внесенных изменениях» (changelog). В коммитах указываем, какие изменения были внесены в код. Semantic-release будет анализировать эти сообщения и формировать список изменений для определенной версии кода: какие изменения попадают в мажор, минор и патч-версию. Semantic-release работает по принципу плагинов и включает в себя девять шагов. На каждом шаге управление передается плагинам для выполнения определенных действий. @semantic-release/commit-analyzer — анализирует коммит-сообщения.@semantic-release/release-notes-generator  — генерирует список изменений (changelog) от последнего релиза до текущего коммита.@semantic-release/npm — публикует пакет в npm.@semantic-release/github — работает с github релизами и комментирует в issues. Стартовать можно с конфигом и плагинами по умолчанию. Semantic-release можно запустить локально с помощью команды (написана ниже). Предварительно нужно сделать коммит в соответствии с соглашениями о коммитах. Эта команда с ключиком --dry-run совершит тестовый запуск semantic-release (без шагов: подготовка, публикация, добавление канала, успех и сбой). Флаг --no-ci позволяет запустить semantic-release с локальной машины (пропускает проверку среды непрерывной интеграции). Если хотите, чтобы эта команда отработала по-настоящему, уберите флаг --dry-run. Как создать github персональный токен (<github_token>) можно узнать — тут. А npm токен (<npm_token>)  —  тут. Строчки, которые будут сгенерированы, надо добавить в команду выше. В результате мы должны получить опубликованный пакет в npm и запись в релизах. @semantic-release/changelog  — вносит изменения в changelog файл, из которого потом можно получить информацию о внесенных в код изменениях (не все же используют github для пакетов).@semantic-release/git  — коммитит изменения в git репозиторий. Здесь нам уже не обойтись без конфигурационного файла. В простом варианте он будет выглядеть следующим образом: Подробнее о конфигурировании можно почитать тут. После запуска semantic-release мы должны увидеть запись в changelog файле и коммит с изменениями в git репозитории. Нужно помнить, что бот делает коммит в ветке, которая у вас указана для деплоя. В нашем случае это master. Следовательно, этому пользователю нужно прописать права на пуш в мастер. Semantic-release имеет много плагинов на разные случаи. С их перечнем можно ознакомиться на официальном сайте. Либо написать свой. Чтобы поиграться с semantic-release, используйте подготовленный для этих целей github repo. Там есть более подробная инструкция. А здесь можно увидеть npm пакет результата. Не получится откатить изменения. Если выпустили версию и в ней обнаружились проблемы, решить это можно только выпуском новой версии. Коммит-сообщения должны быть сделаны по правилам  соглашений о коммитах. Эту задачу можно автоматизировать. Например, с помощью commitlint. К процессу нужно подходить ответственно. Делить коммиты на законченные порции изменений. Любителей коммитов типа «fix fix fix» точно ждут разочарования. На ревью следите, чтобы коммит-сообщения отражали изменения в коде. Для любителей сквош коммитов тоже будут разочарования: соглашение будет нарушено и семантик релиз не увидит изменения. Обкатали новый подход. Да, поймали много проблем, но нам понравилось! Сейчас 10+ библиотек деплоятся с помощью semantic-release в полностью автоматическом режиме. Скорость деплоя увеличилась в разы. Не надо никого обучать публиковать пакеты руками и тратить на это время.",
    "73": "В начале февраля 2024 года вышел Go 1.22. Вот, что нового и интересного принёс новый релиз: сделали более безопасное поведение переменных в циклах, добавили функции-итераторы в качестве rangefunc-эксперимента и улучшили шаблоны роутинга. В этой статье я сфокусируюсь на последнем, самом долгожданном, для многих, обновлении — шаблонах http-роутинга. Роутинг в Go — общая проблема, для решения которой уже построили кучу фреймворков, в этом GitHub-репозитории собраны лучшие. Google сама признаётся, что они вдохновлялись сторонними решениями и лучшее добавили в net/http. С приходом Go 1.22 всё необходимое для роутинга из коробки умеет делать http.ServeMux: он различает HTTP-методы, хосты и домены, а также может шаблонизировать пути через плейсхолдеры. Давайте поднимем сервер на localhost и поэксперементируем с тем, как ведёт себя ServeMux с разными шаблонами. Представим, что у нас есть некоторый сервер блога, у которого есть ручки posts, /posts/{id} и /posts/latest для того, чтобы дёргать посты. Напишем простенький обработчик и настроим сервер на 7777 порт. Теперь будем немного менять муксер и курлом дёргать разные пути. Как было раньше до 1.22. Для пути /posts используется один и тот же обработчик — вне зависимости от метода. Метод определяется уже внутри обработчика, это не очень удобно. Причём Go сам никак не валидирует указанный метод. Можно вызвать тот же путь с методом AVITO, например, — и всё отработает без ошибок. Как теперь в 1.22. Если явно указать метод в шаблоне, то нужный обработчик вызывается только для запроса с этим методом. Обратите внимание: при указании метода GET зарегистрируется обработчик и для GET, и для HEAD. При этом у шаблонов с методом приоритет выше, чем у шаблонов без него. Как было раньше до 1.22. Вне зависимости от хоста для одного пути вызывается один и тот же обработчик. Вернёмся к прежнему шаблону: Как теперь в 1.22. Можно назначить разные обработчики на один и тот же путь в зависимости от того, какой хост используется при вызове. Обратите внимание, между хостом у путём не должно быть пробела. Как было раньше до 1.22. Если требуется обработать пути вида /posts/{id}, то используют слеш на конце пути. Например, при указании шаблона \"/posts/\" все пути, начинающиеся на /posts/ обрабатываются с помощью одного обработчика. Из-за этого в обработчике приходится отдельно вытаскивать id поста. /posts/1 и /posts/latest, скорее всего, должны отдавать разный контент, но оба пути используют один обработчик posts-with-slash. Как теперь в 1.22. Можно использовать плейсхолдеры в шаблоне запроса для более точного роутинга. Например, /posts/{id} будет соответствовать всем URL, которые начинаются на /posts/ и содержат два сегмента. Плейсхолдер может соответствовать целому сегменту, как {id} в примере выше, или, если он заканчивается на ..., — всем оставшимся сегментам пути, как в шаблоне /files/{pathname...}. Для обозначения конца пути можно использовать специальный знак {$}. Например, /posts/{$} будет соответствовать только /posts/, но не /posts или /posts/123/. В Go допустим конфликт шаблонов. Например, шаблоны /posts/{id} и /posts/latest перекрывают друг друга. При вызове /posts/latest непонятно, какой обработчик нужно использовать. Давайте разберёмся, какие шаблоны имеют наивысший приоритет. В версиях до 1.22 выбирается более длинный шаблон — независимо от их порядка. Например, Go предпочтёт /posts/latest, а не /posts/. Теперь в 1.22 при конфликтах выбирается наиболее конкретный шаблон. Например, Go выберет /posts/latest вместо /posts/{id}. А вместо /users/{u}/posts/{id} выберет /users/{u}/posts/latest. Для методов — аналогично. Например, GET /posts/{id} имеет приоритет над /posts/{id}, потому что первый соответствует только запросам GET и HEAD, а второй — запросам с любым методом, то есть такой шаблон менее конкретный. Для хостов — по-другому. Для них пришлось сделать исключение, чтобы сохранить совместимость. Если два шаблона конфликтуют, но у одного явно указан хост, а у другого — нет, то выбирается шаблон с хостом. Если два шаблона конфликтуют, но среди них нельзя выделить наиболее конкретный, вызовется паника. Например, /posts/latest подходит под шаблоны /posts/{id} и /{resource}/latest. В каком бы порядке вы ни зарегистрировали эти шаблоны, при регистрации в обработчике /posts/latest произойдёт паника. То есть до запуска сервера с таким роутингом дело не дойдёт. Изменения в роутинге ломают обратную совместимость. Например, предыдущие версии Go принимали шаблоны с фигурными скобками и трактовали их буквально, а в версии 1.22 используются фигурные скобки для подстановочных знаков. Старое поведение можно вернуть, задав GODEBUG-переменную окружения: GODEBUG=httpmuxgo121=1. Кроме того, проверьте, какая версия Go установлена у вас в go.mod. Если там версия ниже 1.22, то весь роутинг будет работать в режиме совместимости, и все нововведения будут отключены. Поднять версию в go.mod можно просто отредактировав его руками, или командой go mod edit -go=1.22.2 (укажите вашу версию Go).",
    "74": "КНР уже давно реализует программу развития отрасли разработки и производства электроники в стране. Причём достаточно успешно — настолько, что сейчас в ряде государственных учреждений планируется переход с чипов AMD и Intel на китайские процессоры. Кроме того, собираются в Китае замещать и ПО — в большинстве случаев на Linux-дистрибутив UOS. Пока что на собственные чипы и ПО переходят не все государственные учреждения. Речь в первую очередь про школы и университеты округа Хэби в провинции Хэнань. Здесь будут заменять парк компьютеров на отечественные устройства, включая как «железо», так и программное обеспечение. Пилотный проект, а речь именно о нём, предусматривает распределение 10 тысяч китайских ПК между 50 школами. Таким образом, учащихся планируют адаптировать к использованию отечественных устройств и софта. Компьютеры, о которых идёт речь, базируются на чипах Loongson 3A5000. Архитектура здесь уникальная, разработанная с нуля — LoongArch. Она не основывается на американских технологиях. Более того, продукцию компании нельзя продавать за границу, поскольку она признана стратегически важной. С января по февраль 2024 года к настольным и серверным платформам на процессорах Loongson добавилось 107 новых адаптированных продуктов от 76 компаний. В их число входят: 31 бизнес-система, 13 прикладных систем безопасности, 7 облачных платформ, 6 систем искусственного интеллекта, 4 ФС и 46 других продуктов. Кроме того, все 10 тысяч компьютеров получили китайскую же операционную систему UOS (Unity Operating System). Это не новинка, ей уже 5 лет: разрабатывается софт с 2019 года. ОС создаётся компанией UnionTech по заказу Китайской Народной Республики в целях замещения иностранных операционных систем, основан на дистрибутиве Deepin. Разрабатываются две ветки — настольная и серверная. В ОС есть собственный магазин приложений, а в версии для учащихся добавился набор программ, который позволяет заменить привычный для многих софт под Windows OS. Например, вместо Microsoft Windows установлен WPS Office. Есть и специализированное ПО, предназначенное для учебных заведений. Насколько можно понять, это лишь очень небольшой пилотный проект (10 тысяч устройств для Китая — капля в море). Но если он покажет себя хорошо, то программа будет масштабирована и аппаратное обеспечение вместе с ПО станут замещать и в других регионах. Возможно, ПО будет варьироваться, ведь UOS далеко не единственная ОС из Поднебесной, предложенная в качестве альтернативы Windows. Есть ещё и Kylin Linux с Deepin Linux. В конце марта 2024 года власти страны заявили о необходимости уходить от использования процессоров Intel и AMD в государственных структурах. Речь идёт о развитии стратегии импортозамещения, которая актуальна в КНР вот уже несколько лет. Не так давно китайцы решили заменить Windows собственной ОС, а сейчас принялись за процессоры. Министерство промышленности страны выпустило рекомендацию для государственных учреждений, в которой говорится о необходимости найти альтернативу чипам Intel и AMD. Не «ещё вчера», на это даётся около трёх лет. В той же рекомендации сообщается о том, что замена желательно должна быть китайской, например это могут быть процессоры от Huawei и Phytim. На данный момент ведомство уже одобрило сразу 18 разных чипов в качестве замены микросхемам от Intel и AMD. Реализация стратегии должна быть завершена не позднее 2027 года, то есть на это даётся три года, о чём сказано выше. Кроме того, государство предлагает заменить ещё и оставшиеся иностранные сервисы и программное обеспечение. Их должно хватить, поскольку сейчас китайские компании активно развиваются. Фабрики КНР могут выпускать чипы по технологиям вплоть до 7-нм техпроцесса. Сейчас в КНР осваивают и более современные разработки. Так, например, крупнейший в Китае контрактный производитель чипов SMIC действительно находится буквально в шаге от выпуска новых процессоров. Разработала их другая компания — «дочка» Huawei, HiSilicon. Как оказалось, чипы будут производиться на двух новых фабриках, которые построены в Шанхае. Сначала эти процессы установят во флагманских моделях смартфонов. Если всё пройдёт так, как и планируется, то компания SMIC начнёт производство уже более серьёзных чипов, которые предназначены для серверов. Также китайцы планируют начать разработку и, возможно, поставки на внешний рынок ускорителей искусственного интеллекта. создание собственных литографов, пусть и работающих по устаревшим технологиям. По большинству этих направлений у КНР уже есть успехи, причём достаточно значительные результаты достигнуты как Huawei, так и SMIC. Что касается последней, то совершенно точно она смогла освоить 7-нм техпроцесс, хотя и для смартфонов. Более старые 28-нм и 14-нм технологии этой компанией также освоены. Так, о производстве 14-нм чипов внутри страны Китай объявил немногим более года назад. Государство помогает SMIC, да и не только этой компании, выделяются весьма значительные средства. Насколько можно судить, стратегия по развитию собственной отрасли разработки электроники работает. Есть сложности, без них никуда, но в целом задача решается.",
    "75": "С 2008 года моя компания занималась кастомной разработкой сайтов, веб-сервисов и мобильных приложений. Спустя 15 лет я окончательно выгорел, продал долю партнеру и на вырученные деньги решил запустить детский развлекательный центр. Полтора месяца назад мы открылись и вот что из этого вышло. В статье я расскажу, сколько мы потратили на запуск центра и сколько уже зарабатываем. Многие айтишники мечтают создать свой продукт. Обычно это связано с желанием независимости от хотелок начальства/клиентов и стремлением к самореализации и созданию чего-то, исходя из своего видения прекрасного. В 2008 году я открыл IT компанию и за 15 лет мы разработали десятки CRM, СDP, CMS и прочих трехбуквенных управленческих и аналитических систем для клиентов различного размера. С каждым годом экспертиза всё росла, а с ней и желание сделать свой софт. В идеале не просто очередное облачное ПО, а в связке с реальным физическим продуктом или услугой. Этими мыслями я был слегка одержим. В июле этого года основатель Додо пиццы Федор Овчинников устраивал археологическую экспедицию в Усть-Цильме. По счастливой случайности я присоединяюсь к этой экспедиции. Это был не только невероятный опыт, но и знакомство со множеством классных людей из разных сфер бизнеса, среди которых был Алексей Красиков, основатель сети детских развлекательных центров Скалалэнд. У Алексея была схожая с моей проблема, только наоборот. Успешный рабочий бизнес, которому остро не хватает автоматизации. На этой почве мы и нашли много очевидных возможностей для сотрудничества. 30 августа 2023 года мы официально договорились о партнерстве. Я стал частью команды Скалалэнд, в которой отвечаю за IT трансформацию всего бизнеса. Кроме того, помимо IT, я стал франчайзи нового флагманского детского центра в Москве. Такое решение обусловлено двумя вещами: “Skin in the game”, то есть максимальная заинтересованность в успехе компании и качестве будущего IT продукта. Место проверки гипотез и тестирования бизнес-процессов. В этом центре мы внедряем и тестируем все новые технологичные решения и продукты. Средняя стоимость ремонта и оснащения детского центра составляет 37 000 рублей за 1 м2. Это цена, если входить в помещение состоящее только из бетонного пола и стен. В эту стоимость включены уже абсолютно все расходы от строительных материалов до канцелярии и костюмов аниматоров. Главная трудность при открытие нового центра - поиск подходящей локации. Идеальное место это торговые центры, особенно сейчас. Во-первых, там созданы все условия для соответствия всем техническим требованиям нашего законодательству. Во-вторых, сейчас такое время, когда у ТЦ большие трудности с арендаторами, а детские центры генерируют дополнительный трафик (за счёт частых праздников на которые приходят даже те, кто может и не собирался идти в ТЦ). В текущих условиях это позволяет получать неплохие условия по аренде в сравнение со стрит-ритейлом, где на хорошие локации спрос ничуть не упал. С самого начала я рассматривал вариант открытия центра в формате Мини, не более 400м2. Ведь выручить серьезные деньги на продаже сервисной IT компании достаточно сложно. Однако в процессе поиска локации нам попалось идеальное место в 650 м2 в ТЦ в Новой Москве. В итоге было принято решение затянуть пояса и открыть центр на 650 квадратов. Планируемый бюджет вырос с 15 до 24 миллионов рублей. При этом место от нас чуть было не ушло. Конкуренция за хорошие локации настолько высокая, что один из конкурентов даже звонил владельцам ТЦ убеждал их,  что основатель нашей сети умер и не стоит подписывать с нами договор, ведь вся сеть скоро разорится. Пришлось оперативно созваниваться с собственниками ТЦ по видеосвязи. На кухонном оборудование удалось сильно сэкономить. Почти всё мы взяли б/у в специализированных компаниях. С аттракционами, к сожалению, так не получилось. Расходы на демонтаж, транспортировку и адаптацию под новое помещение достигает 50%. Заказать новое выходит так же по стоимости и гораздо проще. Наш развлекательный центр это, по сути, большая крытая игровая площадка. Родители приводят к нам детей, отправляют их играть на аттракционы, а сами могут посидеть в кафе. На входе на каждого ребенка приобретается билет на определенное время, от 1 часа до безлимита на весь день. Выручка от свободных посещений составляет обычно от 40% до 60% всей выручки центра в зависимости от сезона. Летом идёт значительное снижение, сложно конкурировать с уличными площадками в хорошую погоду, особенно в Москве. В основном это Дни Рождения. Мы предоставляем отдельные патирумы, в которых проводится анимация и накрывается праздничный стол. После анимационной программы дети получают доступ ко всем аттракционам центра на 4 часа. Так же часто отмечают выпускные и тематические праздники (Новый Год). Доля выручка от праздников обычно составляет 25-35%. Наш центр первый во всей сети с полноценной собственной кухней и большой зоной кафе на 10 столиков. Кухня в основном работает на праздники. Мы сконцентрировались на небольшом и самом ходовом ассортименте: пиццы и картошка фри. Посетители кафе обычно берут напитки и десерты. 15% приходится на кофе, а самый продаваемый товар обычная вода без газа. При этом мы продолжаем постоянно тестировать ассортимент. Недавно решили попробовать сэндвичи и роллы. Заказали на 5000 рублей, продали два штуки и остальное через 3 дня выкинули по сроку годности. Тестируем дальше. Выручка от кафе и кухни сейчас находится на уровне 25% от общей. Техническое открытие у нас состоялось 10 февраля, но все аттракционы мы запустили только к началу марта. Вчера я подбил итоговые результаты за первый месяц работы: Выручка составила 2 331 450, а EBITDA 298 897 (P&L можно скачать в ТГ). Это весьма неплохой результат для первого месяца. Изначально, мы планировали выйти в операционный плюс к сентябрю, но спрос на удивление оказался таким высоким, что мы даже отключили всю рекламу спустя неделю после её запуска. И дело не в том, что у нас была полная загрузка, а просто мы не успели нанять нужное количество персонала, не ожидая такой ажиотаж в первый месяц. Первые выходные. Неожиданно битком. Несколько выходных я сам работал инструктором на скалодроме, чтобы детям не пришлось стоять в долгих очередях. А 18 марта мы поставили новый рекорд, перевалив за 200 000 рублей выручки за день. Пока рано делать выводы, но радует, что мы можем даже успеть немного накопить прибыли, чтобы спокойно войти в мёртвый сезон. Наша цель по базовой выручке (это когда сезонность 0%) составляет 3 млн рублей. Это почти в полтора раза больше, чем сейчас. Как вы помните, текст начинался с того, что мне хотелось создать собственный IT-продукт. В нашем случае задача этого продукта увеличить выручку и прибыль как нашего, так и всех центров сети. Продуктов, на самом деле, два. Да, краткосрочно, пилить свою CRM-систему это не самое умное решение, но долгосрочно это точно окупится с учетом всей специфики и нюансов бизнеса. Тем более за 15 лет я научился делать такие системы быстро и недорого. Меньше чем за пол года мы уже написали большую часть этой системы и уже во всю её тестируем. Я уверен, что нам удастся значительно увеличить поток заявок на праздники и их конверсию. Ну и в целом над удержанием и возвращаемостью мы тоже будем активно работать У меня уже 3 года есть небольшой стартап - студия разработки игр. Несколько лет мы делали мобильные игры, а год назад даже выпустили полноценный ПК шутер в Steam. В рамках этой студии в конце прошлого года мы решили попробовать собрать интерактивную игру для развлекательного центра. Это такая проекция на стене, к которой подключен датчик, считывающий касания. В результате получается интерактивная панель в виде проекции на стене, на которую можно нажимать руками Первый прототип мы собрали за пару месяцев и так увлеклись, что решили вывести это в отдельное полноценное направление. Площадкой для тестирования всех игр выступает мой центр. Я сам лично тестирую каждую игру и внимательно слежу как в неё играют дети. Первые игры мы переработали уже десятки раз, чтобы было реально интересно и весело. Изначально, у нас в центре должен был быть лазертаг, но в самый последний момент я передумал, решил рискнуть и сделать большую интерактивную зону. Сейчас интерактивная зона - самый популярный аттракцион в центре, так что оно точно того стоило. Кроме того, каждая новая игра это совсем новый опыт, а значит детям каждый раз будет интересно возвращаться к нам, чтобы поиграть во что-то новенькое. Это точно повысит возвращаемость и LTV. Если интересны подробности - напишите в комментариях. В следующем посте я тогда расскажу, как мы создавали интерактивные игры, почему не стоит заказывать датчики слежения в Китае в декабре и как опыт работы с европейскими мобайл паблишерами позволяет нам создавать действительно интересные игры. Оперативно следить за нашими успехами можно в моём ТГ-канале. Пишу там сам и не чаще раза в неделю. Рассказываю про техническую составляющую проекта и публикую ежемесячные финансовые отчеты.",
    "76": "Давно хотите арендовать сервер, но нет такой возможности? Пройдите квест и выиграйте до 15 000 бонусных рублей на услуги Selectel! Регистрируйтесь на сайте и попробуйте себя в роли сыщика: найдите на страницах Selectel спрятанные ссылки и первыми дойдите до финала. Выиграйте эксклюзивный мерч и промокод на сервисы Selectel.",
    "77": "Несколько дней назад на Хабре была опубликована статья Правда ли, что в Европе везде отсталые сервисы, медленные платежи и плохие онлайн-услуги? Которая достаточно быстро набрала много просмотров и вызвала бурные обсуждения в комментариях. Я так же с интересом ее прочел и еле удержался, чтобы не вступить в дискуссию, но потом все же решил написать отдельную статью \"по горячим следам\", так как написать хотелось много чего. Не то, чтобы я согласен или не согласен с автором, а скорее хотелось бы показать, что ситуация сильно отличается от страны к стране, и может даже от региона к региону внутри этой страны. Сам я в настоящий момент живу в Германии и успел уже пожить как в крупном городе-миллионнике (Кельн), так и в маленьком баварском городке на 40 киложителей, название которого никому ничего не скажет (мне оно вообще было неизвестно, до того как судьба занесла меня в него, поэтому для простоты буду ниже по тексту называть его \"город Y\") Почта (в след. статье) Медицина (в след. статье) Онлайн‑ритейл (в след. статье) ДатенШутц (в след. статье) TLDR:  Все не так плохо. Но до в среднем до уровня Чехии недотягивает, до уровня РФ недотягивает сильно. Часть неудобств лежит скорее не в цифровой, а в организационной/законодательной/исторической плоскости. В Германии нет единого портала \"а-ля Госуслуги\". Каждая земля, а то и каждый город - вещь в себе. Поэтому надо найти сайт местного БюргерБюро/КунденЦентра/РатХауза и уже изучать его что-как там устроено. Сайты часто с недружелюбной/непрозрачной навигацией и \"битыми\" ссылками - наберитесь терпения) Для регистрации по месту жительства, что в Кельне, что в городе Y, надо получить сначала ТермИн (= appointment, запись). Вообще в Германии если хочешь куда-то пойти, надо сначала получить Термин для этого. Привыкаем. Получить его можно и нужно онлайн на портале города - \"без регистрации и смс\", просто выбираешь услугу, оставляешь ФИО + email и все. Интерфейс в обоих городах достаточно допотопный, отправляющий нас в район конца нулевых. Несовременно, но работает. В назначенный день и час приходишь и ждешь вызова по номеру электронной очереди, что тебе прислали на почту. Если хочешь отменить запись - либо в письме есть ссылка, на которую надо для этого кликнуть (Кельн), либо звонишь по указанному в письме/на сайте номеру (город Ы). Тут надо еще сказать, что в Германии \"в среднем\" большая проблема с временем ожидания. В Кельне для регистрации по месту жительства (которую ты обязан сделать в течение 2 недель после переезда) среднее время ожидания - 2 месяца, но в городе Y - есть свободные слоты уже \"на послезавтра\". В Фюрершайнштелле (ведомство, которое занимается водительскими правами) в Кельне нас записали (причем запись только по телефону) только через 20 дней, в городе Y - \"приходите хоть завтра\" (и запись онлайн - внедрили в конце 2023 года). Тут еще надо сказать, что все ведомства работают только в будни, только в рабочее время и с перерывом на обед. Никакой возможности посетить их \"после работы\" нет в принципе. Но объективности ради надо сказать, что зачастую работают они с 7:30 или с 8:00 утра - так что посетить их перед работой вполне реально. После регистрации по месту жительства по обычной бумажной почте через дней 20 пришел TaxID (приходит автоматически при первой регистрации на территории Германии). Ничего для этого специально делать не надо. По умолчанию нам с супругой присвоили налоговые классы 4, что в нашем случае немного не выгодно. Поэтому мы ногами пошли в Финанцамт (термин получили онлайн на \"через неделю\", но только потому, что выбирали удобное время), где заполнили бумажное заявление на смену классов. Оформление ВНЖ у меня занимался работодатель, поэтому мало что могу сказать про него. Насколько понял, сотрудник HR просто заполнил за меня какой-то формуляр, скаченный с сайта, и отправил по электронной почте. Далее мы ждали 3 месяца, пока нам придет термин (пришел на \"через 3 недели\"), потом, уже после личного посещения (где снимают отпечатки пальцев, ты даешь фото и подписываешь все лично), ждали еще 2 месяца электронного письма о готовности. Благо, забрать можно в порядке живой очереди. Еще из госуслуг столкнулся с подачей налоговой декларации. Это можно сделать самому через программу от налоговой (Elster) с недружелюбным интерфейсом уровня старой \"3-НДФЛ\" от нашей налоговой, либо воспользоваться платными (в среднем 39,90 евро за одну декларацию), которые есть на любой вкус и цвет (с установкой на компьютер и онлайн, в виде чат-бота, задающего вопросы или просто мастера, где ты последовательно вводишь запрашиваемые данные и т.п.). Плюс таких программ - они подсказывают тебе что, где и как можно \"оптимизировать\", а также, например, распознают загруженные документы от работодателя с информацией о годовой зарплате и автоматически подтягивают из них информацию. Все платные программы берут плату только в момент отправки декларации в налоговую. В моем случае программа еще дала мне PDF-файл с пояснениями касательно части пунктов декларации, который я должен просто распечатать, подписать и отправить в налоговую по обычной почте. Лично меня удивило, что в этих программах я регистрируюсь только по электронной почте и моя личность никак не проверяется. После отправки декларации мне надо ждать ее обработки налоговой. Программа приводит статистику времени обработки: После того, как мы перехали из Кельна в город Ы, через 3 месяца(!) нам по бумажной почте пришло письмо от местного АБХ (ведомство по делам иностранцев) - мол мы знаем, что вы теперь тут живете, поэтому мы теперь за вас ответвенные. И кстати, у вас в условиях ВНЖ ограничение на работы в фирме Х - вы случайно то работу не сменили? Если сменили, то бегом к нам - мы вам новое приложение в ВНЖ сделаем. В письме были все контактные данные некой Фрау, в т.ч. и ее рабочий email - ответил ей на него, что мол что подтверждаю, что продолжаю трудиться на фирму Х. Фрау это устроило. Вообще, у нас карточка ВНЖ с чипом и ПИН-кодом. Теоретически это дает возможность идентификации себя в онлайне через специальное приложение. Мы поставили себе такое, но попытка настроить его провалилась, так как приложение выдавало сообщение, что мол эта функция у нас не включена, хотя бумага с ПИН-кодом, прилагавшаяся к ВНЖ, говорила об обратном. Звонок в ведомство ничего не дал - фрау на том конце провода смогла только посоветовать \"попробовать еще раз через месяц\" :-) Пока решили забить за имением более приоритетных дел. Как резюме по госуслугам. В принципе жить можно. Отсутсвие модных удобных интерфейсов на самом деле не напрягет (после того, как наконец-то разобрался с местным сайтом). Самая большая проблема - получение термина на ближайшее время (вряд ли решается цифровизацией, так как связана с нехваткой служащих) и порою долгое время ожидания результата (может решиться цифровизацией, так как ускорит/упростит/автоматизирует процессы). Ощущение, что \"онлайн\" - это часто скорее некий \"враппер\" обычных оффлайновых процессов, и на \"том конце\" все равно \"работу работает\" какой-то Макс Мустерман. Ну и надо смириться, что зачастую придется звонить по телефону. Но есть плюс - на той стороне трубку возьмет живой человек, а не тупой робот-советник. Если конечно у человека не перерыв на чаепитие :-) Про банкинг могу писать много, ибо самая больная для меня тема. Но попробую покороче :-) В Германии есть два типа банков - традиционные(с отделениями) и нео-банки (чисто онлайн). Как я только прилетел, сразу подал заявки на открытие счетов в несколько нео-банков, но отовсюду получил отказ. Формальная причина - запросили ВНЖ, которого у меня пока еще не было, а как там на самом деле в условиях текущей геополитической обстановки...  Только Револют мне согласился открыть счет по рабочей визе, но он не походил для получения ЗП, так как имеет не немецкий IBAN. Вообще Револют мне очень нравится, но про него все написал автор изначальной статьи, поэтому не буду повторяться. В итоге мне для получения ЗП пришлось пойти в Шпаркассу (этакий местный Сбербанк), где только по паспорту и Анмельдунгу (бумажке, подтверждающей регистрацию по месту жительства) мне открыли счет за полчаса (карта и ПИН пришли по почте через 1-2 недели). Термин, понятно дело, надо получать заранее (за примерно 2 недели, если хочешь желаемые время-день, онлайн).  Приложение Шпаркассы по функциональности (по дизайну получше будет) - как приложения наших банков во второй половине нулевых: смотришь операции, баланс, можешь отправить перевод. Кстати, FaceID поддерживает. Собственно на этом его полезные свойства заканчиваются. Нет даже пушей о совершенных операциях или какой-то квитанции о проведенной операции. Выписку тоже запросить нельзя, но правда она автоматом приходит тебе в личный кабинет в конце месяца за месяц. Часть и без того скудных функций приложения реализованы через iFrame (вроде так это называется, когда внутри приложения открывается по сути веб-страничка, хорошо хоть авторизация при этом автоматическая :-)). Кстати, у другого банка (ING, открыл себе после получения ВНЖ) так же часть функционала мобильного приложения реализована схожим образом. А еще зачем-то у Шапркассы (кстати, и у Santander тоже) для двухфакторной авторизации надо ставить отдельное приложение. В чем смысл, я так и не понял. Выглядит странно - для подтверждения платежа тебя перебрасывает во второе приложение, где ты проводишь сладером по экрану, и тебя возвращает обратно в основное приложение. Если кто знает смысл подобной реализации - напишите в комментариях плз! Если не брать Revolut, то еще я доволен С24. У него даже приложение по внешнему виду сильно похоже на Revolut-овское, только нет всякой ненужной крипты и прочего хлама. Чисто банк. Пуши информативные есть, операции есть, накопительный счет есть, виртуальные карты есть, шаблоны есть (кстати, в Шпаркассе они тоже были, но не работали))), регулярные платежи есть, даже аналитика расходов есть. Но не обошлось и без ложки дегтя, конечно же: банк почему-то решил, что Turkish Airlines - опасный для меня мерчант и в какой-то момент заблокировал оплату картой у него. Поддержка ничего внятного не говорит - стандартное: \"В целях безопасности и сохранности ваших денег мы заблокировали оплату у данного мерчанта\". В общем, что в РФ, что в Германии - не клади все яйца в одну корзину, имей несколько карт/счетов. Обычно стандартная рекомендация - иметь счет в нео-банке и в традиционном. Про оплаты услуг. В Германии почти всё, на что мы в РФ настроили бы автоплатёж (телефон, коммуналка, интернет и т.п.) оплачивается через Lastschriftmandat - разрешение на прямое дебетование твоего счёта. То же самое, как в Чехии Direct Debit - мол, разрешаю такому-то юридическому лицу в рамках такого-то договора списывать с моего счёта деньги. Мы так платим за телефон, интернет, электричество, проездной, различные страховки... Разрешение на это ты даёшь в момент заключения договора - не важно, галочка ли это в онлайн-форме или на бумаге. Что интересно, почти всегда можно указать, что мол, не я являюсь владельцем счёта, а Иванов Иван - никто ничего проверять не будет. Например, супруга указывала мой счёт для оплаты своего проездного, но ни поставщик, ни банк не спросили меня, разрешаю ли я? Просто в начале следующего месяца сумма была списана с моего счёта. То есть теоретически, ничто не мешает указать чужой номер счёта и тем самым совершить мошенничество. С другой стороны, наверное, если владелец счёта заметит и пожалуется в банк, то тебя найдут, и ответственность будет серьёзной... Не знаю, проверять не хочу. Для меня это Lastschriftmandat, конечно же, выглядит как минимум странно и как-то неконтролируемо. Самый большой риск, который я вижу - что в силу разных причин у тебя может в нужный момент не оказаться нужной суммы на счёте (ведь каждый поставщик списывает деньги в разный день месяца, да ещё с погрешностью +/- пару дней), что приведёт к лишнему геморрою и дополнительной комиссии от банка и поставщика за неудачную попытку списания. Поэтому я веду строгий учёт, кому я разрешил это прямое дебетование и по возможности стараюсь избегать его везде, где это возможно. Ещё у большинства банков тут есть \"сервис по смене счёта\" - это когда ты решил сменить свой основной счёт, и банк за тебя уведомляет всех твоих контрагентов (от мобильного оператора до даже работодателя) о том, что мол, ты сменил счёт, и теперь твои платежи надо списывать с другого счёта. Удобно. Переводы. В основном все используют PayPal (их можно, кстати, и оплачивать во многих интернет-магазинах и некоторых приложениях). Или переводят по номеру счёта внутри SEPA - такие переводы бесплатны, идут, как правило, 1-2 дня. За дополнительную комиссию (а у некоторых банков даже бесплатно) можно отправить мгновенный SEPA-перевод. Переводы внутри одного банка мгновенны по умолчанию. И, завершая секцию про банки, надо отметить, что большинство банков берет плату за ведение счета. Как правило, сумма зависит от пакета услуг и колеблется от 3-4 евро в месяц до 10. Плюс могут быть комиссии за снятие наличных, за оплату в другой валюте, у нео-банков почти всегда есть комиссия за внесение наличных на счет (так как это идет через чужие банкоматы или через партнеров). У некоторых нео-банков есть бесплатные пакеты, но с ограничением доступных сервисов. Также немало банков дают тебе какой-нибудь средненький пакет бесплатно, если ты на него перечисляешь зарплату. Я, после ухода от Шпаркассы, умудряюсь ничего не платить за свои 3 счета, распределив свои \"сценарии использования\" между 3-мя банками, взяв от каждого то, что мне нужно в пределах бесплатных или \"условно-бесплатных\" пакетов. Как резюме про банкинг. В целом неплохо. Мне даже нравятся местные минималистичные (по сравнению с русскими) приложения нео-банков - никаких тебе сториз, рекламы в пол-экрана, интегрированного заказа столика в ресторане и т.п. Всё строго и четко - счёт/карта, остаток, операции - всё важное видно без лишних кликов. Чего не хватает, так это нормального кэшбэка, оперативного чата поддержки, может большего юзабилити интерфейса, но это уже \"бантики\". Тут все просто. Сим-карта - только по паспорту с 2017 года. Купить симку можно хоть в супермаркете, хоть в фирменном салоне (где её сразу активируют), хоть заказать по почте через сайт оператора. Активация - после идентификации через видео-звонок или поход на почту. Есть три классических оператора и огромное количество виртуальных. Говорят, что качество связи у виртуальных похуже. Не могу подтвердить это из своего опыта: у меня личная симкарта от виртуального, рабочая - от классического. Разницы за все время не заметил. Цены у классических операторов значительно выше виртуальных. Типичное предложение: безлимитные звонки/смс по Германии + сколько-то интернета. Интернет-роуминг внутри ЕС также включен, а вот за звонки заграницей скорее всего придется платить дополнительно. Обычно есть \"плата за подключение\" - 0/10 или 10/20 евро в зависимости от того, заключаешь ли контракт на 24 месяца (с автоматическим продлением потом) или хочешь иметь возможность расторгнуть в любой момент. Также есть препейд-тарифы, но они мало популярны и более дорогие. Что касается сервиса и качества. Внутри города очень много мест, где есть 5G. Надо сказать, что значок 5G я увидел впервые именно в Германии у своего телефона :-) А вот за городом покрытие может быть плохим. Например, пока мы едем 40 минут из своего города на электричке до другого, примерно на половине пути нет даже 3G. Может быть, это связано с тем, что поезд проходит через леса и поля... С другой стороны, сравнивая с Россией, я тоже помню, как уже через час езды на поезде дальнего следования от Москвы не мог отправить даже СМС. Мобильные приложения для управления номером - минималистичны. Нет никаких сториз, витрин, встроенных маркетплейсов и т.п. Просто просматриваешь остатки пакетов, информацию по договору, и всё. Иногда можешь поменять свои личные данные, например, адрес. Чего не хватает - возможности просмотра и подключения различных опций. Про домашний интернет писать почти нечего - работает и работает. С вероятностью 70% у вас будет DSL. А может, не повезти, и будет подключение через ТВ-кабель. Сейчас в Германии идет какая-то программа по прокладке оптоволокна, но конкретно у нас и у моих знакомых подключить его \"нет технической возможности\", хотя провайдеры уже рекламируют его. В нашем доме по планам, вроде, где-то в 2025 году. У меня тариф 100 Мб/с, плачу за него - первый год 13 евро/мес, со второго - 38 евро/мес. И такая дифференциация цены по годам у многих провайдеров. Наверное, поэтому многие немцы не парятся и просто раз в 2 года меняют провайдера. Благо, их тут тоже много и это не проблема - в отличии от РФ, здесь провайдер не тянет каждый свой кабель в каждый дом, а просто арендует его у ДойчеТелекома - владельца инфраструктуры. Почему-то тут очень распространено брать роутер у провайдера в аренду. Причем 80% рынка занимают неизвестные мне доселе роутеры FritzBOX - может они и хорошие, но на мой взгляд явно оверпрайснутые. Поэтому я купил себе Keenetic за цену годовой аренды непонятного роутера от провайдера. Как резюме по мобильной связи и интернету. Работает без нареканий. Начнем с Дойчебана. Пунктуальность немецких железных дорог осталась в прошлом... Но если говорить про цифровые сервисы, то тут все более-менее нормально: покупка билета через сайт или мобильное приложение, билет также в виде QR-кода в приложении. Само приложение вполне нормально ищет все возможные маршруты, сообщает об отмене поездов или присылает пуш, что ваш поезд будет во время (или опаздывает на сколько-то минут). Также оно в режиме реального времени показывает, на какой путь приходит поезд и его схему вагонов - очень удобно и полезно, когда едешь с пересадками, особенно короткими - не надо на незнакомой пересадочной станции искать табло, так как вы сразу знаете, на какой путь прибывает ваш следующий поезд. Есть также online check-in - актуально для поездов IC и ICE - просто сели на свободное место, зачекинились в приложении и к вам даже не подходят проверять билеты. Кстати, ГуглМэпс также строят маршруты, но не так удобно, как приложение Дойчебана - у Гугла нельзя задать фильтры на виды поездов, что очень важно, когда у вас ДойчландТикет (месячный проездной на весь местный общественный транспорт в Германии за 49 евро - ввели с мая 2023 года). Если не хотите пользоваться приложением, то есть сайт с онлайн-табло всех станций в Германии - у меня он в избранном. Если приложением я пользуюсь, когда у меня сложный маршрут с несколькими пересадками, то на сайте удобно посмотреть перед выходом из дома, не опаздывает ли ваш поезд, когда едете в соседний город. Что касается городского транспорта, то тут у каждого города/ландкрайза может быть свое творчество. Приложения так себе и по функционалу, и по дизайну. В Кельне я пытался пользоваться, но понял, что никакого профита относительно построения маршрутов просто в ГуглМэпс оно не дает. В городе Y смысла пользоваться городским транспортом мало и из-за размера города, и из-за того, что автобусы ходят зачастую 1-2 раза в час - на велосипеде будет быстрее. Такси. Заказать через приложение можно. Есть как минимум taxi.eu и FreeNow (в последнем, вроде как, есть еще и каршеринг, но я не пользовался). Также в некоторых городах работает Убер (но я не пользовался). Приложение покажет, скорее всего, примерную цену, но ты все равно поедешь по счетчику. Оплата привязанной картой возможна. Но, в отличие от РФ, такси тут - достаточно дорогой вид транспорта даже для ИТ-шника (хотя почему \"даже\"? В Германии ИТ-шник ничем не лучше/хуже любого другого квалифицированного работника с высшим образованием). Например, поездка из Кельнского аэропорта в город обойдется под 50 евро - иногда авиабилет стоит дешевле :-) А за 15 минут поездки по Кельну без пробок я отдал как-то 25 евро.. Если же ты хочешь взять такси в аэропорту или от железнодорожного вокзала, то там, как правило, уже на выходе стоит очередь из таксистов, которые ждут пассажиров. Никто не налетает на тебя со словами \"Такси, такси недорого\" - ты просто идешь к началу очереди, садишься и едешь. Когда мы только прилетели в Кельн с 5 чемоданами и сноубордом, мы пошли к самому первому таксисту, который стоял напротив нашего выхода, но он вежливо предложил нам пройти в начало очереди. Еще надо сказать про велошеринг. В Кельне мы им пользовались постоянно, так как если ты купил ДойчландТикет у местной транспортной компании KVB, то тебе дают бесплатно первые 30 минут каждой поездки. Но даже без ДойчландТикета можно купить за 10 евро в месяц абонемент с такими же условиями. Работает велошеринг хорошо. Мы с супругой так как-то 1,5 часа катались, меняясь великами (после сдачи, примерно 5 минут ты не можешь взять этот же велик). Главное следить за временем и зонами, где можно оставлять велик, где нельзя, а где за это с тебя снимут дополнительный 1 евро. Один раз из-за погрешности GPS с меня сняли этот 1 евро, но пара сообщений в службу поддержки - и мне его вернули. Как резюме по транспорту. Приложение ДойчеБана, считаю, заслуживает твердой оценки \"хорошо\". ГуглМэпс хорошо дополняют его в части остальных видов транспорта. Учитывая, что автобусы ходят достаточно точно по расписанию, то в отслеживании положения транспорта в режиме \"онлайн\", как в Яндекс.Транспорте, просто нет надобности. Такси дорогое, а потому здесь это для нас (по крайней мере, на текущий момент) скорее роскошь или крайний вариант в случае отсутствия альтернатив. Велошеринг - тема, много раз пользовались и для прогулки, и просто чтобы доехать от метро до офиса. Но только в больших городах. Получилось очень много, поэтому на этом пока прервусь и остальные аспекты затрону в след. статье. Всем хорошей пятницы и выходных!",
    "78": "Изучение человека в настоящее время проводится множеством разных наук известными и новыми методами и весьма интенсивно. В мире осуществляются многомиллиардные исследовательские проекты. Изучаются геном, протеом, транскриптом человека, мозг человека и другие составляющие организма. Люди поняли, что пришло время серьезно взяться за изучение самих себя, своего организма, состоящего из триллионов взаимосвязанных клеток. Сложность организма, обеспечивается, однако, не только наличием большого количества выполняющих разные функции клеток, но также их взаимодействием на уровне межклеточной среды, тканей и даже целых органов. В рамках проекта Атлас клеток человека (Human Cell Atlas) создан такой атлас и уже используется. Он включил данные, полученные сразу несколькими международными исследовательскими коллективами. Развитие современных технологий секвенирования РНК отдельных клеток (scRNA-seg) показало, что типы клеток человеческого организма очень многообразны, сейчас насчитываются сотни различных типов. В предлагаемой работе приводится характеристика транскриптома, в рамках которого осуществляется картирование клеток, его структура и динамичность. Транскриптом называют молекулу РНК, образующуюся в результате транскрипции (экспрессии соответствующего гена или участка ДНК). Примерами транскриптов являются: матричные РНК (мРНК). В статье приводится характеристика транскриптома, его структура и динамичность. Методы исследования транскриптов. Кодирующие и некодирующие РНК, их классификация, микро РНК, siРНК, нано-РНК, сборка транскриптов кратко рассматриваются в публикации. Цель публикации в первую очередь образовательная, познавательная, популяризация науки, а также стремление привлечь в ряды исследователей, в науку приток новых молодых умов, вызвать в таких умах стремление к поиску ответов на возникающие вопросы.  Масштабность темы требует ввести разумные ограничения на излагаемый материал после краткого панорамного ее рассмотрения. Общая схема, показывающая взаимосвязи генома, транскриптома, протеома и метаболома (липидома, гликома). После определенных успехов и наведения некоторого порядка в геноме, протеоме пришла пора взяться за транскриптом и метаболом. Огромный массив клеток человеческого организма оказался практически неосвещенным наукой, атласы клеток не существовали. Даже на вопрос о количестве типов клеток ответа не было. Основу клеточной (цитологической) диагностики составляет изучение клеток, их типов, изменений их распределения по органам, расположения, взаимодействия и строения. Конечной целью изучения генома конкретного организма является интеграция его генетических, цитогенетических и физических карт, а также их привязка к полной геномной последовательности. Также картирование генома возможно с помощью биоинформатических методов. Для этого сначала проводят секвенирование генома, полученные риды выравнивают, получают контиги и скаффолды, которые затем картируют на геном специальными программами картировщиками. Критерии цитологической диагностики включают анализ клеточного и неклеточного состава организмов: количество клеток, наличие клеток разного типа, их расположение в структурах или разрозненно, вид структур, размер, форма, строение клеток и ядер, наличие или отсутствие клеточного и ядерного полиморфизма и другие параметры. По характеру и степени выраженности отклонения от нормального клеточного состава судят о природе патологического процесса. По признакам, характерным для определенных тканей, судят о тканевой принадлежности опухоли. При этом учитывают фон препарата — элементы крови, бесструктурное вещество, коллоид, жир и др. Следующим шагом является изучение метаболома и разработка теории метаболомики. Каждая клетка человеческого мозга содержит одну и ту же последовательность ДНК, но в разных типах клеток разные гены копируются на нити РНК для использования в качестве белковых программ. Именно из-за разнообразия белков так много типов клеток в организме и в мозге, что и делает столь сложным наш мозг. Раскроем некоторые важные для восприятия контекста термины и понятия. Транскрипт — молекула РНК, образующаяся в результате транскрипции (экспрессии соответствующего гена или участка ДНК).Примерами транскриптов являются: мРНК, рРНК,тРНК,  малые РНК. Транскрипто́м (англ. transcriptome) — совокупность всех транскриптов, синтезируемых одной клеткой или группой клеток, включая мРНК и некодирующие РНК. Понятие «транскриптом» может обозначать полный набор транскриптов в данном организме или специфический набор транскриптов (молекул РНК), представленный в клетках определённого типа. Транскриптом может сильно меняться в зависимости от условий окружающей среды. Он включает в себя все транскрипты данной клетки, а также отражает профиль экспрессии генов в данный момент времени. Транскриптомика — это технология, в которой выполняется идентификация всех матричных РНК, кодирующих белки, определение количества каждой индивидуальной мРНК, определение закономерностей экспрессии всех генов, кодирующих белки. Пространственная транскриптомика — это технология, которая позволяет исследователям наблюдать (и соблюдать) закономерности экспрессии генов в тканях, сохраняя при этом их пространственный контекст. Одной из мощных платформ в этой области является 10x Genomics Visium в соединении с секвенированием Illumina. Ее основные положения иллюстрируются рисунками ниже Разработан метод пространственной транскриптомики, позволяющий анализировать индивидуальные клетки в срезах тканей. Метод включает в себя приготовление среза тканей, окрашивание, распознавание индивидуальных клеток при помощи искусственного интеллекта, автоматическую микродиссекцию клеток лазером, экстракцию белков и анализ образцов при помощи масс-спектрометрии, включает в себя каталогизацию всех РНК в отдельных клетках. Результаты анализа затем можно наложить на изображение среза ткани, таким образом выделив пространственные различия в транскриптомных профилях. Сплайсинг (от англ. splice — сращивать или склеивать концы чего-либо) — процесс вырезания определённых нуклеотидных последовательностей из молекул РНК и соединения последовательностей, сохраняющихся в «зрелой» молекуле, в ходе процессинга РНК. Секвени́рование РНК (англ. RNA sequencing, RNA-seq) — метод определения первичной структуры молекул РНК, представляющий собой высокочувствительный и точный инструмент для изучения транскриптома. Под этим может подразумеваться как секвенирование мРНК, так и определение последовательности некодирующих РНК. Современное полногеномное секвенирование основано на прямом секвенировании фрагментов кДНК. Экспрессия генов — процесс, в ходе которого наследственная информация от гена (последовательности нуклеотидов ДНК) преобразуется в функциональный продукт — РНК или белок. Некоторые этапы экспрессии генов могут регулироваться: это транскрипция, трансляция, сплайсинг РНК и стадия посттрансляционных модификаций белков. Процесс активации экспрессии генов короткими двухцепочечными РНК называется активацией РНК. Способами определения экспрессии генов в данное время являются секвенирование РНК, содержащих поли-А хвост (мРНК), а также применение экспрессионных ДНК-микрочипов. Секвенирование РНК становится всё более распространённым методом в связи с усовершенствованием методов секвенирования нового поколения. Секвенирование РНК не только позволяет определить уровень экспрессии каждого белоккодирующего гена в геноме, но и различать варианты мРНК, получающиеся в результате альтернативного сплайсинга. Транскрипто́мные техноло́гии (англ. transcriptomics technologies) — методы, разработанные для изучения транскриптома (то есть совокупности всех РНК-транскриптов) организма. В состав транскриптома входят все транскрипты, которые присутствовали в клетке на момент выделения РНК. Исследуя транскриптом, можно установить, какие клеточные процессы были активны в тот или иной момент времени. В транскриптомике есть два основополагающих метода: микрочипы, позволяющие выявить наличие и количество определённых транскриптов, и секвенирование РНК (РНК-Seq), в котором используются методы секвенирования нового поколения для получения последовательностей всех транскриптов. Тип клеток в мозге мыши определяется с помощью метилирования: на основе химических маркеров — «штрих-кодов» — расставленных вдоль ДНК, которые определяют, когда гены включаются или выключаются. Транскриптомные базы данных постоянно растут и становятся всё более полезными для исследователей. Это связано с тем, что правильная интерпретация данных, полученных в ходе транскриптомного эксперимента, практически невозможна без опоры на предшествующие исследования. 1970 Библиотеки РНК, кот. конвертированы в комплементарную ДНК (для бабочки) 1979 Коллекция библиотеки кДНК для мРНК шелкопряда1980 Получены транскрипты (метод обрыва цепи по Сенгеру) чтением пар оснований(п.о.)1990 Транскриптомика становится биологической наукой1991 Получена последовательность 609 мРНК из мозга человека1995 1-й транскриптомный метод, осн..на секвенировании, сериальный анализ экспрессии генов1997 Первое исследование, где описано 60633 транскрипта, экспрессируемых у S.serevisiae1998 Введен термин метаболом, придуманный для соответствия существующим терминам2003 Вышла из печати книга по метаболомике2005 Технология секвенирования с длиной прочтения 0,7 млрд п.о. (платформа 454 Life Sciences)2006 Технология секвенирования длина прочтения 50-300 п.о.900 млрд п.о. (платформа Illumina)2008 Получены 2человеческих транскриптома из миллионов последовательностей от 16 тыс генов2008 Технология секвенирования с длиной прочтения 50 п.о. 320 млрд п.о. (платформа SOLID)2010 Технология секвенирования с длиной прочтения 400 п.о. 30 млрд п.о. (платформа Ion Torrent)2011 Технология секвенирования с длиной прочтения 104 п.о.320 млрд п.о. (платформа Pac Bio)2015 Прост. процедурой получаются транскриптомы людей с разл. заболеваниями тканей (клеток)2016 Запуски РНК-Seg, размещенные в базе NCBI SRA 2017 Запущена Сеть переписи клеток (BICCN)– консорциум центров США и Европы2019 Секвенировали РНК из кожи, хрящей, печени и мышц щенка Тумата возрастом 14300 лет.2021 В Интернете опубликован Атлас метаболома мыши на разных этапах ее жизни. Основу цитологической диагностики организмов составляет изучение клеток, изменений в их расположении и строении. Критерии такой диагностики включают анализ клеточного и неклеточного состава: количество клеток, наличие клеток разного типа, их расположение в структурах или разрозненно, вид структур, размер, форма, строение клеток и ядер, наличие или отсутствие клеточного и ядерного полиморфизма и другие параметры. По характеру и степени выраженности отклонения от нормального клеточного состава судят о природе патологического процесса. По признакам, характерным для определенных тканей, судят о тканевой принадлежности опухоли. При этом учитывают фон препарата — элементы крови, бесструктурное вещество, коллоид, жир и др. Задача картирования типов клеток в тканях представляет интерес как с позиций фундаментальной науки, так и для медицины. Новейшие технологии пространственной транскриптомики помогают ее решить: клетки разного типа синтезируют различный набор РНК (транскриптом) и белков (протеом), что позволяет отличить их друг от друга. Принцип 10х Genomics Visium заключается в специальном чипе. Захваченные молекулы РНК из ткани затем помечаются современными молекулярными идентификаторами (UMI) во время процесса обратной транскрипции. Сочетание образцов с пространственным штрих-кодом и UMI обеспечивает точность и специфичность генерируемых данных. Используя этот запрос пространственной транскриптомики, исследователи могут получить более глубокое понимание пространственной организации клеток. и сложные молекулярные взаимодействия, происходящие внутри тканей, бесценную теорию о механизмах, конференции на основе биологических процессов в различных областях, включая онкологию, нейробиологию, биологию развития, иммунологию. и ботанические исследования. В образце ткани выделялись участки для анализа, по результатам которого определялся состав РНК в каждом. Предполагалось, что это позволит определять клетки какого типа размещены в том или ином участке. Как всегда, не удалось избежать сложностей. Во-первых, в ткани может быть множество, мало отличающихся по составу РНК, типов клеток, например, стромального происхождения, что существенно усложняет их идентификацию. Во-вторых, размер исследуемых участков ткани, обычно оказывается значительно больше среднего размера клеток, в результате чего в каждый попадает смесь РНК из различных типов, что затрудняет последующую обработку. Учитывая названные ограничения, нельзя обойтись при обработке только ручным трудом исследователей – необходимы масштабируемые вычислительные методы. Международная группа, в состав которой вошел представитель РФ, разработала инструмент cell2location, который выявляет пространственное распределение типов клеток на основе данных секвенирования отдельных клеток в пространственной транскриптомике. Система сравнивает количество РНК в пространственных данных с эталонными профилями экспрессии РНК для присутствующих в ткани типов клеток, определяя точное количество разных клеток в каждом из экспериментально изученных участков. Инструмент сell2location эффективно корректирует различные источники экспериментальных погрешностей, что позволяет интегрировать клеточную и пространственную транскриптомику с более высокой чувствительностью, чем существующие инструменты. Этот инструмент универсален и позволяет находить редкие типы клеток, которые невозможно обнаружить традиционными для гистологии методами. В рамках проекта ученые опубликовали 21 новую статью в трех журналах: Science, Science Advances и Science Translational Medicine. Проект Исследования мозга посредством развития инновационных нейротехнологий (Brain Research through Advancing Innovative Neurotechnologies (BICCN)).  Его запустили в 2017. В октябре 2021 года Сеть переписи клеток инициативы BRAIN Initiative (BICCN) завершила первую фазу долгосрочного проекта по созданию атласа всего мозга мыши (млекопитающего), включающего 17 исследований, включая атлас и перепись типов клеток в   первичной моторной коре. Ниже показаны проекты по генерированию данных, связанные с BICCN, и их методы профилирования конкретных типов ячеек. Заполненные ячейки указывают на продолжающееся и запланированное получение данных у мышей, приматов, не являющихся человеком, и человека. Более подробная информация по каждому проекту доступна на страницах команды портала. Роль BCDC заключается в обеспечении публичного доступа и организации сложных данных, инструментов и знаний, полученных BICCN. Портал BICCN служит отправной точкой для деятельности BICCN и обеспечивает доступ к наборам данных BICCN, хранящимся в архивах данных R24. По мере того, как будут раскрыты наши знания о взаимосвязи модальностей данных BICCN, будут представлены сводные знания о типах клеток, которые помогут улучшить наше понимание роли типов клеток в мозге. Все данные BICCN, соответствующие принятым стандартам контроля качества, немедленно доступны общественности через несколько архивов данных R24 и доступны через этот сайт. Архивы данных BICCN включают: Транскриптомные и      эпигеномные данные      доступны в Neuroscience Multi-Omic (NeMo) Archive ,      хранилище данных, ориентированном на хранение и распространение -омных      данных, полученных в рамках инициативы BRAIN и связанных с ней проектов      исследования мозга. Наборы данных      модальности изображений доступны через Библиотеку изображений мозга (BIL) ,      общедоступный ресурс, позволяющий исследователям хранить, анализировать,      извлекать, делиться и взаимодействовать с большими наборами данных      изображений мозга. Данные      нейрофизиологии доступны на сайте DANDI: Распределенные архивы для интеграции      нейрофизиологических данных. Данные      электронной микроскопии и рентгеновской микротомографии доступны      в Brain      Observatory and Storage Service (BossDB). и другие не инвазивные методы сканирования для отображения анатомии,физиологии, перфузии, функции и фенотипов   человеческого мозга. Данные собирались в архивах сетью переписи клеток (BICCN) и они позволяют исследователям заняться фундаментальными научными вопросами о человеческом мозге и его генетической организации. Основная задача BICCN– получение характеристик типов клеток и их функций в мозге человека, приматов (NHP) и грызунов. Используя самые передовые технологии, которые до сих пор в основном применялись только к животным изучается клеточный состав взрослого и развивающегося человеческого мозга на транскрипционном, эпигенетическом и функциональном уровнях. (v)            функциональный и анатомический анализ и моделирование типов нейрональных клеток человека, которое включает физиологическую и анатомическую характеристику клеточных свойств в живых тканях человека, а также моделирование типов клеток и специализированных клеточных свойств у людей по сравнению с моделями на грызунах. Благодаря более тонким техникам стало возможно увидеть то, какие гены активны в разных клетках мозга важнейшего органа человека и животных, то есть, развивая мысль о детализации исследования становится возможным понять и оценить разнообразие и сложные взаимосвязи клеток. Особенности любой ткани в организме определяются ее клетками, а их специфика тем, какие гены активны в их ДНК. Как все выглядит на самом деле до настоящего времени неизвестно. О том, как клетки общаются между собой мало что известно. Ученые использовали также другую методику, которая анализировала трехмерную структуру молекул ДНК в каждой клетке, чтобы получить дополнительную информацию о том, какие последовательности ДНК активно используются. Информация взята с портала «Научная Россия» (https://scientificrussia.ru/) В эксперименте на мышах изучались более 3 тысяч клеток головного мозга. Головной мозг мышей состоит примерно из 100 млн клеток. Каждая нервная клетка в диаметре примерно равна 20 мкм, а глиальная — 10 мкм. Для каждой клетки определялось какой из 20 тыс. генов был активен. Все клетки мозга мышей разделились на 47 видов: специализированные нейроны, клетки кровеносных сосудов и вспомогательные клетки нервной ткани, называемые глиальными. Они обеспечивают удаление отходов жизнедеятельности клеток, защиту от инфекций и поставку питательных веществ. Созданная карта мозга позволила ученым определить неизвестные до сих пор типы клеток, например, шесть различных типов олигодендроцитов — клеток, которые образуют электроизоляционную миелиновую оболочку вокруг нервных клеток. Подробная карта клеток мозга мышей, показывает, какие гены обеспечивают появление отдельных типов клеток. Это дает науке ключ для разработки методов эффективной диагностики и лечения многих нервных заболеваний, например, таких как рассеянный склероз Что происходит с наукой о человеке?  Человек как объект природы изучается многими науками и в некоторых из наук получены впечатляющие результаты. В целом же можно сказать, что очень мало наука знает о человеке. Наиболее интенсивно внедряется в область человекознания биология. Но объект так сложен, что даже принципы возникновения, эволюции, развития и функционирования, осознаваемого поведения до конца не ясны. Создание атласов и картирование клеток реально оказывают положительное влияние на ход исследований живых организмов и мозга, так как клетка лежит в основании жизни и возможно разума. Молино Б.Дж., Арлотта П., Менезес Дж.Р., Маклис Дж.Д., Спецификация подтипа нейронов в коре головного мозга. Нат. Преподобный Нейроски. 8 , 427–437 (2007). Клаусбергер Т., Сомоджи П., Разнообразие нейронов и временная динамика: единство операций гиппокампа. Наука 321 , 53–57 (2008). Сугино К., Хемпель С.М., Миллер М.Н., Хэттокс А.М., Шапиро П., Ву К., Хуанг З.Дж., Нельсон С.Б., Молекулярная таксономия основных классов нейронов в переднем мозге взрослой мыши. Нат. Неврология. 9 , 99–107 (2006).",
    "79": "С возрастом всё сложнее быть программистом «в теме». Раньше я любил двух- трёхдневные марафоны, когда надо было что-то понять и написать. Сон урывками, кофе литрами, но при этом ощущение полёта, какого-то такого программистского счастья. Статьи и книги запоем, пет-проекты, и всегда есть что-то интересное, что я ещё не пробовал, но очень хочу попробовать. Постепенно всё уходит в прошлое, и я уже не на острие прогресса. Сейчас во всём разбираются те, кто моложе и те, кто соображает быстрее, а мне осталось только догонять. Но я пытаюсь. Пару лет назад я спросил в одной из тусовок, где собираются люди с горящими глазами: какой дистрибутив Linux сейчас считается самым… скажем, хайповым. И мне ответили — NixOS. Я про такой не слышал. И, конечно, сразу его поставил, работал даже, но долго не понимал главной фишки. Впрочем, нельзя сказать, что в NixOS есть одна главная фишка. Я смотрю на систему, как программист, но у неё есть свои грани для и системных администраторов, и DevOps-инженеров. Если вы никогда не слышали про NixOS, то вот вам несколько вводных. Nix — это пакетный менеджер. Его можно установить в любой Linux или на macOS. NixOS — это дистрибутив Linux, построенный на базе Nix. Проблема, которую пытались решить с его помощью, известна всем программистам и их заказчикам. Она называется «на моей машине всё работает». У нас, у программистов, есть даже анекдот на эту тему. Если бы программисты были врачами, им бы пациенты говорили, например, «у меня болит нога», а они бы отвечали «ну не знаю, у меня такая же нога, и не болит». Проблему осознали и решают с 90-х годов. К решениям в той или иной степени относятся непрерывная интеграция (Continuous Integration), непрерывное развёртывание (Continuous Deployment), DevOps и контейнеризация. Nix тоже в этом ряду. Иногда можно услышать (ошибочное) мнение, что эти решения —  альтернативы, хотя, в действительности, они друг друга дополняют. Вы можете организовать DevOps с помощью Nix и Docker, и вот вам статья, где автор Джилл Торнхилл доходчиво объяснила, что Nix и Docker не только можно, но и нужно использовать вместе. Nix позволяет описать конфигурацию, нужную для работы программы и аккуратно воссоздать её на другой машине. Похоже на Docker, только вам не нужны контейнеры. Nix позволяет настраивать конфигурацию в широких пределах, в частности, у вас может быть несколько конфигураций. В одном проекте вы можете писать на PHP 8.3, а в соседнем — на PHP 8.1. То же самое касается версий .NET, Java, Python, mysql и postgres. В любой момент вы можете включить конфигурацию,  а потом точно также её выключить. Но и это ещё не всё. Nix не перезаписывает изменения конфигурации, а всегда создаёт новые, позволяя вам откатиться в любой момент. Приблизительно также git хранит изменения в проекте. Подобные устойчивые (persistence) структуры данных часто применяют в функциональном программировании. И это неспроста. Дело в том, что язык описания конфигурации Nix — чистый функциональный язык. Разработка Nix идёт полным ходом. За одиннадцать лет, с тех пор, как NixOS вышла на промышленные просторы, она приобрела полноценный реестр пакетов — nixpkgs. Сейчас их порядка 80000 и каждый из пакетов надо было подготовить. Nix или NixOS используют сотни компаний по всему миру, включая таких «монстров», как Atlassian и Yandex. Люди постоянно дорабатывают софт и иногда им хочется поработать вместе. Поэтому вот уже несколько лет разработчики NixOS прилетают на неделю в какую-нибудь точку земного шара и устраивают рабочую сессию. Эти сессии носят название спринты. Чем занимаются программисты во время спринта? Пишут новые пакеты, исправляют ошибки в старых, работают над компилятором, над утилитами. Работы хватает. Последний спринт прошёл в феврале в Таиланде. Теперь самое важное, ради чего, собственно, пост. Следующий спринт ребята хотят провести в России. А мы с @tazjin его организуем. От Москвы мы отказались сразу, не смотря на то, что живём здесь. Москва это дорого. Кроме того, Москва — это не вся Россия, а гостям хочется показать что-то чуть менее мировое, и чуть более местное. После некоторых раздумий мы остановились на Казани. Мультикультурный российский город, где у нас есть «знакомства и связи». Мы рассчитываем, что они упростят нам решение разных вопросов без частых поездок в Казань. Этот спринт мы, по традиции, назвали, опираясь на географию — Volga Sprint. Даты проведения — в 22 по 29 августа. Если у вас есть желание поработать над Nix и NixOS в компании увлечённых программистов со всего мира — следуйте по ссылке выше и регистрируйтесь! Если вдруг у вас по какой-то причине не получится, всегда можно отказаться. С деньгами история такая. Участие в спринте естественно бесплатное, но добираться до Казани и жить там неделю нужно будет самим. Кроме того, придётся на неделю отпроситься с работы. Аренду коворкинга, обеды и полдники, а также кофе и фрукты на местах традиционно помогаю оплатить спонсоры. Которых мы, кстати, тоже ищем! Спринт — мероприятие тёплое и ламповое. Съезжаются обычно 20–25 разработчиков, снимают жильё поблизости от коворкинга, работают целый день с перерывами на обед, а вечером развлекаются. Ну, или продолжают работать — вы же знаете этих программистов! Мы планируем культурную программу, гастрономическую программу, покатаемся по окрестностям, посмотрим на Волгу. План такой. Мы только начали подготовку и не столкнулись, наверное, ещё и с половиной  организационных сложностей. Но мы надеемся, что нам удастся их преодолеть.",
    "80": "Вы когда-нибудь оказывались в ситуации, когда голова была полна идей, но записать их нет возможности? Тогда вы знаете, как бывает сложно быстро и качественно зафиксировать свои мысли. А может вам знакома ситуация, когда собеседник записывает голосовое сообщение на 5 минут с описанием какого-нибудь проекта, и вам приходится переслушивать его снова и снова, чтобы понять все детали. Столкнувшись с этим, я решил сделать Telegram-бота, который может превратить голосовое сообщение в структурированную заметку. Для распознавания речи я решил использовать Yandex Speechkit, а для преобразования неформатированного текста в заметку использую ChatGPT. Так как API из России теперь использовать не разрешают, использую сайт ProxyAPI, однако цены оставляют желать лучшего. Если знаете более дешёвый аналог - буду рад, если поделитесь. Форматирование текста у gpt-4 явно более качественное, чем у gpt-3.5. Заметки у него получаются более понятные, и он качественнее передает исходную информацию. Также неплохие результаты выдает YandexGPT, но цены на него примерно такие же, как и на gpt-3.5. Немного о применении бота. С его помощью можно превратить поток мыслей, записанный в голосовое сообщение, во внятный текст, в котором ваши идеи будут расписаны по пунктам. Здесь же удобные голосовые заметки для студентов, которые автоматически превращаются в конспект, и быстрое фиксирование идей. Интерфейс бота максимально простой - записываем или пересылаем голосовое сообщение, и получаем структурированную, четкую заметку. Таким образом, пользователь может создавать заметки, не прибегая к набору текста на клавиатуре, что особенно удобно в ситуациях, когда руки заняты. Также есть режим \"текст\", в котором бот просто распознает текст и выводит его с правильным форматированием. Позже добавлю возможность преобразовать неформатированный текст из пересланного сообщения в заметку. Собственно бота можете найти здесь. Апдейты по разработке будут у меня в группе в ВК. Если есть идеи по улучшению бота, пишите в комментарии или в сообщения в группу, буду рад вашим идеям.",
    "81": "В прошлом году мы запустили сервис Хабр Эксперты — платформу для IT-менторов и менти, которая помогает специалистам найти наставника, вместе с ним развиваться и двигаться вверх по карьерной лестнице. Вот уже год платформа даёт возможность гуру IT делиться опытом с начинающими, а новичкам и тем, кто хочет лучше разбираться в каких-то вопросах, — учиться с наставником. Недавно мы провели исследование о том, насколько специалисты знакомы с менторством в IT. Результаты показали, что те, кто работал с ментором, в 68% случаев готовы повторить опыт, а интерес к этой практике растёт с уровнем специалиста. Со дня запуска сервиса услугами менторов воспользовалось больше тысячи специалистов. А для тех, кто пока не знает про Хабр Экспертов, мы подготовили этот материал. Менторство в IT — это инструмент наставничества, где опытные профессионалы помогают начинающим и развивающимся IT-специалистам.. Оно включает в себя обмен знаниями, опытом, предоставление советов и рекомендаций. Менторство помогает специалистам быстрее адаптироваться на бурно меняющемся рынке труда, качественнее осваивать новые знания и в конечном итоге становиться востребованнее для топовых компаний рынка. В Хабр Экспертах менторство охватывает множество навыков, специализаций и квалификаций. Помощь получит каждый: и новичок, мечтающий вкатиться в IT-индустрию, и уже сложившийся профи, который хочет сменить направление. Приходить к менторам можно не только за развитием навыков, но и прокачкой софт-скилов, оценкой портфолио и проведением тестовых собеседований. Главное — выбрать подходящего эксперта. Это не очень сложно: сегодня на Хабр Экспертах, более 7 тысяч наставников. построить карьеру за рубежом: составить резюме для иностранного рынка, рассказать про детали трудоустройства. Это общие направления работы с менторами, но помните: каждый ментор индивидуален. В карточках менторов и на их личных страницах вы найдёте конкретные навыки, запросы и темы, с которыми к ним можно обратиться. Вам останется только выбрать самого подходящего. Оставить заявку на консультацию может любой специалист на Хабр Карьере, и даже другой Хабр Эксперт. Если вы решили выбрать наставника, перейдите в раздел «Эксперты». Дальше вы можете найти ментора как по конкретному запросу в поисковой строке, так и при помощи фильтров в меню справа. Фильтры позволяют выбрать запрос, например, «Смена профессии» или «Начало карьеры», специализацию, квалификацию и интересующие навыки. Если вы нашли нужного эксперта, нажмите на кнопку «Связаться» в его карточке или профиле. После этого вы попадете в диалог с экспертом, где можете описать свой запрос, задать вопросы и договориться о встрече. Встреча может пройти очно или онлайн; самая распространённая практика консультаций — часовой созвон. Стоимость часовой консультации указана в карточке эксперта, но в фильтрах вы также можете отметить поиск менторов с бесплатной консультацией — таких сейчас почти 4 тысячи. А еще первая консультация у многих экспертов бесплатная: это позволяет ментору понять, подходит ли его компетенция запросу специалиста, а специалисту — подходит ли ему эксперт и его подход к обучению. Если вы консультируетесь платно, то об оплате вы договариваетесь с экспертом самостоятельно. Пожалуйста, учитывайте, что все риски при оплате вы берете на себя.",
    "82": "7 апреля 1964 года, шестьдесят лет назад, компания IBM анонсировала семейство компьютеров с архитектурой IBM System/360. Автор книги «От хорошего к великому» Джим Коллинз включил System/360 в тройку лучших бизнес-достижений всех времен, наряду с моделью T Форда и первым реактивным лайнером Boeing 707. IBM System/360 стала символом целой эпохи в истории информационных технологий. Именно для System/360 изобрели 8-битный байт. Также IBM/360 стала первой 32-разрядной компьютерной системой. В этой статье погрузимся в историю и узнаем, как IBM System/360 совершила технологический прорыв и превратила производство мэйнфреймов в крупномасштабный производственный процесс. Сейчас идея System/360 кажется простой и очевидной. Всего-то линейка компьютеров, на которых будет работать одно и то же программное обеспечение. Однако до 1964 года компьютерные системы, даже от одного производителя, были вообще несовместимы друг с другом. Программное обеспечение и периферийные устройства старых систем не могли работать с новыми системами. Проводить апгрейд было непросто. Чтобы просто обновить оборудование, приходилось переписывать приложения. «Представьте, что компания потратила целое состояние и два-три года на разработку банковского приложения, а потом появляется новое оборудование, на котором прежнее ПО не работает, и все приходится делать заново», — говорил Пат Тул, один из инженеров первой модели System/360. Да и для самой IBM ситуация была неблагоприятной. Нужно было выпускать устаревшее оборудование и содержать специалистов, умеющих его настраивать. Поддерживать множество несовместимых продуктовых линеек было затратно и по времени, и по ресурсам. К тому же в отрасли была конкуренция: Honeywell, Burroughs и Control Data Corp., Remington Rand представляли свои продукты. А заказчикам было, по сути, все равно у кого купить очередной «уникальный» компьютер, ведь в любом случае все нужно было настраивать заново. Поэтому, когда в январе 1961 года в IBM представили проект новой системы 8000, которая была очень «мощной», но все такой же несовместимой и дорогой, стало понятно, что компания в тупике. Проект реализовывать не стали, IBM взялась за разработку принципиально новых моделей. В 1961 году под руководством нового генерального директора Томаса Дж. Уотсона-младшего IBM создала рабочую группу из 25 человек под названием SPREAD («Системное программирование, научные исследования и разработки»). Целью стало создание новой системы. Бесконечные споры команды сильно тормозили процесс разработки идеи, поэтому руководство отправило членов SPREAD в отель в пригороде Нью-Йорка с ультиматумом, что никто не выйдет оттуда, пока команда не договорится. В итоге 80-страничный отчет был представлен к Рождеству. Идея команды заключалась в создании ряда масштабируемых систем, различающихся только вычислительной мощностью. Каждое устройство совместимо с другими. Программа, работающая на одном компьютере, может работать на всех системах, и все компьютеры используют стандартные интерфейсы. Рекомендации легли в основу System/360. Название «360» было выбрано, чтобы выразить идею полного охвата, буквально на 360 градусов. В ходе обсуждения стало понятно, что процессоры System/360 должны использовать новые технологии. А значит, новое семейство не будет совместимо с уже существующими системами IBM. Это было рискованно. Томас Уотсон долго сомневался. System 360 «была самым важным и рискованным решением, которое я когда-либо принимал, и я мучился по этому поводу неделями, но в глубине души я верил, что нет ничего, чего IBM не могла бы сделать», — писал Уотсон в книге «Отец, сын и компания: Моя жизнь в IBM и за ее пределами». В итоге Уотсон сделал ставку всей компании на разработку System/360. Главным архитектором System/360 стал Джин Амдал, создатель IBM 704, мэйнфрейма, используемого в научных исследованиях. Руководил проектом Фред Брукс. Именно он придумал термин «компьютерная архитектура». Брукс сотрудничал с Бобом Эвансом и Эрихом Блохом, за свою роль в разработке System/360 они получили первую в истории Национальную медаль технологий. Журнал Fortune в 1966 году назвал создание System/360 — авантюрой IBM в 5 миллиардов долларов. И действительно, многое пошло не по плану. Разработка системы заняла значительно больше времени, чем рассчитывали изначально, инженеры буквально жили в компании, работая по 100 часов в неделю. Стоимость System/360 значительно превысила запланированные $625 млн. На разработку системы компания потратила $5,25 млрд (в современных ценах это около $41 млрд). Создание семейства компьютеров обошлось дороже, чем Манхэттенский проект. Риск банкротства для IBM был реален, ведь компания потратила больше, чем зарабатывала за год, но все закончилось благополучно. Анонс системы состоялся 7 апреля 1964 года. Компания представила шесть моделей процессоров, охватывающих широкий диапазон производительности, а также 44 периферийных устройства.System/360 предназначалась для обслуживания всех возможных типов пользователей.Проектирование было отделено от сборки, поэтому системы можно было тиражировать. Совместимость компонентов сделала System/360 модульной, и хотя были анонсированы всего шесть систем, они имели 19 комбинаций мощности, скорости и объема памяти. В IBM стремились, чтобы аппаратное обеспечение было отделено от программного обеспечения, и программа могла работать на разных версиях System/360. Председатель IBM в своих мемуарах отмечал, что не все оборудование, представленное на выставке 7 апреля, было настоящим — некоторые устройства были просто деревянными макетами. Основная работа была впереди. Начался очень опасный, напряженный и сложный период становления производства. Компания наняла больше семидесяти тысяч новых сотрудников, строила заводы и принимала новые и новые заказы. Первые поставки машин низкого уровня были обещаны в третьем квартале 1965 года, а поставки более сложных машин — в первом квартале 1966 года. Компания IBM выпустила 14 моделей System/360, включая отдельные машины для научных вычислений. Первые модели 30, 40, 50, 60, 62 и 70 были анонсированы еще в 1964 году. Модели 30 и 40 были ориентированы на низкий и средний сегмент рынка. Они были самыми прибыльными, на них приходилось более половины проданных единиц System/360 Модели 60, 62 и 70 не были реализованы. Вместо них IBM выпустила модели 65 и 75. В 1966 году вышла модель 20, которая стала самым бюджетным вариантом System/360. Она имела всего 4096 байт основной памяти и восемь 16-битных регистров. В этот же период IBM выпустила специально оптимизированную для научных вычислений модель 44. Изначально для того чтобы все модели использовали один набор команд, в System/360 применили идею «микропрограммирования». Команды разбивались на ряд «микроопераций», специфичных для данной реализации системы. В модели 75 и более крупных системах отказались от использования микрокода. Это сделали для обеспечения более высокой скорости. Среди машин высокого уровня также были модели 67, 85, 91, 95 и 195. Модель 67 стала первым продуктом IBM, который поддерживал аппаратное обеспечение виртуальной памяти. Модель 85 была промежуточной между System/360 и более поздней System/370. Solid Logic Technology (SLT). Метод, используемый IBM для гибридной упаковки электронных схем. Технология позволила компании создавать более быстрые и компактные машины, чем у конкурентов. 8-битные байтовые адреса. В System/360 был стандартизирован байт как равный восьми битам, и использовалась длина слова в 32 бита, что помогло упростить архитектуру. 9-дорожечная лента. Магнитная лента с восемью дорожками для данных и одной дорожкой для бита четности была разработана специально под System/360. Набор символов EBCDIC. Расширенный двоично-десятичный код также был разработан IBM в этот период. Архитектура IBM с плавающей запятой. Числа с плавающей запятой стали поддерживаться в типичных операциях, связанных со сложением, вычитанием, умножением, извлечением квадратного корня и т. д. Создание программного обеспечения стоило IBM многих хлопот. Стоимость проекта OS/360 предварительно оценивалась в 30 и 40 миллионов долларов. В итоге на ПО потратили 500 миллионов долларов. Перед разработчиками стояли сложные задачи: обеспечить совместимость системы, сделать возможным одновременный запуск двух или более программ. IBM привлекла к работе над операционной системой 1000 человек. Было создано более миллиона строк кода. Но увеличение числа программистов не принесло ожидаемых результатов. Свой опыт работы над ОS/360, Брукс отразил в книге «Мифический человеко-месяц». Один из принципов, описанных в книге, гласит: увеличение ресурсов на проекте не всегда приводит к уменьшению его сроков. Брукс утверждает, что разработка компилятора Алгол будет занимать полгода, независимо от количества программистов, задействованных в процессе. Все пошло не по плану. Одну пакетно-ориентированную операционную систему создать не удалось. Не было времени на разработку, аппаратное обеспечение было готово, а OS/360 запаздывала. Было решено выпустить операционную систему в упрощенном виде с обещанием будущих обновлений. OS/360 PCP (Principal Control Program) была самой простой и могла запускать только одну программу одновременно. Ее использовали внутри компании IBM. OS/360 MFT (Multiprogramming with a Fixed number of Tasks) Множественное программирование с фиксированным количеством задач. Система могла запускать несколько программ, но ограничение заключалось в том, что когда одна программа простаивает, выделенная для нее память остается недоступной для других программ. OS/MVT (Multiprogramming with a Variable number of Tasks) Множественное программирование с переменным количеством задач. Система позволяла воссоздавать разделы памяти по мере необходимости. Когда память простаивает, она выделяется другой программе в очереди поиска. В 2000 году OS/360 стала общественным достоянием и теперь бесплатно доступна пользователям и разработчикам для загрузки и усовершенствований. Риск IBM окупился сторицей: всего за первые три месяца после выпуска IBM получила заказов на 1,2 миллиарда долларов. Уже к концу 1960-х годов System/360 стала отраслевым стандартом. Она использовалась в корпорациях, правительственных учреждениях, университетах и исследовательских центрах по всему миру. Доминирование IBM было настолько сильным, что в 1969 году Министерство юстиции США начало антимонопольное разбирательство, утверждая, что компания незаконно монополизировала рынок мэйнфреймов. Система долго оставалась успешной, и у неё появился целый рынок «клонов». В 1960-х и 1970-х годах многие компании производили клоны IBM System/360. Некоторые компании, такие как Memorex, выпускали аппаратные средства-клоны System/360. Например, накопители на магнитных дисках. Даже Джин Амдал, системный архитектор System/360, основал собственную компанию, производящую клоны IBM. В целом System/360 оказала огромное влияние на компьютерную индустрию, установив отраслевой стандарт и закрепив за IBM господство на рынке на два десятилетия. Успех системы привел к росту спроса на мэйнфреймы, поспособствовал технологическим инновациям. Появилась возможность перестать писать ПО для разных компьютеров, и разработчики смогли сосредоточиться на новых приложениях. Для IT открылся совершенно новый мир, мир мегапроектов. -15% на заказ любого VDS (кроме тарифа Прогрев) — HABRFIRSTVDS.",
    "83": "Использование ML в онлайн-шоппинге не ограничивается рекомендациями товаров. Покупать одежду и обувь проще, когда у любого бренда ты знаешь нужный размер, видишь удачное сочетания товаров и легко находишь похожие внешне или по цвету вещи. В Lamoda Tech мы создаем продукты, которые решают самые разные задачи пользователей и бизнеса. На митапе 28 марта мы рассказали, какие ML-модели работают у нас в проде и как мы строили эту работу. Делимся с вами видео выступлений и презентациями. Мы в Lamoda Tech уверены, что онлайн-шоппинг должен быть удобным и вдохновляющим. На нашей платформе работают десятки data-продуктов, которые обеспечивают такой опыт для пользователей: они ранжируют каталог и предлагают персональные рекомендации, собирают образы и управляют ценообразованием на платформе. В выступлении Саша рассказывает, с какими сложностями сталкивается команда при работе с разными задачами и что важно учитывать для успешного внедрения ML в Fashion E-commerce. Онлайн-покупка обуви и одежды кажется простой задачей ровно до тех пор, пока вы не сталкиваетесь с выбором размера. Как выбрать подходящий? Что делать, если у товара незнакомая размерная сетка? А вдруг товар большемерит или маломерит? В Lamoda на помощь в выборе пришли ML-модели. С чего начинались первые тесты и как эволюционировала рекомендация размера — узнаете в докладе Никиты. Поисковые алгоритмы легко справляются с простыми запросами на «спортивные брюки» или «белые кроссовки». Но фантазия и желания пользователей всегда шире, чем мы можем представить. Как найти «изумрудное платье» или «нарядный костюм»? Илья рассказывает о нейросетевом подходе к поиску и дополнении языковых моделей компьютерным зрением. Подробнее о том, как это решение получило развитие в генерацию образов, смотрите в выступлении. После докладов мы успели не только пообщаться за пиццей, но и поиграть в настольный теннис. Ищите себя на фотографиях, если были с нами в офисе! Если вам тоже интересна тема ML в fashion e-commerce, подписывайтесь на канал Lamoda Tech в телеграме. Анонсы новых митапов и все наши выступления можно найти именно там!",
    "84": "Каждый человек хоть раз в жизни, да получал какую-нибудь травму. Это не зависит от нас, часто это просто воля случая. И главное, в этом самом случае, принять правильное решение по тактике лечения, чтобы эта травма не доставила неприятностей в будущем. Сразу оговорюсь, что речь здесь пойдет не о серьезных повреждениях с нарушением целостности структур, таких как переломы или разрывы связок, а о более легких и часто случающихся травмах. Иммобилизацию поврежденной структуры: не кантовать, не трогать, не нагружать пока не заживет. Реабилитация, массаж, упражнения, физиотерапия - все потом и при необходимости. Это решение в корне неверное. Из-за подобных рекомендаций мы получаем «бонусы» в виде постоянных болей, хронического воспаления, а иногда и потери функции. Поэтому, если брать во внимание молекулярные механизмы иммунного ответа при травме, то нужно действовать с точностью до наоборот. Начинать воздействие на травмированную структуру как можно раньше. А вот почему? Давайте разбираться. Самые неприятные последствия любой травмы – это воспаление и боль. Воспаление – это естественный процесс, защитная реакция организма в ответ на повреждение. После получения травмы нарушается целостность тканей, что приводит к активации иммунной системы. Из поврежденных мембран клеток выделяется арахидоновая кислота. Это сопровождается выбросом медиаторов воспаления, таких как гистамин, серотонин, простагландины, лейкотриены. Иммунные клетки, которые находятся в зоне травмы, начинают секретировать сигнальные молекулы: провоспалительные цитокины, которые приводят к увеличению проницаемости сосудов (Il-1, Il-6, TNF) и хемоаттрактанты (Il-8, Il-17, Il-21), которые привлекают специальные клетки - нейтрофилы. Нейтрофилы - самая многочисленная фракция лейкоцитов, которые реагируют как на инфекцию, так и на токсины, появляющиеся в организме после повреждения. Они циркулируют в кровяном русле и готовы быстро прибыть к месту «аварии». Их основная задача – не пропустить врага вглубь организма и любой ценой удержать линию обороны до прибытия подмоги. Поэтому нейтрофилы «работают» как могут, порой не очень мягкими методами, начиная с фагоцитоза и до использования активных форм кислорода. Само по себе присутствие нейтрофилов во внеклеточном пространстве не наносит вреда, скорее наоборот, а серьезный урон организму наносит как раз кислородный взрыв. В результате этого процесса вырабатывается большое количество радикалов, которые не только уничтожают патогенов, а еще и повреждают все, что находится рядом, то есть здоровые ткани. Это называется вторичным гипоксическим повреждением, и это самые неприятные последствия травматизации. Одна группа ученых провела эксперимент. Двум кроликам, контрольному и нейтропеническому (с дефицитом нейторофилов) специально повреждали большеберцовые мышцы. В результате клеточное повреждение у животного с дефицитом нейтрофилов было значительно меньше. Чем больше тканей повреждается в результате кислородного взрыва, тем больше арахидоновой кислоты выделяется в окружающую среду. А по мере увеличения концентрации арахидоновой кислоты, нейтрофилы будут усиливать силу кислородного взрыва. Вот такой порочный круг. Вот таким образом увеличивается первичное повреждение тканей. Поэтому при травме часто назначаются НПВС - нестероидные противовоспалительные средства: диклофенак или ибупрофен которые снижают последствия высвобождения арахидоновой кислоты, и таким образом, уменьшают хемотаксис нейтрофилов. Конечно же нет. В природе ничего не бывает просто так, и блокировка какого-либо процесса всех проблем не решает. Активированные нейтрофилы секретируют интерферон-гамма, который является главным хемоаттрактантом для макрофагов. Макрофаги - крупные иммунные клетки, способные поглощать и уничтожать не только патогенов, но и мертвые или поврежденные клетки организма. В отличие от нейтрофилов, это уже профессиональные «чистильщики». Их задача не убить, а скорее, правильно и чисто утилизировать. Макрофаги подразделяются на два фенотипа - М1 и М2, которые отличаются по функциям и реакциям на различные стимулы. М1 макрофаги - провоспалительные. Они способны уничтожать бактерии и вирусы, продуцировать воспалительные цитокины, стимулировать активацию других клеток иммунной системы. Их основная роль заключается в фагоцитозе некротических тканей. М2 макрофаги, напротив, характеризуются как противовоспалительные. Способствуют ремоделированию тканей, заживлению ран, уменьшению воспаления и регуляции иммунного ответа. Играют важную роль в поддержании гомеостаза. М1 и М2 макрофаги происходят из одной клетки - моноцита, который циркулирует в крови. Когда моноцит попадает в ткани, в зависимости от окружающей среды и сигналов, которые получает, он становится или М1 или М2. Когда воспалительный процесс идет полным ходом, макрофаги активизируются в М1 и вступают «в бой», поглощая патогенов и некротизированные клетки. После того, как «битва» завершилась, а воспаление уменьшилось, макрофаги переключаются на режим М2 и начинают «лечить раненых» - выделять специфические факторы роста, которые улучшают регенерацию тканей. Макрофаги М1 попадают в очаг из сосудистого русла примерно через 24 часа после травмы и резко уменьшаются в количестве примерно через 48 часов, когда их заменяют макрофаги М2. Макрофаги М2 не активируются до тех пор, пока не прекратится процесс фагоцитоза, и все некротические клетки не будут подъедены. Нейтрофилы и макрофаги работают сообща. В доказательство этому провели еще один эксперимент. В мышцу нейтропенического кролика ввели змеиный яд. В результате, в отсутствие нейтрофилов, ее восстановление шло гораздо медленнее, чем мышцы с таким же введенным змеиным ядом у контрольного кролика. Такое ослабление регенерации связано с изменением функции макрофагов, что привело к задержке их рекрутирования в поврежденный участок. Блокировка метаболитов арахидоновой кислоты НПВС конечно уменьшает воспаление, но при этом нарушает хемотаксис нейтрофилов. Это замедляет процессы регенерации и восстановления. Поэтому важно уменьшать кислородный взрыв, при этом не ослабляя хемотаксис нейтрофилов и не препятствуя рекрутингу макрофагов. Такая вот непростая задача. Еще одним неприятным последствием травмы является боль. Причем не острая и относительно кратковременная боль, возникающая в результате повреждения, а боль хроническая, которая еще долго после травмы будет давать о себе знать. Молекулярные механизмы боли связаны с повышенной активностью ноцицепторов. Эти рецепторы определяют реакцию на травму и передают сигнал в головной и спинной мозг, где происходит обработка. Длительная активация ноцицепторов приводит к нейропластическим изменениям в периферической и центральной нервной системе. Именно это влечет за собой развитие хронических болевых синдромов. При травме ноцирецепторы постоянно стимулируются провоспалительными  цитокинам и  метаболитами арахидоновой кислоты. В такой среде происходит изменение порога деполяризации нервных окончаний. А если порог деполяризации снижается, то рецепторы становятся более чувствительными к раздражителям и реагируют даже на слабые болевые стимулы. Происходит сенсибилизация афферентных волокон. Говоря простым языком, увеличение чувствительности к боли и усиление ее восприятия. Кроме этого, во время травмы, миоциты выделяют фактор роста нервов (NGF). Гиперпродукция NGF после сенсибилизации афферентных волокон приводит к росту новых нервных ветвей на периферии и к коллатеральному прорастанию чувствительных нервов в дорсальном роге спинного мозга. Увеличение плотности афферентных волокон всего через неделю после травмы может привести к негативным пластическим изменениям и хронизации боли. Снижение потенциала сенсибилизации афферентных нервов. Привлечение нейтрофилов с ограничением кислородного взрыва. Содействие ранней смене фенотипа макрофагов на М2. Как можно достичь этих целей? Использовать потенциал механотрансдукции. Механотрансдукция – процесс, при котором механическое воздействие на ткань преобразуется в электрические или в химические сигналы, которые передаются через механорецепторы или через активируемые растяжением ионные каналы. Электрические сигналы передаются в ЦНС. Многие, сами того не зная, используют потенциал механотрансдукции для облегчения болевых ощущений.  Легкие растирания при ушибах - известный с детства прием.  Точно так же, благодаря более длительному и более направленному механическому воздействию на структуру, снижается потенциал сенсибилизации афферентных нервов. Это приводит к уменьшению плотности нервных волокон и, таким образом, предотвращается хронизация боли. Химические сигналы предаются внутрь клетки. Они активируют сигнальные пути, которые приводят к изменению экспрессии белков, необходимых для адаптации к условиям окружающей среды. Про нагрузки, запускающие сигнальные каскады, тоже известно многим. Рост мышц после тренировки - результат усиления интенсивности синтеза мышечных белков.  По такому же принципу, механическое воздействие сразу после травмы, будет ограничивать вторичное гипоксическое повреждение за счет снижения экспрессии белков, приводящих к кислородному взрыву. Это будет адаптацией к той среде, которая формируется благодаря манипуляции с поврежденными тканями. В эксперименте с кроликами применение одноосевого пассивного растяжения на поврежденной мышце животного, уменьшило количество клеточной инфильтрации и некроза тканей по сравнению с мышцей без воздействия. Такая мышца восстановилась быстрее и гистологически больше походила на здоровую ткань. Правильно подобранная физическая манипуляция с поврежденной структурой изменяет уровень трех физиологических факторов: снижается потенциал сенсибилизации афферентных нервов; уменьшает силу кислородного взрыва; сохраняет рекрутинг нейтрофилов, что влияет на количество М1 и М2 макрофагов и регенерацию тканей. Существует определенное сочетание силы, техники и времени, которое создает условия для быстрого восстановления поврежденной структуры. От силы применяемого механического стимула зависит количество М1 и М2 макрофагов. Здесь важно не переусердствовать, а работать в ритме тканей. Низкая или умеренная нагрузка способствует ранней активации макрофагов М2 и увеличивает их количество, что предполагает более благоприятные условия для восстановления и регенерации. От техники применения манипуляции будет зависеть весь основной успех мероприятия, поскольку при различных движениях будут активироваться различные клеточные сигнальные пути. Здесь важно выбрать правильный вектор. Отрицательные деформации при осевом сжатии приводят к компенсаторным положительным деформациям. Говоря простым языком, введение в компрессию структуры с одной стороны вызывает удлинение с другой. Это усиливает кровообращение и лимфоток в зоне травмы. А значит улучшится питание и вывод метаболитов, снизится отек.  А если нагрузка будет приложена под углом, то включатся другие механизмы. Из-за бокового движения жидкости, против плоскости ориентации мышечных волокон, мышца становится более жесткой и отек, наоборот, увеличится. Поэтому манипуляции должны быть продуманы, а лучше всего, проведены профессионалами. Многие массажисты и уж точно все остеопаты про это знают. Есть много техник, специально предназначенных для отработки травм. От времени начала воздействия будет зависеть величина вторичного гипоксического поражения. Чем раньше отработана травма, тем меньше тканей будет повреждено, со всеми вытекающими из этого последствиями. Это существенно снизит вероятность получить побочные эффекты в виде хронических болей и потери функции. Не тяните, сразу обращайтесь за помощью. Это намного эффективнее, чем ждать последующей реабилитации и долгого-долгого восстановления тканей. P.S. Для связи в случае необходимости, контакты в первом комментарии.",
    "85": "Все мы работаем в разных предметных областях, и бывает сложно уделить время знакомству с BI. Надеюсь, у Вас есть менее получаса на чтение этой статьи и знакомство с примером, а также есть желание провести графический BI анализ на .NET, в таком случае - добро пожаловать. В этой статье мы создадим .NET приложение для визуализации исторических реальных BI данных компании IBM о стоимости акций на нью-йоркской бирже за последние дни, код примера. С учетом опыта над зарубежными (MercerInsight) и отечественными (Visiology) BI продуктами, а также над оригинальными BI системами для крупных отечественных IT компаний, у меня, честно говоря, сложилось впечатление, что популярным решением для визуализации и чуть ли не стандартом де-факто являются HighCharts. Безусловно, есть альтернативы (даже условно CrystalReports, DevExpress и т.д.), в этой статье будут использованы именно HighCharts, мы увидим их особенности и преимущества. Также для простоты будет просто  JS, но в реальных проектах используются HighCharts в связке с одним из TypeScript фронтендным фреймворком. Создадим новый MVC .NET проект (например, .NET 8) из .NET CLI и добавим dev HTTPS сертификаты: В качестве источника реальных BI данных зачастую используются OLAP хранилища, например, с инфо-кубами. В этой статье мы используем готовые исторические финансовые данные из Alpha Vantage, хотя есть и много аналогов (например, Yahoo Finance и множество других). Построение OLAP хранилища, безусловно, заслуживает отдельного обсуждения вне рамок этой статьи, в то же время Alpha Vantage предлагает бесплатный доступ к данным, за это, надеюсь, ей можно простить какие-то недостатки, которые мы можем заметить. И также заменить Views/Home/Index.cshtml на: https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=IBM&outputsize=full&apikey=demo и заполняются параметры графика в объекте Highcharts из добавленного нами пакета Highsoft.Highcharts (а именно, ID для HTML DOM объекта графика, заголовок Title и подзаголовок Subtitle, настройки осей XAxis и YAxis, всплывающая подсказка Tooltip, настройки отображения столбцов PlotOptions и, собственно, сами данные Series). Во View для HomeController мы импортируем необходимые highcharts.js и exporting.js, и рендерим график через HighchartsRenderer. Как мы видим, всего за несколько минут можно получить BI .NET приложение с реальными данными и их визуализацией. Такие результаты могут быть использованы для дальнейшего анализа и визуализации данных (например, регресионные модели или другие статистические подходы), приятным бонусом выглядит возможность удобного экспорта графика в разные форматы, интерактивность графика и тултип. Также Highcharts не боится больших объемов данных, кроссбраузерный, и содержит множество типов графиков, которые достаточно глубоко и успешно кастомизуются Спасибо за внимание, надеюсь, статья поможет начать путь в BI. Желаю дальнейших успехов! Код примера доступен по ссылке.",
    "86": "Айтишный труд опасен: можно отравиться соевым латте, словить инфаркт от правок и подраться на корпоративе. Можем ли мы рассчитывать на помощь коллег в случае ЧП? Пока вы врубаете VPN, чтобы спросить совета у ChatGPT, ваш коллега отправится на тот свет. В этой статье разобрали несчастные случаи, которые могут настигнуть вас в офисе. Рассказываем, что нужно сделать в таких ситуациях (и что делать ни в коем случае нельзя). Кстати, ВОЗ говорит, что на рабочем месте ежегодно умирает до 600 000 человек. Сам по себе обморок не опасен. Первое, что нужно сделать — убедиться, что коллега не повредил голову и конечности при падении. Второе — предотвратить западание языка (иначе коллега может задохнуться). Положите человека на бок, чтобы язык не перекрыл дыхательные пути. Откройте окно, чтобы обеспечить приток свежего воздуха. Для облегчения дыхания расстегните верхние пуговицы рубашки/снимите галстук/ослабьте ремень. После того, как коллега очнется, предложите любой напиток, содержащий сахар. Сердечно-сосудистые заболевания — самая частая причина смерти. Главное, что надо уметь — быстро их распознать и отличить от обморока или нервного перенапряжения. Тяжелая боль в груди, часто отдает в левое плечо, шею, челюсть или руку + одышка, потливость, тошнота. Частота пульса обычно повышена. Позывы к рвоте, нарушение речи, неустойчивая походка и нарушение функции глотания + онемение конечностей. Частота пульса обычно не повышена. Есть прекрасный алгоритм определения инсульта, называется \"удар\":У - улыбка: попросите человека улыбнуться, оскалить зубы;Д - движение: попросите поднять и удерживать руки перед собой;А - артикуляция: попросите назвать себя, имя и фамилию;Р - решение: если улыбка кривая, перекосило лицо, если не поднимается или ослабела рука, если человек не может назвать свое имя, нарушена речь, значит, случился инсульт. Алгоритм оказания первой помощи одинаковый: немедленно вызвать скорую. Коллега должен принять полусидячее положение. Под голову, плечи и колени положите подушку/сумку/свернутую одежду. Если это инфаркт, дайте человеку разжевать одну таблетку аспирина + таблетку нитроглицерина (положить под язык, не глотать). Если инсульт, не давайте лекарств и ни в коем случае не предлагайте еду или питье. Важный момент: по закону об оказании первой медицинской помощи вы не имеете права «вскармливать» человеку таблетки даже в самых экстренных ситуациях (и даже если у вас мед. образование). Так что лучше, если пострадавший сделает это сам. Если это артериальное кровотечение (повреждена артерия), то из раны будет пульсирующей струей выливаться кровь алого цвета — это самое опасное кровотечение. Ваша задача — быстро наложить жгут. Обязательно засеките время — не больше 60 минут, если вы в теплом помещении. Если на улице зимой — не больше 30. За это время как раз приедет скорая. Обычно в офисных аптеках есть медицинские жгуты. Если нет, используйте полоску ткани, шарф, капроновые чулки. Поднимите пострадавшую конечность выше уровня сердца, чтобы уменьшить кровоток. Поместите жгут выше раны и сделайте несколько витков до полной остановки кровотечения. Витки должны ложиться вплотную один к другому, чтобы между ними не попадали складки одежды. Рекомендуем положить бумажку с временем наложения жгута, чтобы в скорой ничего не перепутали. Если у коллеги венозное кровотечение, то из раны будет медленной струей вытекать темная кровь. В таком случае приподнимите конечность, наложите давящую повязку и согните конечности в суставе (например, так делает врач после того, как взял кровь из вены). Ваша задача — безопасно довезти коллегу до травмпункта или донести до скорой. Если вам показалось, что рука вывихнута, даже не пытайтесь вправить ее обратно, как это делают крутые ковбои в кино. Не двигайте пострадавшую конечность. Постарайтесь зафиксировать её подручными средствами — палкой/доской/шваброй. Наложите холод и держите до приезда скорой. Если это открытый перелом с кровотечением, наложите жгут. Если у коллеги случилась аллергия (зуд, супь, неприятные ощущения в горле), немедленно дайте антигистаминный препарат (Циметидин, Фамотидин, Кестин), а через 1 час после него – активированный уголь, Фильтрум или другой сорбент. Вызвать скорую. Дать ингалятор: Сальбутамол, Вентолин и другие (обычно есть в офисной аптечке). Вдыхать каждые 20-25 минут, но не больше 3 раз подряд. Обеспечить комфортное ожидание врачей: открыть окно в офисе, ослабить ремень и воротник, следить за давлением и пульсом. Если ожог небольшой (покраснение, пузыри на коже), можно ограничиться офисной помощью. Если начался некроз тканей (чернеет кожа), ожог добрался до мышц, срочно вызываем скорую. Как можно скорей снимите все украшения с поврежденной конечности (кольца, браслеты), далее уберите одежду. Держать пострадавшую конечность под струей холодной воды не менее 10 минут. Накройте ожог стерильной повязкой (бинт, марля, подойдет даже полиэтиленовый пакет). Не наносите на бинт медицинские средства, вы можете навредить. Не надо мазать рану йодом, спиртом, перекисью. Предложите коллеге воды и обезбол. Возможно еще ваша бабушка рассказывала вам о секретном народном средстве. Речь о смазывании ожога сливочным маслом. Такой метод раньше был популярен — его рекомендовал прусский хирург Фридрих фон Эсмарх (один из пионеров асептики и антисептики, изобрел кровоостанавливающий жгут, эластичный бинт, кружку Эсмарха, ножницы Эсмарха). В реальности это никак не поможет пострадавшему Один из самых опасных и недооцененных несчастных случаев. Если коллега сам прокашлялся, то все хорошо. Но если еда застряла и мешает дышать, поможет прием Геймлиха — он простой, не требует спец. подготовки. Если ваш весит 100+ кг, у вас не хватит сил его \"продавить\". Тогда применяйте метод Геймлиха в позиции лежа. Положите коллегу на спину. Сядьте верхом на бедра пострадавшего, лицом к голове. Положив одну руку на другую, поместите основание ладони нижней руки между пупком и реберными дугами. Используя вес своего тела, энергично надавите на живот пострадавшего в направлении вверх к диафрагме. Голова пострадавшего не должна быть повернута в сторону. Повторите несколько раз. Важно: не надо стучать или хлопать коллегу по спине — предмет, которым он поперхнулся, от этого может пройти по дыхательным путям ниже. Кстати, мы ведем телеграм-канал про цифровой бизнес: внутри актуальные новости из мира IT, результаты наших собственных исследований рынка, а еще отличные вакансии. Подписывайтесь, у нас интересно! Во всех статьях про первую помощь пишут: сделайте искусственное дыхание/ непрямой массаж сердца. Забудьте об этом и не вспоминайте! Эти техники сложны в реализации, требуют практического опыта, медицинских знаний и физической подготовки (иногда воздействовать на грудную клетку надо больше 30 минут). В лучшем случае вы не добьетесь результата. В худшем — навредите (сломаете ребра в состоянии шока, перекроете дыхание). Ваша задача — быстро вызвать скорую. Блог компании hitch. Аутстаф близко!",
    "87": "Привет! Меня зовут Максим, я руководитель мобильной разработки в KTS. Недавно я попросил рассказать об используемых технологиях бывшего коллегу Сеню Суздальницкого, CTO Sizl — стартапа доставки еды в Чикаго. Я приехал в Москву после учёбы в Красноярске. Сначала работал в «Айтеко» и параллельно учил Android. Однажды прошёл интенсивный курс по Kotlin и понял, как много он привносит в андроидную жизнь. Вскоре приятель позвал меня в KTS на должность Android-разработчика. Во время работы в компании я занимался приложением для технадзора строительной компании ПИК и несколькими другими небольшими проектами. «Рокет-банк» был стартапом, основатели которого потом сделали «Кухню на районе». Когда я начал там работать, «Рокет» уже выкупили QIWI, и в целом это была уже совсем другая компания с другой командой. Хотя общая атмосфера сохранила молодёжный неформальный вайб кальянов и неоновых ламп. Я работал в команде из 2 Android-разработчиков, 3 iOS-разработчиков и 5 разработчиков React Native. React Native интегрировался с API на стороне iOS и Android, но не обеспечивал полную самодостаточность. Например, сложные перфомансные задачи вроде анимаций или отображения видео требовали нативной разработки. По моему опыту, React Native — скорее инструмент унификации, чем супер-буст в скорости разработки. Скорость все равно зависит от личной эффективности каждого разработчика. К тому же из-за наличия трёх разных команд требовались постоянные проверки совместимости и взаимодействия между iOS, Android и React Native. Было ощущение, что с двумя нативными мобильными командами вместо трёх скорость была бы примерно такой же, но тогда потерялась бы унификация между платформами. Получается, что вся польза от React Native — в унификации интерфейса и бизнес-логики, обеспечивая согласованность между платформами. Когда я только устроился в «Кухню», там ещё была оригинальная команда «Рокет-банка». Я написал CTO и спросил, не ищут ли они мобильного разработчика в команду. Оказалось, что место есть. У «Кухни» был backend на Ruby и нативные приложения для Android и iOS. Первое время ребята, как и в «Рокете», продолжали использовать React Native, потому что им казалось, что это классный инструмент. Но к моему приходу от него уже отказались из-за проблем с производительностью и необходимости поддерживать три разных команды. Когда меня взяли на работу, то дали одну общую задачу — сделать работу склада лучше. Это было большое здание, внутри которого находилось производство и разные помещения хранения: холодные склады, склад упаковки, морозильные камеры. Работники склада и курьеры пользовались одним софтом и разделялись по типу учётной записи. У курьеров была мобильная версия, а на складе работали с веб-сервисом. Мне показали склад и познакомили с разработчиком, который писал бэкенд. После этого я сам решал, как именно улучшать складскую работу. Сначала я сделал мобильную версию внутреннего сервиса для работников склада. Пользоваться веб-софтом на складе было неудобно. Сотрудники постоянно перемещались и сканировали бар-коды на физических товарах. А веб-приложение стояло на ПК в одном месте. Получалось, что сотрудник брал в руки Bluetooth-сканер и шёл сканировать коробки и ящики. Но чтобы понять, что именно он отсканировал, надо возвращаться к ПК и смотреть, что показывает веб-приложение. Когда есть мобильное приложение, то в одной руке у вас сканер, а в другой телефон. Сотрудник, как быстрый ковбой на Диком Западе, сканирует ящик и сразу видит: это замороженная курица, которая поступила на склад 3 дня назад в объеме 20 кг. Теперь работники склада могли проводить все операции прямо с телефона — начиная от приёмки до выдачи сырья. Внедрить приложение было просто: достаточно приехать на каждый склад и показать приложение начальнику. Начальник склада — человек умный, потому что тяжело управлять складом, если ты недостаточно разбираешься в деле. Так что я показал приложение ему, а потом он сам всё объяснил своим сотрудникам. После этого я начал работать с той частью сервиса, которой пользовались райдеры-доставщики. Я просто пытался всё сделать чуточку лучше. Например, часто курьер, доставив заказ на высотки, не может отметить заказ как «доставленный», потому что нет Интернета. Я внедрил отложенные отправки статусов доставок и улучшил трекинг локации райдеров. Внедрить приложение для райдеров оказалось сложнее, чем на складе: если все склады находились в одном здании и у них был один начальник, то кухонь было 50 по всей Москве. Есть ответственный за все кухни. Есть ответственные за группы кухонь по районам. И есть ответственный за каждую кухню отдельно. На каждой из них могло быть много курьеров. Если сотрудник склада в каком-то месте не понимал работу приложения, он мог подойти к начальнику, которому до этого всё объяснил я. Но в курьерской части просто невозможно всё объяснить каждому курьеру отдельно. Поэтому райдеров нужно приравнивать к внешним пользователям, которые скачали приложение и хотят сразу понять, как что работает. Поэтому часть для доставщиков я хотел сделать максимально понятной и красивой и ориентировался на дизайн-подходы «Рокета». В «Кухне» работала одна поддержка для клиентов и для курьеров. Если райдер попадает в ситуацию, когда не открывается дверь или что-то ещё, он звонит в поддержку, и они решают эту проблему. Я договорился с одной из сотрудниц, и при добавлении новой версии приложения она оповещала об этом курьеров. А от них требовалось зайти в Play Store и обновиться. Иногда бывали небольшие проблемы, но благодаря качественной работе QA всё проходило хорошо. У меня вообще было много свободы в «Кухне». Не нужно было ни за кем ходить и отчитываться. Однажды я рассказал директору, что у нас будет новое приложение для доставщиков, а он ответил: «Круто!» Sizl придумали несколько ребят из тех, которые сделали «Кухню». Если у «Кухни» была концепция домашней еды, то в Sizl такого нет. Мы готовим и доставляем Real food — это не фаст-фуд, но и не гурманский ресторан. Качество еды лучше, чем если бы её приготовил дома среднестатический человек, и готовится она из хороших, всегда свежих продуктов. Но мы хотим быть не просто историей про доставку, а внести в бытовые процессы приобретения еды более завлекающие вещи. Сейчас мы работаем над игровыми механиками с уже продуманными персонажами и лор. В планах внедрение геймификации в разные взаимодействия, чтобы сделать заказ еды более увлекательным. В основе идеи геймификации — понятная механика в стиле тамагочи. И у нас есть ощущение, что это должно хорошо сработать. Мы запускались очень постепенно, разделив всё по этапам. Запуск в агрегаторах. В первые пару дней мы запустились в агрегаторе и заказывали сами у себя. Этому есть сразу причины: Догфудинг — практика использования сотрудниками компании собственных продуктов и сервисов. Потому что ты — свой самый лояльный клиент. Это нормально: заказать еду у себя, как все остальные клиенты У агрегаторов сложная и никому не понятная система ранжирования. Есть ощущение, что если в ресторане не делают заказы, агрегаторы не показывают его вообще. Поэтому нам показалось важным первое время поднимать активность самим Подключение промоушена в агрегаторах. Для этого нужно выбрать разные поддерживаемые агрегатором акции, например: «Закажи два, получи третье в подарок». Мы начали пробовать разные комбинации, заказов стало больше. Запуск в приложении проходил по той же схеме: сначала заказывали сами, потом конвертировали людей из агрегаторов в пользователей приложения. Сейчас мы полноценно доступны у трёх агрегаторов и в нашем приложении. Когда мы только запустились, у нас не было райдеров. Мы ожидали, что заказов за день сначала будет немного, поэтому не хотелось тратить деньги на курьера, который будет сидеть фуллтайм на кухне и ничего не делать. Из-за этого на кухне должен был всегда находиться ещё один человек кроме повара. Этот человек должен был способен доставить заказ и решить какой-нибудь вопрос, потому что на запуске на кухне много чего происходило: например, нужно съездить к поставщику и докупить какой-то ингредиент. Сейчас у нас несколько поваров и курьеров, но некоторые ребята из операционной команды всё ещё выезжают на кухню. Я тоже выезжаю, но уже не на дежурства, а для какой-то специфичной работы: поменять жёсткий диск в системе видеонаблюдения или посмотреть, как дела с апдейтом приложения для райдеров. Каждая кухня обслуживает один район, в пределах которого можно доставить заказ за 30 минут. Мы всегда доставляем заказ на велосипедах, кроме исключительных случаев. В этом наше преимущество по сравнению с другими доставками, потому что не приходится стоять в пробках и искать парковку. В Штатах в большинстве домохозяйств есть автомобили. В обычное буднее время дороги всё время перегружены, и на велосипеде получается быстро доставить заказ. Если на улице жуткая непогода, мы можем привезти заказ на машине — но это скорее исключение, чем правило. Вообще если можно говорить о дурном вкусе в еде, то американская кухня, безусловно, является выражением дурного, вздорного и эксцентрического вкуса, вызвавшего на свет такие ублюдки, как сладкие соленые огурцы, бэкон, зажаренный до крепости фанеры, или ослепляющий белизной и совершенно безвкусный (нет, имеющий вкус ваты!) хлеб. И. Ильф, Е. Петров, «Одноэтажная Америка», 1935г. Уже по ходу работы мы выяснили две вещи. Во-первых, в целом в Америке посредственная еда. И в доставке, и в ресторанах — еда проигрывает той еде, которая в России. Если верить писателям-соотечественникам, которые были здесь 90 лет назад — за это время как будто ничего не поменялось. Во-вторых, среднему американцу вполне себе заходят русские блюда: борщик, шаурма, сырники. Мы сначала боялись, что американцы не поймут в меню таких позиций и не будут их заказывать. У нас нет фокуса на какой-то специфичной кухне. Поэтому рядом с борщом у нас в меню есть и чизбургер, и хот-дог. Сейчас мы думаем добавить какие-то азиатские направления, вроде роллов и воков. Мы живём все вместе в пятиспаленном пентхаусе. В этом есть и плюсы и минусы. Удобно решать задачи. Когда вы все вместе, можно сесть и быстро решить любой вопрос: планировать, договариваться. Не нужно созваниваться и списываться. Это бывает удобно и для прямой деятельности компании. Несколько раз были ситуации, когда на кухню одновременно прилетало 5-6 заказов одновременно, а там только один курьер. В таких ситуациях мы просто садились в две машины, доезжали до кухни и сами доставляли заказы. Или иногда на склад приходит доставка от поставщика упаковки, а складских сотрудников у нас нет. Можно быстро решить эту задачу вместе: поехать, забрать, привезти, увезти. Это не sustainable-стратегия, но  сейчас это выгодно. То, что все мы поработали райдерами, очень хороший плюс. Мне кажется, что в разработке у многих людей, особенно в больших корпорациях, есть разрыв между тем, что ты делаешь, и непосредственной деятельностью компании. Например, разработчик сидит, делает софт для курьеров, но не знает их боли и вообще не понимает, существуют ли они. А мы все знаем. Изначально я вообще не рассматривал вариант кроссплатформенных подходов, потому что не видел в этом никакой выгоды. Flutter не привлекает меня из-за отсутствия нативных интерфейсов: весь UI в Flutter — это UI Flutter. Например, я хочу, чтобы пружинистый скролл на iOS работал как пружинистый скролл на iOS, а на Android как на Android. Поведение интерфейса во Flutter для меня всегда было блокером к его использованию. KMP мне тоже сначала не нравился, потому что раньше управление памятью в Kotlin/Native (iOS) работало не так, в Kotlin/JVM (Android). Это было связано с особенностями native-рантайма — например, нужно было в явном виде указывать, в какие места памяти будет асинхронный доступ. Мне всё это совершенно не нравилось, но скоро эту проблему решили. Поэтому в начале я выбрал нативную разработку. Кодил на Android и планировал нанять в команду iOS-разработчика. В какой-то момент в Android-части я столкнулся с проблемой задержки в обработке действий. Я хотел, чтобы пользователь мгновенно видел результат каждого действия в приложении. Например, когда добавляет товар в корзину, можно было показать результат: товар лежит в корзине. Но для подтверждения изменений нужно было обращаться к серверу. Мне не нравилось заставлять пользователя ждать ответа сервера, лучше показывать результат сразу локально, отправляя запрос на сервер после. Мой план был в том, что — скорее всего — при добавлении товара в корзину всё пройдёт гладко. Поэтому я сразу же показывал результат пользователю.  А если что-то пошло не так, сервер все равно выдаст актуальную информацию, и локальное отображение скорректируется — приложение покажет корзину без борща и скажет пользователю: «Блин, борщ закончился». Сделать это на Android было сложно, и когда я закончил, показалось явно неудобным сидеть и кодить такую же штуку на iOS. Примерно в то же время происходили ещё две вещи. Во-первых, в KMP обновили memory manager. Всё упростилось, и разработка стала более схожей с обычной Kotlin-разработкой. Во-вторых, я смотрел на работу Telegram. Там уже была библиотека tdlib, на которой сделаны все клиенты. Мне показалось интересным то, что клиенты Telegram — в основном фронтенды для этой библиотеки. Я подумал: «А что, если сделать такой же бэкенд для фронтендов на KMP?» Эти моменты совпали с тем, что я не хотел дублировать сложную логику на iOS и подтолкнули к тому, чтобы начать использовать KMP. Работа с KMP требует единоразовых усилий на начальной настройке, после чего процесс разработки становится более гладким и предсказуемым. Android-часть уже была. В iOS-части нужно было решить несколько задач раз и навсегда. Например, сделать механизм подписки на события из платформенной части, выбрать решение для BigDecimal и UUID, потому что их нет в КМП. После этого у меня лично не возникало ситуаций, чтобы каждая фича потребовала больше времени, чем если бы я писал ее на нативной платформе. Пришлось провести некоторое время, разбираясь в деталях, потому что документации 1,5 года назад не хватало. Сейчас такой проблемы нет. Время на задачу по логике на KMP сопоставимо со временем на эту же задачу на нативном iOS. Работа с KMP, в общем, достаточно безболезненна, но есть и сложности. 1,5 года назад, если была нужна библиотека, которая не является частью KMP, нужно было либо писать свою обёртку, либо использовать существующее решение с GitHub, которое может быть не совсем актуальным. Например, при работе с BigDecimal-значением я нашёл решение для iOS и Android, но KMP-версия была довольно устаревшей и с некоторыми багами. Также для узких нишевых библиотек, например SignalR, KMP-версии нет. В этом случае выбор между использованием голых вебсокетов и написанием нативной реализации может зависеть от конкретных потребностей проекта и сложности работы с библиотекой. В случае с SignalR я открыл доку и один раз накодил библиотеку. Ну как библиотеку — реализовал те кусочки, которые мне были нужны. При этом не было ситуаций, когда один раз написанная общая часть оказывалась неприменимой к другой платформе. Поэтому работа с KMP требует единоразовых усилий на начальной настройке, после чего процесс разработки становится более гладким и предсказуемым. Мы шарим почти весь код логики приложения: управление состоянием, сетевые запросы, хранение локальных данных. Некоторые компоненты, такие как платежи, пуш-уведомления и UI, специфичны для каждой платформы. Логика навигации частично переиспользуется, но реализация переходов тоже остаётся на платформенном уровне. В начале я хотел нанять iOS-разработчика, потому что думал, что придётся отдельно писать два нативных приложения. Но я всё время откладывал этот момент, потому что ещё со времён работы в KTS помнил, как тяжело было найти иосера. Их почему-то вообще не было на рынке. Не знаю, наверное, рынок сейчас изменился, но мои флешбеки из прошлого до сих пор живы. Когда я перешёл на кроссплатформенную разработку, то решил — ладно, сделаю core, а потом найду человека чисто на интерфейсы, на фронтенд. А потом подумал, что можно и фронтенд самому написать. Подходы к вёрстке очень похожи с учётом использования Compose на Android и Swift UI на iOS. В общем, так никого и не нанял. В чем сила, не знаю. О будущей цели думать сложно, могу сказать только о настоящем. Сейчас получается так, что я пишу и на iOS, и на Android, что-то на бэкенде, что-то на DevOps. Эта T-шейперская штука, и она как будто бы реально работает. На практике это очень удобно. Сейчас я задумываюсь о том, что было бы, если бы я, наоборот, сконцентрировался на чём-то одном? Если бы я остановился на Android, я бы мог назвать себя Adnroid-экспертом с 10 годами опыта в этой области. Но я, конечно, объективно не могу этого сделать. Пока я для себя ещё не понял, хорошо это или плохо. В конце хочется сказать про Sizl в целом. Успешный запуск стартапа невозможен без высокого уровня самостоятельности и ответственности каждого из её участников. Хотя формально в команде нет руководителей, но этого и не требуется. Любой из команды может взять под контроль горящую задачу, решить её сам или найти исполнителя. Но главное, что каждый нацелен именно на рост, развитие, а не просто решает проблемы.",
    "88": "Disclaimer: В статье сделан контекст на выборе БД, и хотя контекст можно расширить до выбора языка, фреймворка и тп, предлагаю сконцентрироваться только на одном аспекте. При разработке приложения, сервиса, системы и тп возникает один из главных вопросов: как мне хранить данные (какую БД выбрать). В связи с тем, что чаще всего в получите ответ “зависит” (it depends), предлагаю рассмотреть несколько стратегий, которые будут работать почти всегда. При разработке стоит исходить из того, с чем умеет (и умеет хорошо) работать ваша команда, компания, вы сами. Частенько нам хочется опробовать новую технологию. Как известно, лучшее место и время для этого - новый проект. Поэтому напишите pet-проект и проверьте ее там. Если речь идет про коммерческую разработку, то знания технологии ускорит вас и поможет не нарваться на подводные камни. В крупных компаниях часто (почти всегда) есть определенный скоуп технологий, “поддерживаемых” компанией. В небольших компания такие стандарты могут быть описаны не явно. Выглядят они как “все наши сервисы используют MySQL”. С одной стороны, это может ограничивать вас. С другой, в компании уже набрался пул “экспертов” по этой технологии. “Эксперты” могут помочь, как консультациями, так и предоставлением шаблонов для решения типовых проблем. Очень часто мы не решаете какую-то принципиально новую проблему, а просто адаптируете ее под предметную область. Я считаю, что да. На начальной стадии дизайна надо исходить из текущих условий. Иначе это будет выглядеть как: у нас все Java разработчики, но новый сервис мы будем писать на Erlang. А будет ли это работать, если использовать AWS, Azure, GCP и тп с модными DBaaS? Да, будет. DBaaS решает проблемы развертывания БД и в какой-то степени решает проблему поддержки. Но проблемы работы с конкретной технологий остаются. К тому же, DBaaS обычно более ограничен в плане гибкости конфигураций. Такое и правда может быть. Но лучший вариант, это проверить вашу гипотезу. Реализация двух Proof of Concept со стандартной БД и с узкоспециализированной под ваши задачи. Во-первых, необходимо понять, насколько ваше решение лучше стандартного. Во-вторых, цифры помогут вам аргументировать вашу позицию. И не стоит забывать, что в этом случае необходимо решать вопросы эксплуатации и поддержки новой БД. P.S.: После обсуждений этого поста телеграм канале считаю нужным оставить небольшое дополнение. Однако, я бы прибегал к нему только в случае, если стандартное решение для вас точно работать не будет. А стратегией \"по умолчанию\" стоит использовать подход, описанный в данной статье.",
    "89": "В мире быстро развивающихся технологий и постоянно меняющихся пользовательских ожиданий, ключевым ресурсом для любого дизайнера является его UI Kit. Независимо от того, работаете ли вы над мобильным приложением, веб-сайтом или сложной системой, хорошо структурированный и актуальный UI Kit может значительно ускорить процесс разработки, обеспечивая при этом консистентность и качество дизайна. Однако, чтобы UI Kit оставался релевантным и эффективным, он требует регулярного обновления и улучшения. В этой статье мы рассмотрим практические шаги и советы, которые помогут вам обновить ваш UI Kit, делая его не только более удобным и функциональным, но и вдохновляющим инструментом для создания великолепных пользовательских интерфейсов. Создайте четкую иерархию компонентов в вашем UI Kit, группируя их по функциональности или использованию. Например, разделите элементы на такие категории, как \"Формы\", \"Кнопки\", \"Навигация\", \"Индикаторы\" и \"Типографика\". Для каждой категории предоставьте вариации компонентов (например, разные состояния кнопок: обычное, при наведении, нажатое, отключенное) и четкое описание применения. Каждый компонент в UI Kit должен сопровождаться подробным описанием и примерами использования. Например, для кнопки предоставьте информацию о её размерах, цветовых вариантах, когда и как использовать каждый вариант, а также код для вставки. Добавьте визуальные примеры и best practices, чтобы дизайнеры и разработчики могли легко понять, как правильно использовать компоненты. Разработайте компоненты таким образом, чтобы они адаптировались к различным размерам экранов и ориентациям без дополнительных настроек. Например, кнопка может изменять свой размер и форму в зависимости от ширины экрана устройства. Для этого предусмотрите в UI Kit различные варианты компонентов для мобильных устройств, планшетов и десктопов. Расширьте ваш UI Kit, добавив поддержку темной и светлой темы для всех компонентов. Это позволит приложениям и веб-сайтам легко переключаться между темами в зависимости от предпочтений пользователя или системных настроек. Убедитесь, что все элементы четко видны и читаемы в обеих темах. Разработайте или интегрируйте обширную систему иконок, которая охватывает все потребности вашего проекта или продукта. Это может включать иконки для социальных медиа, инструментов, пользовательских действий и т.д. Убедитесь, что иконки имеют согласованный стиль и размеры для разных сценариев использования. Также предоставьте альтернативы для различных состояний (активное, неактивное, при наведении). Разработайте набор шаблонов страниц и компоновок для наиболее часто используемых сценариев: главная страница, контакты, лендинги продуктов, формы обратной связи и т.д. Это позволит дизайнерам и разработчикам быстро прототипировать и внедрять новые страницы, поддерживая при этом консистентность дизайна. Добавьте в свой UI Kit набор утилит и вспомогательных инструментов, таких как сетки, направляющие, масштабы типографики и цветовые палитры. Это поможет дизайнерам и разработчикам эффективно использовать UI Kit, обеспечивая высокое качество конечного продукта. Регулярно обновляйте свой UI Kit, включая оптимизацию для последних версий операционных систем, браузеров и фреймворков. Например, если появляется новая функция в iOS или Android, рассмотрите возможность добавления соответствующих компонентов или стилей в ваш Kit. Это гарантирует, что ваш UI Kit останется актуальным и полезным для команды на протяжении всего цикла разработки продукта. Создайте каналы для сбора обратной связи от пользователей вашего UI Kit, такие как опросы, формы обратной связи или даже регулярные встречи с командами дизайнеров и разработчиков. Используйте эту обратную связь для идентификации наиболее востребованных улучшений и проблемных мест в вашем UI Kit. Если ваш UI Kit является частью более крупной дизайн-системы, убедитесь, что он интегрируется с другими элементами системы, такими как брендбук, руководства по стилю, ассеты и инструменты. Это обеспечит единообразие дизайна и упростит использование UI Kit в рамках общей дизайн-стратегии компании. Улучшение UI Kit — это непрерывный процесс, требующий внимательности к деталям, открытости к новым идеям и готовности адаптироваться к изменениям в технологиях и предпочтениях пользователей. Следуя вышеупомянутым советам, вы не только сделаете свой UI Kit более универсальным и удобным в использовании, но и обеспечите более эффективное и продуктивное сотрудничество между дизайнерами и разработчиками. Помните, что ключ к успеху — это не только ваши технические навыки, но и способность взглянуть на дизайн глазами пользователя, предвидеть его потребности и ожидания.",
    "90": "Порассуждаем про удаленный формат работы и причины, почему его недооценивают. Кто трудится из дома, может с кайфом начать читать пост прямо сейчас. Если из офиса, то придется дотерпеть до вечера, пока коллеги разойдутся по домам и не будут заглядывать в ваш экран. Кстати, если вы не простой наёмник, а фаундер или топ, тоже почитайте, вдруг найдете что‑то интересное для себя. Осторожно, в конце поста будет ссылка на мой канал в телеграме. Если у вас аллергия на такое — не минусуйте плиз, а просто закройте окошко. Спасибо! В доковидные времена, если кто еще помнит таковые, я работал из офиса, менеджерил какие‑то проекты, общался с коллегами и в 19:00 выключал комп. И вообще не представлял себе как всякие фрилансеры с ноутбуком сидят где‑то по кафешкам и заграницам, и еще делают что‑то полезное. Стильный лофт, печеньки, кофеварка, монитор на коробке из под рутера в качестве подставки. Любой вопрос можно решить просто подойдя к столу коллеги, или поймав его на «водопое» у кулера. Случился ковид и мы с командой переехали на удаленку, где я познакомился с ее плюсами и минусами. Проработав больше трех лет в режиме full remote, у меня тут накопилось что рассказать. Не все компании умеют строить работу в команде на удаленном режиме, и негативный опыт рождает много мифов. Вот с какими я сталкивался. Моя реальность показывает, что это вообще не связано с тем, где они находятся. Ничего не делать можно легко и в офисе. Только это не выглядит так буквально, если бы человек смотрел сериал дома на рабочем ноуте. Люди ходят покурить, к кулеру, в соседний отдел, говорят по телефону в переговорках, задерживаются с обеда, старательно варят кофе по 40 минут, выбирают себе новые кроссовки в Ламоде, устраивают бесполезные встречи и делают еще много‑много «важных дел» вместо работы. На выходе получается аналогичный сценарию с сериалом КПД. Понаблюдайте за своими коллегами. В коллективе почти всегда есть один такой. Эффективная работа зависит только от дисциплины самого сотрудника и процессов внутри команды. Остальное, на мой взгляд, это ерунда, притянутая за уши. Если честно, никто пока мне не смог внятно и с циферками объяснить, что же такое командный дух и как он влияет на работу, метрики и деньги в компании. Путь местные эйчары кинут в меня минус камень, но часто это инфантильные рассуждения про «мои коллеги — это моя семья». Мой опыт подсказывает, что чаще бизнес мыслит немного в другой плоскости: как сделать так, чтобы команда перформила с максимальным ROI как можно дольше. Потому что нанимать других сложно, больно, дорого и вообще не хочется. Для решения таких задачек уже давно есть инструменты и готовые бест‑практис от корпораций, кто перестроил свои процессы на карантине. Изучайте и пробуйте внедрять подходы от тех же FAANG. А банальные штуки вроде сборов в коворкинге 1–2 раза в месяц, кулер‑чатика в телеграме и пиццы в зуме по пятницам, можно вообще внедрить хоть со следующей недели. Просто спросите ваших коллег, а надо ли оно вообще. Возможно, командный дух это одна из ваших фантазий. На одной чаше весов ситуации, когда дома действительно может быть не очень удобно: сосед, которые делает соты в квартире перфоратором, дети, маленькая площадь, отсутствие рабочего места и т. д. На другой располагается освобождение времени на дорогу от 30 минут до часа, экономия на новой одежде «на выход» (актуально для дам), время на спорт или хобби, опция работать из любого города (бывают ограничения по стране, да), комфортный микроклимат — можно забыть про клевый летний кейс, когда вы сидите с выключенным кондеем в жару, потому что одному коллеге дует, и еще много всего. Если вы собственник или топ‑менеджер, попробуйте дать возможность тому, кто хочет попробовать удаленку, сделать это. Начать можно с гибрида, а если эффективность не упадет — подумать о full remote. Если офисный сотрудник, то используйте аргументы из этого поста, чтобы «продать» идею удаленной работы боссу. Во‑первых, перечитайте миф 1. Во‑вторых, есть более прикольные и осязаемые бенефиты от удалёнки, чем мнимый контроль в опенспэйсе. Например, экономия на офисе. Согласен, если есть офлайн рабочие встречи с партнерами и клиентами, полностью от офиса отказаться сложновато. Но тоже можно, используя для приличный коворкинг, например. Или срезание костов на железе и рабочих местах для сотрудников. А еще коллеги не будут пить дорогой зерновой кофе, просить печеньки, или не дай боже еще, «здоровые перекусы» на кухне. А еще сокращение ФОТ. Это когда вместо московского разработчика, можно нанять абсолютно такого же по скиллу, только из условного Саратова. Предложить ему зарплату выше рынка в его городе и заключить win‑win партнерство. Разраб сможет позволить себе ипотеку и ладу гранту через год, а бизнес получит лояльного сотрудника. Все в выигрыше. Если ну очень уж хочется в микроменеджемент, то им можно заняться и на удаленке. Устраивать частые созвоны, просить отчеты и внедрить трекер рабочего времени. Но с этим надо осторожно. Говорят, для адептов трекинга есть отдельный котёл сами знаете где. Да, если разбираться с этим самому, в вакууме. Но если выйти из «пузыря», вам доступны десятки готовых решений на рынке, ведь удаленка появилась не вчера. Это удобный софт, шаблоны документов и регламентов, чужой опыт, который можно достать через невторкинг. Пишите в комментах, с какими мифами вы сталкивались сами. Ну и как вам вообще работается на удаленке. Будет интересно почитать. Этот и другие посты про карьеру можно читать у меня в блоге, где я помогаю айтишникам быстрее строить карьеру и расти в зарплате. Бонусом: собрал 300+ материалов про управление продуктами и проектами. А еще написал гайд по устройству на работу для джунов: забирайте в закрепе канала. Курсы успешного успеха не продаю. Подписывайтесь, чтобы подсматривать за чужим опытом. Всем мир. Менеджер из Санкт-Петербурга. Делаю @postmantg",
    "91": "Во встраиваемых устройствах есть риск извлечения SD карты и использования её для посторонних целей. В этом случае помогает аппаратный пароль SD карты. Установка пароля не даёт проводить с картой никаких операций. Обычные PC такую карту просто не видят. В начальном загрузчике модуля S7V30 с микроконтроллером Synergy S7G2 на базе Azure RTOS реализован механизм установки и снятия паролей с SD карт. В этой статье покажу как использовать встроенный веб-сервер Azure RTOS с TLS для операций с SD картой. В спецификации физического уровня SD карт(Physical Layer Simplified Specification)  в параграфе 4.3.7 Card Lock/Unlock operation есть описание установки и снятия пароля . Установка/снятие пароля производятся командой CMD42. Пароль может содержать до 16 байт.  Пароль записывается в энергонезависимую память карты. Командой CMD42 выполняется как запись и стирание пароля, так и снятие защиты и установка защиты.Если в карту записан пароль, то после подачи питания его надо ввести командой CMD42, чтобы карта начала нормально функционировать. Данные на карте после ввода пароля с флагом снятия зашиты остаются доступны до выключения питания. Если пароль от карты утерян, той же командой CMD42 остается возможность полностью стереть карту с потерей форматирования . Однако есть карты, которые и на команду полного стирания могут содержать отдельный пароль. Драйвер SD карты  Azure RTOS на Synergy в модуле S7V30 был модифицирован таким образом, чтобы при каждом включении питания проверять установку пароля на SD карте и снятие защиты с карты в случае если она запаролена. В начальном загрузчике был добавлен параметр для хранения пароля  SD карты. Исходные тексты проекта доступны в репозитарии начального загрузчика. Параметры загрузчика хранятся во внутренней EEPROM чипа в формате JSON  в сжатом и сдублированном виде. В ходе разработки модуля S7V30 для диагностики и выполнения операций обычно применяется эмулятор терминала VT100. Поэтому установка/снятие пароля и полного стирания карты  были в начале реализованы в режиме терминала. Но работа с терминалом требует соответствующей квалификации.  Для большей доступности было решено реализовать операции с SD картой через WEB приложение. Для предотвращения утечек пароля  в сеть наш веб-сервер защищён протоколом с шифрованием и дополнительной авторизацией. Сложность представляет то, что все файлы WEB приложения находятся на той же защищаемой SD карте (применяется урезанная версия модуля без SPI Flash). Из-за этого команды полного стирания карты в WEB приложении нет, но она есть в терминале. Про особенности реализации защищённого веб-сервера из пакета Azure RTOS NetX Duo рассказано в этой статье.   Встроенный в модуль веб-сервер работает с быстрым аппаратным шифрованием и протоколами TLS 1.2 или 1.3 (настраивается при компиляции). Кроме этого включена базовая аутентификация в http севере стека NetX Duo, т.е. при заходе в приложение нужно ввести пароль. Проект в репозитарии хранится с само-подписанным сертификатом. Чтобы браузеры на PC не выкидывали постоянно предупреждение об этом, сертификат надо явно инсталлировать в систему компьютера. С поры первой публикации о применении jQuery mobile на модуле с Synergy  версии фреймворка уже ушли далеко вперёд. В данном случае применяем версию  1.4.5. JQuery mobile  позволяет разрабатывать  одно-файловые приложения с современным отзывчивым стилем адаптированным под мобильные дивайсы. Всё приложение в одном файле облегчает работу встроенного веб-сервера, поскольку убирает массу повторных запросов при переходе между страницами. Сам  jQuery mobile реализован в нескольких сравнительно небольших .js и .css файлах. Это важно, поскольку экономит трафик и память встраиваемых устройств. На сайте проекта jQuery mobile есть возможность создать кастомные файлы стилей и скриптов с ещё более уменьшенным размером. Привлекает возможность стилизации своих приложений используя билдер стилей  jQuery mobile. Я воспользовался данной возможностью и создал свой файл стилей S7V30_theme.min.css размером всего в 18 килобайт. jQuery mobile по прежнему хорошо поддерживается в  Dreamweaver. В этой среде разработки одно-файловые приложения на jQuery mobile можно разрабатывать и визуализировать постранично. В отличие от обычных редакторов, где страницы приложения сливаются в одно сплошное нагромождение виджетов. В последнее время появилось дополнительное преимущество jQuery mobile и самого базового фреймворка jQuery - это хорошая поддержка со стороны ChatGPT. В ChatGPT можно найти ответы на все вопросы связанные с разработкой на jQuery mobile. Правда в области embedded разработки у ChatGPT ещё получается плохо. Поскольку модуль S7V30 не так уж и слаб, я не стал ужимать файлы  jQuery mobile  до минимума и помещать их во внутреннюю Flash микроконтроллера, но при желании это не трудно сделать. Содержимое html файла приложения можно посмотреть здесь. В jQuery mobile все делается с помощью AJAX, поэтому масса специальных классов и в формах не используются обычные кнопки типа submit.  Прием и обработка сервером методов POST и GET производится в этом файле. Работа непосредственно с SD картой реализована здесь. Установка пароля или смена пароля не приводят к блокировке карты немедленно. Карта окажется заблокированной когда с неё снимут и снова подключат питание. Начальный загрузчик при выполнения программного рестарта принудительно выключает и включает питание у SD карты. Таким образом подачей команды рестарта из приложения можно проверить корректность холодного запуска карты и разблокировку.  Пароль может быть любой длинны от 1 до 16 символов из перечня допустимых символов (перечень придумал я сам исходя из соображений удобства) Для более полного анализа SD карты кнопкой Get SD card CSD на экран выводится содержимое структуры CSD карты. В этой структуре закодированы основные физические характеристики карты. Установка и сброс пароля на содержимое CSD не влияют. Кроме функции работы с SD картой через данное приложение можно загрузить новую прошивку в устройство. Как было сказано в предыдущей статье, прошивки должны быть упакованы в специальный защищённый формат. Для целей отладки в настройках можно отключить шифрование TLS у WEB сервера.  Настройки доступны через терминал. Доступ к терминалу возможен через VCOM порт USB или через Telnet. В Telnet возможны подключения через Wi-Fi, RNDIS, CDC ECM. Поскольку загрузчик оснащен протоколом mDNS, то к модулю в локальной сети можно обращаться сразу по доменному имени S7V30 без необходимости знать IP адрес устройства.",
    "92": "Многофункциональные устройства включают в себя 3, а иногда даже 4 аппарата сразу, и иногда такая универсальность может показаться излишней. Но часто их приобретение в таком комплекте является единственно верным выбором — из-за требований к месту размещения или особенностей работы. Сегодня мы рассмотрим наиболее популярные МФУ, которые продолжают поставляться в Россию и доступны (по крайней мере в нашем интернет-магазине). Несмотря на то, что МФУ являются комплексным видом печатной техники, среди них немало достойных недорогих моделей до 10 000 рублей. Canon Pixma MG2540S - это многофункциональное устройство класса 3 в 1, ориентированное на работу с документами формата А4. Его габариты стандартны для техники этого типа: 426x145x306 мм, а вес МФУ составляет 3,5 кг. Оно включает в себя принтер, печатающий с разрешением до 4800x600 точек, сканер планшетного типа и копир без автоматической подачи бумаги. Струйный принтер Pixma MG2540S печатает цветные изображения со скоростью до 4 страниц в минуту, а при черно-белой печати скорость возрастает вдвое - до 8 страниц. Устройство поддерживает бумагу плотностью от 64 до 275 г/кв. м., поэтому для печати можно использовать как обычную офисную бумагу, так и конверты и даже фотобумагу. Встроенный сканер поддерживает цветное сканирование, а минимальное и максимальное значение масштаба копира составляет 25-400% от величины исходного документа или изображения. Такое решение отлично подойдет для дома, образовательных учреждений и корпоративного использования в условиях, не предполагающих высокой нагрузки. Еще один представитель сегмента бюджетных МФУ - HP DeskJet 2320 размером 425x249x524 мм. Это не самая компактная модель, но зато не тяжелая - всего 3,5 кг. Низкая цена, едва превышающая 6000 рублей, не мешает ей соответствовать званию многофункционального устройства, состоящего из трех видов оргтехники: принтер, сканер и копир. Факса здесь нет, но устройство и не позиционируется как офисное. Это скорее домашний вариант. Принтер HP DeskJet 2320 - цветной термоструйный. Максимальное разрешение, с которым он печатает, - 4800x1200, поддерживаемая плотность бумаги - от 60 до 300 г/кв.2, а скорость - 5,5 и 7,5 страниц в минуту для цветных и монохромных изображений соответственно. Сканер в этом МФУ, как и в большинстве недорогих моделей, - планшетный без автоподачи. Соответственно, скорость сканирования будет зависеть от того, насколько быстро вы сможете подавать ему документы. Максимальное разрешение сканируемых изображений - 1200x1200 точек. Хороший, надо сказать, показатель для бюджетного МФУ. Тем, кто ищет недорогое лазерное МФУ, XCOM-SHOP.RU может предложить не менее достойную модель - Pantum M6500W. Это современное устройство, совмещающее в себе функциональность принтера с монохромной печатью, сканера и копира в пределах 15 тысяч рублей. Несмотря на доступность данного МФУ, оно поддерживает подключение по Wi-Fi. Принтер данной модели использует именно лазерную технологию печати, а не струйную, встроенный сканер позволяет сканировать цветные изображения с разрешением 1200x1200 dpi, а на фронтальной панели предусмотрен монохромный дисплей. Скорость работы принтера  - 22 страницы в минуту. Причем первый отпечаток выходит уже через 7,8 секунды. Скорость сканирования документов у него идентична, независимо от используемого формата исходника: А4, А5, А6, В5 и В6. А встроенная память позволяет сохранять изображения в памяти МФУ. Если вы ищете эффективное и производительное МФУ до 20 000 рублей, то HP Laser MFP 135a под формат А4 на базе процессора с частотой 600 МГц и памятью объемом 128 МБ - как раз для вас. Оно подойдет не только для дома, но и офисного применения с 1-5 пользователями. Поэтому и вместительность лотка тут выше, чем у предыдущих моделей: 150 на подачу и 100 на выдачу. Вес данной модели тоже увеличился: почти 7,5 кг против 3-4 кг у домашних моделей. Принтер, встроенный в HP Laser MFP 135a, основывается не на струйной, а на лазерной технологии со всеми вытекающими, в виде высокой скорости печати до 20 страниц в минуту, большей экономичности и плотности совместимой бумаги от 60 до 163 г/кв.м. Он позволяет печатать монохромные изображения и текст на карточках, конвертах, обычной, тонкой, переработанной, хлопковой и цветной бумаге, а также на этикетках. Сканер в Laser MFP 135a от HP - обычный, планшетного типа, с ручной подачей документов для сканирования. Его максимальное разрешение, что называется, “в рынке” и составляет 600x600 точек. Это стандартное значение для большинства отдельно стоящих моделей и МФУ, которого хватит для классических сценариев использования. Kyocera - очень популярный производитель оргтехники, особенно востребованный в профессиональной среде. Поэтому его лазерное МФУ Kyocera MA2001w, относящееся к новой линейке, подойдет как требовательным юзерам, ищущим универсальное печатное устройство для дома, так и корпоративным пользователям. Это ясно по предельному лимиту печати за один цикл, который равен 99 копиям. Данное МФУ отличается поддержкой Wi-Fi для беспроводного соединения, скоростью печати до 20 стр/мин, быстрым нагревом, который занимает всего 20 секунд от момента включения, и экономичностью. Так, стартовый картридж, который идет в комплекте, имеет ресурс 700 страниц. Его сканер - кстати, планшетный - имеет разрешение 600x600 dpi, а встроенная память составляет 32 МБ. Показатель отнюдь не рекордный, но для базовых задач более чем достаточный. Canon i-SENSYS MF3010 - чрезвычайно популярная модель, представленная на рынке уже не первый год и за это время успевшая зарекомендовать себя только с самой лучшей стороны. При цене менее 35 тысяч рублей она отличается высокой надежностью, быстрой печатью и совместимостью с широким перечнем картриджей, в т.ч. неофициальных. Скорость печати Canon i-SENSYS MF3010 стандартна для лазерного МФУ: 18 стр/мин. До момента выхода первой отпечатки проходит всего 7,8 секунд, а время разогрева устройства не превышает 10 секунд. Разрешение документов, которое его принтер выдает на выходе, достигает 1200x600 dpi, что тоже является значением “в рынке”. Здесь используется сканер планшетного типа, который сканирует изображения, в том числе цветные, со скоростью до 18 стр/мин. По умолчанию его разрешение составляет 600x600 dpi, но при улучшении может достигать 9600x9600. В комплекте с МФУ идет стартовый картридж на 725 печатных копий, что очень и очень неплохо. Емкость выходного лотка Canon i-SENSYS MF3010 - 100 листов. МФУ работает с бумагой плотностью от 60 до 163 г/кв.м., может печатать на карточках, конвертах, офсетной бумаге, а также переработанных листах, этикетках, прозрачных пленках и особых типах плотной бумаги. Объем встроенного накопителя - 64 МБ. На внешней части корпуса расположен монохромный дисплей для управления, а вес устройства составляет всего 8,2 кг. Canon i-SENSYS MF463dw - это классическое лазерное МФУ с монохромной печатью, пришедшее на замену модели MF443dw. В первую очередь устройство ориентировано на формат А4. Тут ничего необычного, но ему есть чем удивить. Модель имеет механизм автоподачи и автопереворота бумаги, чтобы печатать на обеих сторонах. Скорость односторонней печати в его случае достигает 40 страниц в минуту, а двусторонней - 33 страницы в минуту. Скорость сканирования при помощи встроенного сканера планшетного типа  - аналогична, а пиковое разрешение составляет 9600x9600 dpi с глубиной цвета 24 бита. HP LaserJet Pro MFP M4103dw - типично офисное МФУ. Дома его возможности будут избыточными, но в офисных условиях использования его потенциал раскроется как нельзя лучше. Это устройство лазерной монохромной печати с высоким быстродействием, стабильностью и надежностью. Как и полагается МФУ, ориентированным на корпоративный сегмент, HP LaserJet Pro MFP M4103dw располагает сетевым интерфейсом для подключения к локальной сети, модулем Wi-Fi и встроенным хранилищем объемом 512 МБ. Для удобства управления на внешней части устройства предусмотрен цветной дисплей диагональю 2,7 дюйма. Kyocera M2040dn - высокопроизводительное лазерное МФУ для работы с документами формата А4. Оно поддерживает только черно-белую печать с разрешением 1800x600 dpi, но, в отличие от многих других профессиональных устройств, выдает до 40 отпечатанных страниц в минуту, а также может работать в режиме двусторонней печати и имеет механизм реверсивной автоподачи. Плюс в комплекте с ним уже идет стартовый картридж. Встроенный копир МФУ имеет такую же скорость, а также отличается высоким разрешением: 1200x1200. Обычные модели чаще всего ограничиваются вдвое меньшими значениями dpi. Корпоративные пользователи будут рады встроенному накопителю на 512 МБ, а также возможности расширить его до 1.5 ГБ и наличию слота для карт памяти. Подключиться к МФУ можно по локальной сети, используя интерфейс RJ-45. По сути, мы имеем дело с очень удобной и надежной рабочей лошадкой, которая не заставит вас долго ждать и не займет много места благодаря компактному корпусу с габаритами 417x437x412 мм при весе 19 кг. HP LaserJet MFP M443nda - устройство с очень обширными возможностями, несмотря на относительно доступную цену для сегмента профессиональных МФУ класса SRA3. От других моделей того же класса его выгодно отличает сразу несколько аспектов: Кроме того, данная модель может похвастать рядом не эксклюзивных, но чрезвычайно полезных функций. Например, тут из коробки предусмотрен механизм защиты PIN to Print, который позволяет начать печать только при вводе уникального кода безопасности. Кроме того, производителем предусмотрена функция Scan to e-mail/SMB/FTP, благодаря которой можно напрямую отправлять отсканированные изображения на почту, в общие папки и на FTP-сервер. Несмотря на то, что в России Xerox в первую очередь ассоциируется с копировальными машинами, ведь даже слово «ксерокс» стало именем нарицательным, DocuCentre SC2020 - это многофункциональное устройство с полноцветной печатью формата А3. Данное МФУ создано для профессионального использования. На его стороне высокая скорость печати до 20 стр/мин, реверсивный автоподатчик бумаги, возможность подключения к локальной сети, а также комплектный тонер. Выгодное преимущество этого МФУ - высокое разрешение цветной и черно-белой печати, которое равно 1200x2400 dpi. Оно обеспечит высокую четкость изображений, которые вы будете получать. А встроенный сканер планшетного типа позволит отсканировать документы в форматах А3 и А4, независимо от цветности. Это высоконадежное и максимально производительное устройство, поэтому будьте готовы уделить ему достаточно места: его размеры составляют 595x634x586 мм при весе 49 кг. Kyocera M4125idn - сетевое МФУ профессионального уровня от надежного производителя. О направленности устройства говорят и цена, и формат печати SRA3, и специфические для оргтехники этого класса механизмы: автоматическая двусторонняя печать, реверсивный автоподатчик, быстрый разогрев и выход первого отпечатка всего за 5,8 секунд. Встроенный сканер здесь - цветной и, самое главное, протяжный. Сканеры данного типа позволяют облегчить процесс сканирования документов, которые могут иметь форматы А3, А4, А5, В4 и В5, за счет самостоятельной работы. Разрешение - 600x600 dpi. Столько же - у встроенного копира, который копирует изображения со скоростью до 12 стр/мин. МФУ совместим с бумагой плотностью от 45 до 256 г/кв.м. и может печатать на карточках, конвертах, офсетной, переработанной, перфорированной, плотной, цветной бумаге, этикетках и пленках. По части “умной” начинки здесь все просто шикарно. В основе МФУ лежит быстрый процессор с частотой 1200 МГц, а объем встроенного накопителя составляет 1 ГБ с возможностью расширения до 3 ГБ. Устройство имеет слот для карт памяти стандарта SD, а также LAN-интерфейс и 2 Host-USB 2.0. Плюс - цветной дисплей для прямого управления. Пусть вас не смущает цена данного аппарата. Canon imageRUNNER ADVANCE DX C3822i ориентирован на профессиональное использование и полностью себя окупает. Это лазерное МФУ, способное печатать цветные изображения формата А3, а возможность сконфигурировать его под ваши потребности позволит получить на выходе именно то, что вы ждете от оргтехники. Устройство печатает цветные изображения со скоростью 15 стр/мин, а черно-белые - 22 стр/мин. Оно поддерживает автоматическую двустороннюю печать и разогревается всего за 4 секунды. Подключиться к МФУ можно по локальной сети и по Wi-Fi, а те организации, которые по-прежнему пользуются факсом, оценят пропускную способность 33,6 кб/с. Canon imageRUNNER ADVANCE DX C3822i - это высокопроизводительное устройство, рассчитанное на высокие нагрузки. Поэтому здесь установлен довольно мощный процессор с частотой 1800 МГц и двумя вычислительными ядрами, а объем встроенного хранилища МФУ составляет 2 ГБ. Но по-настоящему удобным его делает огромный 10,1-дюймовый экран управления с сенсорным вводом.",
    "93": "Для кодирования данных в десятичном формате требуется гораздо больше символов, чем для тех же данных, но закодированных в base64 — 06513249 против YWJj. Однако это правило не работает, когда речь идёт о QR-кодах. В них гораздо лучше работает использование десятичных чисел. Никакой магии, просто все дополнительные цифры сохраняются настолько эффективно, как если бы кодирования вообще не было. Десятичная кодировка позволяет QR-кодам хранить больше данных, а ещё их легче сканировать. как запихнуть максимум данных в URL в QR-код. В статье «Механическая симпатия к QR-кодам: улучшение регистрации в Новом Южном Уэльсе» я исследовал QR-коды, используемые для отслеживания контактов COVID. Оказывается, несколько штатов включили кучу информации в URL-адреса своих QR-кодов в виде JSON-объекта в кодировке Base64, предположительно потому, что это удобно. Они использовали URL-адреса с 228-символами, типа такого: https://www.service.nsw.gov.au/campaign/service-nsw-mobile-app?data=eyJ0IjoiY292aWQxOV9idXNpbmVzcyIsImJpZCI6IjEyMTMyMSIsImJuYW1lIjoiVGVzdCBOU1cgR292ZXJubWVudCBRUiBjb2RlIiwiYmFkZHJlc3MiOiJCdXNpbmVzcyBhZGRyZXNzIGdvZXMgaGVyZSAifQ== , причём это eyJ...был большим двоичным объектом. На уровне исправления ошибок H это можно закодировать в QR-код 81×81 (версия 16). Если всё же нужно хранить данные в формате JSON (в статье описано, почему мы этого не делаем), есть способы эффективнее base64: переписав данные в десятичный формат, мы получим 353-символьный урл https://www.service.nsw.gov.au/campaign/service-nsw-mobile-app?data=072685680885510189821994892577900638215789419258463239488533499278955911240512279111633336286737089008384293066931974311305533337894591404330656702603998035920596585517131555967430155259257402711671699276432408209151397638174974409842883898456527289026013404155725275860173673194594939. Любой здравомыслящий человек скажет, что это на 50% длиннее. К счастью, для QR-кодов всё немного иначе, и там потребуется на 20% меньше модулей (квадратиков). Вышеуказанный урл умещается в QR-код 73×73 (версия 14). QR-коды с меньшим количеством модулей легче сканировать. QR-коды могут хранить произвольные данные, но обычно они используются для хранения URL-адреса, чтобы при сканировании можно было перейти на сайт и получить оттуда всю полезную информацию. С другой стороны, QR-код может быть размещён где-то, где нет доступа к интернету, и он тоже должен содержать достаточное количество полезной информации. Это приводит к появлению URL-адресов с большим количеством данных. Однако URL-адрес — это ограниченный контейнер: попытка вставить в него произвольные данные может привести к проблемам, связанным с неправильной интерпретацией или искажением спецсимволов. К счастью, URL-адреса в основном представляют собой просто текст, а способов преобразования произвольных данных в текст существует множество: на странице Википедии о кодировании двоичных данных в текст описано аж 28 штук. Один из самых распространённых, пожалуй, Base64: он встроен в каждый браузер. Например тот, с которого вы сейчас читаете эту статью. Base64 кодирует 3 байта в 4, выбранные из сокращённого 64-символьного алфавита. Это разумный выбор по умолчанию. Его можно включать в атрибуты JSON, HTML/XML, CSS и, конечно же, URL-адреса. Всё это означает, что данные base64 могут оказаться в QR-кодах, где закодирован URL-адрес, содержащий большой объем данных. К сожалению, base64 — плохой метод кодирования двоичных данных в QR-коде: выбор алфавита вынуждает QR-код хранить данные неоправданно неэффективным способом. Существует огромное множество способов закодировать набор произвольных байтов в сокращённый набор символов. Мы подробно рассмотрим base64, base10 и base45 из RFC 9285, который разработан для QR-кодов (другие, такие как base16 (шестнадцатеричный), base32 или base36, явно хуже). 1.33 1.33 0–9, A–Z, $%*+-.:/, пробел 2.41 Некоторые из них являются безопасными URL: закодированные данные могут быть внесены непосредственно в URL-адрес, не требуя какого-либо специального экранирования или обработки, в то время как другие таковыми не являются или могут потребовать специальной обработки со стороны сервера. Мы собираемся размещать данные в URL-адресах, поэтому безопасность URL-адресов является обязательным требованием. Так что нам нужно рассматривать следующие варианты: base10 (десятичный). Я придумал «base10» благодаря тому, что рассматривал байты как огромное целое число (с прямым порядком байтов) по основанию 256, а затем напечатал целое число в другой системе счисления. Это не скейлится до больших данных, но отлично работает для пары килобайт, которые можно сохранить в QR-коде. Версия Python может быть такой: В таблице в столбце «Соотношение ввода:вывода» показано, какое количество выходных символов потребуется (в среднем) свыше количества входных символов. QR-код хранит данные в битовом потоке, а входные данные закодированы в сегментах. Каждый сегмент данных может быть закодирован в одном из четырёх различных режимов. Каждый режим поддерживает разные входные данные. Например, «буквенно-цифровой» режим, который поддерживает сохранение только цифр, заглавных букв и некоторых знаков препинания, определяя способ сопоставления этих символов с битами для хранения. 3.33 45: 0–9, A–Z, $%*+-.:/ пробел Для цифрового и буквенно-цифрового режимов несколько входных символов сохраняются вместе, что приводит к дробным битам для одного символа. Например, цифровой режим сохраняет группы из 3 цифр в 10 бит, например, 123456 кодируется двумя фрагментами, 123, а затем 456 — в 20 бит. «Издержки», пусть и небольшие, будут, в любой кодировке. Удобно, что 103 лишь немногим меньше 210 , а 452 меньше 211. Когда QR-код содержит несколько фрагментов данных, он все равно представляет собой один длинный поток символов, поэтому продуманный выбор фрагментов позволяет подобрать оптимальный режим для каждой значимой подстроки входных данных. Режим, необходимый для хранения закодированных данных, зависит от набора выходных символов. Для кодировок, которые мы рассмотрели выше: Base64url содержит строчные буквы и поэтому при сохранении в QR-коде требует двоичного режима. 3 входных байта (24 входных бита) превращаются в 4 выходных символа, и эти 4 символа должны быть сохранены в 4×8 = 32 битах QR-кода. В итоге средние издержки составляют 33%: 1 входной байт сохраняется как 1,33 байта в QR-коде. Каждый байт может хранить до 256 различных значений, но кодировка base64 использует только 64 из них. Это приводит к потере 75% значений, или по 2 бита в каждом сохранённом байте. Входными данными будет некоторое количество 8-битных байтов, каждый из которых имеет 28 = 256 возможных значений. Каждый входной байт log(256, 10)в среднем превращается в ≈ 2,408 выходных цифр. 3 цифры хранятся в 10 битах. В сумме 10 / 3 * log(256, 10)для хранения каждого входного байта требуется ≈ 8,027 бит. В результате 1 входной байт сохраняется в среднем как 1,0034 байта: издержки составляют 0,34%. Это именно издержки самого цифрового режима. На этапе кодирования двоичного текста в кодировке Base10 издержки отсутствуют! Для кодирования тех же данных в Base10 требуется на 242% больше символов, но эти символы можно эффективно сохранить в QR-коде. При хранении не остаётся излишков. Давайте вернёмся к нашему URL-адресу, который был раньше: https://www.service.nsw.gov.au/campaign/service-nsw-mobile-app?data=eyJ0IjoiY292aWQxOV9idXNpbmVzcyIsImJpZCI6IjEyMTMyMSIsImJuYW1lIjoiVGVzdCBOU1cgR292ZXJubWVudCBRUiBjb2RlIiwiYmFkZHJlc3MiOiJCdXNpbmVzcyBhZGRyZXNzIGdvZXMgaGVyZSAifQ== Параметр “data=” содержит некоторый JSON в кодировке Base64. Это можно закодировать и получше, без JSON или base64, но давайте предположим, что нужно сделать именно JSON. Мы можем взять большой двоичный объект eyJ0I… и декодировать его в базовый JSON: {\"t\":\"covid19_business\",\"bid\":\"121321\",\"bname\":\"Test NSW Government QR code\",\"baddress\":\"Business address goes here \"}. Пропуск этих байтов через функцию b10encode даёт очень длинное число: 072685680885510189821994892577900638215789419258463239488533499278955911240512279111633336286737089008384293066931974311305533337894591404330656702603998035920596585517131555967430155259257402711671699276432408209151397638174974409842883898456527289026013404155725275860173673194594939. Вы можете спросить о других кодировках, таких как base16 (шестнадцатеричная), base32 и base36 (которые подходят для буквенно-цифрового режима) или base 8 (подходят для цифрового режима, но с ними легче работать, чем с base10). Они менее эффективны для цифрового режима, чем base10, но в некоторых случаях потенциально более удобны. С использованием Segno 1.6.1 можно увидеть что-то типа такого: В QR-кодах регистрации COVID Нового Южного Уэльса использовался уровень исправления ошибок H. Если взять это за основу и просто поменять кодировку параметра data, мы увидим результат из начала поста: Умение выбирать режимы, кодировки и сегменты позволяет удовлетворить ограничения формата QR-кода. QR-коды могут хранить до 23,6 килобит ≈ 3,0 КБ при использовании версии 40 с уровнем исправления ошибок L. Это соответствует примерно 7 тысячам десятичных цифр в числовом режиме или 3 тысячам байтов в двоичном режиме. Мы можем попробовать это на нескольких тестовых URL-адресах типа http://example.com/{encoded_data}. Максимум, который мы можем уместить в QR-код, выглядит так: 2,9 тыс. 7.0 тыс. Теоретически, гораздо более длинный URL base10 не имеет значения: благодаря QR-режиму он сжимается очень эффективно. Однако после публикации этой статьи было отмечено, что iOS неправильно сканирует огромный URL-адрес base10, читая http://example.com вместо http://example.com/310.... Значит, длина URL имеет значение! Похоже, что это ограничение длины отличается от обычных ограничений URL, поскольку 8 КБ — это разумный нижний предел для «нормальных» браузеров в наши дни, так что 7 тыс. символов должны быть безопасными. Вставка той же ссылки в Safari напрямую работает отлично! Так что лучше проведите собственное тестирование.",
    "94": "Каналы (channels, pipes) – это удобная абстракция для построения приложений, работающих в многопоточной среде. Они используются для передачи сообщений между потоками и, одновременно с этим, как средство синхронизации потоков. Я буду ссылаться на \"Go-style channels\", т.к. на мой взгляд, важная особенность каналов в языке GO – это возможность их мультиплексировать. Реализации каналов на языке C++, конечно же, есть, например, в библиотеке boost::fibers, можно найти реализацию двух видов каналов. В документации boost::fibers можно найти описания способов мультиплексирования, правда не самих каналов, но и к ним можно применить подобную технику. Реализация из boost не предлагает мультиплексирование каналов \"из коробки\" и никак не позиционирует себя на роль \"Go-style channels\", оно и понятно, представлен простой механизм передачи сообщений из одного fiber в другой. Предложенная техника мультиплексирования, которую можно применить и к каналам – это простая реализация \"в лоб\". Она состоит из запуска дополнительных промежуточных fibers по одному на задачу (или в нашем случае, на канал). Применение fibers для задачи мультиплексирования довольно затратно. Ещё одна реализация, моя первая строчка поиска в google \"go-style channels C++\" выдала такой результат. Библиотека использует перегруженный оператор << и >>, есть мультиплексирование, но выполнена через случайный опрос каналов в бесконечном цикле. А класс go::internal::ChannelBuffer содержит ошибку использования std::conditional_variable и поля std::atomic_bool is_closed; (обсудим это ниже). Обе реализации используют циклический буфер для хранения передаваемых сообщений. Продемонстрирую на примерах, что абстракция каналов – это нечто большее, чем просто циклический буфер с примитивами синхронизации. Сформулирую требования для идеальной реализации каналов: Мультиплексирование реализовано без использования \"тяжёлых\" сущностей. Под тяжёлыми сущностями я подразумеваю следующее: отсутствие использования динамической памяти. Дополнительные структуры данных, которые обеспечивают мультиплексирование, должны создаваться только по необходимости. Блокировка потока возникает только в случае отсутствия данных в канале и использует стандартные средства ОС (нет бесконечного цикла \"под капотом\"). Таким образом, канал может использовать lockfree-контейнер, при этом, объекты синхронизации не задействуются, пока в канале есть сообщения. Блокировка длится до тех пор, пока данные не появятся в любом канале (простой round-robin не подходит). Интерфейс канала допускает любую реализацию получения данных. В качестве простого примера представим себе канал, данные для которого, генерируются \"на лету\": Добавлю и одно ограничение, по умолчанию все каналы SPSC (single producer single consumer). Оно упрощает базовую реализацию. Пункт №2 требует отдельного пояснения. Идея заключается в следующем: объединить polling-подход с блокировками. У polling-подхода есть преимущество в эффективности и минимизации задержек, но его недостаток состоит в том, что при отсутствии данных бесполезно тратится процессорное время на постоянный опрос. Объединение подходов работает так: когда в канале (каналах) есть сообщения – использовать polling-подход, когда же сообщений нет, то использовать блокировку. Метод send реализуется схожим образом. Теперь подумаем над тем, как можно добавить возможность мультиплексирования. Мы пока проигнорируем требование не вызывать блокировку, если в канале есть данные. В данной реализации используется пара из std::mutex и std::condition_variable для синхронизации между потоками. Нам нужен класс, объединяющий наши каналы: Следующий шаг – проработать интерфейс у класса ChannelSelect. Напомню, что каналы могут передавать разные типы данных, поэтому простой интерфейс функции recv нам не подходит. В действительности, объединять каналы можно по-разному. Вызов callback для каждого канала, как в библиотeке ChannelsCPP. Пример использования: В примере, Select и Case – это классы из библиотеки. Метод recv возвращает комбинацию типов. Возможны два подварианта: Ожидать готовность всех объединённых каналов и возвращать std::tuple. Объединённый канал считается закрытым, если хотя бы один из его подканалов закрыт (схема \"и\"). Объединённый канал считается закрытым, если все из его подканалов закрыты (схема \"или\"). Подвариант 2 позволяет реализовать \"GO-style\" классы Select и Case. Нужно как-то сообщить каналу, что \"над ним\" кто-то есть. Мы не можем вызвать метод recv, т.к. заблокируемся. Добавим в канал дополнительное поле – указатель (например, на функцию), чтобы \"писатель\" разбудил \"читателя\" после добавления данных. В классе ChannelSelect есть своя пара std::mutex + std::condition_variable. Такая архитектура позволяет удовлетворить все требования из списка, кроме требования №2. Часто я вижу ошибку при использовании связки std::mutex + std::condition_variable – заменить некоторые (или все!) поля класса на атомарные переменные. Действительно, метод notify у класса std::condition_variable можно вызвать без блокировки. Но это не значит, что можно использовать std::condition_variable только для пробуждения другого потока! Проиллюстрирую проблему: \"Писатель\" поместил данные (одно сообщение) в пустой циклический буфер (пусть он будет lock-free) без захвата мьютекса. \"Писатель\" вызвал cv.notify_one(). Больше данных \"писатель\" не предоставит в течение долгого времени (или никогда). \"Читатель\" захватывает мьютекс и проверяет наличие данных в циклическом буфере, ничего не находит. \"Читатель\" засыпает на cv.wait(...). Представим, что хронологический порядок такой: 3, 1, 2, 4. Следовательно, \"читатель\" заснёт, даже если есть данные в буфере, и разбудить его сможет только \"писатель\", когда он добавит следующую порцию данных. Но \"писатель\" мог закончить свою работу, предоставив в буфер последние данные, тогда \"читатель\" заснет навсегда (deadlock), при этом в буфере остались необработанные данные. Пример ошибки можно увидеть тут. Семафор лучше подходит для реализации канала. Свойства семафора позволяют реализовать lock-free канал. Если в канале есть данные, то блокироваться необязательно. Однако, нам всё равно нужно использовать его каждый раз во время записи/чтения в/из канал/а. Требование \"не блокироваться\" (при наличии данных в канале) перетекает в реализацию семафора. Чаще всего, реализация семафора – это тонкая обертка над семафором ОС, а он, в свою очередь, представляет собой некий дескриптор ОС и соответствующие системные вызовы. Конечно, можно положиться на тот исход, что семафор реализован достаточно хорошо и не использует системный вызов ОС, если блокировка не требуется. Но, я решил не использовать его в чистом виде, а вызывать методы семафора только в случае необходимости. Всё, что нам нужно, это уведомить другой поток о том, что новые данные появились (или появилось место в буфере для записи). Для этого лучше всего подходит концепция событий. Объект события, обычно, работает через механизм подписки на событие. У нас есть ограничение SPSC (single producer single consumer), а значит, у события может быть только один подписчик. Представим объект события в виде простого класса: У класса есть единственное поле – signal, которое хранит указатель на объект подписчика. Вот его интерфейс: std::uintptr_t тут не просто так. Обозначим два состояния для события: сигнальное и несигнальное. Хранение сигнального состояния в отдельном атомарном поле слишком расточительно, для этой цели достаточно одного бита. Так как старшие биты указателей не используются, объединим бит-состояния с указателем. Тут мы проверяем, что событие ещё не произошло и, если это так, то читаем указатель с одновременной установкой самого старшего бита. Далее, вызываем у него notify(). Дополнительная проверка в условии handler & ~bit позволяет вызывать emit из разных потоков. Метод универсальный, мы можем передать валидный указатель для подписки, и nullptr для отписки. Во втором случае, нам пригодится возвращаемое значение – это количество \"событий\", которые успели произойти (или точно произойдут в будущем: emit успел установить бит сигнального состояния, но ещё не успел вызвать notify) прежде, чем мы окончательно отписались. Если наш подписчик состоит из семафора (реализация по умолчанию), то нам необходимо дождаться всех событий до того, как разрушить его. Метод сбрасывает флаг события, аналогично методу subscribe, возвращает количество событий, которые успели произойти (или точно произойдут в будущем). Все методы атомарные и неблокирующие. Метод emit вызовет notify, который, в свою очередь, задействует семафор. Важное свойство объекта событие – это то, что повторные вызовы emit только читают атомарную переменную и ничего не делают до тех пор, пока событие вновь не будет сброшено. Тут ничего сложного. Единственный нюанс – метод wait принимает некоторый счётчик. А именно, количество ожидаемых событий, т.е. сумма того, что возвращают методы subscribe и reset. Обосную необходимость счётчика. Когда события переходят в сигнальное состояние, \"подписчик\" не всегда вызывает wait, так в семафоре накапливается свой внутренний счётчик. В момент удаления объекта подписчика, он, конечно же, отписывается от объекта события. Но при этом, метод emit может всё ещё пытаться сделать notify. Теперь-то и пригождается счётчик событий. Перед разрушением \"подписчик\" ожидает все накопленные в семафоре события. Так мы обеспечиваем полную безопасность метода emit и разрушение экземпляра класса Handler. Объект события работает в паре с любым объектом, который может изменять свое состояние, и об этом надо уведомить другой поток. Пока всё выглядит также, как работа с парой std::mutex и std::condition_variable. Вызвать emit. Важное отличие от std::condition_variable – для изменения состояния не требуется захватывать std::mutex. Перейти на шаг 3. Отписаться от события. Пункт 3, можно повторять сколько угодно раз. Объект Handler можно переиспользовать, поэтому к пункту 4 мы переходим тогда, когда выполнили всю работу. Пункт 4 сложнее: недостаточно просто вызвать subscribe(nullptr), необходимо учесть количество произошедших событий, для которых не был вызван wait. Напишем класс, упрощающий работу с объектами событий и подписчиками. Он дополнительно защитит нас от ошибки \"забыть отписаться\": Мы обернули метод wait у класса Handler и метод reset у класса Event. Класс Subscriber реализует идиому RAII и инкапсулирует работу со счётчиком произошедших событий. Как нам теперь объединить несколько объектов событий в один? Довольно просто, но нужно оговориться, что такой тип события переходит в сигнальное состояние тогда, когда хотя бы одно подсобытие переходит в сигнальное состояние. Все методы класса EventMux мы делегируем подконтрольным событиям. Метод poll, для возвращаемого результата, использует оператор логического \"или\". В случае reset и subscribe суммируем возвращаемые значения. Единственное отличие – это отсутствие метода emit, для класса EventMux он не нужен. Каналы могут быть для чтения или для записи, либо и то, и другое. Интерфейсы у них очень похожи, поэтому я использую префиксы \"r\" (recv) и \"s\" (send) для схожих методов. Общий интерфейс канала для чтения можно представить так: rpoll – опросить канал на предмет новых сообщений. true – если канал готов к чтению, false – если в канале ничего нет. urecv – прочитать (или получить) следующее сообщение. Суффикс \"u\" – обозначает \"unsafe\", этот метод нельзя вызывать, если предварительно не был вызван rpoll, который вернул true. close – закрыть канал. Сообщения, которые ещё остались в канале, останутся доступными для чтения. После вызова этого метода, rpoll всё ещё может возвращать true, если что-то осталось непрочитанным. closed – думаю, тут все понятно. revent – возвращает ссылку на связанный объект события. В дополнении к этим методам, интерфейс должен предоставить два типа: Type – тип сообщения. Метод urecv должен возвращать объект именно этого типа. REvent – тип привязанного объекта события. Метод revent должен возвращать ссылку на этот тип. В интерфейсе не используются виртуальные функции, потому что в данной реализации применяется (в основном) статический полиморфизм. Если есть статический полиморфизм, то вариант с виртуальными функциями ничуть не сложно реализовать. Оба интерфейса достаточно \"низкоуровневые\" для прямого использования. Просто так usend/urecv не вызвать, нужно убедиться, что spoll/rpoll вернул true, а если нет, то надо работать с объектом события, ссылку на который возвращает sevent/revent. Обычно, мы просто хотим вызвать что-то вроде channel.send(value) и заблокироваться, если в канале недостаточно места. Для этого сделаем класс-helper: Класс OChannel использует auto& self = *static_cast<Channel*>(this); и наследование от параметра шаблона, а сам параметр, называется Channel. Всё верно, это CRTP. Пример использования: Готово, теперь у нас есть удобный метод send. Для канала-читателя есть аналогичный класс. Заметим, что метод хоть и удобный, но неэффективный, он каждый раз создаёт объекты Handler и Subscriber на стеке и использует их одноразово. Работа через итераторы решают эту проблему. Для выполнения требования №4 необходимо реализовать классы итераторов. Я решил добавить отдельный класс IRange и свободную функцию irange. Класс и функция шаблонные и работают с любыми каналами для чтения. Реализация неидеальная, представлю её целиком: Класс IRange хранит объекты Handler и Subscriber. Это значит, что подписка на событие происходит в самом начале, а отписка – только один раз в конце работы с итераторами. Разыменование итератора должно возвращать ссылку на сообщение, следовательно, сообщение нужно временно где-то сохранить. Используем класс std::aligned_storage_t для этой цели. Конструктор и деструктор сообщения вызываются только в определённые моменты, нет необходимости в дополнительных переменных или использовать класс std::optional. Теперь у нас есть всё необходимое для мультиплексирования каналов. Как было отмечено выше, мультиплексировать можно двумя способами. Класс сохраняет ссылки на объединяемые каналы в список std::tuple. У него есть свой объект события на основе класса EventMux и счётчик для реализации round-robin. Возвращаемый тип – std::variant<typename Channels::Type...>. Все каналы могут иметь разные типы сообщений. Если типы повторяются, то они всё равно будут представлены в std::variant под своим индексом. Простая реализация методов: close – вызывает close для каждого подканала. closed – вызывает closed для каждого подканала и возвращает true, если все из них вернули true. rpoll – вызывает rpoll для каждого подканала, но запоминает на каком остановился. Вызов rpoll в цикле использует массив указателей на функции для стирания типа: urecv – вызывает urecv у того канала, у которого функция rpoll вернула true. Тут тоже используется массив указателей для стирания типа: Класс IChannelAny соответствует интерфейсу IChannelInterface. Следовательно, его можно мультиплексировать с другими каналами. Почти всё то же самое, что у класса IChannelAny. Вместо счётчика используется поле std::bitset для каждого канала по биту – небольшая оптимизация, чтобы повторно не опрашивать каналы. Возвращаемый тип – std::tuple<typename Channels::Type...>. close – вызывает close для каждого подканала. Тут всё так же. closed – вызывает closed для каждого подканала и возвращает true, если хотя бы один из них вернул true. Класс IChannelAll соответствует интерфейсу IChannelInterface, а это значит, что его тоже можно мультиплексировать с другими каналами. Каналы (и события) реализуют паттерн компоновщика: мультиплексированные каналы, в свою очередь, тоже могут быть мультиплексированы и так далее, всё будет работать с любым \"уровнем вложенности\". Все требования к реализации удовлетворены: никакой динамической памяти, никаких дополнительных, сервисных (фоновых) или промежуточных потоков, корутин и т.д. Мы можем обернуть lock-free очередь в канал или сделать тонкую обёртку над функцией-генератором. Посмотреть реализацию можно тут. Архитектурные возможности библиотеки позволяют реализовать каналы и их мультиплексирование на любой вкус. Приведу два примера. Предположим, нам нужно объединить несколько каналов по схеме \"или\", но все каналы передают один тип сообщения или разные типы сообщений, но все они могут быть преобразованы в один общий тип. При этом, \"читателю\" не важно из какого именно канала пришло сообщение. Можно использовать обычный IChannelAny, но тогда итоговым типом будет std::variant. Для такого объединения можно написать отдельный класс: Логика работы этого класса практически идентична классу IChannelAny, разница лишь в создаваемом типе сообщения. Канал для записи, который рассылает копии сообщений в другие каналы. Реализация тривиальна: По умолчанию входной тип вычисляется так std::common_type_t<typename Channels::Type ...>, но можно указать любой другой, главное, чтобы он мог преобразовываться в типы typename Channels::Type. Основа реализации – это класс события и подписчика. Событие может перейти в сигнальное состояние только через вызов метода emit. Данное ограничение не позволяет нам обобщить такие сущности, как файловые дескрипторы или сокеты. Предположим, мы хотим обернуть взаимодействие по сети в абстракцию канала. Создавать сервисный или промежуточный поток – неприемлемо. Что можно с этим сделать в будущем: Добавить новый тип сообщений – тонкая обёртка над объектом ОС. У такого класса не будет метода emit; \"Обычные\" события тоже объединяются, к примеру, в eventfd со своим дескриптором. Метод emit у таких событий будет пробуждать поток через eventfd. Пока общую картину сложно обрисовать, но интуиция подсказывает, что это более, чем возможно. Дополнительные сложности возникают с каналами с динамическим полиморфизмом, необходимо решить, какой тип события будут возвращать методы revent/sevent. Мы можем реализовать классы каналов, преобразующие один тип сообщения в другой, через пользовательскую функцию. Реализация тривиальна, но это работает, если преобразование происходит один к одному. Но иногда, возникает потребность преобразования многие к одному (или один во многие). Например, из некоторого канала приходят буферы байтов (std::span<std::byte> или std::string_view), мы можем написать функцию, которая парсит такие данные и возвращает другой объект, например, json-структуру. Хорошо, если у нас такой парсер поддерживает потоковый интерфейс ( boost::json::stream_parser). В ином случае, нам потребуется промежуточный поток (или stackfull корутина), задача которого, заключается в одном: парсить множество объектов в один и передавать дальше. В C++20 есть базовая поддержка stackless корутин. Интеграция в библиотеку позволит реализовывать сложные преобразователи с минимальными накладными расходами. Генераторы (требование №5) станут более органичными, одна C++ корутина может описать бесконечные и конечные генераторы, и всё это можно завернуть в интерфейс канала. При этом, не нужен пул потоков для выполнения корутин, все они могут выполняться в рамках одного потока, который читает или пишет в канал. На данный момент каналы для записи не поддерживают итераторы. Мы можем их мультиплексировать, но не смешивать с каналами для чтения. По крайне мере, пока такой функционал не разработан. Идея в том, чтобы появилась возможность написать так: Таким образом, тело цикла будет выполняться только тогда, когда оба канала готовы: во входящем канале есть новое сообщение, а в выходящем – место для отправки. Мы можем мультиплексировать сколь угодно много каналов в один, но менять количество каналов динамически нельзя. Это исправляется добавлением отдельного класса мультиплексора событий и каналов. Такой канал принимает на вход контейнер (например, std::vector или std::span) с объединяемыми каналами. Этот класс сможет работать только с каналами с одним типом сообщений, и объединяемые каналы должны быть с динамическим полиморфизмом. Перед каждым вызовом метода usend/urecv мы должны убедиться, что канал доступен, вызвав rpoll/spoll. Можно уменьшить количество вызовов rpoll/spoll, если они вместо bool будут возвращать количество доступных сообщений для чтения/записи. Это количество необходимо где-то хранить, так мы выиграем в производительности, немного увеличив потребление памяти. Это мой первый пост на Хабре. Любая критика, замечания, указания на ошибки и неточности строго приветствуются.",
    "95": "В России на разные виды деятельности нужны лицензии. Например, чтобы производить и продавать алкоголь и табачные изделия, требуется разрешение. Это касается и защиты конфиденциальной информации, разрешение на работу с которой выдает Федеральная служба по техническому и экспортному контролю (ФСТЭК России). Лицензии ФСТЭК России — это обязательное требование для компаний, которые оказывают услуги по защите информации или создают средства защиты информации ограниченного доступа, например ПО, которое обрабатывает конфиденциальную информацию. Такие лицензии необходимы и «Фланту», потому что мы разрабатываем Deckhouse Kubernetes Platform и другие продукты в рамках экосистемы Deckhouse, которые планируем внедрять в окружение клиентов, работающих с конфиденциальной информацией. Ещё лицензии ФСТЭК России нужны, чтобы сертифицировать средство защиты информации, в нашем случае — Kubernetes-платформу. Это даст нам право устанавливать её в окружение клиентов, которые работают с конфиденциальной информацией, она должна быть сертифицирована во ФСТЭК России. Если у нашего ПО не будет такого сертификата, мы не сможем оказывать услуги владельцам информационных систем, в которых использование сертифицированных продуктов является обязательным, например государственных информационных систем или информационных систем персональных данных. Для нас это важно, так как благодаря импортозамещению у нас появилась возможность оказывать услуги в новых областях. Как раз-таки первым и важным шагом на пути к этому будет получение лицензий. Если мы хотим сертифицировать во ФСТЭК свои программные продукты, у нас как у вендора должна быть лицензия ФСТЭК. Проще говоря, лицензия выдаётся компании, а сертификацию проходит продукт. В этой статье не будет захватывающих кейсов из практики наших SRE-инженеров. Мы погрузимся в бюрократический мир, с которым столкнулись при получении лицензий ФСТЭК России. Мы расскажем, лицензии каких видов бывают и как определить, какие из них необходимы. Также пройдёмся по основным требованиям к получению лицензий и рассмотрим, как мы приводили компанию в соответствие с ними и с какими проблемами столкнулись. секретной информации, составляющей государственную тайну, например сведений в области внешней политики и экономики, распространение которых может нанести ущерб безопасности государства. «Флант» оказывает услуги клиентам, работающим с конфиденциальной информацией, поэтому мы сконцентрировались на получении лицензий для защиты такой информации. К этому типу информации относятся две лицензии, которые мы и решили получить: на деятельность по технической защите конфиденциальной информации, так как мы планируем устанавливать, настраивать, сопровождать сертифицированное ПО и выполнять другие работы в области защиты информации. Лицензируемые виды деятельности, которые нужно проанализировать и выбрать те, которые потребуются при оказании услуг / выполнении работ. В лицензиях будет указано, на какие работы и услуги она распространяется. Требования, которым должна соответствовать компания. Они касаются штата, помещений, оборудования, документации. Подробнее об этом поговорим дальше. Нарушения, которых стоит избегать, чтобы получить лицензии. Например, нарушения, повлекшие за собой нанесение ущерба правам граждан, допустим, при утечке конфиденциальной информации. Список документов, которые нужно направить в лицензирующий орган для получения лицензий. Например, документы, подтверждающие, что штат компании соответствует требованиям постановления: приказы о назначении, копии трудовых книжек, дипломов, удостоверений, свидетельств. Изучить Постановления и документы в части лицензирования, опубликованные на сайте ФСТЭК России. Привести компанию в соответствие требованиям Постановлений и подготовить необходимые свидетельства о их выполнении. Подать заявления о предоставлении лицензии. Получить лицензию. Нам удалось получить лицензии за пять месяцев: мы начали процесс в феврале 2023 года, документы подали в конце мая, а получили лицензии в июне. В этой статье мы подробно рассмотрим нормативные требования, с которыми возникают основные сложности. Мы расскажем, как их решали, и дадим советы для избегания проблем. Чтобы получить лицензию, в штате должны быть специалисты, имеющие профильное образование, например «информационная безопасность» или «инженерное дело», и прошедшие переподготовку по направлению «информационная безопасность». Также у них должен быть стаж работы (обычно это или не менее трех, или не менее пяти лет) в области защиты информации. Например, чтобы в трудовой книжке были записи, которые соответствуют следующим профстандартам: специалист по информационной безопасности в кредитно-финансовой сфере. Указанные должности не обязательно должны называться именно так. Эти профстандарты установлены в 2023 году, поэтому записи в трудовых книжках могут отличаться, но должны соответствовать сведениям этих видов деятельности. Чтобы подтвердить, что у нас есть такие специалисты, мы заполнили соответствующую форму и приложили необходимые копии документов, подтверждающие квалификацию и стаж работы. Чтобы продолжать соответствовать лицензионным требованиям, нужно проводить повышение квалификации персонала каждые пять лет. Это возможно только в организациях, которые аккредитованы во ФСТЭК России в рамках осуществления образовательного вида деятельности и могут проводить соответствующие курсы по защите информации. Ещё для получения лицензии у компании должно быть аттестованное по требованиям защиты информации помещение, которое будет использоваться для обработки конфиденциальной информации. Для выполнения требований мы нашли и арендовали подходящее помещение, установили средства защиты (о них мы поговорим позже) и провели его аттестацию. Здесь стоит отметить, что не получится самостоятельно установить средства защиты и провести аттестацию. Чтобы выполнить эти работы, нужно обращаться в сторонние компании, которые лицензированы во ФСТЭК России. При заключении договора аренды нужно обратить внимание на следующие тонкости. Обычно арендодатели заключают договор аренды на 11 месяцев. Но так как у нас бессрочные лицензии, срок аренды должен быть больше. Если отправить в составе документов на получение лицензии копию договора аренды, в котором указан срок 11 месяцев, ФСТЭК России может отклонить заявление, так как не понятно, где компания будет находиться через 11 месяцев. При этом по Гражданскому кодексу, если договор аренды заключается более чем на 11 месяцев, он должен регистрироваться в Росреестре. А это длительная процедура, что нам не подходило, и не все собственники помещений готовы на заключение договоров с регистрацией в Росреестре. Поэтому нам нужно было заключить договор с преимущественным правом продления, так как он подходит под требования ФСТЭК России и его не нужно регистрировать. В итоге мы договорились с юристами собственника помещения о таком виде договора. Поэтому, если у вас помещение не в собственности и нет регистрации договора аренды в Росреестре, обратите внимание на условия договора аренды и его продления. Перечень технической документации, национальных стандартов и методических документов. Её нужно заказывать в Росстандарте и во ФСТЭК России с пометкой ДСП (для служебного пользования). Часть документации открытая, она есть в интернете. Но есть и документация, которая имеет ограничительные пометки, так как по мнению ФСТЭК в ней содержится информация, которая не подлежит публичному распространению. Она платная и её нужно заказывать, поэтому это занимает определённое время. Мы писали письма во ФСТЭК России о том, что просим обеспечить нас документацией, и они уже передавали наше письмо в свой подведомственный институт ГНИИИ ПТЗИ. Институт выставлял нам счёт на оплату, и после оплаты мы уже получали документы. А в Росстандарте мы заказывали ДСП ГОСТы, которые мы тоже получили после оплаты. На первый взгляд всё кажется просто, но это не так. Например, в перечне литературы есть ГОСТ, который отменён Росстандартом. При этом ФСТЭК России свой перечень литературы не обновил, и данный ГОСТ там числится. Но купить его невозможно, потому что он уже отменён. В этом случае нужно написать официальное письмо в Росстандарт и включить в перечень заказов этот ГОСТ. На это ответят, что выставят счёт на два ГОСТа, а на третий — отменённый — не смогут. В итоге нужно оплатить два ГОСТа и предоставить свидетельства приобретения документов во ФСТЭК России. К ним прикладывается копия товарной накладной о том, что приобретены два ГОСТа, а также письмо-запрос и ответ Росстандарта о ситуации с третьим ГОСТом. Если нет доказательства, что отменённый ГОСТ запрашивался, то ФСТЭК может «развернуть», так как он указан в перечне и его тоже нужно заказывать. В общем, на каждый подобный случай должно быть обоснование, почему ты это сделал, и подтверждение, что ты это купил либо хотел купить, но тебе не продали. Сложности могут возникнуть и со средствами защиты. Они нужны, чтобы осуществлять лицензируемые виды деятельности. На сайте ФСТЭК России есть перечень этих средств: Перечень контрольно-измерительного и испытательного оборудования, программных (программно-технических) средств. Например, у компании должна быть аттестована автоматизированная система, с помощью которой будет обрабатываться конфиденциальная информация. На технических средствах автоматизированной системы должны быть установлены средства защиты информации, такие как средство защиты от несанкционированного доступа, средство антивирусной защиты, средства анализа защищённости и другие. Для создания самих средств защиты информации должны быть приобретены средства для проведения анализа исходных текстов ПО, среды разработки (IDE), средства проектирования и так далее. При этом на сайте ФСТЭК России не указываются вендоры и средства защиты, которые нужно приобрести, а только классы средств и для некоторых изделий их характеристики: для лицензии на разработку и для лицензии по защите конфиденциальной информации. На основании этих данных компания выбирает, какие программные и программно-технические средства нужно купить. Также эти средства нужно покупать только у официальных дистрибьюторов либо вендоров, то есть у тех, кто может официально продать эти решения. Например, нам нужно было приобрести статический анализатор исходного кода, который проверяет исходный код и указывает, где могут быть опасные конструкции. И этот анализатор должен быть сертифицирован. В отделе лицензирования к требованиям относятся формально: если средство есть, то оно должно быть официально приобретено и на это средство нужно предоставить товарную накладную или другой документ, подтверждающий, что оно было приобретено официально. Ещё в помещении должны быть средства, которые предотвращают утечку акустической информации. Они генерируют различные волны, в том числе белый шум, и предотвращают возможность прослушивания. Например, у нас установлен «Муссон» — система виброакустической защиты: Чтобы подтвердить, что наше помещение и автоматизированная система соответствуют требованиям к обработке конфиденциальной информации, нужно получить об этом заключение в форме аттестации. Для этого мы привлекали компанию, которая имеет лицензию на проведение аттестационных испытаний, и они давали нам заключение. Полный список таких компаний можно посмотреть в реестре лицензий на деятельность по технической защите конфиденциальной информации. Аттестацию могут проводить только те организации, у которых в столбце про виды услуг и работ указан пункт «г» — работы и услуги по аттестационным испытаниям и аттестации на соответствие требованиям по защите информации. Сами себя мы проверить не могли, потому что у нас не было лицензий и мы не планировали заявлять данные виды деятельности, так как это не является профильным бизнесом компании. В рамках одних из измерений выяснилось, что дверь, которая стоит в нашем помещении, пропускает все шумы, потому что она была сделана из бумажного наполнителя в виде сот. И несмотря на то, что мы вешали колонки для белого шума, всё равно слышимость по частотам позволяла прослушивать. В итоге нам пришлось менять дверь, и мы поставили сейфовую металлическую дверь, которая уже соответствовала всем показателям. Организации-лицензиаты должны поддерживать в актуальном состоянии технические и программные средства. Если лицензия на них закончилась, нужно продлевать её. Что касается аттестованных помещений и автоматизированных систем, нужно проводить инспекционный контроль о том, что требования продолжают соблюдаться. После выполнения всех требований мы сформировали лицензионные дела: заполнили специальные формы (по разработке и производству и по технической защите) и приложили соответствующие документы, например, о пройденных аттестациях, технические паспорта, копии платёжных документов по закупкам средств защиты. Эти документы передали во ФСТЭК России. Далее мы ожидали информацию об оформлении лицензий, которые нам в итоге одобрили и выдали. Получение лицензий ФСТЭК России может быть длительным и трудоёмким процессом. Компания должна быть хорошо подготовлена и соответствовать определённым требованиям, при соблюдении которых могут возникнуть сложности. В этой статье мы рассказали, в каких ситуациях могут возникнуть проблемы и как стоит действовать, чтобы избежать их. В июне 2023 года мы получили лицензии ФСТЭК. Для нас это не только обязательное требование законодательства, но и гарантия безопасности данных клиентов и партнёров. «Флант» успешно прошёл все этапы получения лицензии, что позволяет нам предоставлять высококачественные услуги в области информационной безопасности. Это важный шаг для дальнейшего развития и укрепления доверия наших клиентов, а также большой шаг к сертификации наших продуктов. В будущей статье мы расскажем о том, как получали сертификат на ПО Deckhouse Kubernetes Platform и как совмещали лицензирование и сертификацию, чтобы не затягивать эти процессы. Обновления Kubernetes-платформы Deckhouse версий 1.45–1.47: Istio в Community Edition и поддержка ALT Linux",
    "96": "Привет, меня зовут Павел Кардаш, я IT-архитектор в «Магните» и занимаюсь управлением мастер-данными. Ранее я уже рассказывал о лучших практиках при работе с мастер-данными. В этой статье хочу поделиться, чем грамотно построенные процессы управления мастер-данными могут помочь продажам. https://habr.com/ru/companies/navicon/articles/260927/ https://habr.com/ru/articles/324148/ https://habr.com/ru/articles/720738/ Если в магазине есть что-то особенное, это всегда дополнительный мотив для клиента купить именно там. Только наличия товара на складе или полке недостаточно: покупатель всё реже ищет товар «глазами». Информация о товаре должна быть доступна для поиска, но в лучшем случае особые характеристики указываются в текстовом описании. Даже если для интересующего свойства сделали отдельное поле, то его заполняют левой задней пяткой без гарантий достоверности. Давайте разберемся, почему так происходит и что с этим возможно предпринять. Как ни удивительно, я тоже покупатель и лично не раз сталкивался с проблемами поиска особенного товара на различных площадках. Например, всё большую популярность набирают оранжевые вина. Это вино из белых сортов винограда, но приготовленное по технологиям красного и от этого приобретающее золотистый оттенок и новые нотки вкуса. Но найти такую категорию или фильтр получилось только на ресурсах компаний, плотно специализирующихся на алкоголе. Другой пример: при поиске смесителя в ванную мне был важен тип переключения между изливом и душем. Этот функционал бывает реализован дезертиром, переключателем, встроенным в кран или поворотом излива. Но поискать смеситель по типу переключения не получилось: даже в специализированном интернет-магазине если фильтр и есть, то качеством заполнения свойств товара никто не занимается. В результате куплен тот товар, по которому нашлась информация в сети: обзор блогера или подробное описание у производителя. Когда специалистам по мастер-данным приходит запрос на добавление нового свойства, срабатывает реализация процесса по умолчанию: руки сразу тянутся добавить атрибут. Реже встречается практика, когда добавляемые атрибуты структурированы, но всё так же неотделимо связаны с карточкой товара. Добавлением товара обычно занимаются закупщики, но ассортимент канала продаж и сами продажи часто уже вне их ответственности. Разграничить ответственность владельцев информации на атрибутном уровне и обеспечить консистентное состояние свойств товара при приемлемых сроках заведения новой карточки — задача нетривиальная. В результате ответственность за весь состав навешивается на закупщиков, которые часто не замотивированы разбираться в тонкостях потребительских свойств и бороться за качество заполнения: у них и без этого хватает задач. Ещё потребительские свойства совсем не нужны, например, бухгалтерам и логистам. Приходится либо загружать лишнюю информацию в профильные системы, либо на интеграционном слое срезать лишнее. Если при этом интеграция сделана без учёта возможных расширений атрибутов, то при добавлении новых свойств «вздрагивают» связанные системы. Хорошее решение такой проблематики: выделение доменов мастер-данных, создание для разных доменов своих связанных сущностей с недублирующимися атрибутами. Организовать связанные, но не блокирующие друг друга процессы заведения и изменения свойств, принадлежащих разным доменам. Сформировать не конфликтующие наборы правил качества данных. Разнести по времени заполнение блокирующих характеристик на этапе закупки и на этапе включения в ассортимент. Ускорить процесс ввода новых позиций. Разделить ответственность подразделений за набор базовых (ключевых) характеристик и пользовательских свойств. Улучшить качество информации за счёт заполнения заинтересованными в результате сотрудниками. Если специфика компании требует (читай: если в компании не в курсе значения термина «омниканальность») — развести свойства товара для отдельных каналов продаж. При необходимости даже развести ведение карточки товара и потребительских свойств в разные системы. Не могу и не стану никому рекомендовать такую реализацию, но ситуации бывают разные. Раз упомянул логистов, стоит сказать, что у товара должно быть ровно одно логистическое свойство: связь с видами упаковки, в которой он транспортируется. Вес, габариты, количество единиц в коробке, условия перевозки (температура, хрупкость, вертикальная ориентация, возможность штабелирования) — это всё свойства упаковки. Логистика возит упаковку, не товар! Вообще придумана такая мощная вещь, как «Правила продажи»: сущность, связанная с товаром, рынком, объектом продажи. Правила могут иметь срок действия и дату начала. Их можно классифицировать, чтобы разделить правила бухгалтерского учёта продаж, правила взаимодействия с ГИС при продажах, временны́е и вре́менные ограничения продаж, правила выкладки товара в офлайн-магазине. Но это уже не мастер-данные, а настройки бизнес-процессов. Главное обеспечить для них связанность с мастер-данными и ни в коем случае не копировать них значения из карточки товара. Управление мастер данными редко имеет прямое прослеживаемое влияние на бизнес, достаточное для финансового обоснования эффективности переустройства модели данных или даже обновления средств автоматизации. Выстроить цепочку от модели мастер-данных до результатов продаж сложно и, если в компании нет выделенного бюджета «на модернизацию ИТ», то выявленные проблемы ложатся в ящик отложенным долгом. Цена ошибки проектирования модели растёт. Вместе с ней всегда растёт стоимость устранения, так как данные обрастают потребителями. Чтобы не попасть в этот замкнутый круг, есть принцип: Хорошее архитектурное решение не отрезает путь к новым возможностям, возникающим при росте бизнеса. А если ресурса на качественную проработку архитектуры мастер-данных не выделили, то «всё придумано до нас» – есть референсные модели данных, где учтены множество потребностей. Я всегда призываю сверяться с ними перед любыми изменениями. Например, для розничной торговли мне нравится очень хорошо проработанная модель от Association for Retail Technology Standards.",
    "97": "Всем привет, меня зовут Саша Комбаров, я CEO в веб-студии. Мы создали бота, который делится регламентами, которые полезны и агентской, и клиентской стороне. Сегодня расскажем про ежедневное планирование. Календарь помогает не промотать звонок, не забыть выполнить задачу, поставить задачу другому исполнителю. Ведение календаря при работе в команде – мастхэв. Каждый участник может поставить встречу в свободный слот, закинуть задачу с описанием, ознакомиться с задачами, которые вы выполняли. Запланировать долгосрочные задачи. Для того, чтобы ведение календаря повышало эффективность сотрудников, а не вызывало боль, мы составили регламент. Ссылка на него в конце материала. Рабочее время планируется самостоятельно в Google Calendar. Утром до стендапа необходимо запланировать задачи на день в календарь. Поставь в календарь регулярную задачу для этого; Планирование не должно занимать больше 10-15 минут. Расставляй задачи так, как планируешь их делать. Делай по порядку (по возможности), старайся не перепрыгивать с задачи на задачу. При одинаковых приоритетах выше ставь более сложные и объемные задачи — в конце дня морально сложней браться за сложные задачи; Те задачи, которые не планируешь делать в текущий день, переноси на следующие дни. Не теряй их; Задачи на день ставятся блоками с указанием в квадратных скобках времени, которое займет задача — [1h30m]. Чтобы максимально точно оценить время, которое нужно на задачу — декомпозируй; Если задача успешно выполнена, то она красится зеленым. Если по задаче что-то делалось, но она не доделана, она красится желтым. Если задача не была сделана, она красится красным и дублируется на время во время следующего планирования. Если задача занимает несколько рабочих дней, то нужно ее разбить на подзадачи. Например: нужно создать прототип, это займет 36 часов. Разбей задачу на отдельные подзадачи по каждой странице/блоку и планируй в календаре отдельными задачами: «[2h] Проект — Прототип. Страница \"Помощи\"» Необходимо планировать 7 часов своего рабочего времени, так как могут появятся срочные или незапланированные задачи — например, помочь команде, ответить на вопрос и т.д.; Обязательно ставь задачу «Обед», чтобы понятно было, когда ты собираешься на обед. В описание задачи добавляется ссылка на задачу в Kaiten, ссылки на документы, относящиеся к задаче, чтобы тебе или исполнителю было проще и быстрее ее решить. Или менеджеру понять суть задачи; Все контрольные точки, напоминания, письма планируй в календарь на будущие дни — не стоит рассчитывать на память, а потерянная задача может стоит дорого. Просто записывай задачи с постановкой, добавляй материалы, не планируя, в какое именно время дня ты будешь выполнять задачу — сделаешь это на ежедневном планировании; Если задача заняла больше времени, чем планировалось, оставляй в скобках время, которое ты на нее планировал и растягивай в календаре так, чтобы показать, сколько реально времени она у тебя заняла. Так ты будешь видеть, где ошибся в оценке, и сможешь в следующий раз оценить задачу точнее; Если задач не хватает на день, то закидывай задачи по изучению нового материала или сообщи куратору. Крупные задачи — провести тестирование, составить смету и КП, написать статью. Средние задачи — провести стендап с командой, обработать новые задачи от клиента и перенести в доску. Мелкие задачи — ответить на письмо (если не делаешь это сразу и быстро), проверить, пришла ли оплата. Регулярные задачи в течение дня — например, несколько раз позвонить клиенту, который долго отвечает, чтобы узнать подробности по задаче — ставь в верх календаря. Время не нужно указывать, т.к. ты будешь возвращаться к этой задаче несколько раз, и неясно, сколько она у тебя займет. Если задача общая (требует участия нескольких людей), то добавляй в «гости» к задаче все остальных по корпоративной почте. Найти адреса все исполнителей можно в этой таблице. После этого у всех задача продублируется в их календарях в это же время. При редактировании основной задачи, она также будет обновлена у остальных. Если необходимо поставить задачу другому исполнителю, то выбираем его из списка. В задачу другим исполнителя точно так же пишем время на выполнение этой задачи. Если не знаешь, сколько займет, то получил оценку у исполнителя. В задачу тоже закидываем ссылку на Kaiten с постановкой задачи. Задачу другим исполнителям ставим минимум на следующий день. На текущий не закидываем. Учись планировать необходимые ресурсы заранее. Нереалистичные оценки. Важно проанализировать, какие задачи отнимали времени больше запланированного. Вспомнить, почему они заняли больше времени, какая возникла проблема, что не было учтено. Системная ли проблема. Можно ли с этим что-то сделать. Сильно большие задачи (больше 4ч) необходимо декомпозировать. Оценивать необходимо объективно исходя из своих возможностей. Кто-то быстро в уме складывает, кто-то пишет, а кто-то анализирует большие объемы данных. Написать ТЗ на 20 листов за день не каждый сможет. Но если оценки двух компетентных специалистов отличаются в 2-3 и более раз, то необходимо разбирать постановку и процесс выполнения будущей задачи — вероятно, понимание задачи у специалистов разное; Частые переключения. Давно доказано, что переключение на другую задачу даже на 1 минуту или просто ответить в мессенджере отнимает не менее 10-15 минут продуктивного времени. Отлично помогает метод помидора и его производные. Проверить почту или мессенджеры можно между задачами, а на рабочий цикл при выполнении задачи включать режим «Не беспокоить». Кому очень надо - тот подойдет.При прочих равных лучше закрыть одну задачу целиком, чем получить на следующий день две выполненные наполовину. Принцип Парето здесь не работает; Незапланированные задачи. Важно понимать, что большая часть прилетающих задач в течение дня — менее приоритетны, чем выполнение дневного плана. Новая задача – это срочная и важная, чтобы сделать сегодня (см. матрицу Эйзенхауэра)? Ее можно делегировать? Если задача не терпит переноса на завтра, то вполне вероятно на нее можно переключиться между другими задачами либо вечером, а не прямо сейчас; Переутомление. Попытка сделать все и сразу часто приводит к общей усталости и низкой эффективности. Симптомами могут быть: неосознанные переключения, слабая концентрация, проблематично въехать в предметную область. Причиной может быть: частые задержки по вечерам, отсутствие отдыхов в течение дня, постоянная сверхвысокая нагрузка. Помним, что невыполненный план оказывает негативное влияние на самооценку и впечатление от проделанной работы в течение дня. К тому же невыполненные задачи имеют свойство лавинообразно накапливаться. «Как привести дела в порядок. Искусство продуктивности без стресса», Дэвид Аллен. «Жесткий тайм-менеджмент. Возьмите свою жизнь под контроль», Дэн Кеннеди. «Максимальная концентрация. Как сохранить эффективность в эпоху клипового мышления», Люси Джо Палладино. Ссылка на регламент. Этот и другие регламенты уже доступны в Телеграм-боте. Напишите ему и он пришлет документы!",
    "98": "На самом деле фантастика уже давно стала реальностью. Теперь человечество (правда, не все) думает о строительстве городов не в контексте «где бы нам всех их расселить», а ради более глубинных целей. Например, спасти экологию планеты, разгрузить движение в ближайших перенаселенных городах. Ну и, конечно, прорекламировать свое государство и отправить всем послание: «У нас лучше, чем у других». Об одном из таких проектов уже недавно писали на Хабре. Тут предлагаем кратко разобрать 10 амбициозных проектов, которые должны реализовать в ближайшем будущем. В этой статье уже хорошо всё описали, но кратко мы повторимся. Что за проект? Главная проблема большинства старых городов вроде Парижа или Рима, привлекающих миллионы туристов — то, что в них легко запутаться. Потому что их застраивали столетиями и не думали о планировке и удобстве. Проект саудитов по-настоящему амбициозен: почему бы не растянуть город в длину? Планируется, что он растянется почти на 170 км и пройдет сквозь пустыню прямо к морю. Высота этой «линии» составит 500 метров, а ширина — почти 200 метров. По сути город будет представлять собой два параллельных здания-небоскреба, в которых смогут разместиться до девяти миллионов человек. Зеркальные стены подчеркнут уникальность объекта и будут дополнительно отражать солнечный свет. Одна из главных фишек — в городе не будет автомобильных дорог. Совсем. Значит, нет никакого загрязнения окружающей среды и проблем со смертностью на дорогах. Вместо этого люди будут передвигаться на высокоскоростных поездах. Планируется, что они будут развивать скорость до 480 км/ч — из конца в конец можно будет преодолеть «Линия» всего за 20 минут. Жители города при этом будут иметь шаговую доступность к магазинами и зеленым зонам (правда не очень ясно, как будут решать вопрос с доступом к солнечному свету). Саудовская Аравия стремится показать всему миру, что она не просто сидит на огромной бочке с нефтью. Поэтому принципиальным моментом станет использование только возобновляемых источников энергии (например, солнечных панелей — уж солнца в пустыне с избытком). Можно сказать, что строительство такого города — больше репутационная штука, чем необходимость. Объем инвестиций? 500 млрд. долларов Какие есть сложности? Их много. Например, существуют политические факторы, которые могут влиять на доступ к различным технологиям или другим ресурсам. Но другая общая проблема этой стройки — это дефицит... песка. Да, все верно. Для строительства нужен бетон, а для него песок из пустыни не годится. Поэтому Саудовская Аравия покупает песок в Австралии — там как раз нужного прибрежного песка с избытком. Однако на строительство той же «Линии» песка потребуется столько, что в мире может возникнуть его дефицит. Но тут уж, как говорится, поживем — увидим. Что за проект? Продолжим про идеи саудитов. Город-стена — не проблема. Почему бы тогда не выполнить задачу попроще и построить небоскреб выше 1000 метров, заодно обойдя конкурентов из ОАЭ с их небоскребом Бурдж-Халифа высотой 828 метров? Потом вокруг этого можно создать настоящую агломерацию. Башню Джидда проектировал тот же архитектор, что и Бурдж-Халифу — Эдриан Смит. В самой башне будет 167 этажей с суммарной площадью 530 000 м2. При этом здание планируется сделать многофункциональным, что должно стать магнитом для бизнеса и туризма: вплоть до 156 этажа будут располагаться гостиницы. На 157-м этаже установят самую большую смотровую площадку в мире, обращенную на Красное море, а также самую высокую обсерваторию. Чтобы люди могли свободно перемещаться по этажам, не отстаивая огромные очереди, в Джедде будет 69 лифтов (некоторые даже будут двухэтажными). Объем инвестиций? 20 млрд. долларов Какие есть сложности? Ну собственно, сложность одна — стройка в 2018 году была остановлена из-за того, что наследный принц устроил чистку в правительстве и собственной семье. На сегодняшний момент есть только 71 этаж из 167. В прошлом году объявили о продолжении стройки и в данный момент определяют кто продолжит возводить небоскреб. Что за проект? Главная проблема прежней столицы Египта — его перенаселенность и транспортная загруженность. Как город Каир существует уже больше 1000 лет, и кардинальная перестройка для него — настоящая проблема. Нужно сносить много старых домов, строить на их месте новые, перестраивать всю инфраструктуру и где-то временно размещать жильцов (общее население Каира порядка 9 млн человек). Спрашивается — зачем? Можно же построить рядом новый современный город и переселить часть людей туда (кого не переселят — извините, не повезло, но вы держитесь). В 2015 году власти объявили о том, что будут строить новую столицу, которая вместит 5 млн человек. Генеральный план разработала архитектурная фирма SOM. В столице планируется разместить самую крупную мечеть в регионе, огромный парк (в 6 раз больше Центрального парка в Нью-Йорке), небоскреб Iconic Tower высотой 385 метров и самый высокий в мире флагшток (прежний рекорд сейчас — 202 метра). Кроме разгрузки старого Каира, власти планируют привлечь в страну больше зажиточных семей, потому что туризм — огромная составляющая ВВП страны. Объем инвестиций? 45-60 млрд. долларов Какие есть сложности? Во-первых, столь масштабные траты на постройку новой столицы означают, что старой никто заниматься не планирует. Стоимость квадратного метра бедные семьи не потянут — значит, им придется еще много лет жить в условиях ужасной инфраструктуры и десятилетиями не ремонтирующихся коммуникаций. Во-вторых, строительство новой столицы — процедура довольно рискованная. Дело в том, что не всегда жители и бизнес сразу оценят преимущества и поспешат перебираться туда. Именно так было с Бирмой, которая потратила 5 млрд на строительство новой столицы Нейпьидо — сейчас ее население меньше 1 млн человека при том, что площадь в 3 раза больше, чем Москва. Третий момент — это парк. Дело в том, что для такого огромной зеленой зоны потребуется большое количество пресной воды. В странах Африки она вообще в серьезном дефиците, поэтому есть подозрения, что часть воды заберут у жителей. В общем, на нашем веку мы это творение точно увидим — стройка уже вовсю идет. Что за проект? Нельзя сказать, что у Кувейта есть те же проблемы, что и у Египта с Каиром. Эль-Кувейт — город с довольно хорошо развитой инфраструктурой, поэтому ни о какой перенаселенности речи нет. Строительство «Города шелка» вблизи столицы больше имиджевая штука — планируется, что в городе будет проживать не более 700 тысяч человек (треть от населения Эль-Кувейта). То есть стройка — это гимн экономической мощи Кувейта: на секундочку, речь идет о 17-м месте в рейтинге богатейших государств мира. Ну и дополнительно Кувейт хочет привлечь бизнес, не связанный с нефтедобычей — стремления изменить отношение к ближневосточным государствам очень похожи на Саудовскую Аравию. По задумке это даст больше 200 000 рабочих мест Центр города будет представлять собой искусственный остров, окруженный рекой. Посередине расположится огромный небоскреб высотой 1001 метр (отсылка к «тысяче и одной ночи»), что позволит Кувейту обойти ОАЭ с их Бурдж-Халифой высотой 828 метров. «зеленый» район с парками для прогулок горожан. Уже вовсю идет строительство технологичного порта Мубарак аль-Кабир за 9 млрд долларов. В 2019 году был сдан мост Джабера длиной 48 км, соединяющий столицу с «Городом шелка». Объем инвестиций? 132 млрд. долларов Какие есть сложности? Их немного. По известной сейчас информации, проблемы пока носят больше политический характер. Город станет настоящей экономической зоной, и ей нужны инвесторы. Власти Кувейта заключили предварительное соглашение с Китаем о совместном управлении строящимся портом. Заодно предложили ряду китайских компаний переместить свои офисы в деловые районы «Шелкового города». Однако эти инициативы натолкнулись на серьезное сопротивление оппозиции в парламенте. Дело в том, что Китай «славится» притеснением уйгуров — тюркской народности, исповедующей ислам. Вот что на возможное сотрудничество сказал член партии исламистов: «Если они не прекратят убивать китайских мусульман, мы приостановим переговоры с ними по Шелковому городу». Конечно, вопрос может решиться просто: правительство не захочет терять деньги и закроет глаза на деликатные вопросы. Но это создаст ненужный прецедент, говорящий о том, что в Кувейте не соблюдают собственные законы. У многих компаний появится вопрос: «А стоит ли им доверять? Еще национализируют нас, когда прижмет». Но кажется, что эта проблема за следующие 25 лет строительства должна решиться. Что за проект? В 2006 году ОАЭ презентовали идею первого экологичного города будущего с нулевым углеродным выбросом, а в 2008 году приступила к строительству. Вот в чем кратко заключалась идея: Летом в ОАЭ очень тяжело жить — температура достигает 50 градусов, что резко отгоняет туристов. Чтобы решить эту проблему, архитекторы Foster + Partners предложили построить здания максимально близко друг к другу — так делали в ближневосточных городах, что снижало количество солнечного света на улицах и создавало прохладу. Дополнительно в центре установили ветровую башню высотой 46 метров — по задумке она должна всасывать более прохладный воздух сверху и распределять по нагретым улицам. Город будет потреблять электроэнергию, вырабатываемую солнечными электростанциями. Для этого на площади в 21 га установят 87 000 солнечных панелей. По задумке этого должно хватить для того, чтобы обеспечить нужды 50 000 жильцов и нескольких исследовательских предприятий (Масдар должен стать IT центром Ближнего Востока). Как и в случае с «Городом линией» в Саудовской Аравии, в «Городе солнца» не предполагается перемещение на личных автомобилях. Однако здесь будут не скоростные поезда, а автономные электрокары (PRT). Они должны будут передвигаться со скоростью до 40 км/ч — не быстро, но и сам город будет иметь площадь всего лишь 5 км2. Объем инвестиций? 22 млрд. долларов Какие есть сложности? В этом проекте их валом. Хотя бы тот факт, что в 2022 году (строительство стартовало в 2008 году) построен буквально один квартал — только 5% от запланированного. В нем располагается несколько домов, в которых живут студенты Института науки и техники Масдар (организован совместно с Массачусетским Технологическим), работники шестиэтажного офиса SIEMENS на Ближнем Востоке и обслуживающий персонал. Всего чуть больше 300 человек. Во многом на такие темпы строительства повлиял мировой кризис. Но все-таки большую роль сыграл тот факт, что сделать 100% экологичный город просто невозможно при современном развитии технологий (в 2006 году про это никто не думал). Поэтому есть ощущение, что и к 2030 году город построен не будет. Почитать подробнее, как обстоят дела со строительством Масдар, можете в этой статье. Что за проект? Переходим уже к более фантастическим сценариям. Проблема перенаселенности, особенно актуальная для Индии и Китая, также затрагивает и Японию — в ней проживает 126 миллионов человек на площади 380 тыс. км2 (меньше Республики Коми). У строительной компании Shimzu появилось решение — почему бы не использовать океан, покрывающий большую часть поверхности планеты? Ведь можно построить город прямо в воде! Проект назвали Ocean Spiral из-за конструкции. Близко к поверхности будет располагаться огромная сфера диаметром около 500 метров, способная вместить до 5000 человек. В самой сфере будут располагаться жилые комнаты, гостиницы, магазины и вся нужная инфраструктура для жизни. Воздух будет браться с поверхности, при этом ураганы или цунами грозить сфере не будут (в отличии от жителей прибрежных регионов Японии). Но сфере нужна, как минимум, электроэнергия. Не тянуть же ее с берега? Поэтому японские инженеры предложили использовать принцип преобразования тепловой энергии океана при помощи ОТЭС (океанические тепловые электрические станции). По сути, океан представляет собой неиссякаемый источник энергии, поскольку его поверхность постоянно подогревается солнцем. Разница температур на поверхности и глубине 3-4 км может достигать 20 градусов. Соответственно глубина может быть разной, поэтому сфера и модуль соединяются гибкой спиралью длиной 15 км. В ней проходят коммуникации, а также могут перемещаться люди. Станция имеет опреснительные установки, а питаться можно той же рыбой. Объем инвестиций? 26 млрд. долларов Какие есть сложности? Идея очень крутая, однако больше походит на произведение Жюля Верна. Тем более технологий, которые позволяет реализовать эту задумку, в ближайшие лет 10-15 ждать не стоит. Что за проект? По прогнозам, к 2050 году Индия обгонит Китай по числу жителей, достигнув отметки в 1,7 млрд человек. Уже сейчас она сталкивается с огромными проблемами в плане логистики и инфраструктуры, да и высокая плотность населения способствует росту инфекционных заболеваний. Поэтому сейчас в Индии идет настоящий строительный бум, призванный решить проблему и распределить проживание более равномерно. Одним из них стал международный финансово-технический центр Гуджарат (Gujarat International Finance Tec) — сокращенно GIFT. Его главная фишка в том, что это будет первый «умный» город. Условно говоря, управление светом, водой и канализацией станет возможным с помощью смартфонов (правда автор полагает, что поиск смартфона для смыва воды в унитазе — это то будущее, которое мы заслужили). Мусор будет собираться при помощи вакуумной системы, а система охлаждения будет централизована. На площади всего в 15 км2 построят небоскребы на 25 000 умных квартир. Самым заметным из них станет центральная башня высотой около 400 метров. Главная задача Гифта — стать новым финансовым центром Индии. Например, Международный валютный фонд и ООН планируют открыть свои представительства и поддержать Индию в своем стремлении, Bank of America и IFC уже это сделали. Объем инвестиций? 7,5 млрд. долларов Какие есть сложности? Не замечено. Строительство идет полным ходом — индусы молодцы. Где планируют построить? Столица Шри-Ланки, г. Коломбо Что за проект? С проектом города под водой мы уже знакомы — та еще фантастика. А вот строительство города на поверхности воды — что-то новенькое. Именно так посчитало правительство Шри-Ланки и решило расширить площадь прибрежной зоны столицы. Для этого решили создать искусственную насыпь площадью 269 (!!!) га из песка и специальных смесей. Чтобы реализовать инициативу обратились к компании China Harbour Engineering Company. Китай инвестировал в первую стадию 1,4 млрд долларов и взял на себя большую часть работ по созданию насыпи. Работы начались в 2014 году и закончились в 2023 году, спустя 9 лет. Вся новая область тут же была передана Китаю в аренду сроком на 99 лет. Теперь на новом полуострове начнут возводить коммерческие здания, а всю область объявят особой экономической зоной (ОЭЗ) и предоставят налоговые льготы бизнесу. По задумке это способствует привлечению в экономику Шри-Ланки иностранного капитала (разумеется, по большей части китайских компаний). Объем инвестиций? 14 млрд. долларов Какие есть сложности? Китай преследовал далеко идущие цели. Во-первых, получить промежуточный порт в рамках проекта «Один пояс, один путь» и установить прочные экономические связи с развивающимися рынками Индии, Африки и Ближнего Востока. Во-вторых, продемонстрировать всему миру преимущества сотрудничества с Китаем. «Смотрите, что мы создали меньше, чем за 10 лет. Хотите также?». экологов — мол, нужно изучить воздействие новой насыпи на морскую экосистему. Но как водится, никто их вопли всерьез не воспринял. Индии, потому что усиление влияния Китая в регионе вызывает тревогу. Это уже посерьезнее, потому что Шри-Ланка — ближайший сосед и очень зависит от индийского импорта. Как скажется передача земли Китаю на отношениях Индии и Шри-Ланки, покажет время. Но пока кажется, что беспокоится не о чем. Что за проект? Экологичный город-утопия, который задумал создать миллиардер Марк Лор — бывший директор компании Walmart. Концепцию в 2021 году разработала датская компания Bjarke Ingels Group. Весь город будет располагаться на площади 610 км2. В строительстве будут использоваться только экологичные материалы (какие, не уточняется). Передвигаться по городу люди будут, как водится, на велосипедах или электрокарах — никаких дизельных или бензиновых автомобилей. По небу будут летать воздушные такси — так называемые eVTOL (electric vertical take-off and landing). В центре комплекса будет красоваться огромная башня, которую миллиардер называет «маяком города». Это будет настоящее чудо инженерной мысли: автономность здания будут обеспечивать солнечными панелями на крыше. Там же будут выращиваться растения на принципе аэропоники и храниться вода путем отбора и фильтрации конденсата и осадков (никого не смущает, что действие происходит в пустыне). Но город не зря называют утопическим. В основе идеи Марка Лора лежит книга Генри Джорджа «Прогресс и бедность», написанная еще в 1879 году. Автор предполагает, что проблема капитализма — в частной собственности. Если бы вся земля в городе управлялась неким фондом, то прибыль от ее сдачи можно было бы направить на социальные нужды: транспорт, медицину и образование. Подробнее с идеей можно ознакомиться здесь. Когда запуск? 2030 год — 50 тыс. человек, 2070 — 5 млн человек Объем инвестиций? до 400 млрд. долларов Какие есть сложности? Есть много вопросов к проекту Лора, который больше напоминает его детские мечты. Во-первых, утопические идеи уже пытались реализовать на большей части земного шара. Но настоящий коммунизм и общество «от каждого по способностям, каждому по потребностям» так никогда и не было построено. Да, речь в проекте Лора идет о модернизированном капитализме. Но по большому счету, утопия и есть утопия — она либо не жизнеспособна, либо ее просто не дадут реализовать. Во-вторых, все это просто проблематично реализовать. Кто захочет вложить 400 млрд в такой амбициозный проект? Допустим, лет через 50-70 может быть. Но чтобы в 2030 году в городе уже жили 50 тыс. человек? Кстати, счастливчиков будут выбирать в лотерее. Главное, чтобы они знали про участие в ней. Где планируют построить? В 56 км от г. Остин, штат Техас Что за проект? Информацией поделилась только газета Wall Street Journal, и новость растиражировали другие СМИ. Насколько оригинальному сообщению WSJ можно доверять, пока неясно. Но по слухам Маск планирует построить город для сотрудников его компаний SpaceX, Tesla и Boring Company и сдавать жилье с хорошими скидками (порядка 800$ в месяц). Рядом как раз располагаются крупные объекты Space X и Boring Company — это даст возможность сотрудникам быстрее добираться до работы. Строительство планируется на земле площадью 14 км2, которую Маск скупал в течение нескольких лет. Когда запуск? Неизвестно. Объем инвестиций? Неизвестно. Какие есть сложности? Неизвестно. Но зная господина Маска, сложности он себе придумает даже одним твитом. Делитесь в комментариях, в каком из этих городов вы хотели бы жить в будущем? Либо предложите свой вариант.",
    "99": "Привет, это Влад Силантьев, основатель агентства по рекламе в Tg Ads. Хочу немного поторопить вас с запуском рекламы в Telegram. По-хорошему, с Telegram мы тоже уже опаздываем. Идеальное время, чтобы раскачивать каналы было пару лет назад. Но еще можно разобраться, как работает Tg Ads, и привести подписчиков по адекватной цене. Иначе еще через год мы будем с грустью вспоминать об этом времени — как сейчас вспоминаем о подписчиках во ВКонтакте по 50 копеек. Telegram Ads — это внутренний рекламный кабинет Telegram. Если запустить в нем рекламу, плашка с вашим креативом появляется под последним постом в разных каналах — о том, как их выбрать, я расскажу чуть позже. По этой ссылке подписчики смогут перейти к вам на канал и даже на внешний сайт. Сейчас, в феврале 2024 года, CPM (Cost per mile или стоимость за тысячу показов) в Telegram примерно 350 ₽. Это дешево. К примеру, цены за рекламу в Яндекс Директе или во ВКонтакте раздулись уже настолько, что в первые месяцы после запуска она может просто срабатывать в ноль — на окупаемость выйти непросто. Поясню, как происходит рост цен. Цены на рекламу устроены по принципу аукциона — кто больше платит, того чаще показывают. Так что цены за показы растут всегда: одни ставки перебивают другие. Условно, если бы на площадке работали только два рекламодателя, они по очереди повышали бы ставку на 1 копейку. А если рекламодателей 1000, за это же время цена на рекламу вырастет вместо 1 копейки на 10 рублей — ставку будет повышать каждый из них. По сравнению с ВКонтакте или Яндекс Директом, рекламодателей в Telegram еще не так много, поэтому цена за просмотры относительно невысокая и поднимается медленно. Но с запуском рекламы лучше не затягивать — новые рекламодатели приходят из других соцсетей в Telegram каждый день. Высокие цены на других площадках заставляют рекламодателей приходить в Telegram. Это неизбежный процесс миграции бизнеса с площадки на площадку — как перенаселение. Рекламодатели уходят из-за высокой цены, но своим появлением поднимают цены на новом месте. Конкуренция за внимание подписчиков тоже растет: за 2023 год среднесуточные охваты в Telegram выросли с 47,2 млн до 56,9 млн — это больше, чем во ВКонтакте и даже на YouTube. После того, как в Telegram тоже случится перенаселение, появится новый тренд. Но пока что это адекватный вариант для рекламы. Подписчиков еще можно набрать быстро и недорого — к примеру, в прошлой статье я рассказал, как мы с нуля привели 24 000 подписчиков по средней цене в 48,7 ₽. Есть еще несколько факторов, почему сейчас многие идут в Telegram. Кто-то волнуется, что инcту все-таки заблокируют, поэтому надо успеть перевести часть аудитории. В Telegram появились сторис, пользователи стали проводить там больше времени. Скорее всего, между сторис со временем начнут показывать рекламу. Пока это еще в новинку, у первых рекламодателей шансов запомниться будет больше. Чаще всего рекламный кабинет на площадках устроен так: нужно закинуть на счет деньги, и они будут постепенно тратиться на показы. Во ВКонтакте или Яндекс Директе нет ограничений, с какой суммы можно запускать рекламу — буквально, можно попробовать закинуть в личный кабинет «сколько есть». В Tg Ads есть минимальный порог для старта. Если работать без посредника, это 2 млн евро — мало кто мог бы это себе позволить. Но есть реселлеры, которые позволяют заходить в Tg Ads от 1500 евро. За это они берут комиссию 15—30%, в зависимости от ниши. Если бизнес рекламодателя в России, нужно будет заплатить НДС — 20%. Получается, начать запускать рекламу можно с суммы примерно в 2000 евро. Может прозвучать пугающе, но на самом деле это большое преимущество. Если бы вход был от 1000 ₽, рекламировались бы просто все, кому это придет в голову. Как вы помните, по принципу аукциона чем больше рекламодателей, тем выше цена показа. Так что порог в 2000 евро пока еще защищает предпринимателей от высоких цен на рекламу. Важно: если кто-то еще не работал с рекламными кабинетами, можно представить, что все эти 2000 евро нужно потратить на запуск одного конкретного креатива. Но это совсем не обязательно. Обычно на эти деньги можно провести достаточное количество тестов, чтобы выбрать лучшие креативы и запустить сотни рекламных кампаний по удачным вариантам. На первый взгляд, можно подумать, что у Tg Ads не так много шансов привлечь внимание подписчиков — плашки с рекламой выглядят примерно одинаково. А у нас всех уже и так баннерная слепота, так что читатели могут заметить только что-то особенное. Чтобы люди реагировали на Tg Ads, Telegram добавляет новые инструменты. К примеру, сейчас на плашки можно добавить аватарку канала, картинку или эмоджи и поменять фон объявления. Все эти визуальные детали повышают CTR — конверсию в клики из показов, то есть подписчики замечают рекламу и переходят по ней. Помимо того, что Tg Ads — это в принципе сейчас недорого, есть еще несколько способов быстро подобрать нужную аудиторию для показов и сэкономить деньги. В чем суть: важно, чтобы реклама показывалась «нужным» людям, которых она потенциально может заинтересовать. Иначе деньги за показы будут тратиться впустую. Конечно, можно провести много тестов, чтобы подобрать аудиторию, но каждый тест — это тоже трата денег. Совсем без тестов не обойтись, но сэкономить можно. И пока эти способы работают. При этом показываться она может совсем в других местах. У популярных каналов стоимость показа выше, чем у небольших. Но обычно рекламодателю не принципиально показываться именно в большом канале — можно просто догнать его аудиторию в канале поменьше. Предприниматель хочет прорекламировать службу доставки в своем городе. В городе есть канал-афиша с большим количеством подписчиков. Показы в нем стоят дорого. Предприниматель может запустить рекламу своей доставки на тех же людей в каналах подешевле — к примеру, кто-то ее увидит в «Клубе любителей утренних пробежек», в котором только 1000 подписчиков и показы дешевые. Один из вариантов, как выбрать, кому показывать рекламу — таргет по интересам. Но если задавать только один параметр, может получиться много нецелевых показов. Деньги за них будут списываться просто так — без переходов. Чтобы сузить выборку для показов, в Tg Ads можно задать сразу несколько интересов. Школа языков разработала курс делового английского. Она запускает рекламу на пользователей с интересом «Иностранные языки». Это не очень эффективно: показы тратятся на школьников, которые подписаны на группы для подготовки к ЕГЭ, или на путешественников, которые учат необходимый минимум для общения. А вот если указать интересы «Иностранные языки», «Бизнес», «Marketing & PR», целевых показов будет больше. Недавно в Telegram появилась автоматическая подборка «Похожие каналы». Если человеку интересна тема, ему теперь не обязательно искать больше каналов по ней самому — можно просто кликнуть на подборку и посмотреть, какие варианты предлагает сам Telegram. Этот же принцип можно использовать для настройки рекламы — указать для показов каналы, похожие на свой, или на любой другой нужный. Маркетинговое агентство может показывать свою рекламу на каналах других агентств — вдруг их подписчики еще только выбирают подрядчика или захотят попробовать разные варианты. А теперь этих конкурентов еще и не нужно искать вручную. Рассказываю про Telegram Ads и цифры в бизнесе.",
    "100": "Это 2024 год, и GraphQL на подъеме, чтобы стать важным игроком в экосистеме API. Это идеальное время, чтобы поговорить о том, как сделать ваши GraphQL API безопасными и готовыми к производству. Итак, вот мой тезис: GraphQL по своей природе небезопасен. Я докажу это в течение всей статьи и предложу решения. Одно из решений потребует некоторого радикального изменения в том, как мы думаем о GraphQL, но это принесет много преимуществ, которые выходят далеко за рамки просто безопасности. Если вы выберете случайный фреймворк GraphQL и запустите его с настройками по умолчанию в производстве, катастрофа не заставит себя ждать. Почему? Почему GraphQL гораздо более уязвим, чем, например, REST? Давайте сравним URL с операцией GraphQL. Согласно Википедии, концепция URL была впервые опубликована в 1994 году, это 30 лет назад. Если мы ищем в том же источнике дату рождения GraphQL, мы видим, что это сентябрь 2014 года, примерно 10 лет назад. Это дает разбору URL преимущество в 20 лет перед разбором операций GraphQL. Весьма заметное преимущество! Далее, давайте посмотрим на грамматику antlr для обоих. Грамматика для разбора URL составляет 86 строк. Грамматика для разбора документа GraphQL составляет 325 строк. Так что можно справедливо сказать, что язык GraphQL примерно в 4 раза сложнее, чем тот, который определяет URL. Если мы учтем обе переменные, очевидно, что в разборе URL должно быть гораздо больше опыта и компетенции, чем в разборе операций GraphQL. Но почему это вообще проблема? Недавно мой друг проанализировал некоторые популярные библиотеки, чтобы увидеть, насколько быстро они разбирают запросы GraphQL. Меня порадовало видеть, что моя собственная библиотека работала довольно хорошо. В то же время меня удивило, что некоторые библиотеки не принимали тестовые операции, в то время как другие могли их разобрать. Что это значит для нас? Человек, который проводил тесты, выбрал несколько библиотек GraphQL и провел несколько тестов. Этого было достаточно, чтобы найти некоторые ошибки. Что, если мы выберем все библиотеки и фреймворки GraphQL и протестируем их против многочисленных операций GraphQL? Помните, что мы все еще говорим только о простом разборе операций. Что, если мы добавим построение действительного AST в уравнение? Что, если мы добавим выполнение операций? Мы почти забыли о проверке операций, это отдельная тема. Несколько лет назад небольшая группа людей начала удивительный проект с открытым исходным кодом: CATS (The GraphQL Compatibility Acceptance Test). Это довольно сложно, но идея блестяща. Идея заключалась в том, чтобы создать инструмент, чтобы различные реализации GraphQL могли доказать, что они работают, как предполагалось. К сожалению, последний коммит проекта датирован 2018 годом. Хорошо, разбор URL кажется простым и хорошо понятным. Разбор операций GraphQL - это кошмар. Вы не должны доверять любой библиотеке GraphQL без тщательного тестирования, включая фаззинг. Мы все люди. Создание библиотеки GraphQL сложно. Я владелец реализации, написанной на Go. Это не просто, это много кода. Много кода означает много потенциальных ошибок. И не поймите меня неправильно, речь идет не о ручном написании парсеров против генерируемых парсеров из грамматики. Превращение строки в AST - это всего лишь одна маленькая часть головоломки. Есть множество возможностей для ошибок. Вам не нужно нормализовать URL. Если вы можете разобрать его на вашем выбранном языке, он действителен, в противном случае - нет. С GraphQL история другая. Вот пример: Много foo! Давайте нормализуем запрос. Это гораздо меньше foo, отлично! Я мог бы усложнить это большим количеством фрагментов, вложенностью и т.д... В чем смысл? Как мы можем доказать, что все библиотеки и фреймворки корректно нормализуют запрос? Что произойдет, если здесь что-то пойдет не так? Это может дать злоумышленнику возможность запросить поля, которые он/она не имеет права использовать. Возможно, есть скрытое поле, и, обернув его странным комбо inline fragment @skip, мы сможем его запросить. Вывод: Нет нормализации для URL. Больше кошмаров для GraphQL. Я сам реализовал валидацию операций GraphQL. Один из файлов модульного теста содержит более 1000 строк кода. Что я сделал, так это скопировал полную структуру из спецификации GraphQL по одной и превратил ее в модульные тесты. Есть различные способы, как это могло бы пойти не так. Ошибки копирования и вставки, общее непонимание, реализация логики для прохождения тестов, в то время как логика все еще неверна. Есть много подводных камней, на которые вы могли бы наступить. Другие библиотеки и фреймворки, вероятно, принимают разные подходы. Вы также можете скопировать тесты из референсной реализации, но это также не гарантирует, что логика на 100% правильная. Опять же, поскольку у нас больше нет проекта, как CATS, мы действительно не можем доказать, что наши реализации верны. Я надеюсь, что все делают все возможное, чтобы сделать все правильно. До тех пор, не доверяйте любой библиотеке валидации GraphQL, если вы сами ее не тестировали. Используйте много операций для тестирования. Вывод: Если стандартная библиотека может разобрать ваш URL, он действителен. Если ваша выбранная библиотека проверяет операцию GraphQL, вы все равно должны быть осторожны, особенно когда имеете дело с PII (лично идентифицируемой информацией). На этом этапе мы, вероятно, уже прошли через несколько ошибок, передав наш запрос через парсер, нормализацию и валидацию. Настоящая проблема все еще впереди, выполнение операции. При выполнении операции GraphQL ответственность за правильное выполнение лежит не только на фреймворке. На этом этапе также есть большой шанс для пользователя фреймворка ошибиться. Это связано с тем, как разработан GraphQL. Операция GraphQL может перемещаться от узла к узлу, где бы она ни хотела, если вы ничего с этим не делаете. Так что диапазон возможных атак варьируется от простых атак отказа в обслуживании до более сложных подходов, которые возвращают данные, которые не должны быть возвращены. По этой причине мы дадим этому разделу немного больше структуры. Если вы хотите ограничить скорость пользователя REST API, все, что вам нужно сделать, это сохранить их IP в истории в памяти, например, Redis, и ограничить их скорость вашим выбранным алгоритмом, например, сложным ограничением скорости окна. Каждый запрос считается одним запросом, это звучит глупо, но имеет значение в контексте GraphQL. С другой стороны, с GraphQL вы не можете применить тот же шаблон. Одной единственной операции достаточно, чтобы остановить сервер GraphQL. Перемещение туда-сюда, бесконечно. Каждый слой вложенности запрашивает больше вложенных данных, отсюда экспоненциальный рост сложности выполнения. Обычно операции GraphQL представлены в виде JSON через HTTP POST запрос. Этот JSON может выглядеть так: Первое, что вы должны сделать, - это ограничить количество байт JSON, которые вы принимаете. Какой может быть ваша самая большая операция? Несколько килобайт? Мегабайты? Говоря о максимальном количестве узлов при разборе операции. Выбранный вами фреймворк действительно ли позволяет вам ограничить количество узлов, которые он прочитает? Далее, давайте поговорим о вариантах, которые у вас есть, когда операция разобрана. Вы можете рассчитать \"сложность\" операции. Вы можете \"пройти\" по AST и применить некий алгоритм для определения сложности операции. Один из способов определения сложности - например, вложенность. Этот алгоритм - хорошее начало. Однако у него есть некоторые недостатки. Вложенность сама по себе не является хорошим показателем сложности. Чтобы лучше понять сложность, вам придется посмотреть на возможное количество узлов, которое может вернуть поле. Это похоже на EXPLAIN ANALYZE в SQL. Он дает вам некоторые оценки того, что планировщик запросов думает, как будет выполняться запрос. Имейте в виду, что эти оценки могут быть совершенно неверными. Так что оценка не плоха, но вы также должны посмотреть на реальное количество возвращаемых узлов во время выполнения. Компании с публичными GraphQL API, например, GitHub, реализовали довольно сложные алгоритмы ограничения скорости. Они учитывают количество узлов, возвращаемых каждым полем, и дают вам некоторые ограничения на основе своих расчетов. Есть одна важная вещь, которую мы можем узнать от них в отношении дизайна схемы GraphQL. Если у вас есть поле, которое возвращает список, убедитесь, что есть обязательный аргумент для ограничения количества возвращаемых элементов, например, first, last, skip и т.д... Только тогда можно рассчитать сложность перед выполнением операции. Кроме того, вы также захотите подумать о пользовательском опыте вашего API. Это будет плохим пользовательским опытом, если операции GraphQL случайно не удаются, потому что из поля списка возвращается слишком много данных для некоторых экземпляров. В конце статьи мы вернемся к этой теме и поговорим о еще лучшем подходе, подходе, который хорошо работает как для поставщика API, так и для потребителя. Это должно быть довольно известно, но это все равно должно быть частью списка. В случае плохо написанной реализации db.loadHumanByID, SQL-запрос может выглядеть так: Поскольку 1=1 всегда истинно, это вернет всех пользователей. Вы, возможно, заметили, что функция может вернуть только одного пользователя, а не список пользователей, но для иллюстрации я думаю, это понятно, что мы должны иметь дело с данной проблемой. Будьте либеральны в том, что вы принимаете, и консервативны в том, что вы отправляете. [Закон Постеля] Решение проблемы не специфично для GraphQL. Вы всегда должны проверять входные данные. Для доступа к базе данных используйте подготовленные выражения или ORM, который абстрагирует слой базы данных, чтобы вы не могли внедрить произвольную логику в выражение по умолчанию. В любом случае, не доверяйте пользовательским вводам. Недостаточно проверить, является ли это строкой. Еще один вектор атаки - неполная логика аутентификации. Могут быть разные пути запроса для перехода к одному и тому же объекту, вы должны убедиться, что каждый путь покрыт. В резолвере для поля me вы извлекаете идентификатор пользователя из объекта контекста и разрешаете пользователя. До сих пор с этой схемой нет проблем. С этим изменением вы должны убедиться, что поле userByID также защищено промежуточным программным обеспечением аутентификации. Это может звучать тривиально, но вы на 100% уверены, что ваш GraphQL не содержит ни одного незащищенного пути доступа? Мы вернемся к этому пункту в конце статьи, потому что есть простой способ исправить проблему. Атаки обхода очень просты для эксплуатации, но их трудно обнаружить. Глядя на предыдущий пример, допустим, вы должны иметь право просматривать только id и name своих друзей; Поскольку мы сами вводим идентификатор пользователя в резолвер me, злоумышленник не может сделать много. С этим запросом мы загружаем всех друзей и их друзей. Как мы можем предотвратить \"обход\" этого пути пользователем? Вопрос в этом случае: защищаете ли вы рёбра (друзья) или узел (User)? На первый взгляд кажется, что защита ребра - это правильный путь. Так что, каждый раз, когда мы входим в поле \"друзья\", мы проверяем, является ли родительский объект (User) текущим аутентифицированным пользователем. Это сработало бы для запроса выше, но у него есть несколько недостатков. Один из которых - если вы защищаете только рёбра, вам придется защищать все. Вот еще один запрос, который не был бы защищен этим подходом, но это не единственная проблема. Если вы не защитили поле userByID, мы могли бы просто угадать идентификаторы пользователей и собрать их данные. Переходя к следующему разделу, вы увидите, почему защита рёбер - это не очень хорошая идея. Ваш серверный фреймворк GraphQL может реализовывать спецификацию глобальной идентификации объектов Relay. Эта спецификация является расширением вашей схемы GraphQL, чтобы сделать ее совместимой с клиентом Relay, клиентом, разработанным и используемым Facebook. Спецификация Relay определяет, что каждый узел в графе должен быть доступен через глобально уникальный идентификатор. Обычно этот ID является комбинацией __typename и полей id узла, закодированных в base64. С возвращенным узлом вы можете использовать фрагменты для запроса конкретных полей узла. Это означает, что даже если ваш сервер полностью безопасен, включая расширение Relay, вы открываете еще один вектор атаки. На этом этапе должно быть ясно, что защита рёбер - это игра в кошки-мышки, которая не в вашу пользу. Лучшим решением проблемы является защита самого узла. Так что, каждый раз, когда мы входим в резолвер для типа User, мы должны проверить, разрешено ли текущему аутентифицированному пользователю запрашивать поля. Как вы видите, вы должны принимать решения очень рано при проектировании вашей схемы GraphQL, а также схемы базы данных, чтобы иметь возможность надлежащим образом защищать узлы. Каждый раз, когда вы входите в узел, вы должны быть в состоянии ответить на вопрос, разрешено ли текущему зарегистрированному пользователю видеть поле или нет. Таким образом, возникает вопрос, должна ли эта логика действительно находиться в резолвере. Если вы спросите создателей GraphQL, их ответ будет \"нет\". Поскольку они уже решили проблему на слое ниже резолверов, слое доступа к данным или их \"Entity (Ent) Framework\", они не затрагивали проблему с GraphQL. Это также причина, почему авторизация полностью отсутствует в GraphQL. Сказав это, решение проблемы на слое ниже не является единственным допустимым решением. Если это сделано правильно, вполне нормально решать проблему из резолверов. Прежде чем мы продолжим, вы должны взглянуть на отличный фреймворк entgo и его архитектуру. Даже если вы не собираетесь использовать Golang для построения слоя вашего API, вы можете увидеть, сколько мысли и опыта вложено в дизайн фреймворка. Вместо того чтобы разбрасывать логику авторизации по вашим резолверам, вы можете определить политики на уровне данных, и нет никакого способа обойти их. Политика доступа является частью модели данных. Вам не обязательно использовать фреймворк, как entgo, но имейте в виду, что тогда вам придется решить эту сложную проблему самостоятельно. Опять же, мы вернемся к этой уязвимости позже, чтобы найти гораздо более простое решение. Многие серверы GraphQL также являются API-шлюзами или прокси-серверами для других API. Внедрение аргументов GraphQL в подзапросы - это еще одна возможная угроза, с которой мы должны иметь дело. Представим, что эта схема реализована с использованием REST API с GraphQL API в качестве API-шлюза спереди. Резолвер для поля userByID может выглядеть так: Почему это возможно? Скаляр ID должен быть сериализован в виде строки. В то время как \"7\" является допустимой строкой, \"7/friends/1\" тоже. Чтобы решить проблему, вы должны проверить ввод. Поскольку система типов GraphQL проверяет только то, является ли ввод числом или строкой, вам нужно сделать еще один шаг вперед. Если вы принимаете строки в качестве ввода, например, потому что вы используете UUID или GUID, вы должны убедиться, что вы проверили их перед использованием. Опять же, нам нужно проверить входные данные. WunderGraph предлагает вам простой способ настроить валидацию JSON Schema для всех входных данных. Это возможно, потому что WunderGraph сохраняет ваши операции полностью на сервере. Но мы вернемся к этому позже. Все остальные должны убедиться, что проверяют любой ввод перед его использованием из ваших резолверов. Интроспекция GraphQL - это удивительная возможность GraphQL сообщать клиентам все о схеме GraphQL. Инструменты, такие как GraphiQL и GraphQL Playground, используют запрос интроспекции, чтобы затем предоставить пользователю функции автозавершения. Без интроспекции и схемы такие инструменты не существовали бы. В то же время у интроспекции есть и некоторые недостатки. Схема GraphQL может содержать конфиденциальную информацию. Есть вероятность, что ваша схема GraphQL раскроет внутреннюю информацию или поля, которые используются только внутри. Возможно, одна из ваших команд работает над новым MVP, который еще не запущен. Ваши конкуренты могут сканировать ваш API GraphQL, используя запрос интроспекции. Каждый раз, когда происходит изменение в схеме, они могут немедленно увидеть это, используя diff. Что мы можем сделать с этим? Большинство руководств советуют отключать запрос интроспекции в продакшене. То есть вы разрешите его во время разработки, но запретите запросы интроспекции при развертывании в продакшен. Однако из-за дружелюбия некоторых реализаций фреймворка GraphQL, включая референсную реализацию graphql-js, отключение интроспекции не решает проблему. Имейте в виду, что каждая реализация, зависящая от референсной реализации graphql-js, также затрагивается этим. Так что, если отключение интроспекции не помогает, что еще мы можем сделать с этим? Если ваш API используется только вашими внутренними сотрудниками, вы можете выполнить запросы интроспекции с помощью промежуточного программного обеспечения аутентификации. Таким образом, вы добавите слой аутентификации перед выполнением GraphQL. Очевидно, что это работает только для API, которые всегда требуют аутентификации, иначе пользователи не смогут сделать ни одного запроса. Если вы создаете приложение, которое может использоваться пользователями без аутентификации, предложенное решение не работает. В итоге, отключая интроспекцию во время выполнения, вы немного усложняете интроспекцию схемы, но с большинством фреймворков это все еще возможно. Следующая уязвимость также будет использовать эту проблему. В конце будет представлено универсальное решение. Существует ряд сервисов и инструментов, таких как, например, Postgraphile или Hasura, которые генерируют API из схемы базы данных. Обещание простое: укажите инструмент на базу данных, и вы получите полностью функциональный сервер GraphQL. Как мы уже обсуждали ранее, не всегда легко и иногда невозможно полностью отключить интроспекцию во время выполнения. Сгенерированные API GraphQL обычно следуют общей структуре для создания резолверов CRUD. Это означает, что нам довольно легко определить, имеем ли мы дело с API, созданным на заказ для конкретного варианта использования, или сгенерированным API. Почему это проблема? Если мы не можем отключить интроспекцию, мы раскроем информацию о нашей полной схеме базы данных в публичный доступ. Это уже вызывает сомнения в подходе, если вы хотите иметь тесную связь между клиентом и сервером, что является случаем, если мы генерируем API из схемы базы данных. Сказав это, с точки зрения безопасности, это означает, что мы выставляем всю нашу  схему базы данных на публику. Выставляя вашу схему базы данных на публику, вы предоставляете злоумышленникам много информации для поиска уязвимостей, попыток SQL-инъекций и т.д... Я знаю, что это повторяющаяся схема, но мы также обсудим эту проблему в конце. Эта проблема не является прямой уязвимостью GraphQL, но общей угрозой для приложений, основанных на HTTP, с механизмами аутентификации на основе Cookie или сессий. Если вы используете фреймворки, такие как NextJS, аутентификация на основе Cookie довольно распространена (и удобна), поэтому стоит об этом рассказать. Представьте, что мы создаем приложение, которое позволяет пользователям отправлять деньги другим пользователям. Мутация для отправки денег может выглядеть так: Если мы создаем одностраничное приложение (SPA) на app.example.com с API на другом домене (api.example.com), первое, что вам нужно сделать, чтобы этот сценарий работал, - это настроить CORS. Убедитесь, что вы разрешили только ваш домен SPA и не используйте подстановочные знаки! Следующая вещь, которая может пойти не так, - это правильная настройка свойств SameSite для домена API, который устанавливает Cookies. Вы захотите использовать SameSite lax или strict, в зависимости от пользовательского опыта. Для запросов может иметь смысл использовать lax, что означает, что мы можем использовать запросы из доверенного домена, например, другого поддомена. Для мутаций strict вариант определенно был бы лучшим вариантом, так как мы хотим принимать их только от источника. SameSite none позволит любому веб-сайту делать запросы к нашему домену API, независимо от их происхождения. Если вы сочетаете плохую конфигурацию CORS с неправильными настройками Cookie SameSite, вы попадаете в беду. Наконец, злоумышленники могут найти способ составить ссылку на наш сайт, которая приведет уже аутентифицированных пользователей к совершению транзакции, которую они на самом деле не хотят делать. Чтобы защититься от этой проблемы, вы должны добавить промежуточное программное обеспечение CSRF вокруг мутаций. WunderGraph делает это из коробки. Для каждой конечной точки мутации мы настраиваем промежуточное программное обеспечение CSRF. Кроме того, мы генерируем наши клиенты таким образом, чтобы они автоматически обрабатывали CSRF. Как разработчик, использующий WunderGraph, вам не нужно ничего делать. Это еще одна распространенная проблема с API GraphQL. У GraphQL есть хороший и выразительный способ возвращения ошибок. Однако некоторые фреймворки по умолчанию просто немного слишком информативны. Это сообщение об ошибке довольно выразительное, кажется, что оно исходит из SQL-базы данных и связано с нарушением ограничения уникального ключа. Хотя это полезно для разработчика приложения, на самом деле это дает слишком много информации потребителю API. Это сообщение можно было бы записать в журналы, если бы они были. Кажется, что пользователь приложения пытается создать контент с уже существующим идентификатором. В правильно спроектированном API GraphQL это вообще не должно быть ошибкой. Лучший способ спроектировать этот API - вернуть объединение, которое охватывает все возможные случаи, например, успех, конфликт и т.д... Но это просто общая проблема сгенерированных API. В любом случае, сгенерированный или нет, всегда должно быть промежуточное программное обеспечение на самом верху вашего HTTP-сервера, которое перехватывает подробные ошибки, такие как эта, и удаляет их из ответа. Если возможно, не используйте просто общий объект ответа \"errors\". Вместо этого используйте выразительную систему типов и определите типы для всех возможных результатов операции. REST API имеют богатую систему HTTP-кодов состояния для указания результата операции. GraphQL позволяет вам использовать определения типов Interface и Union, чтобы потребители API могли легко обрабатывать ответы API. Очень трудно программно анализировать сообщение об ошибке. Это просто строка, которая может измениться в любое время. Создавая типы Union и Interface для ответов, вы можете явно охватить все результаты операции. Затем потребитель API сможет переключаться по полю __typename и правильно обрабатывать \"известную ошибку\". Это много проблем, которые нужно решить перед переходом в продакшен. Пожалуйста, не относитесь к этому легкомысленно. Если вы посмотрите на HackerOne, вы увидите, что проблема реальна. Так что, мы хотим получить преимущества GraphQL, но пройти через весь этот список - это слишком много работы. Есть ли лучший способ использовать GraphQL? Есть ли способ использовать GraphQL по-другому, чтобы мы не сталкивались со всеми этими проблемами. Ответ на этот вопрос - да! Все, что вам нужно сделать, - это скорректировать свое видение GraphQL. Большинство из нас используют GraphQL API внутри. Это означает, что разработчики, которые используют GraphQL API, находятся в той же организации, что и люди, которые предоставляют API. Кроме того, я предполагаю, что мы не меняем наши операции GraphQL во время выполнения. Все это сводится к корневой причине проблемы. Разрешение клиентам API отправлять операции GraphQL по HTTP - это корень всех зол. Все это можно полностью избежать, это не приносит никакой ценности и только создает вред. Вполне нормально разрешать разработчикам в защищенной среде отправлять произвольные операции GraphQL. Однако большинство приложений не меняют свои операции GraphQL в продакшене, так зачем вообще это разрешать? Давайте посмотрим на архитектуру, которая вам наиболее знакома. Клиент GraphQL общается с сервером GraphQL. Теперь давайте внесем небольшое изменение в архитектуру, чтобы исправить все 13 проблем. Вместо общения GraphQL между клиентом и сервером мы говорим на RPC, более конкретно на JSON-RPC. Затем сервер обрабатывает для нас аутентификацию, авторизацию, кэширование и т.д. и пересылает запросы на исходные серверы. Мы не изобрели это. Это не что-то новое. Так делают компании, такие как Facebook, Medium, Twitter и другие. То, что мы сделали, это не просто сделали это возможным и исправили проблемы, перечисленные выше. Мы создали удобный для использования опыт разработчика. Мы собрали все в одном месте. Вам не нужно устанавливать множество зависимостей. Давайте немного разберем решение, чтобы вы могли полностью понять, как мы смогли решить все уязвимости. Самый безопасный код - это код, который вообще не нужно запускать. У каждого кода есть ошибки. Чтобы исправить ошибки, нам приходится писать больше кода, что означает, что мы вводим еще больше ошибок. Во время разработки разработчики определяют все операции, которые требуются для приложения. В то время, когда приложение готово к развертыванию, мы анализируем, нормализуем и проверяем все операции. Затем мы генерируем конечные точки JSON-RPC для каждой операции. Как уже упоминалось, мы нормализовали операции. Это позволяет нам рассматривать все входные данные (переменные) как объект JSON. Затем мы можем проанализировать типы переменных и получить схему JSON для ввода. Кроме того, мы можем проанализировать схему ответа запроса GraphQL. Это дает нам вторую схему JSON. Эти две будут очень удобны позже. Делая это во \"время развертывания\", нам не нужно делать это во время выполнения снова. Мы можем \"предварительно скомпилировать\" дерево выполнения. Все, что остается во время выполнения, - это ввести переменные и выполнить дерево. Мы позаимствовали эту идею у систем баз данных SQL, она довольно похожа на \"Подготовленные выражения\". Хорошо, это означает, что мы решили три проблемы. К сожалению, мы также ввели новую проблему! Нет простых в использовании клиентов, которые могли бы использовать наш API JSON-RPC. К счастью, мы извлекли две схемы JSON на конечную точку. Если мы подадим их на генератор кода, мы сможем генерировать полностью типобезопасные клиенты на любом языке. Эти клиенты не только очень малы, но и очень эффективны, так как им не нужно делать много. Итак, в итоге мы не только решили три проблемы, но и сделали наше приложение более производительным. Как еще один побочный эффект, вы также можете генерировать формы из этих определений схемы JSON. Это полностью интегрировано в WunderGraph. Большинство уязвимостей GraphQL DOS происходят от того, что злоумышленники могут легко создавать сложные вложенные запросы, которые запрашивают слишком много данных. Как мы обсуждали выше, мы просто заменили GraphQL на RPC. Это означает, что нет динамических операций GraphQL. Для каждой операции мы можем настроить определенный лимит скорости или квоту. Затем мы можем легко ограничивать скорость наших пользователей, так же, как мы делали это с REST API. Мы извлекли схему JSON для каждой конечной точки RPC. Эту схему JSON можно настроить по вашему желанию, чтобы позволить вам проверять все входные данные. Посмотрите нашу документацию и как вы можете использовать директиву @jsonSchema для настройки схемы JSON для каждой операции. Проблема с аутентификацией заключалась в том, что вы должны убедиться, что каждый возможный путь запроса покрыт промежуточным программным обеспечением аутентификации. Введение слоя RPC по умолчанию блокирует все возможные пути запроса. Кроме того, вы можете по умолчанию заблокировать все операции за промежуточным программным обеспечением аутентификации. Если вы хотите открыть операцию для публики, вы должны сделать это явно. С точки зрения злоумышленников, атаки обхода возможны только в том случае, если есть что-то, что они могут \"обойти\". Защищая слой GraphQL с помощью слоя RPC, эта функция удаляется из общедоступного API. Самой большой угрозой теперь является сам разработчик, так как он может случайно раскрыть слишком много данных. Вспоминая проблему выше, проблема со спецификацией Relay возникает с двух сторон. Одна из них - неполная защита аутентификации, другая - защита ребер, когда защита узлов была бы правильным способом сделать это. Спецификация Relay позволяла вам запрашивать любой узел с глобально уникальным идентификатором объекта. Это дает разработчикам (и клиенту Relay) мощный инструмент, но также создает еще одну проблему, которую нужно решить. Вы, возможно, видите некоторую повторяемость здесь, но уязвимость Relay также покрывается фасадом RPC. Это одна из более сложных проблем для решения. Если мы используем пользовательские входные данные в качестве переменных для подзапросов, мы должны убедиться, что эти переменные точно соответствуют тому, что мы ожидаем, и не пытаются эксплуатировать нижележащую систему. Чтобы помочь смягчить эти проблемы, мы сделали очень простым определения схемы JSON для переменных. Таким образом, вы можете определить шаблон Regex или другие правила для проверки входных данных перед их внедрением в последующие запросы, запросы к базе данных и т.д... Система типов GraphQL абсолютно замечательна и очень полезна, особенно для облегчения жизни потребителей API. Однако, когда дело доходит до интерпретации запроса GraphQL, все еще есть несколько пробелов, которые мы пытаемся исправить. Как мы видели в описании проблемы, отключение интроспекции GraphQL может быть не таким простым, как кажется, в зависимости от используемого вами фреймворка. Тем не менее, интроспекция полагается на слой GraphQL. Если вы посмотрите, как работает интроспекция, это просто еще один запрос GraphQL, даже если это особый запрос, с довольно большим количеством вложений. Помните, что мы все еще разрешаем интроспекцию во время разработки. Инструменты, такие как GraphiQL, будут продолжать работать, просто не в продакшене, или, по крайней мере, не с особым токеном аутентификации. Если API GraphQL является точной копией схемы вашей базы данных, вы раскрываете внутренние элементы вашей архитектуры через интроспекцию GraphQL. Но мы уже решили эту проблему. Способ смягчения этой проблемы заключается в правильной настройке CORS и настроек SameSite на вашем API. Затем добавьте промежуточное программное обеспечение CSRF к слою API. Это добавляет зашифрованные CSRF-куки для каждого пользователя. Как только пользователь войдет в систему, передайте им их токен csrf. Наконец, если пользователь хочет вызвать мутацию, они должны представить свой токен CSRF в специальном заголовке, который затем может быть проверен промежуточным программным обеспечением CSRF. Если что-то идет не так, или пользователь выходит из системы, удалите CSRF-куки, чтобы заблокировать все мутации, пока снова не будет действительной сессии пользователя. Все это может звучать немного сложно, особенно взаимодействие между клиентом и сервером, отправка токенов CSRF и заголовков туда и обратно. Вот почему мы добавили все это в WunderGraph по умолчанию. Все мутации автоматически защищены. На стороне сервера у нас есть все промежуточные программные обеспечения. Клиент генерируется автоматически, поэтому он точно знает, как обращаться с токенами CSRF и заголовками. Эта проблема, вероятно, одна из больших угроз, хотя ее легко исправить. После разрешения операции, прямо перед отправкой ответа клиенту, убедитесь, что вы удалили всю конфиденциальную информацию из объекта ошибок. Это особенно важно, если вы проксируете к другим службам. Не просто передавайте все, что у вас есть от вверхнего потока. Если вы обращаетесь к REST API, и код ответа не 200, не возвращайте ответ как общий объект ошибки. Кроме того, подумайте о вашем API как о продукте. Как должен выглядеть пользовательский опыт вашего \"продукта\" в случае ошибки? Помогите вашим пользователям понять ошибку и то, что они могут с ней сделать, по крайней мере, для \"известных хороших ошибок\". В случае \"плохих ошибок\", тех неожиданных, не будьте слишком конкретными для ваших пользователей, они могут быть не дружелюбными. Хорошо, вы видели, что, изменив нашу архитектуру и развив наше понимание GraphQL, мы можем устранить почти все проблемы, которые исходят от самого GraphQL. Остается самая большая уязвимость всех систем - люди, которые их создают, мы, разработчики! Хорошо, мы почти закончили. Мы пропустили одну небольшую группу API. Мы говорили о частных API почти весь этот материал, но мы сделали это по хорошей причине, вероятно, 99% API являются частными. Так делают компании типа Shopify и GitHub. Что мы можем узнать у них? На данный момент Shopify решил 1273 отчета. Они выплатили в качестве вознаграждений 1.656.873 долларов хакерам в диапазоне от 500 до 50.000 долларов за вознаграждение. Twitter решил всего 1364 проблемы на общую сумму 1.424.389 долларов. Snapchat выплатил только 439.067 долларов с 389 решенными отчетами. GitLab выплатил поразительные 1.743.639 долларов с общим числом решенных проблем 845. Эти вознаграждения не только связаны с GraphQL, но все перечисленные выше компании можно найти в списке сообщенных проблем GraphQL. Всего на GraphQL есть 70 отчетов с большим количеством выплаченных вознаграждений. Если вы ищете на других веб-сайтах с вознаграждениями за ошибки, вы, вероятно, найдете больше. Компании, такие как GitHub, имеют людей, которые создают специализированную инфраструктуру, чтобы лучше понять, как используется их API GraphQL. Мне было приятно встретиться с удивительной Клэр Найт и послушать ее доклад на последнем саммите GraphQL, это было довольно давно... Я представил вам все эти данные, чтобы сделать два вывода. Во-первых, действительно ли вам нужно раскрывать ваш API GraphQL? Если нет, отлично! Вы можете применить все решения из этой статьи. Если, при всех обстоятельствах, вы абсолютно хотите раскрыть API GraphQL, убедитесь, что у вас есть необходимая экспертиза для этого. У вас должны быть эксперты по безопасности внутри компании или вы должны их нанять, вы должны регулярно проводить аудиты и тестирование на проникновение. Не относитесь к этому легкомысленно! Мы также с удовольствием ответим на звонок с вами и дадим вам демо. Забронируйте сейчас бесплатную встречу! Вы заинтересованы, как работает концепция GraphQL-to-RPC в реальности? Посмотрите нашу быструю стартовую инструкцию за одну минуту и попробуйте концепции, обсуждаемые в этой статье.",
    "101": "Итак, я полностью поменял то, как происходит процесс разработки сайта. Для начала, в корне проекта должен быть файл скрипта ntw3 index.*, где вместо * должно быть любое из расширений скрипта ntw3 Итак,  в нём должны быть объявлены пути и классы для элементов сайта. Вот пример такого файла: Итак, едем дальше. Классы для элементов интерфейса. Тут всё преобретает основной смысл. в них могут быть смешаны и стили, и код, и даже разметка. Вот, например, код для кнопки в темной теме: строка 7: создание класса Button, которому прилинковывается встроенный тип button, чтобы его нельзя было применить к другому типу. Это нужно скорее для самоконтроля программиста, ведь если он прилинкует этот класс к не тому типу, всё может сломаться. Это необязательно, можно написать просто class Button { ..., но это может усложнить код и отладку в будущем. итак, всё, что заключено в //{ ... }, это просто комментарий. внутри них можно использовать подсветку кода, заключив его в нечетное кол-во символов `. строка 13: распаковка всех имен из super, которые имеют тип event. Это удобно тем, что далее в теле класса можно не писать super., а писать сразу имя аттрибута. строка 14: прилинковка к событию под названием click более сложного события. в скобках расположен сценарий, при котором событие можно считать активированным. символом & требуется одновременное выполнение ранее известных событий, символом >> объявляется переход к следующему этапу, символом ? объясняется, что условие необязательно. в будущем эта система будт боле проработанной. строки 24-34: применение \"сложного стиля\" - стиля, состоящего из многих элементов. зачем тут auto? чтобы привести json-компонент к нативному типу $__backgroundStyle$inherit_from_parent_allowed. Такие типы недоступны по названию, это так задумано, и таких типов огромная масса. Пояснение насчет json-компонентов: Это тоже скрытый тип, который применяется ко всем словарям (надеюсь не нужно объяснять, что это) Итак, а что, если я хочу сделать красивый свитчер, как в андроиде, например? Допустим, у нас есть спрайты включенного и выключенного состояния. Тогда код будет выглядеть так: Тут уже открывается куча возможностей. с помощью private можно указать, что вне тела класса аттрибут недоступен. следом идет тип аттрибута, а потом имя. что дальше объяснять не ндо, там интуитивно понятно. Методы так же объявляются интуитивно понятно. new - это специальный метод, который вызывается при инициализации класса, для каждого экземпляра отдельно. потом идут строки 78-85, вот так уже самый сок: link привязывает хендлер к чему угодно, что можно \"спровоцировать\" (от англ. raise), будь то исключение, событие или что либо ещё, что имеет метод raise. anonfn  это алиас над более сложной конструкцией, которая создает анонимную функцию (да, её просто так не сделать :)), объявление функции похоже на что-то среднее между Honey и C. exists проверяет имя на доступность или существование, тут понятно. потом идет стрелка влево (часть синтаксиса для link) и то, что может быть спровоцировано. И наконец, мы хотим красную кнопку. Это можно изменить и в разметке, но почему бы не сделать ей отдельный класс, чтобы подробнее разобрать наследования? Итак, вот её код: Что ж, тут особо объяснять нечего, кромя слова global в строке 90. Нет, оно не аналог public в том же месте в C++. Оно лишь говорит о том, что если есть какой-то блок с классом RedButton, то ему присваивается и класс Button (Это может пригодится в будущем для селекции по классу) Вот мы и разобрали скриптовую часть. теперь к файлу pages/index.nml! строка 2: здесь просто даётся подсказка компилятору, что если эта страница используется, то нужно автоматически подгружать скрипт ui.ntw3. строки 11-13: здесь объявляются классы, которым пренадлежит блок (да, это уже не тэги :)). второй класс писать необязательно, ведь мы написали global в ui.ntw3. строки 14-19: здесь перечисляются скрипты (могут быть как inline, так и импортированные и привязанные к имени), содержимое inline скрипта должно быть окружено ${ ... }. Само содержимое кода в примере интуитивно понятно. Я очень старался сделать всё понятным, но если остались вопросы, вы всегда можете задать их в комментариях к посту.",
    "102": "Начнем пожалуй с того, что решать дифференциальные уравнения вручную может быть не совсем просто. На данном ресурсе http://mathprofi.ru/differencialnye_uravnenija_primery_reshenii.html есть шпоры как это делать вручную, там есть множество подсказок по различным разделам высшей математики, например мне это пригодилось, потому что помнить все и вся не всегда представляется возможным. Конкретно в данном посте я попытаюсь прорешать 10 примеров из первой темы данного предмета. Дифференциальные уравнения первого порядка. Примеры решений.Дифференциальные уравнения с разделяющимися переменными Учитывая что на mathprofi уже есть решения и ответы к этим 10 примерам, я предложу дополнить её решениями данных уравнений с помощью персонального компьютера, а конкретно с помощью языка программирования Python и библиотеки символьного вычисления и решения уравнений SymPy. В точности то, что написано в 7 строке вывода. Это задача с условием, поэтому код немного усложнится. В данном примере 6 на сайте надо было выразить через функцию выше ответ, а код решает сложный диффур и выражает его через зависимую переменную y. Оставим данное уравнение и ответ потомкам. Данный ответ 7 автор сайта называет общим интегралом и делает проверку дифференцируя этот ответ, а код решает сложный диффур и выражает его через y. Данный пример 8 я не смог решить с условием, у кого будут идеи где я ошибся, прошу в комментарии. В 9 примере в ответе опять общий интеграл. Очень полезная библиотека этот SymPy - упрощает жизнь и решения сложных дифференциальных уравнений.",
    "103": "Первая система для обработки больших объемов данных появилась в конце 19 века. Американский инженер Герман Холлерит создал ее для того, чтобы обрабатывать результаты переписи населения США. Компания Холлерита — первый ИТ-стартап — нашла частных инвесторов и государственные заказы, создала новую отрасль, и привлекла сотни клиентов. Однако ее монопольное положение на этом рынке было недолгим — вскоре появился конкурент, который смог предложить пользователям более низкие цены и новые технологии. О первой в истории ИТ войне систем рассказывает исследователь истории науки и техники, автор Центра непрерывного образования факультета компьютерных наук НИУ ВШЭ Антон Басов. Согласно Конституции США, «исчисление фактического населения штатов» должно производиться каждые десять лет. Первая перепись населения бывших тринадцати колоний прошла еще в 1790 году. С тех пор количество американского населения росло, считать его становилось все сложнее. Обработка результатов переписи 1880 года заняла восемь лет и закончилась незадолго до проведения новой переписи. Стало ясно, что считать население вручную больше невозможно. Вопросом механизации переписи занялись двое — Джон Шоу Биллингс и Герман Холлерит. Биллингс был крайне разносторонним человеком: хирург по образованию, он стал одним из главных врачей американской армии, сыграл важную роль в создании больницы Джонса Хопкинса при одноименном университете и Нью-Йоркской публичной библиотеки. Как представитель руководителя медицинской службы армии США, Биллингс участвовал в анализе данных переписей 1870 и 1880 годов. Как раз во время работы над результатами переписи 1880 года, Биллингс познакомился с сотрудником Бюро переписи населения Германом Холлеритом. Холлерит, недавний выпускник Колумбийского горного училища, работал в Бюро над статистикой машин и механизмов в промышленности. Работа счетчиков в Бюро переписи была утомительной. Видимо, наблюдая за ней, Биллингс сказал Холлериту: «Должна же быть какая‑то машина для выполнения этой механической работы». Холлерит загорелся идеей. Спустя некоторое время он показал Биллингсу модель счетной машины. Вдвоем они потратили много времени, пытаясь улучшить систему, пока Биллингсу не наскучило — он все‑таки был врачом, а не механиком. Однако он выполнил главную задачу — заинтересовал Холлерита вопросом и даже обрисовал в общих чертах решение: каждая единица данных записывается с помощью перфораций на отдельную карточку, которая затем обрабатывается с помощью специальных устройств. В 1884 году Холлерит увольняется из Бюро переписи и подает первую заявку на патент. К 1889 году он получает три патента — US395 781, US395 782, US395 783 — в которых описываются все части его системы: Данные заносятся на карточки с помощью перфоратора. Каждое отверстие соответствует наличие какого‑либо признака — пола, возраста, цвета кожи. Оператор пропускает карточки через табулятор. Когда в определенном месте карточки есть отверстие, оно позволяет замкнуть электрическую цепь и прибавить единицу на счетчике. Таким образом, каждый счетчик подсчитывает количество карт, на которых имеется тот или иной признак. Когда карточка прочитывается, автоматически открывается одна из ячеек сортировального ящика, куда помещается карточка (сортировальный ящик может сортировать только в пределах одного признака, например по возрасту: до одного года, от 1 до 10 лет, от 11 до 20 и так далее). Перфокарты можно пропускать через табулятор снова и снова, сортируя их по разным признакам и их сочетаниям. Как и любой основатель технологического стартапа, Холлерит с переменным успехом ищет спонсоров и клиентов. В 1886 году он получает первый крупный подряд — его система используется для анализа демографической статистики в Балтиморе. Затем последовали испытания в Нью-Джерси и Нью-Йорке, а также в офисе руководителя медицинской службы армии США. Скорее всего, Холлерит получил все эти подряды не без поддержки Биллингса. В 1889 году Холлерит отправил набор оборудования для обработки перфокарт на всемирную выставку в Париже, где тот удостаивается золотой медали. Тогда же Бюро переписи населения объявляет конкурс систем для обработки данных переписи 1890 года. Система Холлерита выигрывает и он подписывает договор на поставку оборудования. Скорость работы системы была огромной для того времени. Первые результаты были обнародованы уже через шесть недель после дня переписи. В целом обработка результатов переписи 1890 года заняла три года (против восьми для переписи 1880 года). Использование системы Холлерита позволило сэкономить порядка пяти миллионов долларов при общей стоимости переписи около 12 миллионов. Холлерит решил не продавать оборудование Бюро переписей, а давать его в аренду. После окончания обработки данных машины вернулись к нему и он начал искать других клиентов. Холлериту удалось продать новую технологию двум потребителям — иностранным правительствам и крупному бизнесу. С одной стороны, его машины начали применять для переписей населения в Австрии, Канаде, Италии, России и других странах. С другой, Холлерит начал внедрять счетно-аналитические машины для работы с перфокартами в страховых компаниях, на железных дорогах и других предприятиях, где нужно было быстро и эффективно обрабатывать большие массивы информации. При подготовки переписи 1900 года никаких сомнений, использовать ли систему Холлерита, не осталось. Холлерит предложил Бюро переписей усовершенствованное оборудование: табулятор с автоматическим считыванием карт (до этого карты считывались по одной на ручном прессе), автоматическую сортировочную машину вместо сортировального ящика, новый перфоратор. Однако последствия переписи 1900 года ознаменовали конец монополии Холлерита на поставку оборудования для работы с перфокартами. В 1902 году появилось постоянное Бюро переписи населения, руководителем которого в 1903 году стал статистик Саймон Норт. Изучив договоры с Холлеритом, Норт приходит к выводу, что арендная плата слишком высока. Холлерит отказывается идти на уступки, и Бюро не перезаключает договор. Вместо этого Норт создает при Бюро переписи собственную механическую мастерскую. Задачей мастерской стало создание собственного оборудования. Времени до следующей переписи было достаточно, а срок действия первых патентов Холлерита уже истекал — поэтому мастерская могла просто скопировать его старые машины и внести в них небольшие усовершенствования. Сотрудники мастерской начали именно с этого, постепенно переходя к разработке нового, более совершенного оборудования. У табуляторов Холлерита, даже с автоматическим вводом перфокарт, была проблема — данные с карт не записывались, а только отображались на счетчиках. Переписывать их приходилось вручную, что было утомительно и приводило к ошибкам. К 1907 году мастерская Бюро разработала первый печатающий табулятор. Другой проблемой переписи было перфорирование карт, то есть пробивка в них отверстий. Кажется, что проколоть отверстие в куске бумаги несложно — но попробуйте проделать это несколько тысяч (или даже десятков тысяч) раз за день, да еще так, чтобы отверстия были только в нужных местах. Перфораторы Холлерита хотя и помогали пробить отверстие в нужном месте карты, но требовали от оператора большого физического усилия. Для переписи 1910 года сотрудник мастерской Джеймс Легранд Пауэрс предлагает перфоратор новой конструкции — с автоматической подачей перфокарт, удобной клавиатурой и электродвигателем для пробивания отверстий. Пауэрс, талантливый инженер и изобретатель, приложил руку ко многим разработкам мастерской Бюро переписей и получил несколько патентов. Пауэрс по праву может считаться одним из пионеров ИТ-индустрии. Несмотря на это, он совершенно забыт, а в его биографии зияют многочисленные дыры. Точно известно немногое: Пауэрс родился в 1871 году в Одессе. В 1889 году он переехал в США и работал механиком в крупных компаниях, в том числе в Western Electric. Неизвестно, когда именно он стал сотрудником мастерской Бюро переписи, но он определенно работал там между 1907 и 1911 годами. По всей видимости, правительство США дало ему разрешение запатентовать его изобретения, хотя они были сделаны им на государственной службе. Во время переписи 1910 года было собрано все имевшееся тогда оборудование. Для пробивки карт использовались как новые перфораторы Пауэрса, так и старые перфораторы Холлерита. Сортировальные машины были тоже Холлерита, построенные им для переписи 1900 года и сильно переделанные (сам Холлерит, узнав о переделке своих машин, подал в суд за нарушение своего патента. В итоге — неожиданный поворот — суд аннулировал патент, так как Холлерит сначала построил сортировальные машины для Бюро переписи, а только потом запатентовал их). А вот табуляторы были уже собственной разработки мастерской Бюро. Несмотря на эту пеструю смесь, перепись прошла успешно. Впрочем, уже к переписи 1920 года стало понятно, что силами одной только мастерской обойтись все равно не получится. Во время следующих переписей Бюро вернулось к практике аренды или покупки оборудования, которое не имело смысла изготавливать своими силами. Другие правительственные учреждения тоже арендовали или покупали оборудование вместо того, чтобы делать его самостоятельно (например, почтовая служба использовала оборудование и Холлерита и Пауэрса). В 1911 году в отрасли перфокарт произошли два важных события. Во-первых, Tabulating Machine Company Холлерита слилась с тремя другими компаниями в холдинг Computing-Tabulating-Recording Company. Холлерит продал контрольный пакет акций и стал в новой компании консультантом. В 1915 году президентом компании стал Томас Уотсон-старший, который занимал этот пост до 1956 года. Во-вторых, Джеймс Пауэрс уволился из мастерской Бюро переписи и учредил Powers Tabulating (с 1914 года — Accounting) Machine Company. Итак, на рынке оборудования для обработки перфокарт появились две конкурирующие компании. Почему же именно это событие можно считать началом первой войны систем в истории ИТ? Все дело в патентах. Пауэрс не мог использовать наработки Холлерита (да, срок жизни первых патентов уже истек, однако и сортировочная машина, и автоматический ввод карт в табулятор все еще были защищены), а поэтому был вынужден изобретать собственные решения. В основе всех машин Холлерита лежал принцип электрического считывания перфокарт: отверстие в карте позволяет замкнуться контактам и активировать тот или иной механизм. Пауэрс решил не пытаться обойти решение Холлерита, а просто заменить электричество механикой. В разработанных им сортировальных машинах и табуляторах считывающий механизм напоминал пишущую или счетную машинку: на карточку опускались подпружиненные штифты; там, где в ней были отверстия, штифты проходили насквозь и приводили в действие счетные или печатающие механизмы. К 1914 году Пауэрс предлагал набор из четырех машин для обработки перфокарт: перфоратор, новоизобретенный контрольник для проверки правильности пробивки, сортировочную машину и печатающий табулятор. С технической точки зрения его оборудование было совершеннее, чем у Computing-Tabulating-Recording, которая предлагала клиентам три машины — клавишный перфоратор без мотора, сортировку и табулятор со счетчиками. Однако и Computing-Tabulating-Recording не собиралась сдаваться. Для начала они обложили Powers Accounting Machine Company данью: в 1914 года Пауэрсу пришлось купить лицензию на использование патентов в обмен на 25 % выручки от аренды машин и 18 % выручки от продажи перфокарт. Не совсем ясно, какие патенты Computing-Tabulating-Recording мог использовать Пауэрс; видимо, он опасался дорогостоящих судебных процессов, которые бы подорвали и без того слабые финансы компании. Сработала и ставка Холлерита на бизнес. Он начал поставлять свое оборудование частным компаниям еще в 1890 году, одновременно с проведением тогдашней переписи. Спустя двадцать пять лет у Computing-Tabulating-Recording было уже 550 клиентов, использовавших 1076 табуляторов и 827 сортировальных машин для обработки 660 миллионов перфокарт в год. Пауэрс, хороший инженер, но плохой предприниматель, не мог похвастаться такими цифрами. К 1918 году коммерческие неурядицы ему надоели и он ушел из основанной им компании, новым руководителем которой стал инженер Вильям Ласкер. Хотя Герман Холлерит и Джеймс Пауэрс отошли от дел, их компании продолжали развиваться. Вступление Америки в Первую мировую войну дало бизнесу новый толчок — правительству понадобились системы для обработки данных о солдатах на фронте и экономическом положении в тылу. После войны этот тренд продолжился — задача обработки данных переписи населения отошла на второй план, а счетно-аналитические машины стали использовать для самых разных задач, от бухгалтерского учета до классификации растений. В 1924 году Computing-Tabulating-Recording сменила название на International Business Machines. А в 1927 году Powers Accounting Machine Company объединилась с семью другими компаниями в Remington Rand. Еще до войны у обеих компаний появились подразделения в Великобритании, Франции, Германии. В 1929 году британское и французское представительство компании Пауэрса объединились под названием Powers-Samas (или Samas-Powers, от французского Société Anonyme des Machines à Statistiques). Забавно, что Пауэрс, совершенно забытый как человек, продолжал жить в названии компании до 1950-х годов, причем не только в Европе, а по всему британскому Содружеству. В завершении этого исторического разбора попробуем ответить на вопрос — кто же все-таки выиграл первую войну систем в истории ИТ? Видимо, она окончилась ничьей. Именно конкуренция заставила компании Холлерита и Пауэрса продолжать совершенствовать свою продукцию и создавать инновации — сперва в области счетно-аналитических машин, а затем в компьютерах. Показательно, что Бюро переписи населения США, с которого началась вся эта история, пользовалось продукцией обеих компаний: до Второй мировой войны — оборудованием IBM; а после, в 1952 году Бюро приобрело свой первый компьютер — UNIVAC I — у Remington Rand. Основанные больше ста лет назад компании продолжают работать и сегодня — IBM сама по себе, а Remington Rand — как часть компании Unisys. «Перфокарты. Технико-исторические заметки» Евгения Колесникова. Одна из наиболее подробных книг по вопросу истории перфокарт и счетно-аналитической техники. The Development of Punch Card Tabulation in the Bureau of the Census, 1890–1940, Leon Edgar Truesdell. Подробная история механизации работ в Бюро переписи населения США. Punched-Card Systems and the Early Information Explosion, 1880–1945, Lars Heide. История первых систем обработки больших объемов данных в социально-историческом контексте.",
    "104": "В данном руководстве, подготовленном Брайаном Тимэном, рассматривается кейс, как можно применить настраиваемые поля для фильтрации материалов Joomla, видоизменив макет шаблона. Создайте одну категорию под названием «Поездки». Создайте группу полей с тем же именем, что и у категории. Создайте настраиваемые поля, необходимые для фильтрации контента, например «Отправления», «Навыки», «Доступность», и обязательно поместите их в только что созданную группу полей. Создайте несколько статей в категории «Путешествия». Добавьте значения для настраиваемых полей в каждую статью. Добавьте «Подробнее» к каждой статье. Создайте пункт меню типа Статьи->Категория Блог. Дата создания (и т. д.) — Скрыть Создайте переопределения для components->com_content->category. Откройте макет для /templates/template_name/html/com_content/category/blog.php. Вставьте приведенный ниже код перед строкой 16. Вставьте приведенный ниже код перед закрывающим тегом php в строке 51. Сохраните и закройте файл. Откройте переопределение для /templates/template_name/html/com_content/category/blog_item.php. Вставьте приведенный ниже код перед закрывающим тегом php в строке 35. Отредактируйте следующий div, чтобы он выглядел, как показано ниже. Возможно, вы захотите вывести дополнительную кнопку, которая сбрасывает все фильтры. Повторно откройте переопределение для /templates/template_name/html/com_content/category/blog.php. Вставьте следующий код сразу после второго добавленного вами блока кода: Вакансии и предложения работы по Joomla: фуллтайм, частичная занятость и разовые подработки. Размещение вакансий здесь. Менеджер проектов. Создание сайтов, ecommerce",
    "105": "В каком-то смысле человеческая печень в операционной Северо-Западного мемориального госпиталя в Чикаго была живой. Кровь, циркулирующая по её тканям, доставляла кислород и выводила отходы, а орган вырабатывал жёлчь и белки, необходимые организму. Но донор умер днём раньше, и печень лежала в пластиковом устройстве. Своей жизнеспособностью орган был обязан этой машине, которая сохраняла его для трансплантации нуждающемуся пациенту. «Это немного похоже на научную фантастику», — говорит доктор Даниэль Борха-Качо, хирург-трансплантолог из больницы. Хирурги экспериментируют с органами генетически модифицированных животных, намекая на будущее, когда они могут стать источником для трансплантации. Но в этой области уже происходит смена парадигмы, вызванная широко распространёнными технологиями, позволяющими врачам временно хранить органы вне тела. Перфузия, как её называют, меняет все аспекты процесса трансплантации органов: и методы работы хирургов, и типы пациентов, которые могут пожертвовать органы, и результаты для реципиентов. Что особенно важно, хирургические программы, внедрившие перфузию, позволяют трансплантировать большее количество органов. С 2020 года объём трансплантаций печени в Северо-Западном университете увеличился на 30 процентов. В 2023 году число трансплантаций лёгких, печени и сердца в стране увеличилось более чем на 10 процентов, что является одним из самых значительных годовых приростов за последние десятилетия. Без кровотока органы быстро разрушаются. Именно поэтому врачи долгое время считали идеальным донором органов человека, который умер при обстоятельствах, прекративших деятельность мозга, но сердце которого продолжало биться, сохраняя жизнеспособность органов до тех пор, пока они не будут подобраны реципиентам. Чтобы минимизировать травмы органов после их извлечения из донорской крови и до подключения к реципиентной, хирурги охлаждают их до температуры чуть выше нуля, значительно замедляя метаболические процессы. Это продлевает период, когда органы можно пересаживать, но лишь на короткое время. Печень сохраняет жизнеспособность не более 12 часов, а лёгкие и сердце — не более шести. Учёные давно экспериментируют с методами сохранения органов в более динамичных условиях, при более высокой температуре и перфузии кровью или другим насыщенным кислородом раствором. После многолетних разработок первое устройство для сохранения лёгких с помощью перфузии получило одобрение Управления по санитарному надзору за качеством пищевых продуктов и медикаментов в 2019 году. Устройства для перфузии сердца и печени были одобрены в конце 2021 года. Устройства по сути перекачивают кровь или насыщенную кислородом жидкость через трубки в кровеносные сосуды донорского органа. Поскольку клетки перфузируемого органа продолжают функционировать, врачи могут лучше оценить, приживётся ли орган в организме реципиента. Опираясь на эту информацию, хирурги-трансплантологи начали использовать органы пожилых или более тяжело больных доноров, от которых в противном случае они могли бы отказаться, говорит доктор Крис Крум, профессор хирургии в клинике Майо во Флориде. «Мы используем органы, которые раньше никогда бы не взяли, и видим хорошие результаты», — говорит он. Перфузия также облегчает изнурительный процесс извлечения и пересадки органов — многочасовые операции, которые врачи часто проводят на скорость, начиная работу посреди ночи и приступая к следующей сразу по завершению предыдущей. Теперь хирургические бригады могут извлечь орган, поставить его на ночь на перфузию, спокойно поспать, и завершить трансплантацию утром, не опасаясь, что задержка с пересадкой повредит орган. Возможно, самое важное, что перфузия открыла возможность донорства органов коматозными пациентами, чьи семьи прекратили поддерживать жизнь, позволив их сердцам в конце концов остановиться. Ежегодно десятки тысяч людей умирают таким образом, после остановки кровообращения, но они редко становились кандидатами в доноры, поскольку в процессе умирания их органы лишались кислорода. Теперь хирурги перфузируют эти органы, либо перенося их в машину, либо, что менее технологично, рециркулируя кровь в этой области тела донора. Это делает их гораздо более привлекательными для трансплантации. С 2020 года количество пересадок печени, совершённых после смерти донора, удвоилось, согласно анализу данных United Network for Organ Sharing, некоммерческой организации, управляющей системой трансплантации в Соединённых Штатах. Когда-то хирурги никогда не использовали сердца таких доноров из-за чувствительности этого органа к кислородному голоданию; в 2023 году благодаря перфузии они пересадили более 600 сердец. По словам специалистов центров трансплантологии, задействовав этот новый круг доноров, они смогут быстрее находить органы для большого количества пациентов, нуждающихся в них в срочном порядке. Доктор Шимул Шах говорит, что программа трансплантации органов, которой он руководит в Университете Цинциннати, практически обнулила список ожидания на пересадку печени. «Я никогда не думал, что за всю свою карьеру смогу сказать такое», — сказал он. Одним из препятствий на пути внедрения технологии может стать стоимость. При тех расценках, которые сейчас требуют производители устройств, перфузия органа вне тела может добавить к цене трансплантации более 65 000 долларов; небольшие больницы могут оказаться не в состоянии предоплачивать эту процедуру. Одна из ведущих компаний, TransMedics, значительно подняла цены после того, как регуляторы одобрили её устройство, что вызвало осуждающее письмо представителя Пола Госара, республиканца из Аризоны, который написал: «То, что начиналось как многообещающая инновация в области медицинского оборудования и возможность увеличить объёмы трансплантации по всей стране, сейчас находится в заложниках у публичной компании, которая потеряла ориентиры». Однако некоторые хирурги заявили, что технология может сэкономить деньги, поскольку пациенты, получающие перфузированные органы, как правило, быстрее покидают больницу с меньшим количеством осложнений и демонстрируют лучшие среднесрочные и долгосрочные результаты. Хирурги все ещё исследуют верхние пределы того, как долго перфузированные органы смогут выживать вне тела, и, хотя технологии уже существенно меняют трансплантацию, некоторые говорят, что это только начало. Доктор Шаф Кешавджи, хирург из Университета Торонто, чья лаборатория стояла у истоков разработки технологий по сохранению лёгких вне организма, говорит, что в конечном итоге эти устройства могут позволить врачам не заменять лёгкие, а удалять, восстанавливать и возвращать их больным пациентам. «Я думаю, мы сможем создавать органы, которые переживут реципиента, в которого вы их поместите», — сказал он. Доктор Ашиш Шах, председатель отделения кардиохирургии в Университете Вандербильта, где проводится одна из самых оживлённых программ по пересадке сердца в стране, согласился с ним, назвав это «святым Граалем». «У вас дефектное сердце, — сказал он. — Я вынимаю его. Я помещаю его в свой аппарат. Пока у вас нет сердца, я могу некоторое время поддерживать в вас жизнь с помощью искусственного сердца. Затем я беру ваше сердце и чиню его — клетки, митохондрии, генная терапия, что угодно, — а потом вшиваю его обратно. Ваше собственное сердце. Вот к чему мы стремимся». Научпоп. Проповедую в храме науки.",
    "106": "Выжимка требований и обязанностей после исследования кучи вакансий, частые хотелки бизнеса и реальные задачи системного аналитика. Отсеиваем лишнее. Покажите своему руководителю, чтобы перестать делать всё, везде и сразу. Привет! Меня зовут Кир @akxkir — системный аналитик в Монете — и за 5+ увлекательных лет в IT я успел примерить шляпы фронтендщика, техписателя, лида QA, тимлида и архитектора. Сегодня хочу поделиться взглядом на роль САнов в команде и компании в целом: кто они такие, кому и зачем нужны и какие задачи выполняют. Рассуждения будут полезны новичкам в системном анализе и всем тем, кто имеет счастье работать с САнами. * Вы уже догадались, но «САном» я обозначаю роль Системного Аналитика (а «СА» — это Системный Анализ). Начнём с парочки хрестоматийных определений СА / САна (цитаты сокращены, расставлены акценты). Совокупность методик и средств, используемых при исследовании и конструировании сложных систем, разработка методов выработки, принятия и обоснования решений при проектировании, создании и управлении системами. The system analyst role leads and coordinates requirements elicitation and use-case modeling by outlining the system's functionality and delimiting the system; for example, establishing what actors and use cases exist, and how they interact. Специалист по решению сложных организационно-технических проблем, имеющих междисциплинарную природу. Ответственный за анализ потребностей пользователей на предмет возможности их удовлетворения. Также его называют «постановщик задач». Основным продуктом являются организационно-технические решения, оформляемые как техническое задание. Звучит уже достаточно убедительно и монументально, чтобы у начинающего САна случился преждевременный кризис самоопределения. Но ничего, дальше будет хуже! А потом лучше. Но вы держитесь! Продолжим накидывать на вент фактуру — посмотрим на обязанности и требования к САнам, которые чаще всего указывают в вакансиях на hh (примерно по 30 вакансиям). Также с акцентами. Интервьюировать заказчиков и пользователей. Выявлять, собирать, анализировать и документировать требования. Прорабатывать варианты оптимизации текущего функционала. Предлагать решения возникающих проблем. Планировать RoadMap и вести бэклог. Выступать связующим звеном между разработкой и заказчиком. Прототипировать интерфейсы и проектировать бизнес-процессы. Участвовать в восстановлении документации проекта. Разрабатывать ТЗ. Ставить задачи разработчикам и дизайнерам. Координировать работу нескольких команд разработчиков. Разрабатывать и проектировать архитектуру. Разрабатывать сценарии тестирования и участвовать в процессах тестирования. Контролировать сроки выполнения и этапы работы. Составлять функциональные спецификации. Проектировать use cases. Проектировать внутрисистемные и внешние интеграции (REST API). Проектировать реляционные БД на уровне логической модели. Составлять пользовательскую документацию (руководства, инструкции). Проводить презентации разработанного функционала. Знание архитектурных подходов проектирования высоконагруженных систем. Представление о форматах данных, применяемых в веб: XML, HTML, JSON, WSDL. Понимание способов взаимодействия браузера и сервера: куки, сессии, HTTP, AJAX, сокеты. Знания в области технологий: REST, SOAP, SQL, шифрование, балансировка нагрузки. Примеры самостоятельно разработанных ТЗ, алгоритмов, интерфейсов. Высокая коммуникабельность. Умение работать в Jira, Confluence, вести подробную документацию требований и задач. Хорошее понимание технического процесса разработки по Scrum и Agile. Умение разбираться в новых сервисах и интерфейсах. Знание английского языка не ниже Upper-Intermediate. Опыт работы с брокерами сообщений (rabbit, kafka). Понимание жизненного цикла процесса разработки. Общее понимание микросервисной и монолитной архитектуры. Опыт проектирования REST API. Умение писать Swagger, навыки работы в Postman. Навыки декомпозиции работ по аналитике и для продукта в целом. Знание и опыт использования инструментов визуализации: Figma, Visio, Miro, Draw.io. Опыт ведения переговоров, проведения презентаций. С фактурой закончили. Теперь давайте разбираться, как со всем этим добром должен управляться САн (спойлер: не должен, ну или точно не со всем). Поговорим о системах. Границы системы, как правило, чётко не определены — можно найти «серую зону», которая напрямую и не находится в самой системе, но существует ради её поддержки. Назовём эту зону контекстом. Наша система может пересекаться контекстом с какой-либо другой. Это что касается внешних границ системы. Рассмотрим теперь её внутренности. Состав системы рассмотрим на примере IT-продуктов, хотя основные принципы будут те же и для других областей. Интерфейсы — что-то, что воспринимает входные данные от пользователя (GUI, файлы для импорта, физические кнопки, пины и т. п.). Обработчики — собственно, код или его аналог (например, электронная схема, чип, шестерёнки, сообщающиеся сосуды — всё зависит от рассматриваемой системы), выполняющий основные бизнес-функции системы. Данные — данные, порождаемые и/или используемые самой системой (записи в БД, сообщения в очереди, перфокарты, диски и т. д.). Документы — некоторые артефакты реального мира (распечатки отчётов, справки, билеты и т. п.). Связи между элементами — обозначают поток данных или управления, это процессы внутри системы. Добавляем сигналы — это могут быть простые сообщения, вызовы, мигания лампочек и прочее. Сигнал может быть входным или выходным. … а потом обработчики делятся на сервисы или модули, которые могут общаться через сигналы и использовать общие данные и разные интерфейсы... Уже начинает сосать под ложечкой? Не будем доводить пример до реальной системы, да и цель не в этом. Давайте обратим внимание не столько на «объекты» внутри системы, сколько на связи между ними. Заметили, как быстро растёт количество зелёных дорожек? А ведь это именно то, как система работает, без знания этих процессов нам сухой перечень модулей и сигналов ничем не поможет, нам нужно знать взаимосвязи. Финальная картинка — сложность есть не только внутри самой системы, но и в её контексте: какие роли за что отвечают, какие артефакты используются или порождаются системой. И да, информационной системой, по-хорошему, является весь окружающий его контекст. Хотя для удобства восприятия такое разделение кажется вполне сносным. Получаем, что для успешного анализа системы (цель анализа тут опустим) САн должен разбираться в составных элементах системы и связях (процессах) между ними. Разумеется, это даёт огромное преимущество и в разборе инцидентов, и в написании документации, и в тестировании — практически для любой работы с системой. Теперь понятно, из-за чего на рынке такой широкий спектр требований к САну. Ровно поэтому на практике САны часто могут заниматься не своими обязанностями, а это совсем не гуд. И где-то в этой суматохе потерялось написание спецификаций API, и теперь фронту и бэку приходится работать не просто последовательно в режиме с взаимными прерываниями, уступками и костылями, но ещё и периодически переключаться на вечные баги от техпода или QA. Но не думайте, что с появлением архитектора и САна всё сразу поменяется — обязанности нужно разделять, компетенции развивать, а границы отстаивать. САн — это междисциплинарный специалист, который точечно применяет знания об общем контексте системы. САн физически не сможет быстро выполнять задачи в любой точке системы (нужно погружение), но он способен оказывать консультации по общим принципам работы её компонентов. Перейдём к конкретной конкретике. Конечно, всё зависит и от специфики проекта, и от наличиствующих ролей, и от так-истерически-сложилосьстности — поэтому вот список ролей внутри нашей команды для ориентира: Лид продактов (продукт сложный, нужен координатор-вижионёр). Продакты (по одному на направление внутри продукта, отвечают за бизнесовую часть). САны (в идеале по одному на продакта, но сейчас нас меньше). Два специалиста QA (расшарены на несколько команд). Разработчики (тоже +/- делятся по направлениям). Приём задачи или проблемы (от бизнеса её нам приносит продакт). Поиск затронутых частей системы (собственно, сам анализ). Выявление и формализация требований, техническая постановка задачи (есть хотелка — мы выдаём требования и задачи). Документирование и актуализация требований (по результату работы над задачей смотрим, что фактически сделали, дополняем базу знаний реализованными решениями). Непонятные сроки задачи, кому и зачем она нужна. Придерживаться шаблона заполнения задач всей командой. Тех. поддержка Требуется поиск по логам вместо глубокой аналитики. Вести реестр частых ошибок и способов их решения, брать задачу после однозначного указания ошибки по логам. Составление тестовых сценариев, выявление граничных условий. Передавать функциональные сценарии на проработку QA-инженеру до отправки задачи в разработку. Выбор конкретных программных решений. Не привязываться к конкретной реализации, если её ещё нужно выбрать. Детализация до уровня наименований в коде, привязка алгоритма к технической реализации. Согласовывать оптимальный вариант решения задачи с разработчиком, предлагать несколько вариантов. Сегодня мы лишь слегка приоткрыли дверцу к такой удивительной, многогранной и неоднозначной роли, как Системный Аналитик. И сколько ещё впереди! А сейчас велком в комментарии — всегда интересен опыт коллег по цеху (как САнов, так и тех, кто за ними наблюдает — не просто так ведь вы открыли эту статью).",
    "107": "Привет. Гоу разберем почему вам может быть полезно вносить вклад в сообщество программистов. Речь пойдет про запросы на внесение изменений через форк проекта. Запросы на принятие изменений позволят поближе познакомиться с системой Гит, освоить незнакомый функционал. Запросы на принятие изменений не ограничиваются лишь целью внести вклад в общество: в крупных компаниях запросы – обязательная часть работы, одобрение работы программиста как сотрудника, работающего над проектом совместно с другими. Очевидно, вклад в чужой репозиторий требует прочтения чужого кода, что само по себе учит новому. Подход к написанию, объявление полей и свойств, применение атрибутов, событий, делегатов, пространств имен, assembly definition (в проектах Unity); шаблоны проектирования на конкретном примере, вложенные методы, способы инициализации, приемчики старой школы программистов по типу использования адреса ячейки памяти в качестве индексации коллекции… Да просто обнаружить, что лучше в методе Awake() проводить инициализацию игрового объекта (внутренняя работа), а в Start() сделать ту часть работы, которая  требует связи с внешним миром (подписаться на событие, подтянуть параметры другого объекта). Пример из Unity/C# проекта. Параметр персонажа, такой как здоровье или броня, можно прописать в одну строчку, ограничив доступ к изменениям и не потерять возможность отображения в инспекторе. Могу порекомендовать посмотреть ролик с фишками C# на эту тему. Раз вы хотите внести изменения в чужой код, значит это… Рефакторинг! Отличный способ попрактиковаться в этом направлении. Приятное с полезным. Видишь повторяющиеся строки кода? Вынеси в отдельный метод. Видишь смешение логики? Извлеки одну логику в отдельный класс. Схожее поведение у разнотипных классов? Вынеси интерфейс. Неразлучно передается много данных? Можно собрать в структуру. Метод принадлежит чужому классу, нелогичное расположение? Перенеси в более очевидное место. Прежде чем менять чужой код, стоит почитать методы рефакторинга и запахи кода. Бесполезные или неграмотные запросы никому не нужны. Примечание. Мы же все понимаем, что бездумно следовать всем выше перечисленным примерам не стоит? Не все поголовно повторяющиеся строки кода выносятся в отдельный метод. Пример с принципами SOLID. Если неукоснительно следовать принципу Брабары Лисков (если в функцию вместо базового класса подставить наследника, то логика не должна быть нарушена; наследники не меняют поведение предков), то можно сказать, что виртуальные методы вам строго запрещены. Конечно это не так. Конечно же при внесении изменений проект становится лучше. Я не стал писать этот пункт первым в списке, т.к. хотел прежде всего подчеркнуть полезность запросов для самого автора этих реквестов. Вклад в сообщество программистов учит уважать чужой стиль написания кода. Твой запрос не одобрят, если вы напишите все по-своему, без учета уже написанного. Уважение чужого труда поможет не только одобрить запрос на вливание, но и позволит более эффективно работать в команде на постоянной работе (или при участии в джемах), будет легче наладить общение с коллегами по цеху. Например, тут меня ругают за то, что я, помимо прочих изменений, удалил скобки во всех однострочных конструкциях. Это нарушило общий стиль написания. В процессе изменения понадобится общаться с владельцем проекта. Даже самые полезные изменения не будут приняты в отсутствие должного обращения. Вежливость, конструктивность помогут вам. Конечно это тоже софт-скиллы, которые помогут наладить общение с коллегами. Понравилась статья на тему как правильно делать запросы на принятие изменений, советую. Стоит почитать как авторам запросов, так и ревьюерам. Запросы на принятие изменений – мощный способ совершенствоваться, помогать в этом другим. Умение писать грамотный и понятный код, знать тонкости игрового движка и языка программирования, налаживать общение в процессе работы – вот чему можно научиться в процессе. Главное – получайте удовольствие от проделанной работы!",
    "108": "Во всём мире активно развёртывают инфраструктуру мобильной связи 5G. В то же время различные компании ведут разработку 6G-технологий, реализуются и промежуточные проекты. Речь идёт прежде всего о сетях 5.5G, или 5G-Advanced. В Китае уже начали практическое освоение этого типа связи. О том, что это за технология и как работает, читайте под катом. Для обычного пользователя, то есть нас с вами, разницы почти нет. Загрузка файлов — мгновенная. Наибольшую ценность новые технологии несут для бизнеса, ведь на их основе можно реализовать очень много всего, включая автономные автомобили, IoT-сети едва ли не глобального масштаба и т. п. 5G во многих случаях не может обеспечить работу крупных предприятий, например автоматизированных фабрик, целых парков беспилотных автомобилей или морских портов. Так что следующие поколения мобильной связи — отличный выход. Кроме того, что касается 5G, то в сетях большинства компаний пропускная способность канала ниже пикового значения в 20 Гбит/с, иногда — намного меньше максимального значения. Вот результаты измерений скоростей в разных странах, по данным Ookla. 5.5G, или 5G-Advanced, называют промежуточной ступенью при переходе к 6G. От существующих сетей связи пятого поколения её отличают повышенная скорость приёма и передачи данных и меньшая задержка сигнала. 5G-Advanced обеспечивает до 10 Гбит/с на скачивание и до 1 Гбит/с на отправку. Это, конечно, пиковые значения, на практике достичь их будет крайне сложно, да и незачем. Задержка, к слову, составляет до 4 мс. Технология эта не новая, спецификации 5.5G были заданы ещё три года назад, в 2021 году. 2024 год был указан как время коммерческого внедрения стандарта — и, похоже, планы реализуются в срок. Активнее всего работают в этом направлении китайские операторы. Основное достоинство «промежуточного» стандарта связи — бесперебойность. Она сможет обеспечить коннектом «без единого разрыва» сотни миллионов подключённых устройств — от пользовательских смартфонов до промышленного оборудования, где задержки недопустимы. Аналогичным образом будут обеспечиваться надёжной связью и IoT-сети с их миллионами или даже миллиардами датчиков. Точность позиционирования, кстати, повышается до сантиметров. Также стандарт предусматривает поддержку спутникового интернета без наземных станций. Очень высокая пропускная способность сети 5.5G в сочетании со сверхмалой задержкой даёт возможность значительно расширить мониторинг любых IoT-датчиков, работать с дополненной/виртуальной реальностью, новыми типами интерфейсов. В этой стране постоянно внедряются всё более современные стандарты связи. Сейчас многие китайские провайдеры работают над развёртыванием технологий 5.5G. Поддерживать этот стандарт первыми станут телефоны брендов из КНР, в первую очередь Oppo. Более того, в соцсетях Поднебесной распространяется фотография смартфона Find X7 Ultra, который отображает значок поддержки сетей 5.5G. Китайцы утверждают, что Find X7 также получит возможность работы в сетях 5.5G. Так что среди пользовательских устройств именно эта серия первой станет поддерживать новую технологию. Кроме этого телефона, в 2024 году появится 15–30 моделей мобильных гаджетов, поддерживающих новейший стандарт. Если говорить о поддержке сетей 5.5G операторами, то China Mobile планирует к концу года покрыть сетью 5.5G более 300 городов в Китае. Уже созданы демонстрационные зоны по всей стране, как раньше это было с сетями 5G. Также развивает новые сети и компания Huawei. Она представила первое в мире корпоративное решение для опорной сети 5.5G. Руководство компании считает, что 5G-стандарт и все последующие постепенно развиваются и реализуются. Так, несмотря на все сложности развёртывания 5G-инфраструктуры, уже около 20% абонентов мобильной связи в мире используют сети пятого поколения. К слову, Huawei представила 8 «инновационных практик» 5.5G, которые помогут операторам строить сети 5.5G во всех диапазонах частот. По мнению представителя Huawei, президента по продажам и обслуживанию информационно-коммуникационных технологий (ИКТ) Ли Пэна (Li Peng), развитие постепенно продолжается: «Пользователи генерируют 30% всего сотового трафика и обеспечивают 40% доходов от мобильных услуг. Коммерческое использование 5.5G начнётся в 2024 году, и по мере слияния 5.5G, искусственного интеллекта и облачных технологий операторы смогут раскрыть потенциал новых приложений и возможностей». Кроме Китая, тестовое внедрение 5.5G идёт и в других странах, включая Ближний Восток, Европу, Азиатско-Тихоокеанский регион. Вполне вероятно, что в этом году вступят в работу и прочие сети, кроме уже названной выше. Эксперты прогнозируют, что обновление инфраструктуры 5G будет идти быстрее в потребительском секторе, нежели в корпоративном. Так, к 2030 году около 75% базовых станций, обслуживающих рядовых абонентов, переведут на технологию 5G Advanced. В коммерческом сегменте показатель окажется ниже — приблизительно 50%.",
    "109": "Несколько внеочередная заметка... Со времён Короны я работаю дома на подстольной дорожке. Когда я искал что же купить, я наткнулся на небольшой местный магазинчик FitOffice, где выбор был TR1200 или TR5000. По отзывам обе хороши, но одна требует регулярного смазывания и предполагает использование на несколько часов в день -- а вторая не требует смазывания и подходит для ходьбы хоть весь день. Жена настояла на второй :) Если вкратце, то за 4 года использования, я находил на ней всего около 10 тысяч км, но она начала шуметь до невозможности -- по замерам телефона более 60 дБ на 5км/ч, да еще с жуткой высокочастотной составляющей, которая делала шум еще хуже. Ну то есть я мог за ней работать (стол экранировал от прямого шума), но шум рядом с ней становился отвратительным. Официальный суппорт по почте сказал мне \"это нормально\" -- а FitOffice сказали что это похоже на проблему мотора -- аналогично высказался кто-то на reddit, да и на мой слух звучало как проблема какого-то из подшипников. В качестве простой меры мне предложили поменять щётки мотора, эту инструкцию я нашел (осторожно: видео). Но щетки оказались почти не израсходованными (меньше 1мм съелось), и на звук замена не повлияла. Осталась опция только замены мотора. Учитывая, что дорожка уже за гарантией, а замена сама выглядела как простое дело -- я решил поменять его сам, хоть и не нашел никакой инструкции. Так что теперь заполняю этот вакуум :) Учтите: я тыжпрограммист и не должен уметь менять моторы. Ну то есть заменить мотор в дорожке я могу, но не потому, что тыжпрограммист. Если решитесь что-то делать по инструкции, делайте это на свой страх и риск и действуйте по обстоятельствам! Я не мастер и эта инструкция нифига не официальная. Телефон, чтобы зафотать как оно выглядело до начала работ -- чтоб знать как всё скидать обратно потом. Перед вскрытием дорожки её следует ОБЕСТОЧИТЬ. Проверьте, что всё обесточено и все кабеля от неё отключены. Следует выкатить дорожку из-под стола. Слева: управляющая электроника. Мотор подключен разъёмами на концах толстого красного (плюс) и черного (минус) проводов, плюс сине-зеленый провод прикручен к болту заземления. В середине расположено большое ферритовое кольцо. Обратим внимание как идут провода: земля проходит прямо через него, а питание делает три витка. Справа: собственно, мотор. Во-1х натяжение ремня передачи в зоне (1). Следует понажимать пальцем, чтоб запомнить сколько примерно надо будет в конце натягивать. Система натяжения организована в зоне (2). Она состоит из плоской гайки (a), которая держится в пазу металла, болта с шестигранной головой (b) (и да, места тут мало, надо короткий шестигранник на 6), плюс зажимается чтоб не разбалтывалось гайкой (c) после того как натянуто как надо. Мотор прикручен к металлическому основанию, которое в свою очередь расположено в длинных отверстиях (3), то есть мотор может двигаться вперед-назад, чтоб можно было натянуть ремень как надо. Разумеется, эти 4 болта прикручиваются из-под дорожки снизу, для удобства обслуживания. Выключить дорожку. Убедиться, что дорожка выключена и все провода от неё отцеплены. Ослабить контрагайку (c) в системе натяжения, затем вращая болт натяжения (b) снимаем гайку (a) совсем. Выкручиваем 4 болта, которые держат мотор. Для этого дорожку надо поднять на 8-10см, чтоб рука с ключом туда пролезала. Я поднял на боковых ножках дорожку на максимум что они позволяют, а потом складывал на левую руку чтоб приподнять и подпирал ногой, а правой выкручивал, но удобнее может оказаться взять большие деревянные бруски на 10см и положить по углам дорожки -- главное зафиксировать так, чтоб дорожка не могла с них соскочить и свалиться всеми 50 килограммами на руки! Отключить провода от мотора: выдернуть черный и красный из платы, вытянуть из феррита и открутить землю (мотор там верхний, так что снять легко). Металлическая пластина, на которой лежит мотор, прикручена к мотору двумя болтами под шестигранник -- откручиваем её от мотора. Вот теперь можно затянуть все 4 болта накрепко -- и еще раз проверить натяжение. (Если что-то подкладывали под дорожку -- теперь уже можно убрать). Затягиваем контрагайку (c) в системе натяжения. Протягиваем синезеленый провод от мотора через феррит и накручиваем на болт заземления. Протягиваем провода питания, делая три витка на кольце, и подключаем к плате к тем же местам что до этого. У меня черный внизу (на плате \"-\") а красный вверху (плюс написан на плате). Прокручиваем саму дорожку рукой -- мотор должен легко вращаться и не должен издавать никаких посторонних звуков. Складываем крышку на место и затягиваем болты по периметру. Перед затягиванием боковых болтов чуть-чуть приподнимаем крышку над дорожкой, чтоб крышка не царапалась за ленту, иначе опять же будут странные звуки. После замены мотора шум упал с 63 дБ когда она вращается на 5км/ч вхолостую -- до 52 дБ. Божественно! Программист / сисадмин (Sr. SRE)",
    "110": "Привет! В этой статье c помощью простого чек листа за пару шагов выясним, являются ли функции, которые вы пишете в своем vue коде, настоящими composables. Согласно документации, composables это функции, которые благодаря использованию внутри себя composition API, инкапсулируют и переиспользуют логику, в работе которой задействовано состояние приложения (локальное или глобальное). Если ваша функция принимает на вход определенные числа и возвращает наименьшее значение из них, то у такой функции нет работы с состоянием, а значит это не может называться composable, в действительности это функция утилита. Лишь одного в попадания в пункт будет достаточно, чтобы пройти тест, вам не нужно попадать во все три. Ваша функция внутри себя использует методы жизненного цикла. Функция выше не манипулирует с состоянием, но по-прежнему является composable, потому что использует методы жизненного цикла компонента. Она удовлетворяет первому пункту. Ваша функция внутри себя использует другие composables. Конечно, вы вполне можете выстраивать композицию из composable функций, и если единственная работа функции заключается в том, чтобы внутри себя вызвать другую composable функцию, то она тоже является таковой без исключений. Например, функция может использовать внутри себя написанный вами useFetch , использовать useSomething из библиотеки vue-use и тому подобное. У вашей функции есть логика работы с состоянием (stateful logic). Чаще всего это работа с ref, который хранит определенное состояние, и изменение или преобразование этого состояния происходит в функции. Composables - это функции на composition API, которые переиспользуют логику с методами жизненного цикла, другими composables и/или хранят и преобразуют состояние. ✌️ Всегда рад предложениям и обратной связи - bronnikovmb@gmail.com",
    "111": "Привет! Меня зовут Андрей Чучалов, я работаю в билайне, и в этом посте я расскажу про оптимизацию параметров запуска приложений в Spark, поиск проблем и повышение производительности. Разберем запуск приложений Spark в базовой и расширенной версиях, покажу методы расчёта основных параметров работы приложения для производительности и эффективности использования доступных ресурсов кластера. Бонусом — о том, как всё это привязано к деньгам, и где сэкономить можно, а где — не стоит. Спараметризировать приложение — это не такая уж грандиозная задача, а вот попытаться понять взаимосвязь эффективности работы приложения со стоимостными параметрами такой работы — это уже сложнее. Тут вам пригодится своеобразное «боковое зрение». В рассказе и на примерах я буду исходить из того, что у нас по умолчанию процесс ETL-обработки данных правильно, с самой программой всё ОК и она корректно спроектирована. И оборудование в составе кластера тоже рабочее и достаточное для запуска приложения. Это позволит говорить именно о влиянии параметров на эффективность. Есть приложение и кластер. Мы получаем контекст, а через него— доступ к тем ресурсам, которые осуществляют непосредственный расчёт. Кластер — это совокупность виртуальных серверов, зачастую без привязки к физической технике. Нод, которые являются воркерами, может быть несколько — одна, две, N, в зависимости от объёма используемого физического кластера. Внутри нод есть экзекьюторы, которые обладают свойствами, такими как количество ядер и объем памяти. Этот кластер пытается загрузить определенные данные, которые лежат в определённом месте (хранилище объектов, HDFS-хранилище, в общем, где вам привычнее). И после этого возвращает нам результат. В нашем случае объектами параметризации будут процессор, память и параллелизм, который принимает во всём этом непосредственное участие. Вот пример конфигурации. А теперь давайте немного на живом коде, у меня есть пример на Яндекс Облаке. Входим в spark shell. Сделаем приложение, которое будет работать из коробки. Здесь у нас spark сконфигурирован в процессе создания кластера, так что никаких особых параметров использования ресурсов мы ему не задаем. У нас включена динамическая аллокация ресурсов — spark сам для себя решит, какое количество ресурсов он использует для проведения наших расчётов. В моём примере были общедоступные данные по ковидной статистике, загруженные в HDFS, один CSV-файл занимает 12 гигабайт. Это нужно, чтобы за адекватное время проверить, сколько будет идти расчёт. Подгрузить данные очень просто — мы определяем наш датасет в виде некоторых комментариев, связанных с тематикой ковида. Запустим простую операцию, которая подсчитает количество данных в датасете. На кластере параллельно работает 10 тасок: у нас параллельно используются 10 ядер для расчета вычисления количества данных внутри нашего датасета. Каждый мой кластер состоит из одного мастера и трёх воркеров. На каждый воркер приходится 4 ядра и 8 гигабайт памяти. Грубо говоря, на вычислительный расчет у меня получается 12 ядер. За 25 секунд с использованием 10 ядер мы получили количество данных — порядка 17 миллионов. Всё это spark посчитал за 25 секунд именно с учётом динамического распределения ресурсов. Тут сразу возникает главный вопрос — почему spark не дал нам для работы все 12 ядер, а использовал лишь 10? Мы же, вообще-то, оплачиваем мощности из разряда 12 ядер, куда ещё 2 делись? Выходит, что мы теряем какие-то средства, платя за мощности, которые не используем. Понятно, что spark оставил тут себе точку входа для второй задачи. На случай, если кто-то еще придёт на этот кластер с желанием что-то запустить, тогда новый расчёт сразу же запустится, просто с меньшим количеством ресурсов. Но он не будет дожидаться окончания моих вычислений, которые уже выполняются. Звучит здорово, но для меня это невыгодно — раз я запустил кластер для одного приложения, то я хочу использовать все доступные мне и оплаченные ресурсы. Значит, spark надо настроить. Сделать это можно через передачу определенных параметров, где я укажу количество экзекьюторов. Первое, с чего начинаем, это драйвер, так как у нас драйвер — машинка послабее, там всего два ядра, Собственно говоря, так мы их и указываем — driver cores 2 executor cores 4. Самих экзекьюторов у нас три. Плюс для каждого экзекьютора я определяю объем памяти и, соответственно, могу передать некоторые параметры — партишены в нашем случае. Если бы мы запускали join, это было бы полезно. Но так, как мы рассчитываем просто count, то здесь они никакой роли не играют. Главное, что я хочу сделать — выключить динамическую аллокацию ресурсов на spark как раз для того, чтобы вот эти передаваемые параметры работали, и spark их не переписывал собственными значениями. Проверим, сработают эти настройки или нет. Чтобы все эти настройки вступили в силу, все должно быть хорошо на самом кластере. Итак, у меня здесь executor-cores 4, а самих экзекьюторов — 3. Значит, spark должен использовать по-максимуму все 12 ядер. Проведу ту же самую операцию — определю датасет. При таких настройках у нас заработают все 12 ядер одновременно вместо 10, которые были в прошлый раз. Это же будет работать, если кластер будет побольше, например, 24 ядра, 48 или вообще до 750 — в свое время была такая практика на очень достаточно тяжелых задачах. Вместо 25 секунд в первый раз сейчас это заняло 21 секунду, Положим всё это на вот такой график. У нас есть зависимость количества времени работы нашего приложения от количества ресурсов, которые мы используем. И, соответственно, чем больше ресурсов мы используем, тем меньше времени тратится на выполнение нашей программы. Поэтому график времени здесь именно падающий, в зависимости от количества ресурсов, которые мы используем. Всем известно, что мы не можем бесконечно ресурсы увеличивать и так же не можем их бесконечно уменьшать. Просто потому, что у нас найдётся такой этап времени, когда увеличение количества ресурсов станет бесконечным — оно не приведет к обозримому уменьшению времени, за которое наша программа будет рассчитывать результат. Ровным счетом правдива и обратная ситуация. Например, если мы посмотрим слева от области А, то мы в принципе не можем не использовать ресурсы для вычисления. Поэтому существует и ситуация, когда уменьшение ресурсов даже в небольшом количестве будет существенно увеличивать время работы нашего приложения. Так что обозначим ту область, где мы можем эффективно менять параметры для нашего приложения, как область А. Перемещаться по этому графику можно с помощью параметров. Spark воспринимает переданное нам количество параметров для экзекьюторов, количество памяти и количество ядер. То же самое для драйвера и плюс дополнительные какие-то параметры. С количеством ядер для экзекьюторов всё понятно — в каждом экзекьюторе мы должны использовать определённое количество ядер. Само собой, чисто физически мы можем быть ограничены тем сервером, который используем. На своем сервере для примера я мог бы сделать и больше ядер, на то он и кластер. На самом деле, могло бы быть и два экзекьютора по шесть ядер. Но, скажем так, у нас есть определенная зависимость — чем больше данный параметр, тем меньше количество экзекьюторов. То есть на шесть ядер было бы уже два экзекьютора. Все, что кратно 12, мы можем все эти пары получить. Соответственно, при уменьшении экзекьюторов у нас падает параллелизм, но малое количество ядер увеличивает объем операций ввода вывода. Практическим путём мы для себя этот параметр стали устанавливать равным пяти, реже четырём, еще реже — шести. В общем, 5 — это такой де-факто получившийся стандарт. Но он не так сильно влияет на производительность. А вот что влияет — так это память. О ней (и о параллелизме) и будет вторая часть.",
    "112": "В середине марта ко мне в руки попал смартфон realme 12 Pro. Несколько недель я изучал его возможности, а сейчас готов поделиться обзором, своим опытом использования и мнением. В статье оцениваем дизайн, производительность и качество снимков на камеры смартфона. У realme 12 Pro богатая комплектация, которая на фоне даже нынешних флагманов выглядит очень хорошо. В ярко-жёлтой коробке находится сам смартфон, прозрачный силиконовый чехол, метровый кабель USB-A на USB-C, 67-ваттный адаптер питания, скрепка для извлечения лотка SIM-карты и документация. Есть всё необходимое, чтобы прямо сейчас начать пользоваться устройством. Особенно удачно, что чехол прозрачный. Мне очень нравятся именно такие. Сквозь них видно дизайн задней крышки, но и всё под защитой. Из минусов — силикон со временем желтеет, но стоят такие чехлы недорого. Можно  часто менять, если критично. Чехол realme 12 Pro защищает не только заднюю крышку, но и блок камер. Вокруг него сделали бортики, чтобы камеры не касались поверхности. Такие же бортики есть и на передней части для защиты экрана. За комплектацию точно можно похвалить. Может быть, для пользователей Android это стандартный набор допов к смартфону. Но для меня, пользователя iPhone, прям удивительное разнообразие. Это надо же, положить в коробку что-то кроме смартфона и кабеля! Над дизайном realme 12 Pro работали в коллаборации с Оливье Савео (Ollivier Savéo) — дизайнером премиальных часов. Он успел принять участие в проектах часовых компаний Rolex, Roger Dubuis, Piaget, Breitling и Quentin. В работе над realme 12 Pro дизайнер стремился сделать из повседневного гаджета люксовый аксессуар. Задняя крышка покрыта веганской кожей. Это материал из силикона, который повторяет текстуру кожи. Он устойчив к загрязнениям и мягкий на ощупь. У меня на обзоре оказалась версия в цвете «Синее море»: синяя кожа и золотые вставки. Цвет приятный, в руке смартфон лежит удобно. Блок камер массивный, но не перевешивает. Сам блок камер выполнен в виде рифлёного безеля из 300 металлических линий по всей окружности. Отделка задней крышки сделана в стиле часовых браслетов Jubilee. Ромбовидный узор выглядит интересно. Мне, любителю минимализма, дизайн показался очень нагруженным. Но в целом вопросов к нему нет. Всё лаконично, а главное, что элементы оформления не мешают. На боковых гранях realme 12 Pro находится кнопка блокировки и качелька громкости, снизу — лоток для двух SIM-карт, микрофон, динамик и порт USB-C. Смартфон защищён по стандарту IP65. Это спасает его от пыли и капель. Нырять и вести подводные съёмки не выйдет. Во всяком случае, без специального кейса. Изображение выводится на 6,7-дюймовый OLED-экран с разрешением 2412×1080 пикселей. Частота обновления составляет до 120 Гц. Это зависит от выбранных настроек и разработчиков конкретного приложения. В настройках доступны следующие режимы: Автовыбор — баланс между продолжительностью работы и плавностью. Экран покрывает 100% цветового диапазона DCI-P3, и это видно сразу. Цвета очень сочные и приятные. Плавность при 120 Гц тоже радует, но работает не везде. Интерфейс полностью адаптирован под высокую частоту, а приложения — далеко не все. К примеру, в Telegram всё «летает», а в картах 2ГИС сразу заметна просадка. Но это не проблема смартфона, тут больше вопросов к разработчикам самих приложений. В настройках есть режим защиты глаз — фильтр синего. Можно включить его по умолчанию или настроить расписание. К примеру, чтобы защита активировалась только в тёмное время суток. Тут ничего необычного, всё просто хорошо работает. Экран сам по себе качественный и приятный, но сейчас очень сложно найти смартфон с экраном, к которому будут вопросы. Даже на бюджетные модели ставят приличные дисплеи. Запаса яркости хватает для комфортного использования на улице в солнечную погоду. Анимации выглядят приятно, цвета  насыщенные. Устройством приятно пользоваться. Читал на нём статьи, листал соцсети и смотрел видео. Дискомфорта не возникало даже при длительном использовании. Видео особенно приятно: есть поддержка стерео по технологии Dolby Atmos. Ещё важно отметить, что под экраном установлен сканер отпечатка пальцев. Работает он шустро, и не было случаев, когда вместо отпечатка приходилось вводить пароль. В верхней части экрана есть небольшой вырез под камеру в виде точки. Выглядит органично и не мешает. Эта же камера используется для разблокировки телефона по лицу. На экран из коробки наклеена плотная защитная плёнка. В смартфоне установлен аккумулятор на 5000 мАч. Заряда хватает на полный день использования, но всё зависит от сценария. Если активно играть в требовательные проекты, то понадобится подзарядка. В моём случае realme 12 Pro выдерживал почти три часа в режиме навигатора в машине с подключенной музыкой и весь день использования в режиме чтения статей и Telegram-каналов в перерывах. В конце дня оставалось 20-30%. Смартфон оснащён функцией быстрой зарядки, а в комплекте есть адаптер на 67 Вт. С ним до 100% аккумулятор заряжается за 40-45 минут. В качестве операционной системы на realme 12 Pro установлен Android 14 с оболочкой realme UI 5.0. Приятный плюс в том, что компания официально присутствует на российском рынке. Это значит, что всё полностью локализовано и адаптировано. Смартфон можно доставать из коробки и сразу пользоваться. Не надо искать русскоязычную прошивку и возиться с установкой. Если говорить об общих впечатлениях, то всё работает плавно, приятно, а интерфейс не вызывает вопросов. Не приходится думать, куда надо нажать и как свайпнуть. Это не чистый Android, но оболочка продуманная. Есть и собственные фишки realme. К примеру, конструктор анимированных аватаров OMOJI. Можно создать собственных персонажей, которые повторяют движения пользователя. Не выглядит, как очень нужная функция, но для общения в социальных сетях — почему бы и нет. Особенно порадовала технология O-Haptics. Она имитирует текстуру материалов с помощью вибрации. Выглядит очень приятно и влияет на восприятие системы. В настройках есть демонстрационный ролик, с помощью которого можно почувствовать всю прелесть функции. В остальном realme UI меня порадовала. Очень удобная оболочка без обилия лишних функций, которыми пользуешься только в первый день. Практически всё нужно и по делу. Из плюсов можно отметить открытость Android. Это значит, что можно сразу же установить RuStore и скачать нужные приложения, которые пропали из Google Play. Оплата смартфоном через платёжные сервисы банков или MirPay работает. За производительность realme 12 Pro отвечает 4-нанометровый процессор Snapdragon 6 Gen 1 5G. Предусмотрен модуль Qualcomm ISP для вычислительной фотографии. Компания отмечает, что адаптировала его специально для ПО смартфона. Лучше всего о производительности устройства могут сказать тесты в бенчмарках. Я прогнал его в Geekbench и Geekbench ML, результаты ниже. Если говорить не о голых цифрах, а опыте использования, то всё тоже хорошо. За всё время использования не возникало торможений. Хорошо работает в ежедневных сценариях, но, по отзывам пользователей, бывают проблемы в требовательных играх. Если быть честным, то это не геймерский смартфон и не флагман. Поэтому требовать от него невероятных результатов не стоит. Он просто стабильный и не подводит. Я пользовался версией на 512 ГБ постоянной памяти и 12 ГБ оперативной, но у realme 12 Pro предусмотрен своп на 4-12 ГБ. Можно заранее настроить его объём. В некоторых сценариях полезная функция.  К примеру, с ней можно держать больше приложений в фоновом режиме. Камеры — главная гордость компании в realme12 Pro. Их тут четыре, но два модуля особенно интересны: Основной ​​Sony IMX882 с разрешением 50 Мп и размером сенсора 1/2\". Есть оптическая стабилизация. Портретный телефото Sony IMX709 с разрешением 32 Мп и размером сенсора 1/2.74\". Есть 2х-кратный оптический зум и внутрисенсорное увеличение 4Х без потери качества. Ниже подборка фотографий на realme 12 Pro. Делал их в разных местах и на разные линзы. Под каждым снимком есть плашка с метаданными. Если говорить о качестве фотографий, то оно радует. Получаются красивые кадры как при дневном освещении, так и в тёмное время суток. Есть пресеты для быстрой коррекции цвета. Может быть удобно для случаев, когда нет времени возиться с Lightroom. Я несколько недель пользовался смартфоном и остался доволен. realme 12 Pro хорошо подходит для ежедневных сценариев и в качестве дополнительных возможностей предлагает продвинутую систему камер. Для меня он может стать отличным спутником в поездках, когда надо сделать красивый снимок, но камеры нет рядом. Остальные функции в виде общения и потребления контента тоже выполняются. realme 12 Pro определённо понравится фанатам бренда, которые пользуются другими моделями. Для особенно требовательных пользователей в линейке есть модель 12 Pro+. В ней прокаченный процессор и более продвинутые модули камер. Мне хватает возможностей 12 Pro. После опыта с realme я впервые задумался о том, что хочу перейти с iPhone на что-то более открытое. За последние несколько лет уже забыл, что в магазине можно расплатиться смартфоном, а приложения из сети устанавливаются и не просят переподписывать их каждую неделю.",
    "113": "В этой статье рассказывается, как настроить ваши проекты SFML С++ со статической компоновкой используя интегрированную среду разработки программного обеспечения Visual Studio. Загрузите библиотеку SFML c официального сайта. Выбираем самую последнюю версию софта, на сегодня это версия SFML 2.6.1. Скачанный архив Вы можете распаковать в любую папку. Я распаковываю на диск С: в папку IT (C:\\IT). Создаём проект \"Консольное приложение\" в Visual Studio. Настраиваем созданный проект. Если вы используете 32 разрядную библиотеку SFML тогда выбираете платформу win 32, я использую 64 разрядную и выбираю платформу x64. Связываем наше приложение с файлами SFML *.lib sfml-graphics-s-d.lib sfml-window-s-d.lib sfml-system-s-d.lib sfml-audio-s-d.lib sfml-network-s-d.lib opengl32.lib openal32.lib freetype.lib winmm.lib gdi32.lib flac.lib vorbisenc.lib vorbisfile.lib vorbis.lib ogg.lib ws2_32.lib sfml-system-s.lib sfml-window-s.lib sfml-network-s.lib sfml-audio-s.lib sfml-graphics-s.lib opengl32.lib openal32.lib ws2_32.lib winmm.lib ogg.lib vorbis.lib flac.lib vorbisenc.lib vorbisfile.lib freetype.lib gdi32.lib Чтобы исчезала консоль при запуске проекта SFML, вносим дополнительные настройки, обычно их вносят в конфигурацию Release. Незабываем в конце нажать кнопку применить. Для работы с 3D звуком в играх, копируем файл openal32.dll из папки SFML-2.6.1\\bin\\  в папку Windows\\System32\\ По умолчанию в операционной системе Windows данный файл отсутствует. Если после выполнения кода программы Вы увидите четырёхугольник, значит Вы сделали всё правильно, создав свой первый проект SFML C++ со статической компоновкой.​ В появившемся окне выбираем шаблон проекта и нажимаем кнопку далее. Заполняем графы: имя шаблона, описание шаблона, изображения значка, просмотр изображения. Ставим ниже все галочки и нажимаем кнопку готово. В появившемся окне проводника находим архив шаблона и распаковываем его. Заходим в папку с распакованными файлами и с помощью блокнота открываем файл MyTemplate.vstemplate. Вносим изменения обозначенные на картинке ниже. Вносим переменную в два последующих файла. Sample SFML Static C++1.vcxproj Sample SFML Static C++1.vcxproj.filters Копируем изменённые файлы обратно в архив шаблона. Запускаем Visual Studio 2022, находим наш шаблон SFML и используем его для своих проектов.",
    "114": "LLM продолжают свое пребывание в центре технологических дискуссий. Они трансформируют наши взаимодействия с технологиями, поскольку предоставляют возможность усовершенствованной работы в обработке и генерации текстов. Однако и упомянутые модели не идеальны, так как одна из их самых значительных проблем - галлюцинации, критическое препятствие в развитии LLM, возникающие в основном из-за качества обучающих данных, поскольку они могут быть неполными или противоречивыми. Для эффективной работы с LLM крайне важно понимать что такое, эти \"галлюцинации\" и как их обнаружить. В статье мы опробуем обнаружение галлюцинаций, исследуя различные метрики сходства текста, и проанализируем их релевантность. Дадим следующее определение галлюцинации (hallucination) - это сгенерированный контент, который не соответствует оригинальному предоставленному материалу и не имеет логической ценности. Исходником, или оригинальным предоставленным материалом, может служить, например, мировое знание, если задача представляет собой вопросно-ответную систему. (JI, Ziwei et al. Survey of hallucination in natural language generation. ACM Computing Surveys, v. 55, n. 12, p. 1–38, 2023) Начнем с более простых в проверке утверждений, которые маловероятно могут служить галлюцинацией. Здесь мы не задаем контекста, оттого получаем неверный ответ, точнее он может быть правильным, но не в нашем случае, поскольку мы спрашивали именно про модели (мы можем это легко исправить, расписав наш вопрос более детально). На первый взгляд выглядит как последовательный и убедительный ответ, однако, если мы зададимся вопросом \"а правда ли это?\", мы не сможем ни подтвердить, ни опровергнуть ответ модели, поскольку не обладаем достаточным количеством информации. Мы не имеем контекста с подтвержденными фактами для проверки полученного утверждения. Вполне возможно, что такого человека в принципе не существовало. Можем считать это той самой галлюцинацией. Предлагаю перейти к метрикам сходства текста, с помощью которых мы сможем обнаружить галлюцинацию LLM. BertScore представляет собой метод оценки качества суммаризации текстов, который оценивает степень схожести текстового резюме с исходным документом. BertScore успешно преодолевает две ключевые сложности, характерные для метрик, основанных на анализе n-грамм. Во-первых, методология n-грамм зачастую неадекватно оценивает парафразирование, так как семантически верные выражения могут существенно отличаться от формулировок в эталонном тексте, что порождает ошибки в оценке качества. В отличие от этого, BertScore использует контекстно-зависимые векторные представления слов, что демонстрирует большую точность в распознавании смысловых нюансов. Во-вторых, методы, базирующиеся на n-граммах, не способны уловить дальнодействующие лексические связи и часто неправильно оценивают семантически обоснованные изменения порядка слов. Шаг 1: Создание контекстуальных векторов: Эталонные тексты и предлагаемые варианты текста кодируются в контекстуальные векторы, которые учитывают окружающие слова. Эти векторы получаются с помощью передовых моделей, таких как BERT, Roberta, XLNET и XLM. Шаг 2: Определение косинусного сходства: Векторные представления эталонных и кандидатских текстов сравниваются, чтобы определить степень их схожести, используя метод косинусного сходства. Шаг 3: Выравнивание токенов для оценки точности и полноты: Каждый токен из предложенного текста сопоставляется с наиболее подходящим токеном из эталонного текста и наоборот. Это позволяет оценить точность и полноту перевода. Результаты этих оценок затем объединяются для расчёта итогового показателя F1. Шаг 4: Придание веса редким словам: Применяется обратная частота документов (IDF), чтобы учесть значимость редких слов, повышая их вес в расчёте BERTScore. Это внедрение не является обязательным и может варьироваться в зависимости от специфики задачи. Шаг 5: Нормализация результатов: Чтобы сделать результаты BERTScore более понятными для человека, они подвергаются линейному масштабированию. Таким образом, значения приводятся к диапазону, который легче интерпретировать, с использованием данных из монолингвальных корпусов, таких как Common Crawl. Задействуя метод расчета косинусного сходства между векторами каждого слова в тексте запроса и ответа, оценка BERT дает возможность точнее измерить семантическое соответствие. Этот метод показывает особенно хорошие результаты в случаях, когда ответы сформулированы иными словами: хотя буквальное совпадение слов может быть невелико, сохраняется общий смысл высказывания. Анализируя распределение оценок BERT и рассматривая случаи с низкими значениями, мы можем обнаружить потенциальные отклонения в ответах, которые значительно меняют семантическое содержание по сравнению с запросом. Тем не менее, следует учесть, что иногда даже корректные ответы могут получать низкие оценки BERT, если тематика или контекст ответа отличается от того, что представлен в запросе. BLEU (Bilingual Evaluation Understudy) представляет собой метрику для оценки качества автоматического перевода текстов, сравнивая его с одним или несколькими эталонными переводами. Оценка BLEU, выраженная числом между 0 и 1, отражает степень соответствия машинного перевода эталонным версиям: 0 указывает на отсутствие совпадений (низкое качество), тогда как 1 свидетельствует о полном совпадении (высокое качество). Как правило, оценка BLEU снижается по мере удлинения переводимого предложения. Тем не менее, степень этого снижения может отличаться в зависимости от конкретной модели перевода, применяемой в процессе. Для наглядности приведем график, который демонстрирует изменение оценки BLEU в зависимости от длины предложения: Необходимо учитывать, что, несмотря на полезность оценок BLEU, они обладают рядом ограничений. Величина оценки может сильно изменяться в зависимости от выбранного набора данных, что делает сложным определение общих стандартов качества. Помимо этого, оценки BLEU основаны исключительно на сравнении отдельных слов или фраз, что может привести к упущению из виду семантических нюансов. Через процесс визуализации распределения оценок BLEU и детального рассмотрения примеров с низкими показателями можно выявить возможные случаи галлюцинаций в переводе. Однако крайне важно учитывать, что низкие оценки BLEU не всегда свидетельствуют о галлюцинациях; они могут также отражать различия в стиле изложения или понимании смысла. Сопоставление запросов и ответов может предоставить значимые взгляды, но для глубокого понимания феномена галлюцинаций более продуктивно сравнивать различные ответы, созданные одной языковой моделью на один и тот же запрос. Такая методика, которую называют Response Self-Similarity, дает возможность оценить устойчивость и консистентность результатов, предоставляемых языковой моделью. Для расчета Response Self-Similarity мы применяем векторные представления, которые отражают смысловую нагрузку целых фраз или текстовых фрагментов, вместо анализа индивидуальных слов. Измеряя косинусное расстояние между такими векторными представлениями оригинального ответа и его вариаций, мы можем точно определить уровень их взаимного сходства. Через визуализацию распределения оценок Response Self-Similarity и детальный анализ случаев с низкими значениями, мы имеем возможность обнаружить ситуации, когда ответы, сгенерированные языковой моделью, заметно отличаются друг от друга. Это может свидетельствовать о наличии галлюцинаций или нестыковках в логике модели. Существующие метрики для анализа текстовых данных часто опираются на фиксированные алгоритмы и модели для определения схожести текстов. Однако мы можем также задействовать возможности самих LLM для оценки уровня однородности и взаимосвязи их собственных ответов. Этот метод, известный как LLM Self-Evaluation, включает в себя обращение к LLM с просьбой оценить последовательность и актуальность её же ответов. LLM Self-Evaluation охватывает ряд ключевых аспектов: языковую выразительность, логическую согласованность, понимание контекста, точность в отражении фактов и способность создавать ответы, которые не только соответствуют теме, но и несут в себе значимую информацию. Для того чтобы инициировать самооценку с помощью LLM, мы можем сформулировать запрос, который представляет разнообразие ответов, и попросить модель оценить их взаимную согласованность или схожесть. В качестве примера можно привести следующее: Запросив у LLM оценку степени согласованности и похожести между его собственными ответами, мы можем эффективно задействовать его языковые аналитические способности для выявления возможных недочетов в логике или галлюцинаций. Однако следует учитывать, что достижение однородности в ответах LLM может быть затруднено из-за переменчивости того, как модель интерпретирует запросы. Один из возможных путей усовершенствования этой стратегии — попросить LLM предоставить оценку отдельных фраз или конкретных элементов ответа, вместо выдачи универсальной числовой оценки. Подводя итоги, мы поговорили о галлюцинациях, что затормаживают нашу работу с нейросетью, а также рассмотрели различные методы выявления галлюцинаций в LLM. Проблема не сказать чтобы очень новая, но я совру, если обзову ее старой, а потому крайне важно искать возможные решения для ее обнаружения. Методы имеют свои плюсы и минусы (как впрочем и все, что нас окружает), надеюсь, эта статья смогла дать ответы на ваши вопросы или хотя бы подтолкнула к размышлениям в поиске своего решения этой проблемы.",
    "115": "В компании Циан (где я, Клюшев Александр, и работаю в роли ML-инженера) проводятся внутренние хакатоны, и один из таких проходил в начале лета 2023. Достаточно давно в компании обсуждали идею по реализации поиска объявлений через текстовую строку, и было принято решение эту идею воплотить в жизнь. По итогу мы провели пару бессонных ночей, кто-то потусил в загородном отеле (часто внутренние хакатоны организуются на выезде) и заняли 1 место. В статье я расскажу, как выглядит флоу поиска, какую мы использовали модель и какие результаты получили. превращается в предзаполненные фильтры. Не все пользователи дружат с фильтрами. Например, в клиентскую службу часто приходят запросы по типу «подберите мне квартиру в Москве», и одна из причин — сложность с фильтрами. Хотелось дать возможность через полнотекстовый поиск искать очень узкие кейсы (например: «однокомнатная квартира со светлой кухней на 7 этаже»). Спойлер: не все фильтры из этого запроса будут применены, но база для них заложена. Ещё мы ожидали сокращение времени, которое пользователь затрачивает на поиск необходимого объявления, и увеличение конверсии в целевое действие (в данном случае это нажатие на кнопку «показать телефон»). Пользователь вводит запрос: «однокомнатная квартира со светлой кухней на 7 этаже». Модель (NER) подхватывает запрос и возвращает размеченные сущности. Сущности приводятся в соответствие со значениями в наших фильтрах. Фильтры уже отправляются в микросервис, который выдаёт список подходящих объявлений. Сортируем полученные объявления по мере сходства текстового описания объявления и непротегированной части самого запроса. Отсортированный список объявлений отдаём пользователю. Так как в основе флоу лежит модель по разметке сущностей в запросе, начнём с неё, а именно с данных, которые нам для неё нужны. Первый вопрос, с которым мы столкнулись: какие данные нам использовать для обучения/теста Немного подумав и пообщавшись с коллегами, узнали, что гугл и яндекс при переводе со страницы выдачи пробрасывают нам поисковый запрос, который и привёл на наш сайт. Например, если пользователь ввёл «купить квартиру в Москве»: То при переходе по ссылке поисковая система передаст нам и сам запрос, а мы уже залогируем его. Так у нас появился достаточно большой (3723 записи, если быть точным) датасет, но не было никакой разметки. Изначально думали использовать те страницы, на которые попадал пользователь при переходе из поисковой системы. Т.е. в примере выше пользователь попадёт на выдачу, где уже будут указаны основные активные фильтры (тип действия: «покупка», тип недвижимости: «квартира», где: «Москва»). В целом неплохо, но работает только для основных фильтров. Если добавлять в запрос в гугле имя жилого комплекса (ЖК), комнатность и цену, то при переходе на сайт потеряется фильтр только по цене. Поэтому решили использовать ручную разметку, сделанную своими руками. Развернули у себя инструмент doccano (удобный инструмент для разметки, решающий разные задачи по текстам/картинкам/аудио + позволяет управлять права доступа/считать стату по выполненным заданиям), закинули в него уже имеющуюся выборку и разметили своими силами по категориям. town — населённый пункт (город, деревня, пгп и т. д.); street — название улицы/проспекта и т. д.; poi_name — название достопримечательности или любого другого важно объекта. И уже имея размеченный датасет (пример ниже), приступили к обучению модели. где слово, помеченное лейблом с префиксом B_, означает начало какого-либо лейбла, а I_ — данное слово не первое для данного лейбла и ранее где-то есть слово с префиксом B_ этого же лейбла. Плюс лейбл O — для всех слов, которые не отнеслись к каким-либо категориям.После такого преобразования датасет стал выглядеть примерно так: В качестве самой модели использовали класс BertForTokenClassification из transformers и модель rubert-base-cased-conversational от DeepPavlov (за что им отдельное спасибо). В качестве финальной метрики смотрели на долю верно предсказанных лейблов для каждого текста, а потом усреднили по всему датасету. Прогнали первый цикл (модель обучали на nvidia A100, заняло порядка 10 минут.), посчитали точность (0.75) и решили, что можно лучше. Особенно бросались ошибки на тех названиях улиц/городов, которых не было в обучающей выборке. Решили попробовать аугментировать датасет: так как в Циан большая база объявлений, то мы из одного текста, например, «купить квартиру в Краснодаре, можем получить ещё штук 5, заменив «квартиру» на «комнату», «гараж» и т. д. Так и поступили для большинства тегов. Датасет увеличился в несколько десятков раз, качество тоже подросло (выросли до 0,81). Эту модель и решили финально заюзать для хакатона. Следующий шаг — маппинг значений из сущностей в наши фильтры. Здесь были идеи по реализации через zero-shot модели, но выиграла комбинация регулярок и поиска наиболее похожего значения через расстояние Левенштейна. Взяли все уникальные значения из размеченного датасета для каждого типа сущности и привели их к значениям в наших фильтрах, например: текстовые значения «однушка», «однокомнатная», «1-комнатная» превращается в int “1” для фильтра по числу комнат. Теперь та часть, которая, к сожалению, не удалась, а именно сортировка объявлений по мере сходства текстового описания объявления и непротегированной части самого запроса. Пользователь ввёл «купить квартиру с белой кухней». Мы выделили сущности «купить», как тип действия, и «квартира», как вид недвижимости. Остался текст «с белой кухней». Этот остаток прогоняется через какой-нибудь эмбеддер. Оцениваем расстояние от него до эмбедингов описаний ранее полученных объявлений. Идея звучала хорошо, и, как казалось, позволяла учесть те части запроса, на которые у нас нет фильтров. Но возникло две сложности: Объявления отдаются нам уже в отсортированном порядке, и проранжированы они не только по вероятности клика клиента, но и с учётом платности объявления. Встроиться в такую логику за 2 дня хакатона было слишком сложно. По основной логике успевали слишком впритык и решили сконцентрироваться на основном флоу. По итогу мы выиграли хакатон в категории лучший продукт и затащили первую версию текстового поиска на прод. Сейчас текстовый поиск раскатан в хорошо доработанном виде на мобильный веб (можно и с десктопа потестить, но нужно через DevTools в браузере сменить тип страницы на мобильный) и приложение для ios (раздел с поиском на карте). По итогу выкатки на ios получили статзначимый прирост к количеству начатых поисков и прирост к количеству открытых карточек. Добавление текстового поиска на десктоп. Увеличение гибкости текстового поиска за счёт нереализованной части с полнотекстовым поиском. Спасибо, что дочитали статью до конца, приглашаю вас в комментарии, отвечу на все вопросы.",
    "116": "В последнее время многие блокчейн-платформы для исполнения смарт-контрактов переключились на WASM — WebAssembly. Мы не стали исключением, и в последнем обновлении тоже добавили WebAssembly как альтернативу привычному Docker. В этом посте мы расскажем, для каких задач нам потребовался именно WASM, что мы достигли с ним на сегодня и как WASM отражается на производительности блокчейна. Что такое WebAssembly? Это переносимый бинарный формат для исполнения программ, известный с 2015 года. Изначально основной целью WASM — исполнять более компактный и быстрый код в вебе, в клиенте браузера. Разработчикам удалось ее достичь, но на этом WebAssembly не остановился. Сегодня он уже дорос до расширения WebAssembly System Interface: оно позволяет создавать виртуальные машины, вполне способные конкурировать с зарекомендовавшими себя решениями типа JVM, .NET, BEAM. Для WebAssembly нашлось применение не только в браузерах, но и в блокчейне, для смарт-контрактов. WebAssembly Runtime Environments (RE) даже использует в названии Assembly, а байт-код здесь представляет собой некий набор инструкций, что роднит его с другими ассемблерами. Но фактически это стековая машина, где регистр не используется и мы оперируем четырьмя типами значений — i32, i64, f32, f64. С более сложными типами данных работает линейная память, в которой хранятся нужные данные самого байт-кода. Они собираются в виде условной «кучи», которая может расти до определенных пределов, исходя из ограничений самого байт-кода и виртуальной машины, на которой он исполняется. Для удобства чтения и редактирования в WebAssembly предусмотрен текстовый формат. Он представляет собой одно большое S-выражение, имеет Lisp-подобный синтаксис, напоминая Common Lisp и Scheme: Текстовый формат позволяет нам читать и при необходимости редактировать байт-код, понимая, какие инструкции и как в нем используются. Есть несколько причин, по которым в принципе стоит обратить внимание на WebAssembly. Как виртуальная машина для исполнения смарт-контрактов, WASM имеет очень высокую производительность. Байт-код сам по себе имеет очень малый размер, что важно для блокчейна, поскольку так мы можем не раздувать транзакции и стейт. Любой рантайм по факту является изолированной средой. В базовом представлении WASM не имеет доступа ни к чему снаружи: в него загружается байт-код, который затем исполняется. На WASM можно добиться детерминированного исполнения: для этого достаточно просто отказаться от использования значений с плавающей точкой. WASM поддерживает компиляцию множества языков, что позволяет разрабатывать смарт-контракты на любом языке программирования. Мы хотели увеличить производительность по сравнению с текущими докер-контрактами. Было понятно, что это сработает, так как в докере уходит очень много времени на сетевое общение контрактов с нодой, поднятие контейнеров и многое другое. Мы хотели реализовать вызов одним смарт-контрактом методов другого, но по тем же причинам, что и в предыдущем пункте, в докере такие вызовы не представляются реалистичными. При этом важно было не допустить сильного увеличения размера транзакции. Байт-код нам придется хранить он-чейн, поэтому минимизация байт-кода стала очень приоритетна: как можно меньше инструкций в самом байт-коде, как можно больше делегирования инструкций и тяжелых вычислений на уровень выше. Наконец, важно было не сломать то, что уже работает. Поэтому в итоге мы не стали создавать новые транзакции, а обновили существующие, добавив в них необходимые поля для WASM-контрактов. Внутри ноды мы также постарались сохранить весь текущий флоу контрактов и просто подселили туда еще один движок для исполнения. Таким образом получилось сохранить все наработки докер-реализации. Результатом работы с WASM стала Waves Enterprise Virtual Machine — новый движок для исполнения смарт-контрактов на нашей платформе. Здесь стоит пояснить: хотя основным продуктом нашей компании является блокчейн-платформа «Конфидент», компоненты, общие с публичным блокчейн-протоколом Waves Enterprise, носят оригинальное название open-source проекта. набор функций для работы с нодой, который расширяет интерпретатор и позволяет байт-коду взаимодействовать с нодой, выполнять перевод, создание ассетов, получение баланса и т. п.; механизм для управления выполнением смарт-контрактами с целью вызова одним контрактом методов другого. При исследовании мы поняли, что почти все существующие виртуальные машины и интерпретаторы разработаны на Rust, и решили тоже начать развитие WEVM с Rust. В качестве интерпретатора мы использовали wasmi, разработанный компанией Parity. По большей части его используют именно для смарт-контрактов, в нем нет ничего лишнего. Для общения виртуальной машиной и ноды используется Java Native Interface (JNI) и crate jni v.0.21.0. Если вкратце, WEVM работает следующим образом. Майнер в сети запускает новую виртуальную машину, по аналогии с реализацией в докере. В нее майнер загружает транзакцию с байт-кодом из UTX-пула. Виртуальная машина возвращает результат, который используется майнером как результат выполнения смарт-контракта и, соответственно, транзакции. В состав Rust CDK входит расширение cargo-we для пакетного менеджера с утилитами для удобного создания и сборки проекта. В будущем мы хотим ее сильно расширять, и об этом я расскажу чуть позже. Эту реализацию WASM мы сравнивали с докером в разных бенчмарках, с шардингом и без, с включенным и выключенным MVCC и т. д. В лучшем для докера прогоне производительность сети в среднем достигала 50–60 tps — и это при 300–400 tps у WASM. В конце поста мы покажем бенчмарки конкретного смарт-контракта. О том, как развернуть у себя опенсорсную версию нашей платформы, мы рассказывали в одном из предыдущих постов (и в другом, более доступно). CDK для смарт-контрактов на Rust доступен на гитхабе. Если интересно, можете повторить смарт-контракт, который я для примера напишу далее. И с помощью нашей утилиты cargo-we расскажу о некоторых его особенностях. cargo install --git https://github.com/waves-enterprise/we-cdk.git --force В дальнейшем мы планируем опубликовать утилиту не только на GitHub, но и в публичном репозитории пакетов Rust. В утилите доступна инициализация, сборка проекта и отладка в WASM и текстовом формате. Cargo.toml — это обычный файл манифеста, как package.json для JS и TypeScript. Он включает имя проекта, версия и все необходимые настройки. Так как CDK еще не опубликован, мы в последней строке вручную дописали, что его нужно ставить с гита: Также у нас автоматически создается gitignore и сам контракт — lib.rs: Здесь используется базовый синтаксис Rust, ничего специфичного; no_std означает, что мы не используем стандартную библиотеку — с ней бинарник получится слишком объемным. Импортируем собственную библиотеку и в lib.rs начинаем писать логику контракта. Возьмем для примера стандартную функцию и допишем атрибут action, чтобы показать, что она доступна снаружи при вызове контракта: Добавим параметры. В CDK описаны четыре стандартных типа данных, используемых в нашей платформе: integer, boolean, string и binary. Выберем из этого списка: Функции, доступные снаружи для вызова, не дают в ответ никаких результатов: это все аккуратно спрятано в CDK. Под капотом функция возвращает код ошибки, а если его нет, то возвращает ноль. Поэтому нам не нужно использовать возвращаемые значения, достаточно только описать аргументы. В контракте обязательно должна присутствовать функция _constructor, помеченная как action, — ее можно найти в базовом коде lib.rs выше. Именно она вызывается через CreateContract Transaction. Даже если нам нечего написать в constructor, мы можем оставить ее пустой; тогда функция просто не будет ничего делать. Далее мы можем описывать любые функции контракта и использовать набор функций, доступный из CDK для работы с нодой: чтение и обновление значений контракта, работа с токенами. Подход здесь несколько отличается от подхода смарт-контрактов в докере. Там контракт получает транзакцию на вход и выполняет действие в зависимости от заданных параметров. Здесь же при использовании транзакции CreateContract вызывается функция constructor. Она может быть только одна; в ней описывается некий код, который инициализирует состояние контракта. В остальных случаях может быть использован CallContract с указанием конкретных функций для вызова. Этот подход больше похож на подход в Solidity. В CDK мы уже реализовали все базовые функции — создание, сжигание, перевыпуск, лизинг токенов и др. Отдельно я хочу показать, как реализовать вызов одного контракта другим — это именно то, что доступно в WASM и неприменимо в докер-реализации. Сначала необходимо описать функции, которые мы будем вызывать. Вернемся к lib.rs и добавим в него стандартный интерфейс, как в Java. Пометим его атрибутом interface: Далее мы можем вызвать функцию другого контракта. Делается это через макрос call_contract!: мы указываем интерфейс, который мы хотим использовать, и адрес контракта (base58), затем функцию, которую хотим вызвать. И наконец, передаем в нее аргументы. К транзакции call_contract можно приложить платеж. Для этого используем константу системного токена SYSTEM_TOKEN. Также мы указываем, что хотим провести платеж совместно с вызовом. Вот как будет выглядеть итоговая функция flip: Сначала выполнится логика, описанная в начале. Потом вызовется другой контракт, который выполнит действия. А в случае ошибки функция завершит свою работу с кодом ошибки, который произошел при выполнении другого контракта. Функция flip будет считаться успешно завершенной, если ошибок нет — это прописано в CDK. Нода получит эти данные, предоставит пользователю, и он сам примет решение. Код контракта простой, но довольно показательный. Здесь происходит два запроса к ноде: получить сторадж и записать сторадж. Логика простая: мы берем из стораджа номер шарда (счетчика) и перезаписываем его, увеличив на 1. Нам также нужно получить SHA-сумму. Так мы удостоверимся в том, что задеплоили именно то, что хотели задеплоить: Вот в таком виде хранится контракт. По API получить его очень просто: Вызовем транзакцию 103, создание контракта на ноде. Всё так же, как при работе с докером, только вместо imageHash используем bytecode и bytecodeHash: В тестовом конфиге будем использовать один этот контракт, который создаст десять счетчиков при параллелизме, равном восьми. Сравнивать транзакции 103, создание смарт-контракта, смысла не будет: здесь докер, очевидно, сильно отстанет за счет времени на создание контейнера. А у WASM это время будет сопоставимо с вызовом смарт-контракта. По этой причине, кстати, функциональность вызова одного смарт-контракта через другие актуальна только для WASM — представьте, сколько времени это будет занимать на сложных проектах с докером. Вернемся к бенчмаркам. Посмотрим, сколько занимают вызовы смарт-контракта в докере: Видно, что многие вызовы занимают больше 50, а один вообще занял 170 мс. Медиана — 30 мс. Максимальное время на вызов у WASM — 2 мс. Медиана — 0,391 мс. Получается, что без сетевого оверхеда gRPC в докере производительность увеличилась в 76 раз. Сейчас такой показатель возможен только в лабораторных условиях, по факту мы получаем разницу в 3–4 раза. Чтобы приблизиться к идеальным показателям в рабочих кейсах, в этом году мы займемся улучшением обработки и валидации блоков (наборов транзакций при их создании) — в итоге планируем ускориться как минимум в 30 раз. В перспективе производительность WASM будет упираться лишь в возможности сети. Виртуальная машина WEVM стала главным функционалом недавнего обновления 1.14.0 open-source платформы Waves Enterprise. Вскоре мы добавим эту функциональность в приватную блокчейн-платформу «Конфидент». В будущих релизах планируем расширять возможности WEVM: реализовать локальное тестирование контрактов, чтобы WASM-контракты проверялись на виртуальной машине с симуляцией блокчейна. Так можно будет убедиться, что контракт собрался правильно и не использует ничего лишнего. Будем расширять нашу стандартную библиотеку — набор функций, доступный байт-коду для работы с нодой — и дорабатывать утилиту cargo-we. Мы не хотим останавливаться только на Rust для написания смарт-контрактов, так что в будущем расширим поддержку языков: JS, TypeScript, AssemblyScript (который применяется в том числе для WebAssembly), а также Java, Kotlin, Scala. Мы обязательно вернемся к теме WebAssembly в блоге и более подробно рассмотрим возможности нашего нового инструментария.",
    "117": "Ссылка на оригинал, попытался дополнить и объяснить некоторые не понятные детали. Эта статья фокусируется на примере использование библиотеки DCMTK при создании DICOM-файлов. Как говорит Википедия, DICOM - Digital Imaging and Communications in Medicine, это стандарт создания, хранения, передачи и визуализации медицинских изображений. Стандарт включает в себя часть, которая описывает структуру DICOM-файла, и другую, описывающую передачу DICOM-данных по сети. DCMTK обеспечивает строгую совместимость с DICOM-стандартом, предоставляя широкий спектр функциональности для обработки изображений, текстовой информации и метаданных. Библиотека поддерживает различные форматы изображений, унифицирует данные и обеспечивает эффективный обмен информацией в медицинском сообществе. Современные МРТ и КТ устройства по умолчанию создают медицинские изображения и передают их на PACS-сервер для хранения, используя стандарт DICOM. Но цифровые медицинские изображения не обязательно должны быть топографическими, а могут быть обычными цветными или черно-белыми фотографиями, например, снимок сетчатки глаза. Такие снимки зачастую хранятся в виде: описание пациента + jpg снимок. Чтобы хранить такие изображения на PACS-серверах, их нужно преобразовать в DICOM. В данной статье мы углубимся в практическую сторону вопроса, рассмотрев конкретный пример создания файла DICOM из изображения формата *.dcm на языке C++ для последующей его отправки на PACS-сервер. MSVC v. 14 CMake v. 3.28.2 DCMTK v. 3.6.6 Для начала нужно скачать и установить библиотеку DCMTK. После сборки библиотеки и подключения её к своему проекту, стоит уточнить, что необходимо всегда открыть Visual Studio 2022 с правами администратора - поможет избежать большей части ошибок. (https://brandres.medium.com/building-a-simple-dicom-application-with-c-and-dcmtk-in-visual-studio-2019-5aacc1e0854e) dcmtk/config/osconfig.h - содержит настройки конфигурации для операционной системы; dcfilefo.h - определяет класс “DcmFileFormat”, который представляет DICOM-файл в формате File Format (включая метаданные и изображения). Содержит функции и методы для чтения, записи и манипуляций с данными DICOM в рамках файла формата File Format. Пример использования “DcmFileFormat” может включать создание нового DICOM-файла, добавление метаданных и изображений, а также сохранение или чтение DICOM-файла. i2d.h - Предоставляет средства для кодирования изображений в формат DICOM, чтобы они могли быть интегрированы в DICOM-формат; i2djpgs.h - Предоставляет средства для преобразования изображений в формат JPEG и интеграции их в DICOM-датасет для последующего сохранения в DICOM-файл с использованием JPEG-сжатия; dctk.h - Основной заголовочный файл для компонентов, относящихся к обработке данных. Массив символов длиной в 100 элементов. Зачем он нам нужен поговорим подробней далее. Создается объект “I2DImgSource” с именем “inputPlug”, который инициализируется конструктором “I2DJpegSource”. Этот объект представляет источник изображения в формате JPEG. Все точно так же, как и в переменной, указанной выше, только этот объект представляет собой выходной плагин для DICOM. Класс, реализующий движок image2dcm. Преобразование происходит путем объединения входного плагина, считывающего формат изображения общего назначения, и выходных плагинов для преобразования в определенные классы DICOM SOP Перечисление (enum), представляющие различные синтаксисы передачи данных. Этот перечислитель определяет различные стандартные синтаксисы. хранилище метаданных пациентов и их изображений. (чуть подробнее на этой переменной остановимся дальше) После объявления всех переменных необходимо загрузить изображение, к которому и будут цепляться метаданные. В нашем случае – test.jpg. E_TransferSyntax - представляет собой перечисление (enum) в библиотеке DCMTK, которое определяет различные форматы передачи данных (transfer syntax) в стандарте DICOM. Формат передачи данных определяет, как данные закодированы и передаются между устройствами или программами. Этот тег необходим для передачи DICOM файла по сети, например, для хранения на PACS-сервер. Без него в принципе возможно создать DICOM файл, но невозможно передать его по сети, PACS-сервер не примет передачу. По умолчанию, для DICOM изображений используется синтаксис VR Little Endian, который задается следующей строкой: \"1.2.840.10008.1.2\". В нашем случае, для передачи JPEG существует отдельный набор правил кодирования - JPEG Baseline (Process 1), что означает использование базового профиля JPEG для сжатия изображений, который задается такой строкой: “1.2.840.10008.1.2.4.50”. Функция i2d.convert предназначена для конвертации данных в формат DICOM. Рассмотрим аргументы: inputPlug: Переданный в функцию входной плагин, представляет собой полностью настроенный компонент, способный читать данные пикселей из исходного формата (.jpg). outPlug: Переданный в функцию выходной плагин, представляет собой полностью настроенный компонент, определяющий конечный формат-DICOM. resultDset: Этот параметр представляет собой выходной набор данных, созданный в результате конвертации. writeXfer: Этот параметр предоставляет предложенный формат передачи данных, необходимый, например, при использовании JPEG входного плагина. Он возвращается из функции. Таким образом, функция i2d.convert выполняет конвертацию изображения из одного формата в другой. В этой части кода добавляем имя и пол пациента. Первым аргументом идет название DICOM тега, вторым – значение, которое в этот тег записывается. Причем данные могут быть любых типов, например, “string”, как показано у меня в примере. Также могут передаваться числа (целые и с плавающей точкой), дата, время и так далее. Как правило, добавляется еще и дата рождения пациента и/или ID для упрощения последующей идентификации изображения. SOPClassUID = '1.2.840.10008.5.1.4.1.1.77.1.4' Логика DICOM следующая: есть пациент с уникальным ID (PatientID), у пациента проводится исследование с уникальным ID (StudyInstanceUID) под номером StudyID, в данном исследовании была серия изображений с уникальным ID (SeriesInstanceUID), в которой находится файл с уникальным ID (SOPInstanceUID). Рассмотрим, как сгенерировать UID на примере SOPInstanceUID, который задает глобально уникальное значение (больше нигде в мире он не должен повторяться) для нашего DICOM файла. Запишем в него значение с помощью генератора уникального идентификатора (uid): UID — это уникальные строки, которые используются в стандарте DICOM для однозначной идентификации различных ресурсов, таких как изображения или серии. Функция dcmGenerateUniqueIdentifier является частью библиотеки DCMTK и предназначена для генерации уникальных идентификаторов объектов DICOM (UID). uid: это выходной параметр, в который будет записан сгенерированный уникальный идентификатор. root: это корневая часть UID. Это может быть ваша уникальная строка, например, идентификатор вашей организации. Корневая часть обычно представляет собой префикс UID. В DCMTK строка «1.2.840.10008.1.2.4.50» синтаксиса передачи данных DICOM (Transfer Syntax UID) для JPEG Baseline (Process 1) также представляет собой уникальный, но не глобально идентификатор. Это означает, что другие DICOM файлы могут также содержать в себе эту строку, в случае, если используют тот же синтаксис передачи. Итак, мы все добавили, всё вписали. Мы почти молодцы, остался один момент - сохранить файл с расширением *.dcm. Как это сделать?! В этом нам помогут следующие две строки. Первая строка создает объект типа “DcmFileFormat” с именем “dcmff” и ему передает все наши данные, записанные в “resultDset”. Вторая строка вызывает метод “SAVEFILE” объекта “dcmff” для сохранения DICOM-изображения в файл с названием \"test.dcm\" с использованием заданных параметров. Получившийся .dcm файл можно открыть одним из множества бесплатных просмотрщиков, например, MicroDicom. В итоге должно получится нечто подобное: Для проверки возможности передачи файла по сети использовался локальный PACS-сервер Orthanc 23.2, который после добавления всех указанных тегов успешно сохранил DICOM файл. Хотя добавление тегов *UID и Transfer Syantax не гарантирует того, что сервер примет файл. В конечном итоге конфигурация сервера может запрещать сохранять некоторые типы SOPClassUID, или некоторые Transfer Syntaxes. Для знакомства и написания этой программы я пользовался источником. В отличие от оригинальной статьи, хотелось привнести дополнительную информацию о генерации uid, детальнее рассмотреть функции и возможности DCMTK, а также сделать акцент на некоторых важных тегах при создании своего DICOM файла.  В итоге получилась программа, создающая файл формата .dcm из изображения .jpg, который можно послать на хранение в PACS-сервер. Ссылка на оригинал, попытался дополнить и объяснить некоторые не понятные детали.",
    "118": "Обсуждая удаленку, мы часто говорим о том, что комфортно в таком режиме работается людям с высоким уровнем самостоятельности - тем, кто может сам спланировать время, мотивировать себя делать задачи (да и в целом понимает, что удаленка - это не фриланс, а “фриленд”). В этой статье поговорим о том, чем на мой взгляд должен заниматься руководитель команды на удаленке. Сразу отмечу, что здесь нет никаких ноу-хау, я просто собрал наработанный за много лет опыт и выделил несколько самых важных пунктов. Но прежде чем переходить к сути, хочу ответить на один фундаментальный вопрос. В этом вопросе я категоричен. Глубоко убежден, что ничего в этом мире в принципе не существует без управления. Лично я не верю ни в какие самоорганизующиеся структуры - холакратии и прочее. Ряд людей убеждал меня, что возможно организовать некие распределенные системы, в которых все работает по-другому (и при этом работает). Но я уверен, что для любого процесса нужен тот, кто, условно говоря, поднимет знамя и возьмет процесс под свой контроль. Если людей немного, структура под его управлением может быть одноуровневая. Но если команда большая, требуется также четкая иерархическая структура управления. На ум приходят два исторических примера, отлично иллюстрирующих эту идею. Первый пример - Сергей Павлович Королев. Возможно, ученым и конструктором он был не самым лучшим - в то время некоторые его коллеги генерировали идеи намного быстрее. Но Сталин остановился на этой кандидатуре, потому что С.П. Королев был человеком “в теме” с управленческим опытом и амбициями. Он был довольно жестким человеком, даже в чем-то жестоким. Но это была отличная кандидатура на роль главы всего ракетного проекта. И он добился определенных успехов. Второй пример - Ростислав Евгеньевич Алексеев - разработчик судов на подводных крыльях, которые до сих пор используются на многих водных маршрутах. Он был выдающимся конструктором - генерировал идеи и горел ими. Но при этом он был не очень хорошим управленцем. По воспоминаниям люди из его коллектива уходили пачками. И он умер из-за травмы, полученной в ходе испытаний очередной конструкции. КБ Алексеева до сих пор продолжает его разработки, но направление так и не стало масштабным, поскольку он не вытащил его, как управленец. И таких примеров можно вспомнить много. Любому проекту нужен человек, который возьмет на себя управленческую ответственность. К сожалению, природа так распорядилась, что способных на это людей в обществе от 1 до 4% (это моя личная статистика). Все остальные могут быть гениальными художниками, программистами, администраторами, врачами, но не могут запускать проекты. К слову, если в команде одни менеджеры и никто не хочет работать, ничего хорошего из этого тоже не выйдет. Это палка о двух концах. Работая лично и управляя командами на удаленке, я пришел к нескольким поинтам относительно функций менеджера именно в удаленном формате. Если рассматривать разработку, как бизнес-процесс, т.е. как некую структуру, которая движется по этапам в соответствии с определенными правилами, задача менеджера - выстроить и управлять этим процессом. В офисе на определенные аспекты процесса можно не обращать внимания. В любой момент люди могут быстро между собой пообщаться и скомпенсировать это. В удаленном формате сложнее. Такое случайное исправление процессов затруднительно, а значит все должно быть более формализованным, даже если сама компания небольшая. В итоге на удаленке процесс как таковой, его формализация и жесткое соблюдение выходят на первый план. Сюда же стоит отнести объяснение процессов - новому человеку сложно влиться во все и сразу, поэтому нужна памятка, где указано, как все это устроено в компании. На мой взгляд на удаленке хорошо работает канбан или скрам. Не обязательно канонические, но нужна итеративность - у тебя есть задачи, ты за них отчитываешься и раз в определенный период выдаешь результат. Соответственно, должно присутствовать и некоторое напряжение, связанное со сроками сдачи очередной итерации. Без него на удаленке особо ничего не работает. Для людей на удаленке нужно создавать ритуалы общения. Это огромный пласт бизнес-процессов, связанных с коммуникациями. И выстраивать их должен именно менеджер. Человек так устроен, что он любит следовать ритуалам. Мы встаем утром, кто-то чистит зубы, кто-то пьет кофе, кто-то сразу хватает телефон и читает новости. В работе простые ритуалы также важны - мы в первую очередь люди, а уже потом работники. В офисе внутрикомандные ритуалы формируются сами собой  - в курилке, в общей кухне во время обеда, где угодно. На удаленке этого нет, поэтому задача менеджера - создать ритуалы и следить за их исполнением. Дейлик, на мой взгляд, обязателен. Даже если сказать нечего, менеджеру надо прийти на дейлик и просто поговорить с командой. Выше я говорил про итерации - по окончании каждой из них нужно обязательно подводить какие-то итоги. И должны быть созвоны, связанные с запуском нового спринта. допустимо ли опоздание на встречи и т.п. Это должны быть непоколебимые правила, приемлемые для всей команды. Они имеют большое значение, поэтому их надо соблюдать неукоснительно. Если используем Slack, значит общаемся в нем. И это значит, что у всех должны срабатывать оповещения, чтобы не было необходимости тэгать человека еще и в Telegram, чтобы он увидел сообщение. Из личного опыта отмечу, что удобно, когда команда небольшая и внутренние договоренности позволяют в экстренном случае без предупреждения набрать человека по телефону и быстро что-то обсудить. Если собеседник при этом не может разговаривать, он просто скидывает звонок и перезванивает, как только появляется такая возможность. Обязательно нужны встречи менеджера или тимлида один на один с командой. Нужен процесс обмена впечатлениями - все ли хорошо, есть ли замечания. И общение должно быть с камерой - собеседнику важно считывать эмоции и какой-то невербальный фидбек. Из этого правила можно делать исключения, если именно сегодня у коллеги нет технической возможности (проблемы с интернетом, встреча на бегу и т.п.). Но это должно быть именно исключение. Регулярное общение без камеры приводит к потере эффекта личного общения - степень вовлечения совершенно другая. Еще один инструмент, который должен драйвить менеджер - дашборды, показывающие, как и куда мы двигаемся. Если взять среднестатистического разработчика и спросить его, важна ли общая картина, скорее всего он скажет, что он просто пилит свои задачи, а на остальное ему плевать. Но как правило, он просто не осознает, насколько ему важна big picture. Каждый человек должен иметь возможность видеть, что происходит. Менеджер может показывать это на каких-то схемах или диаграммах, рассказывать в письмах, на коротких совещаниях и так далее. В своей текущей команде я взял за правило в понедельник утром присылать то, что мы называем “бодрящим письмом” - короткое сообщение, где указано количество закрытых задач и в целом текущая ситуация по работе. Хотя письмо в большей степени формальное, некоторый след оно оставляет - люди видят, зачем мы все это делаем, понимают результат своей работы за предыдущую неделю. Они не отрываются от контекста. Возможно, менеджеру стоит иногда дополнительно говорить о том, куда и как мы идем. Людям нужно постоянно чувствовать некоторый нерв движения. Не должно быть задач “отсюда и до обеда”. Наставничество - это хорошая история, которая помогает коммуникациям внутри команды. Люди должны делиться на небольшие группы и общаться между собой. В наставничестве тоже должны быть определенные правила - регулярность, договоренности и т.п. И за этим тоже предстоит следить менеджеру. Хочу отдельно отметить (и я уже говорил об этом в своих статьях), что на мой взгляд удаленка неприменима для джунов. В больших командах мидлы и сеньоры еще могут попробовать его вытащить, но если команда маленькая, джун просто будет ее топить. Речь идет о наставничестве на более высоких уровнях. Люди проводят на работе больше трети своей жизни. А в ИТ-индустрии так получается, что ноутбук-то ты закрыл, но работа остается в голове. В это время хочется чувствовать с коллективом какое-то родство, а значит проводить с коллегами немного больше времени (в том числе за рамками рабочих задач - в каких-то других активностях). Можно общаться вне работы онлайн - играть в игры, проходить квизы и т.п. Но на мой взгляд, эффективность их очень низкая. Очень хорошо работают оффлайновые истории - локальные или общие встречи коллектива. Их сложно организовать, это дополнительные затраты, которые компания должна вкладывать. Но раз в квартал или полгода можно себе их позволить. По моей практике люди с удовольствием на них съезжаются. Задача менеджера - организовать этот процесс. Ему могут помогать HR-ы, но менеджеры обязаны подключаться к этим процессам, потому что в повседневной жизни HR-ы проводят с разработчиками меньше времени и могут хуже понимать коллектив. А менеджер лучше знает настроения внутри, может корректнее подобрать активности. Подведу итог. Менеджер на удаленке необходим, но в своей работе он должен учитывать формат работы, акцентируя внимание на том, что помогает коммуникациям внутри. Автор: Кирилл Антонов, один из основателей Maxilect. P.S. Мы публикуем наши статьи на нескольких площадках Рунета. Подписывайтесь на нашу страницу в VK или на Telegram-канал, чтобы узнавать обо всех публикациях и других новостях компании Maxilect.",
    "119": "Это исследователи из NSS Lab ИТМО и создатели сообщества ITMO.OpenSource. Мы считаем, что важно делать не просто науку, а открытую науку, результатами которой можно легко воспользоваться. Поэтому пишем об открытом коде, проводим митапы «Scientific Open Source» и разрабатываем научные решения с открытым кодом. Эта статья написана на материале исследования, которое мы презентовали на последнем митапе. В ней поговорим об открытом научном коде, посмотрим, как с ним обстоят дела в разных организациях, и поделимся подборкой репозиториев, которые могут решать различные научные задачи. В машинном обучении научная статья перестаёт быть достаточным артефактом исследования. Можно собрать данные, провести эксперименты, написать статью и даже опубликовать её, но вашими результатами никто не сможет воспользоваться, потому что датасеты закрыты, моделей нет, эксперименты и методы описаны недостаточно подробно, чтобы их воспроизвести… Список можно продолжить. А открытые данные (включая код) к статье резко повышают вероятность, что вашу статью не просто прочитают и скажут: «Ага, хорошо», ― но и попробуют воспроизвести результаты, использовать идеи и методы в своём исследовании и – last but not least – процитировать. Мы делаем все эти заявления не на пустом месте. Сообщество научного опенсорса Университета ИТМО объединяет более 700 разработчиков и пользователей открытого ПО по всей России. Среди наших разработок ― более 30 репозиториев с сотнями звезд и сотнями тысяч скачиваний из более чем 40 стран. Наши регулярные митапы привлекают широкий круг опенсорс-энтузиастов из Санкт-Петербурга и других городов, а чат является популярной площадкой для общения. При сообществе действует одноименный студенческий клуб, мы помогаем в менторстве проектов в ИТМО и за его пределами. Также мы активно участвуем в сообществе Open Data Science и уже не один год организуем опенсорс-секции в рамках конференции ODS DataFest. Казалось бы, всё довольно неплохо ― но это не совсем так. В России создается много интересных открытых решений, реализующих или использующих различные технологии ИИ. Однако зачастую их разработчики ничего не знают друг о друге. Даже существующие опенсорс-сообщества весьма разрозненны ― они формируются вокруг энтузиастов, но часто так и не выходят за пределы своих «информационных пузырей». Это затрудняет популяризацию новых проектов, привлечение аудитории в уже существующие, усложняет продвижение интересов открытого кода, да и в целом не дает понять, не изобретаем ли мы велосипед в виде yet-another-community. Поэтому мы решили провести небольшое исследование и более тщательно разобраться, кто есть кто в мире российского ИИ-опенсорса. Мир открытого кода в ИИ не ограничивается библиотеками и фреймворками. Свободно распространяются модели, бенчмарки, код к статьям. Возможно, в будущем мы напишем и про них, но в этой статье (и в нашем исследовании) рассматриваются только библиотеки и фреймворки, предназначенные для решения исследовательских задач. На то есть причина: модели, код к статьям и бенчмарки не нуждаются в постоянной доработке, они выкладываются на github или hugging face и часто консервируются. Библиотеки, напротив, дают больше информации о ходе разработки и о развитии и поддержке проекта. Кроме того, библиотеки встраиваются в другие проекты и могут расширяться до решения задач, о которых core-разработчики не предполагали. Анализ этой информации даёт более глубокое понимание контекста научного open source как динамической сети взаимосвязанных акторов (людей, научных групп, лабораторий и так далее). Как выяснилось в первые часы исследования, найти репозитории с научными ИИ-библиотеками ― весьма нетривиальная задача. Некоторые проекты и лаборатории мы вспомнили, другие пытались найти через Google, Хабр и сайты университетов. Некоторые проекты было сложно найти из-за того, что о них нигде не писали, некоторые лежали на личных аккаунтах разработчиков, а репозитории МФТИ и Сколтеха были вовсе удалены из-за санкций. Мы проанализировали более 50 проектов из 5 университетов (ИТМО, МФТИ, ВШЭ, Сколтех, Иннополис) и 4 компаний (Яндекс, Сбер, Тинькофф, AIRI). До сих пор не существует общепринятого способа оценить open source проект. Звёздочки на гитхабе мало говорят о качестве кода, частота коммитов зависит от числа заинтересованных разработчиков (а для нишевых библиотек оно может быть кратно меньше, чем для более широких), наличие readme и документации показывает, что авторы задумались об удобстве использования их проекта, но это не значит, что пользоваться проектом стало понятно и удобно. В качестве компромиссного варианта мы взяли метрику SourceRank агрегатора репозиториев libraries.io. Это кумулятивная метрика, учитывающая как базовые вещи типа наличия readme и лицензии, так и частоту релизов, количество зависимых репозиториев и число контрибьюторов. Для уравновешивания нишевых и массовых проектов SourceRank использует логарифмическую шкалу при расчёте некоторых её составляющих. SourceRank не учитывает наличие канала поддержки проекта (телеграм-чата и прочее), хотя его наличие важно. Также она не учитывает наличие примеров и туториалов по использованию фреймворков. Отсутствие однозначно хорошего способа оценить проекты и низкая осведомленность научного open source сообщества о разработках коллег – причины, склонившие нас к мысли, что главный результат нашего исследования – это не рейтинг open source проектов, а их обновляемый список. А теперь – к результатам. Большинство исследованных репозиториев имеют небольшое число звёзд (до 100) и малое количество контрибьюторов. Помимо этого, существует явный дисбаланс между количеством репозиториев от каждого университета (доля репозиториев ИТМО составляет 40% общей выборки, некоторые организации представлены 1–2 репозиториями). Поэтому для сравнительного анализа были выбраны по 3 самых популярных (число звёзд) проекта. Исследуемые репозитории университетов представлены в таблице. ITMO AIM.CLUB Сводная оценка по метрике SourceRank представлена на рисунке. У репозиториев ИТМО высокое значение метрики с низкой дисперсией. Это объясняется наличием базовых элементов (readme, лицензии) в каждом фреймворке, а также частыми релизами и наличием библиотек, имеющих зависимости от этих фреймворков. У репозиториев ВШЭ тоже присутствуют базовые элементы, но они релизятся не так часто и не имеют зависимостей. Для расчёта итогового значения метрики по каждому университету мы решили учитывать не только SourceRank самого популярного репозитория, но и общее количество «значимых» проектов (брали все проекты, у которых было 20+ звёздочек на гитхабе). Если упорядочить репозитории по числу звёзд, то формула предлагаемого критерия имеет вид: где SRUNI – итоговое значение метрики SourceRank для университета, SRn – значение метрики SourceRank для n-ного проекта. Результаты представлены на рисунке. Можно заметить, что итоговая оценка ИТМО выше, чем у остальных организаций. Это связано с бóльшим числом проектов. У МФТИ и ВШЭ показатели близки, но МФТИ представлен только двумя проектами, а DeepPavlov широко известен за пределами Физтеха, привлёк научных и индустриальных партнёров и имеет большое число внешних контрибьюторов. При этом состав репозиториев ВШЭ достаточно диверсифицирован, хотя их средний SourceRank ниже. Некоторые крупные российские компании достаточно прочно связывают разработки в ИИ и полезность публикации их исходного кода. Например, фреймворк CatBoost, возникший в Яндексе, давно вошёл в число базовых инструментов ML-инженера. Исследуемые репозитории представлены в таблице. Нельзя сказать, что небольшое число репозиториев говорит о низком вкладе этих компаний в опенсорсный ИИ. Просто они концентрируются преимущественно на выпуске моделей и бенчмарков (примеров масса: Russian SuperGlue Benchmark, датасет и модели РЖЯ Slovo, датасет для распознавания эмоций Dusha, YaLM-100B…). Также проекты ClickHouse и YTsaurus, хотя и используются в работе с big data, не рассматривались из-за большей ориентированности на продуктовый ML, нежели на ресёрч. Всё это показывает, что компании направляют силы на создание проектов, слишком ресурсоёмких для академических групп. У вас могло возникнуть ощущение, что это исследование особо ни о чём не говорит. Это в какой-то мере правда: сделать обобщённые выводы об уровне развития научного open source в университетах и институтах нельзя, потому что могут быть интересные проекты, которые мы не смогли найти. Хочется верить, что в ИТМО больше открытого кода, но, возможно, проекты наших коллег просто лучше представлены в информационном поле. Про коммерческие компании вообще трудно сказать хоть что-то: они делают акцент на моделях и бенчмарках, которые мы вообще не рассматривали. Но не бывает неудачных исследований. Мы упомянули много интересных проектов и постарались очертить поле научного open source. Набросок ландшафта – это площадка для будущих дискуссий и исследований. Надеемся, что они возникнут. Перечни проектов научных организаций и компаний выложены в нашем репозитории open-source-ops. В этом репозитории также содержатся гайды, лучшие практики и многое другое, связанное с open source разработкой. Если вы знаете библиотеки, которых там нет, – пишите в комментариях или смело создавайте PR, мы будем вам очень благодарны! Если вам интересна деятельность сообщества ITMO.OpenSource – заходите в наш чат и следите за обновлениями. Уже совсем скоро мы проведём онлайн-митап «Scientific Open Source», на который вас с удовольствием приглашаем.",
    "120": "Индивидуальный план развития (ИПР) - документ с конкретными шагами для развития компетенций, которые необходимо прокачать сотруднику, чтобы приблизиться к достижению бизнес-показателей. Например, линейного сотрудника назначили на руководящую позицию. У него появляется команда, новый функционал и потребность в развитии управленческих компетенций. В такой ситуации новому руководителю необходим конкретный план, где будет указано, что и когда надо делать для ликвидации недостающих навыков. ИПР дисциплинирует и задает вектор движения сотруднику к намеченной цели; он позволяет не размазывать усилия, а сосредоточиться только на том, что нужно здесь и сейчас. Цель. Определение конкретных целей, которые сотрудник должен достичь. Сроки. Постановка плановых сроков по достижению целей. Действия по развитию. Разработка мероприятий и описание конкретных действий для достижения поставленных целей. Поддержка. ФИО коллег(и), кто будет помогать сотруднику в развитии. Контроль. Показатели или параметры, которые будут показывать прогресс \"прокачки\" сотрудника. Отметка о выполнении. Оценка достижения сотрудником намеченных результатов. Сопоставление целей развития сотрудников с целями компанииИПР должен быть ориентирован на достижение целей и реализацию стратегии компании, также он должен коррелировать с общими требованиями компании. Взаимодействие между руководителем и сотрудникомОчень важно, чтобы руководитель поддерживал развитие сотрудников, не препятствовал ему и оказывал содействие. Также важно, чтобы руководитель принимал непосредственное участие: отмечал изменения в поведении сотрудника в правильном направлении и предоставлял конструктивную обратную связь. Учет индивидуальных особенностей сотрудникаСотрудник должен быть сам готов к изменениям. Ему крайне необходимо понимать, для чего ему нужно менять привычное поведение. Для этого в ИПР включается мотивационная часть, ведь если сотрудника не интересует его развитие, то необходимо принимать определенные управленческие решения. Постоянный мониторинг и корректировкиИПР - своего рода список действий, направленных на постоянное развитие сотрудника. Такие действия могут носить единичный характер, например прохождение интенсива или тренинга. Чтобы новые знания хорошо усвоились, они должны применяться на практике. Это значит, что после прохождения тренинга необходимо продумать способы закрепления и контроля полученных навыков. Адаптация к изменениямИПР должен пересматриваться в том случае, когда происходят существенные изменения в компании, внешней среде и т.д. В идеале стоит учитывать еще и изменения в жизни самого сотрудника. Для этого, например, каждый раз при проведении ежегодной оценки и формировании ИПР на следующий год необходимо соотносить долгосрочный ИПР с текущим положением дел. SMART-целиSMART-цели - цели, которые являются конкретными, измеримыми, достижимыми, релевантными и ограниченными по времени. Установка SMART-целей поможет сделать план развития более эффективным и реалистичным. Прежде чем сформировать ИПР, нужно четко понимать, с чего начать это делать. Необходимо определить зоны для роста, т.к. это ключевой этап в создании эффективного ИПР. Анализ результативности и эффективности работы (в т.ч. через KPI, OKR и т.д.)Оценка производительности и текущих компетенций сотрудника помогает определить его сильные и слабые стороны. Результаты такой оценки следует рассматривать как один из инструментов для выявления областей, где требуется дополнительное развитие, но не как единственный. Метод оценки “360 градусов”Обратная связь от коллег, руководителей и заказчиков - ценный источник информации о зонах роста сотрудника. Важно анализировать рекомендации и советы, чтобы определить, какие навыки и компетенции следует улучшать. Стоит помнить, что важно соотнести полученную информацию с целями команды, компании. Самооценка и развитие навыковСтоит предложить сотруднику провести оценку своих навыков и определить области, над которыми ему хотелось бы поработать. Это поможет включить личные предпочтения и интересы сотрудника в план развития. Анкетирование и проведение опросовРегулярно проводите опросы по уровню удовлетворенности, вовлеченности и выгорания среди сотрудников, чтобы своевременно отслеживать изменения и динамику. Те, кто сталкиваются с выгоранием, могут быть менее мотивированы к развитию. Удовлетворенные и активно участвующие в жизни компании сотрудники станут агентами изменений на местах. Они будут адаптироваться и меняться, чтобы лучше соответствовать целям компании в области развития. Составление ИПР может быть сложным и трудоемким процессом. Вот некоторые лайфхаки, которые помогут упростить эту задачу: Используйте инструменты и программыМожно использовать специализированное ПО для автоматизации процессов оценки, составления ИПР, а также контроля их исполнения. По сути система берет на себя решение однотипных задач, освобождая время сотрудника для саморазвития. Так появляется мотивация и время для улучшения навыков, которого в рутине будней иногда так не хватает. Внедряйте техники мотивации и стимулированияПри составлении ИПР нужно учитывать индивидуальные предпочтения и интересы сотрудников. Это сделает процесс развития более эффективным и результативным. Стоит продумать мотивационную схему. Это может быть: гибкий график работы, возможности для профессионального роста и т.д. Такое комплексное внимание к потребностям сотрудников поможет стимулировать их активное участие в жизни компании и стремление к личностному росту. Привлекайте менторов и наставниковНужно убедиться, что у сотрудника есть необходимые ресурсы и поддержка для реализации ИПР. Помимо доступа к обучающим материалам, стоит помнить про систему менторства, где более опытные коллеги будут сопровождать и направлять новичков. Также стоит обеспечить их возможностью посещать специализированные тренинги, мастер-классы и профессиональные конференции. Комплексный подход усилит мотивацию сотрудника и ускорит достижение поставленных целей. Когда у сотрудника есть опытный ментор или наставник, это значительно повышает его уровень уверенности и мотивацию развиваться. Регулярно обсуждайте и корректируйте планЧтобы эффективно реализовать ИПР, важно не только его составить, но и регулярно мониторить. Рекомендуется организовывать ежемесячные встречи руководителя с сотрудником, чтобы обсудить текущий прогресс, возникающие сложности и при необходимости скорректировать план. Такое взаимодействие помогает поддерживать мотивацию сотрудника, а также гарантирует актуальность ИПР. Для реализации плана развития используются различные методы и инструменты. Например, это может быть обучение на курсах, участие в тренингах и семинарах, получение обратной связи от коллег и руководителя, наставничество, коучинг, менторство, самостоятельное развитие и другие. Все эти инструменты помогают сотруднику развиваться и достигать своих целей в карьере. Далее рассмотрим наиболее популярные из них. 10% обучения через внешние источники: литература, обучающие курсы и т.д. Обучение на рабочем месте (on-the-job training) - метод обучения, при котором новые или уже работающие сотрудники получают необходимые знания, умения и навыки непосредственно на своем рабочем месте. Оно может быть организовано в различных формах, таких как инструктаж, решение новых рабочих задачи, реализация проектов, трансформация рабочих процессов и т.д. - Может не иметь четкой структуры, что может затруднить контроль качества обучения и оценку результатов. - Слишком много информации может привести к перегрузке и снижению эффективности обучения. - В некоторых случаях обучение на рабочем месте может занимать слишком много времени, что может снизить производительность сотрудников. - Возможность получить практический опыт работы в реальных условиях, что может быть полезно для развития профессиональных навыков и быстрой адаптации к новым обязанностям. - Получение обратной связи от коллег и руководства позволяет быстро исправлять ошибки и улучшать навыки. - Может быть адаптировано к индивидуальным возможностям каждого сотрудника, что делает его более эффективным и гибким. - Может повысить мотивацию сотрудников, поскольку оно позволяет им развивать свои профессиональные навыки и достигать новых целей. - Более экономичное по сравнению с другими формами обучения, поскольку не требует значительных затрат на организацию и проведение. Обратная связь (ОС) - один из наиболее эффективных методов обучения и развития сотрудников. Она позволяет не только улучшить их профессиональные навыки, но и способствует созданию благоприятной рабочей атмосферы. - Хотя ОС стремится быть объективной, она все же может быть субъективной, особенно если основывается на личных предпочтениях, предубеждениях или эмоциях. - Не все сотрудники могут получать достаточное количество ОС, что может привести к неравномерному развитию навыков и знаний. - Предоставление и получение ОС может быть стрессовым процессом для некоторых людей, особенно если критика является негативной. - Некоторые сотрудники могут сопротивляться изменениям, предлагаемым в рамках ОС, что может замедлить процесс развития. - Поскольку ОС основывается на реальных наблюдениях и фактах, она позволяет получить более точную и объективную оценку работы сотрудника. - Информация, полученная в процессе ОС, обычно излагается четко и ясно, что позволяет сотруднику понять, что от него требуется и какие ошибки были допущены. - Предоставление ОС сразу после выполнения задания или работы позволяет сотруднику быстро исправить ошибки и улучшить свои результаты. - Обычно ОС содержит не только критику, но и предложения по улучшению работы, что помогает сотруднику развиваться и расти профессионально. - Положительная ОС может стать мощным стимулом для сотрудника, мотивируя его на дальнейшие успехи и достижения. - Помогает наладить эффективную коммуникацию между сотрудниками и руководством, что способствует созданию благоприятной рабочей атмосферы и повышению уровня удовлетворенности работой. Самообучение - процесс получения знаний и навыков самостоятельно, без помощи коллег, руководителя, преподавателей и т.д. Оно может включать в себя изучение новых предметов, развитие навыков, а также самосовершенствование. Самообучение может быть как формальным (например, посещение курсов или тренингов), так и неформальным (например, чтение книг, просмотр видео, общение с другими людьми). Необходимо предоставлять сотруднику возможность самостоятельно определять, например, специализированную литературу, которая поможет формировать общие представления о развиваемой компетенции и искать формы для эффективной работы. - Может быть хаотичным и беспорядочным, поскольку сотрудник сам определяет темп, порядок и содержание обучения. - В самообучении обратная связь от учителя ограничена или отсутствует вовсе. - Самообучающиеся люди часто сталкиваются с отвлекающими факторами, которые могут мешать концентрации и результативности обучения. - Многие люди могут испытывать трудности с сохранением мотивации при отсутствии внешних стимулов. - Самообучение иногда требует доступа к ресурсам, которые могут быть недоступными или дорогими для некоторых людей. - Самообучение требует больше времени для достижения тех же результатов, что и традиционное обучение. - При самообучении трудно проверить и оценить собственные знания. - Отсутствие внешней оценки и поддержки может привести к эмоциональному выгоранию. - Позволяет сотрудника адаптировать свое обучение к своим индивидуальным потребностям, интересам и темпу. - Дает сотрудникам гибкость в планировании своего обучения и возможность изучать материалы в удобное для них время. - Самообучающимся людям постоянно приходится обновлять свои знания и навыки, чтобы оставаться актуальными в своей области. - Поощряет независимость и самодисциплину, поскольку учащиеся должны самостоятельно планировать, организовывать и контролировать свое обучение. - Самообучение менее затратно, чем традиционное обучение, т.к. не требует оплаты услуг учителей, учебных материалов или посещения занятий. Тематические конференции - важный инструмент для обучения и развития сотрудников. Они предоставляют возможность для обмена опытом, налаживания деловых контактов, получения новых знаний и вдохновения. Конференции также могут помочь сотрудникам оценить конкурентов и улучшить репутацию компании. Таким образом, тематические конференции являются ценным ресурсом для обучения и развития персонала. - Конференции могут быть дорогостоящими, особенно если они проводятся в других городах или странах. - Конференции обычно проводятся в течение ограниченного времени, что может ограничить количество времени, которое сотрудники могут уделить обучению. - На конференциях может предоставляться слишком много информации, которая может быть избыточной или ненужной для некоторых сотрудников. - Некоторые конференции могут больше фокусироваться на теории, а не на практическом применении полученных знаний. - Вернувшись на работу, сотрудникам может потребоваться время, чтобы интегрировать новые знания в свою текущую работу. - Обучение на конференциях часто является групповым, и индивидуальный подход к обучению может быть затруднен. - Качество и актуальность информации на конференции во многом зависит от докладчиков, их опыта и знаний. - На тематических конференциях сотрудники могут обмениваться опытом с коллегами из других компаний, узнавать о новых трендах и технологиях. - Участники могут устанавливать деловые связи, находить потенциальных партнеров, клиентов и наставников. - На конференциях часто выступают эксперты из разных областей, которые могут поделиться своими знаниями и опытом. - Участники могут наблюдать за тем, как другие компании используют полученные знания и навыки в своей работе. Тренинги и семинары - метод обучения, который используется для повышения квалификации сотрудников. Тренинги обычно проводятся в течение короткого периода времени и сосредоточены на развитии конкретных навыков или знаний. Семинары же могут длиться дольше и охватывают более широкий круг тем. Они могут быть организованы как внутри компании, так и внешними организациями. - Тренинги и семинары могут быть дорогостоящими, особенно если компания проводит их регулярно или если они проводятся за пределами офиса. - Участие в тренингах или семинарах может отвлечь сотрудников от выполнения их основных рабочих обязанностей. - Некоторые тренинги и семинары могут не предоставлять достаточного количества практических заданий или кейсов, которые помогут сотрудникам применить полученные знания на практике. - Если тренинг или семинар не является актуальным или полезным для сотрудников, они могут чувствовать, что их время тратится впустую. - Некоторые тренинги или семинары могут не учитывать индивидуальные потребности и предпочтения каждого сотрудника. - Некоторые тренинги или семинары не предоставляют достаточной обратной связи, которая могла бы помочь сотрудникам улучшить свои навыки или знания. - Тренинги и семинары предоставляют участникам возможность получить практический опыт в определенной области. - Тренинги и семинары могут предложить участникам новые идеи и информацию, которую они могут использовать в своей работе. - Участие в семинарах и тренингах может быть мотивирующим для сотрудников. - Тренинги и семинары часто предоставляют участникам возможность узнать о новых возможностях или получить новые навыки, которые могут быть полезными для их карьеры. - Тренинги и семинары позволяют сотрудникам расти личностно, поскольку они учатся новым вещам и развивают новые навыки и способности. Временная ротация - процесс, при котором сотрудники меняются своими рабочими обязанностями или позициями на определенный период времени. Это может быть полезно для обучения сотрудников, поскольку позволяет им получить новый опыт и расширить свои знания и навыки. - Временная ротация может вызвать конфликты между сотрудниками, если они не согласны с изменениями или если их новая роль вызывает стресс. - Временная ротация требует дополнительных затрат на адаптацию сотрудников и обучение их новым навыкам. - Некоторые сотрудники могут потерять мотивацию к работе, если они знают,что их роль может измениться в ближайшем будущем. - Временная ротация не всегда обеспечивает достаточное количество возможностей для профессионального развития, особенно если она ограничена по времени. - Временная ротация позволяет сотрудникам получить новый опыт и знания, которые могут быть полезны в их текущей должности и в будущем. - Временная ротация может стимулировать сотрудников к обучению, т.к. они понимают, что их работа может измениться в будущем, и они должны быть готовы к этому. - Временная ротация повышает гибкость компании, т.к. сотрудники могут быть перемещены на другие позиции в зависимости от потребностей компании. - Временная ротация также может снизить уровень стресса у сотрудников, т.к. они знают, что их рабочие обязанности могут измениться в будущем. Наставничество - метод развития, при котором более опытный и знающий сотрудник, называемый наставником, помогает менее опытному или знающему, называемому учеником, в приобретении определенных навыков и знаний. Наставничество может быть формальным, когда наставник и ученик договариваются о конкретных целях и задачах, которые ученик должен достичь, и неформальным, когда опыт и знания наставника передаются ученику в процессе обычного общения и взаимодействия. Наставничество может осуществляться в разных формах: индивидуальные консультации, групповые занятия, работа над проектами, участие в тренингах и семинарах. - Наставники могут быть заняты своими собственными задачами и не иметь достаточно времени для постоянного сопровождения ученика. - Качество наставничества может варьироваться в зависимости от опыта и знаний наставника. - Ученик может не получить достаточной обратной связи от наставника, что может замедлить его прогресс. - Наставник может помочь ученику определить его слабые места и разработать стратегию для их устранения. - Наставничество помогает развить такие навыки, как коммуникация, критическое мышление, решение проблем и самоорганизация. - Наставники могут оказывать эмоциональную поддержку и мотивировать учеников к достижению их целей. - Ученики могут устанавливать связи с другими людьми в своей области, что может способствовать их профессиональному росту и развитию. Регулярно собирайте и анализируйте данныеЧтобы делать выводы о продвижении в реализации индивидуального плана развития, необходимо собирать данные о достижениях сотрудника и анализировать их. Это могут быть показатели производительности, результаты обучения, отзывы коллег и пользователей и т.д. Используйте метрикиОпределите ключевые показатели, которые будут отражать прогресс в реализации плана развития. Например, это может быть количество выполненных задач, процент успешных проектов, уровень профессиональных навыков и т.п. Сравнивайте результаты с планомСравните полученные данные с теми, которые были запланированы при составлении плана развития. Если результаты соответствуют плану, то это может говорить о том, что сотрудник движется в правильном направлении. Обратите внимание на обратную связьОбратите внимание на отзывы коллег, подчиненных и руководителей о работе сотрудника. Это может дать дополнительную информацию о его успехах или проблемах в реализации плана. Обсуждайте результаты с сотрудникомОбсудите полученные данные с сотрудником. Это поможет ему понять, где он достиг успеха, а где есть проблемы, и поможет разработать стратегии для улучшения результатов. Определите корректирующие действияНа основе полученных данных и обсуждений, определите, какие корректирующие действия необходимо предпринять для улучшения реализации плана. Это могут быть дополнительные тренинги, изменение обязанностей, улучшение коммуникации и т.д. Проведите беседуСначала попытайтесь понять причины, по которым сотрудник не хочет развиваться. Возможно, у него есть какие-то опасения или страхи, которые мешают ему расти профессионально или личностно. В ходе разговора можно выявить эти препятствия и найти способы их преодоления. Предложите помощь и поддержкуЕсли сотрудник столкнулся с трудностями, предложите свою помощь и поддержку. Возможно, ему нужны дополнительные ресурсы или обучение, чтобы продвинуться в карьере. Направляйте сотрудникаЕсли сотрудник не знает, как развиваться, помогите ему определить цели и составить план действий. Вместе с ним разработайте план развития, который будет включать в себя конкретные шаги и сроки. Обучайте и развивайте сотрудникаРегулярно проводите обучающие мероприятия для сотрудников, чтобы помочь им в развитии. Это могут быть как внутренние, так и внешние курсы, семинары или конференции. Оценивайте результаты работыРегулярно оценивайте результаты работы сотрудника, чтобы увидеть его прогресс и понять, что нужно улучшить. Обратная связь поможет сотруднику понять, над чем нужно работать и как достичь успеха. Мотивируйте сотрудникаПоощряйте успехи и достижения сотрудника, это поможет ему чувствовать себя значимым и мотивированным для дальнейшего развития. Создайте благоприятную средуСоздайте условия, в которых сотрудник сможет развиваться и расти профессионально и личностно. Обеспечьте прозрачность карьерного роста, возможность обучаться и обмениваться опытом с коллегами. Не давите на сотрудникаНе стоит давить на сотрудника, если он не хочет развиваться, это может вызвать стресс и нежелание работать в компании. Будьте терпеливы и поддерживайте его на пути к развитию. Рассмотрите возможность перевода на другую должностьЕсли сотрудник все еще не желает развиваться и препятствует этому другим сотрудникам, возможно, стоит рассмотреть вариант его перевода на другую позицию или даже в другое подразделение, где он сможет применить свои навыки и развиваться в новом направлении. При необходимости, обратитесь к эксперту по развитию в компанииЕсли все вышеперечисленные методы не помогли, возможно, сотруднику потребуется помощь специалиста, который поможет разобраться в причинах его нежелания развиваться и найти пути преодоления этих препятствий. Про практические методы и инструменты развития мы с коллегами из OTUS рассказываем в рамках онлайн-курсов по менеджменту и управлению. С полным каталогом курсов можно ознакомиться по ссылке.",
    "121": "Социальная сеть Google+ должна была стать альтернативой проекту Марка Цукерберга — платформой, помогающей людям оставаться на связи и находить друзей по интересам. Но спустя семь лет после запуска проект пришлось закрыть. Разбираемся, что пошло не так и почему новая соцсеть оказалась не только непопулярной, но и небезопасной. Создателем Google+ стал Вик Гундотра — акционер и руководитель подразделения мобильных сервисов Google. Он убедил основателя корпорации, Ларри Пейджа в том, что собственная социальная сеть — единственный способ конкурировать со стремительно развивающейся платформой Цукерберга. Заручившись поддержкой частного инвестора Брэдли Горовиц, Вик Гундотра приступил к разработке новой платформы и спустя год его идея была воплощена в жизнь. Google+ стала четвертой попыткой поискового гиганта занять нишу сервисов для общения и обмена контентом. Предыдущие проекты — Orkut (2004-2014), Friend Connect (2008-2012) и Buzz (2010-2011) не оправдали ожиданий и вскоре были закрыты. Инновационная соцсеть должна была исправить это и сделать корпорацию лидером рынка. Основная «фишка» Google+ — circles или «круги». Эта функция позволяла пользователям объединять людей в группы и делиться с ними избранным контентом. Например, можно было показать фото с корпоратива только коллегам, а видео из отпуска — близким друзьям. А ленту новостей отфильтровать, оставив в ней посты из определенных кругов и сообществ. Кроме того, Google+ была интегрирована с другими продуктами корпорации, такими как Gmail и YouTube. Это позволяло за пару кликов отправлять сообществам или отдельным пользователям электронные письма, фото и видео. А еще — способствовало расширению аудитории сервисов. Открытие регистрации в Google+ состоялось в июне 2011. Первое время она была доступна только по приглашениям. Атмосфера «закрытого клуба» всего за месяц привлекла на платформу 10 млн пользователей, а к концу года их число выросло до 90 млн. Несмотря на ажиотаж, первые проблемы настигли соцсеть уже через несколько недель после старта. Google запретила пользователям указывать псевдонимы вместо настоящих имен и потребовала раскрыть свой пол. Аккаунты, не соответствующие этим требованиям, были удалены. Потеря возможности сохранить анонимность вызвала волну возмущения. Люди, несогласные с новыми правилами, лишились доступа не только к новой соцсети, но и к другим сервисам, включая YouTube, Gmail, Google Maps, Google Play, Google Music, Google Wallet, Google Local. В процессе «чистки» корпорация допустила ошибку, еще больше подорвавшую ее репутацию — временно заблокировала все бизнес-профили, нанеся им существенный ущерб. В 2012 году Google решила любой ценой ускорить темпы роста аудитории своего детища. Теперь люди, не зарегистрированные в ней, не могли использовать Gmail. А в 2013 году аккаунт Google+ стал допуском к комментированию видео на YouTube. Эти приемы сработали — количество пользователей превысило 500 млн. Но многие профили были «мертвыми» — их создавали только для того, чтобы получить доступ к нужным сервисам. По данным исследовательской компании Nielsen, посетители сайта Google+ проводили на нем около семи минут. А The New York Times сравнила соцсеть с городом-призраком. В 2014 году Google+ покинул Вик Гундотра, занимавший должность исполнительного директора. В своем прощальном посте он поблагодарил «непобедимых коллег» за то, что они «упорно занимались развитием соцсети несмотря на скептицизм критиков». Вскоре после ухода основателя политика корпорации резко изменилась — с 2015 года ее сервисами смогли воспользоваться все желающие, а не только владельцы аккаунтов Google+. Кроме того, Google Photos и мессенджер Hangouts стали самостоятельными продуктами. По словам Google, из соцсети были удалены функции, не соответствующие ее миссии — «получению социального опыта, основанного на общих интересах». При этом обновлений, способных заполнить образовавшуюся пустоту и заинтересовать аудиторию, не появилось. Напротив, некоторые перемены лишь усложнили использование сервиса. Так, отказ от хештегов затруднил поиск нужного контента. Очередным неоднозначным нововведением стала фильтрация новостной ленты в соответствии с интересами людей, а не с хронологией событий, как раньше. В результате актуальные публикации часто оставались незамеченными, а продвижение коммерческих аккаунтов превратилось в невыполнимую задачу. На фоне этих перемен большинство пользователей окончательно потеряли интерес к Google Plus и вернулись к соцсетям Марка Цукерберга. В августе 2019 года Google Plus перестала существовать. К этому моменту около 90% ее посетителей проводили на сайте менее пяти секунд. Согласно официальному заявлению корпорации, причиной закрытия проекта стала низкая вовлеченность аудитории. А еще — сложности в обслуживании соцсети: Google не обеспечила безопасность базы данных, и-за чего в общий доступ попали имена, email-адреса, пол, возраст и должности пользователей. Этот инцидент мог навредить более 50 млн людей. Еще одна версия окончательного провала Google+ — отставание от конкурентов. К моменту ее запуска люди уже успели привыкнуть к другим соцсетям и не хотели бросать свои аккаунты. Кроме того, поисковый гигант буквально навязывал пользователям новый продукт, не позволяя своему детищу развиваться естественным образом. Как отмечает аналитическая компания Startup Genome, «Google хотела слишком многого и слишком быстро». Руководство по товарным знакам в 2024 году.",
    "122": "Привет, Хабр! Я Аня Анциферова, продакт «Цифрового вагона». Я уже рассказывала о том, зачем ПГК пошли в разработку и какие продукты мы делаем. Несмотря на то, что сейчас у ПГК существует «дочка» — ПГК Диджитал, и там трудится порядка 400 человек, мы — не ИТ-гигант. А это значит, что каждый проект, за который мы беремся, и даже каждую фичу, которую дорабатываем, мы должны оценить на предмет эффективности. И доказать, почему разработка оправдана и целесообразна. Сегодня расскажу о том, как такую базовую оценку может провести тимлид. Когда мы говорим о разработке в ПГК Диджитал, обычно подразумеваем один из двух сценариев. Либо речь идет о доработке и расширении существующего проекта. Либо — о новом продукте. Например, над «Цифровым вагоном» мы работали несколько лет. Соответственно, мы проходили ежегодную «сверку»: показывали, какие цели перед нами ставились, чего мы достигли, куда идем дальше. Готовы ли какой-то модуль передавать в поддержку, нужно ли переходить к разработке нового модуля, что нам это даст. В общем, ежегодно защищали бюджет на разработку решения. Возможен и другой вариант — когда бизнес приходит с новой идеей об автоматизации. Есть определенный порядок рассмотрения таких идей: в компании изначально принимают решение, чьими ресурсами будет идти проработка проекта, а дальше выбранная команда (проекта или конкретного продукта) забирает идею на реализацию. Если это небольшая фича, то рассматривается возможность включения ее в текущий бэклог команды. Если это масштабная доработка, она проходит по процедуре защиты бюджета, так как на разработку нового решения требуется больше ресурсов. В обоих случаях нужно решить вопрос о том, как (хотя бы примерно) оценить эффективность будущей разработки. Для этого мы можем: Собрать прототип и протестировать его. У нас есть команда прототипирования, к которой может обратиться бизнес-заказчик, чтобы собрать пилотную версию проекта. Отличие этого варианта в том, что можно не проходить через все этапы разработки, а локально решить, в каком виде будет представлено решение для проверки гипотез. Дать оценку по аналогии с готовыми разработками. Если проект схож с тем, что делает команда, или это логическое продолжение текущей функциональности, то у нас уже есть понимание о том, как взаимодействуют системы, что нужно сделать, какие ресурсы потребуются. В таком случае мы можем максимально точно прикинуть бэклог и дать оценку как минимум на год, запросить бюджет и дать коммит на эффект (о том, что это и как мы можем такой эффект подсчитать, я расскажу чуть ниже). Небольшой дисклеймер: конечно, когда речь идет о полноценной инвестиционной оценке проекта, мы разбираем вопрос комплексно. Наша задача — рассчитать, как он будет чувствовать себя на протяжении минимум пяти лет. В этом случае используется целый ряд показателей, в том числе внутренняя норма доходности, чистая приведенная стоимость и другие. А сегодня я хотела бы рассказать о тех методах оценки эффекта, которые доступны тимлиду на первом этапе работ. В самом простом варианте эффективность можно трактовать как разницу между доходами от полученного результата и расходами на использованные ресурсы. В таком случае задача лидера команды — посчитать (пусть и приблизительно) эти самые доходы и расходы (правильность расчетов и адекватность выбранной методики — как на данном этапе, так и при комплексной оценке — подтверждают наши экономисты). Как считать расходную часть, относительно понятно: это затраты на ФОТ, инфраструктуру, лицензии и т. д. Некоторые показатели мы рассчитываем, исходя из приближенных значений. Другие расходы оцениваем как долю от общих затрат (например, расходы на инфраструктуру). А какие-то из расходов можно вычислить с высокой точностью — это, в частности, ФОТ. Доходную часть оценить сложнее. Сейчас у нас есть четыре основных подхода к такой оценке. Мы пришли к ним опытным путем и не считаем, что это единственно возможные варианты — наоборот, постоянно тестируем новые способы и методы. Но в настоящий момент используем следующие. Сначала мы сравниваем автоматизированное решение с ручным. Мы можем взять что-то, что делали вручную, автоматизировать (например, собрать прототип) и посмотреть, где мы начали лучше зарабатывать. В случае с сервисом «Цифровой вагон» сравнение выглядело так: Мы используем нормативные документы владельца инфраструктуры и локальные документы ПГК. Опираясь на них, рассчитываем, когда ориентировочно тот или иной вагон должен выбыть в ремонт. Далее берем статистику предыдущих периодов и составляем прогноз: где будет находиться вагон, когда придет время отправлять его в депо. И на основании этого формируем план заадресации. [Кстати, о том, что такое матрица заадресации и как она выглядит, мы недавно рассказывали в нашей статье «Оптимизатор ремонтов грузовых вагонов, что за зверь?».] Когда это происходило вручную, мы получали на выходе одну сумму затрат на ремонт в следующем месяце (план). После автоматизации расчетов мы получили другую сумму. Разница — результат, который мы засчитали как обоснование разработки. Подобная автоматизация может снизить затраты на 1–2% — на первый взгляд немного, но в денежном исчислении это ощутимая экономия. Постоянно сравнивать автоматизированное решение с ручным неудобно. Все-таки, мы планируем развиваться и уменьшать количество ручных операций. В таком случае включается расчет базисного периода. Мы можем зафиксировать значения в среднем за прошлый период и сравнивать с ними показатели, полученные по итогам разработки. Как на практике может выглядеть ситуация с расчетом базиса: Вагон уходит на ремонт в двух случаях: если возникла поломка, и ее нужно устранить прямо сейчас, либо если подошел срок планового обслуживания. В некоторых случаях эти два типа ремонта можно совместить так, чтобы не увозить вагон в депо два раза подряд. Наш «Цифровой вагон» позволяет оценить состояние вагона и, при необходимости, объединить текущий и плановый ремонт (вагон забраковали в текущий ремонт, а потом перебраковали в плановый). Как мы определяем эффект от использования этой функциональности: оцениваем, сколько раз в среднем за заданный период мы перебраковывали вагоны до того, как внедрили автоматизированное решение. Эти данные у нас уже были, такую оценку проводили и раньше, вручную, просто сам процесс отправки вагона на ремонт был намного менее управляемым. Дальше смотрим, как изменился уровень перебраковок — это наш результат, и это тоже существенная экономия для компании. Одна из метрик, которые мы оцениваем — длительность простоя вагона в ремонте. Чем меньше вагон стоит в депо, тем лучше. Для того, чтобы адекватно оценивать ситуацию, нашему бизнес-заказчику требовался дашборд, на котором было бы видно, что происходит с вагонами, в каких депо и как долго они находятся. Перед стартом проекта мы сделали предварительную оценку его эффективности: предположили, насколько могут уменьшиться простои, если у специалистов будет удобный инструмент ежедневного мониторинга. Новый сервис позволил оперативно отслеживать длительно простаивающие вагоны. Через некоторое время мы увидели, что им активно пользуются, у сотрудников подразделения появилась возможность совершать меньше «ручных» операций. Одновременно мы стали отмечать, что и простой вагонов действительно начал снижаться (а чем быстрее вагон покидает депо после ремонта, тем больше он зарабатывает). Прямолинейно оценить все выгоды от внедрения было сложно — на результат могли повлиять и другие решения. Поэтому тут на помощь пришли эксперты-экономисты. Они использовали факторный анализ, подсчитали, как новый инструмент повлиял на работу подразделения и насколько его использование отразилось на снижении процента простоев. Вначале считаем дельту между данными текущего периода и базового. На основе ранее полученного опыта определяем факторы, повлиявшие на дельту, в том числе факторы изменения в работе компании (не-ИТ, например, изменение структуры парка вагонов) и выделяем фактор ИТ (в данном случае — изменения, связанные с использованием нового дашборда). При этом все достигаемые эффекты по цифровым продуктам считаем общими эффектами совместно с бизнес-подразделениями, поскольку наши инструменты позволяют повысить скорость и качество принимаемых управленческих решений. «Цифровой вагон», помимо прочего, отслеживает состояние каждого вагона, чтобы предотвращать поломки и оптимизировать ремонты. В частности, у нас есть задача наиболее точно предсказывать, когда толщина гребня достигнет 25 мм. Это важный показатель технического состояния вагона (колесо должно плотно прижиматься к рельсу). Если гребень тонкий, вагон могут отцепить. Но если вагон отцепляют по другой причине, а не из-за колеса, мы смотрим, в каком состоянии гребни и, при необходимости, сразу же обтачиваем колеса. Чтобы оценить результат нововведений такого плана (правильно ли мы поступаем, или все же стоило подождать), мы проводим A/B-тесты: Группе А мы даем рекомендацию обтачивать колеса, группе B — не даем. И дальше отслеживаем, когда вагоны добегут до следующего планового ремонта. Смотрим коэффициент отцепки (это процентное соотношение количества отцепленных вагонов к общему парку вагонов в компании в определенном периоде). Если коэффициент снижается, то разница в снижении — наш эффект. Можно сказать, что «Цифровой вагон» позволил предотвратить N ремонтов (умножаем N на среднюю стоимость аналогичного ремонта, с учетом потерь на отвлечение вагона от эксплуатации, и получаем экономию за период в денежном выражении). Даже приблизительная оценка эффективности проекта помогает понять, нужно ли браться за полномасштабную разработку или игра не стоит свеч. А заодно лучше сформулировать наше видение и требования к будущему продукту. И наоборот — чем больше мы собрали данных о новой системе на старте, тем точнее можем ее оценить. Не говоря уже о том, что качество оценки растет с каждым новым проектом, вместе с компетенциями всей команды. Как минимум, такой подход позволяет разработчикам взглянуть на продукт с другой стороны — глазами бизнеса, и не просто развивать какой-то сервис, а разбираться в том, зачем он нужен компании и что он ей реально может дать.",
    "123": "Мне 21 год и я работаю программистом всего 4 года, за это время я побывал на 2-3 мелких проектах и 3-4 проектах крупных компаний, таких как: Luxoft (упокой его душу), Альфа, ОТП, ГПБ и др. Так же я часто прохожу собеседования и в другие компании, чтобы «держать себя в форме», собирать статистику и т. п. Прохожу собеседования, в том числе и в крупные финтехи и пока что не хочу туда идти. А почему, это тема для отдельной статьи. В разговорах со своими коллегами я очень часто слышал фразу «алгоритмы программистам не нужны», «алгоритмы никогда не используются в работе» и т.п. Я не являюсь ярым приверженцем того, что алгоритмы нужно учить и использовать их повсеместно, но я понимаю, как они могут помочь мне в жизни программиста. Ещё со школы мы учим математику, физику, биологию и другие б̶е̶с̶п̶о̶л̶е̶з̶н̶ы̶е, как нам кажется, бесполезные предметы и даже после школы не все понимают, для чего всё это было нужно. Как часто вы слышали фразу «мне математика в жизни не нужна, калькулятор всегда под рукой»? С одной стороны, всё верно, калькулятор всегда под рукой, любую формулу или теорему тоже можно найти за 2 минуты. Но как эти аргументы могут оспорить мнение, что все эти предметы просто меняют мышление. Как человек знает, что колесо проще катить, чем тащить булыжник, так и я знаю что бинарный поиск быстрее\\лучше, чем линейный. Хотя ни разу ни разу не тащил булыжник, не катил колесо и не писал алгоритм бинарного поиска в рабочем коде. Все эти алгоритмы реализованы в каждой второй библиотеке, и писать самостоятельно это практически никогда не нужно. Но как я пойму, что его тут вообще можно применить? Как я пойму, что он тут улучшит работу? Я могу не помнить, как этот алгоритм написан, где он реализуется и т.д, но я знаю, что могу вспомнить что это такое только посидев в интернете пару минут, я знаю, что такой алгоритм существует, что он применим в той или иной задаче. Но как бы я узнал эту информацию, если бы не зашел на LeetCode\\CodeWars\\Codeforces (нужное подчеркнуть) и не провел бы там хотя бы 20 минут своего времени, даже просто смотря на топики задач? Алгоритм не нужно учить, его нужно реализовать самостоятельно. Написать (можно даже с гайдами), с дебагом попробовать что-нибудь посмотреть внутри, поменять что-нибудь и понять что именно сломалось. Любая информация запоминается в разы лучше, если ёе записать\\нарисовать\\реализовать. Так и с алгоритмами, не нужно заходить на математические форумы и прошерстывая сотни страниц запоминать какой-либо алгоритм. Достаточно всего лишь зайти на любой сайт с алгоритмическими задачками открыть уровень Easy и решить 2-3 задачи на тот или иной алгоритм. Тут не идёт речь про алгоритмы уровня корневой оптимизации, которая помимо того, что просто специфична, но и абсолютна не реализуема в 99% задач в программировании. Бинарный поиск, сортировки, жадные алгоритмы и т.п. - наши лучшие друзья. После их реализации в голове появятся трейдоффы многих из них. Жадный алгоритм не всегда точно находит ответ, для бинарного поиска нужен отсортированный список и т.д. Наш мозг запомнит условия, в которых этот алгоритм был написан и в следующий раз на уровне ассоциаций подскажет решение для новой функции. А там уже скорее всего и встроенное решение можно найти. Я очень часто прохожу собеседования и очень часто слышу вопрос про HashMap, про индексы в БД. Знаю как на них отвечают другие люди, и в большей части люди могут только пересказать выученное предложение с какого-нибудь сайта. Возможно такое влияние на статистику имеют выпускники IT курсов, но и прожжённые опытом программисты не всегда понимают каким образом бинарный поиск лучше линейного, или из-за какой структуры данных индекс вообще ускоряет работу. Они знают их сложность алгоритмов, знают, что алгоритм бинарного поиска применим на сортированном массиве (хотя это тоже не всегда), но не знают почему. И тогда становится непонятно, как такой работник сможет на глаз определить узкое место какого-нибудь процесса или хотя бы предложить использовать другой, более продуктивный вариант функции. Тут опять может прозвучать аргумент «я же не реализую эти индексы, зачем мне знать какая там структура внутри? Мне сказали, что быстрее работает и это самое главное. А, как и почему это уже дело второстепенное». Всё верно, в современных реалиях программистам зачастую можно не знать как работает та или иная функция, но «экспертное» мнение обязательно должно быть в каждой команде. А что если такого человека нет у вас в команде? Тогда начинаются проблемы. Индексы вдруг начинают работать дольше потому, что никто не знает, что они замедляют операции вставки, поиск по списку начинает занимать секунды, если не десятки секунд и вся программа работает в разы медленнее только потому, что данных стало больше, а алгоритмы остались те же, т.к. никто не знает об существовании других или не знает куда их применить. Теперь ваш тестировщик вас постоянно пилит: «А почему теперь страница 2 минуты открывается?». Бесконечные созвоны, лживые оптимизации по типу «убрать лишний if из метода» и т. п. И все это продлится до того момента, как вас не уволят или не наймут человека с той самой «экспертизой». Так почему бы тогда не ждать человека, который вас может заменить, а самим стать человеком, который может заменить кого‑нибудь. Это так же может стать триггером для повышения зарплаты, так что никогда не поздно. Это так же проблема компаний, которые нанимают людей. Взять человека, который знает, что зачастую HashSet работает быстрее, чем TreeSet одно дело, но взять человека, который знает почему HashSet работает быстрее, чем TreeSet дело абсолютно другое. Один из них знает накладные расходы данной структуры данных, и сможет понять, что именно в данном случае нужна одна или другая коллекция, а второй просто знает, что зачастую HashSet работает быстрее, чем TreeSet. Но кроме крупных финтехов никто не спрашивает людей о таком. Все хотят спросить на собеседовании «в ширь», чтобы человек мог сказать поверхностно, но побольше, а там уже разберется. Хотя хорошо углубившись только в одну тему можно понять насколько человек может разобраться и в других темах.",
    "124": "⚠ Внимание ⚠Вся информация, представленная ниже, предназначена только для ознакомительных целей. Автор не несет ответственности за вред, который может быть причинен с помощью предоставленной им информации. В этой подборке представлены самые интересные уязвимости за март 2024 года.Подведем вместе итоги первого месяца весны, поехали! Об уязвимости:В набор утилит для сжатия данных XZ Utils, который уже успел попасть во многие популярные мартовские сборки Linux, неизвестными злоумышленниками был внедрен вредоносный код. Затронутые версии продукта —  5.6.0 и 5.6.1. Бэкдор состоит из перехвата функции RSA_public_decrypt с последующей проверкой подписи хоста с фиксированным ключом Ed448. При успешной проверке —  выполнение вредоносного кода, переданного хостом, через функцию system() , при этом без оставленных следов в логах SSHD. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, при определенных условиях выполнять произвольный вредоносный код. Исправление:Всем пользователям рекомендуется немедленно прекратить использование продуктов уязвимых версий и сделать откат к безопасной версии xz-5.4.x (подробнее). Оценка уязвимости по шкале CVSS 3.1 — 10.0 баллов. Об уязвимости:В продуктах Atlassian Bamboo Data Center и Server обнаружена уязвимость внедрения SQL-кода, возникающая в результате ошибки в pgjdbc, драйвере JDBC PostgreSQL. При создании соответствующей полезной нагрузки имеется возможность внедрения SQL-кода для изменения запроса, минуя защитные механизмы, включающие параметризованные запросы от атак с внедрением SQL-кода. Затронуты следующие версии продукта —  8.2.1, 9.0.0, 9.1.0, 9.2.1, 9.3.0, 9.4.0 и 9.5.0. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности,  внедрять SQL-код при условии использования PreferQueryMode=SIMPLE, что приводит к несанкционированному доступу к базам данных, их краже, а также манипулированию и другим вредоносным действиям. Исправление:Всем пользователям рекомендуется как можно скорее обновить продукты до версий 9.6.0 (LTS), 9.5.2, 9.4.4 и 9.2.12 (LTS) (подробнее). Оценка уязвимости по шкале CVSS 3.1 — 10.0 баллов. Об уязвимости:В компоненте ftpservlet веб-портала FileCatalyst Workflow обнаружена уязвимость обхода каталога, позволяющая загружать файлы за пределы предполагаемого каталога uploadtemp с помощью специально созданных POST-запросов. Опубликована проверка концепции, которая доступна по ссылке. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, выполнять произвольный код. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление продукта до версии 5.1.6. Оценка уязвимости по шкале CVSS 3.1 — 9.8 баллов. Об уязвимости:В Ivanti Neurons for ITSM обнаружена уязвимость небезопасной загрузки файлов, используемая для записи файлов в конфиденциальные каталоги. Затронуты следующие версии продукта — 2023.3, 2023.2 и 2023.1. Эксплуатация:Уязвимость позволяет злоумышленнику, прошедшему проверку подлинности, совершать удаленное выполнение произвольных команд в контексте пользователя веб-приложения. Исправление:Всем пользователям рекомендуется как можно скорее применить исправление (подробнее). Оценка уязвимости по шкале CVSS 3.1 — 9.9 баллов. Об уязвимости:В Ivanti Standalone Sentry обнаружена уязвимость выполнения команд операционной системы, возникающая из-за непринятия мер по нейтрализации специальных элементов. Затронуты все версии продукта до 9.19.0 включительно. Эксплуатация:Уязвимость позволяет злоумышленнику, не прошедшему проверку подлинности, совершать удаленное выполнение кода в системе. Исправление:Всем пользователям рекомендуется как можно скорее применить исправление (подробнее). Оценка уязвимости по шкале CVSS 3.1 — 9.6 баллов. Об уязвимости:В драйвере appid.sys утилиты Windows AppLocker обнаружена уязвимость, связанная с повышением привилегий в системе. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, прошедшему проверку подлинности, повышать свои привилегии в системе до уровня SYSTEM. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление (подробнее). Оценка уязвимости по шкале CVSS 3.1 — 7.8 баллов. Об уязвимости:В приложении NextChat (ChatGPT-Next-Web) обнаружена уязвимость, которая позволяет проводить атаки SSRF (подделка запросов на стороне сервера). Затронуты все версии продукта до 2.11.2. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, получить доступ для чтения к внутренним конечным точкам HTTP, а также позволяет скрывать свой исходный IP-адрес, перенаправляя вредоносный трафик через открытые прокси. Исправление:Всем пользователям рекомендуется как можно скорее обновить продукт до версии 2.11.3 (подробнее). Оценка уязвимости по шкале CVSS 3.1 — 9.1 балл. Об уязвимости:В ПО для виртуализации Hyper-V обнаружена RCE-уязвимость, связанная с отправкой специально созданных запросов на выполнение файловых операций на виртуальной машине к аппаратным ресурсам. Эксплуатация:Уязвимость позволяет злоумышленнику, прошедшему проверку подлинности, при определенных условиях использовать гостевую учетную запись виртуальной машины для удаленного выполнения произвольного кода на базовом хост-сервере. Исправление:Всем пользователям рекомендуется как можно скорее обновить продукт до доступной версии (подробнее). Оценка уязвимости по шкале CVSS 3.1 — 8.1 балла. Об уязвимости:В ПО для виртуализации Hyper-V обнаружена уязвимость, связанная с отказом в обслуживании (DoS). Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, вызвать отказ в обслуживании (DoS). Исправление:Всем пользователям рекомендуется как можно скорее обновить продукт до доступной версии (подробнее). Оценка уязвимости по шкале CVSS 3.1 — 5.5 баллов. Об уязвимости:В процессорах Intel, AMD, ARM и IBM обнаружена уязвимость с кодовым именем GhostRace, связанная с возникновением в спекулятивном режиме состояний гонки (SRC), приводящих к обращению к уже освобождённым областям памяти при условии неверного прогнозирования процессором ветвления в код. Опубликована проверка концепции, которая доступна по ссылке. Эксплуатация:Уязвимость позволяет злоумышленнику, имеющему физический или привилегированный доступ к целевой машине, получить доступ к защищенной памяти путём создания условий неверного предсказания ветвей выполнения. Исправление:Компания AMD рекомендует использовать типовые приёмы защиты от атак класса Spectre v1 (подробнее). Об уязвимости:В функции sys_membarrier компонента membarrier ядра Linux обнаружена уязвимость, связанная с неконтролируемым расходом ресурсов. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику вызвать отказ в обслуживании (DoS). Исправление:Пользователям доступна для установки функция ограничения скорости IPI, реализуемая разработчиками Linux для защиты от применения техники IPI Storming (Inter-Process Interrupt Storming). Рекомендации доступны по ссылке. Сразу четыре уязвимости, две из которых оцениваются как критические, были обнаружены в марте в гипервизорах VMware. Компания в срочном порядке исправила раскрытые уязвимости в своих продуктах, подробности доступны по ссылке. Возможность использования освобожденной памяти (use-after-free) в USB-контроллере XHCI позволяет удаленному злоумышленнику с правами локального администратора на виртуальной машине выполнять произвольный код в качестве процесса VMX виртуальной машины, запущенной на хосте. VMware Workstation и Fusion(подробнее CVE-2024-22252, оценка по шкале CVSS 3.1 — 9.3 балла) VMware ESXi(подробнее CVE-2024-22253, оценка по шкале CVSS 3.1 — 8.4 балла) Возможность записи за пределами границ в VMware ESXi позволяет удаленному злоумышленнику с привилегиями процесса VMX осуществить выход из песочницы (подробнее CVE-2024-22254, оценка по шкале CVSS 3.1 — 7.9 баллов). Возможность раскрытия информации в USB-контроллере UHCI используется удаленным злоумышленником с административным доступом к виртуальной машине для утечки памяти из процесса vmx (подробнее CVE-2024-22255, оценка по шкале CVSS 3.1 — 7.1 балла). Об уязвимости:В Cisco IOS XR Software обнаружена уязвимость, связанная с повышением привилегий в системе, возникающая при отправке специально созданных команд SSH в CLI. Затронуты маршрутизаторы серии 8000 и система NCS серий 540 и 5700. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, прошедшему проверку подлинности, повышать свои привилегии до root-пользователя. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление продукта до версии 7.10.2 (подробнее). Оценка уязвимости по шкале CVSS 3.1 — 7.8 баллов. Об уязвимости:В Cisco IOS XR Software обнаружена уязвимость, связанная с воздействием на функцию завершения PPP через Ethernet (PPPoE). Затронуты маршрутизаторы серии ASR 9000. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, аварийно завершить ppp_ma, что приведет к отказу в обслуживании (DoS). Исправление:Всем пользователям рекомендуется как можно скорее провести обновление продукта (подробнее). Оценка уязвимости по шкале CVSS 3.1 — 7.4 балла. Об уязвимости:В Cisco IOS XR Software обнаружена уязвимость, связанная с некорректной обработкой определенных кадров Ethernet, принимаемых на линейных картах с активированной функцией служб уровня 2. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности,  вызывать отказ в обслуживании (DoS) путем сброса сетевого процессора линейной карты. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление продукта (подробнее). Оценка уязвимости по шкале CVSS 3.1 — 7.4 балла. Об уязвимости:Обнаружена уязвимость, связанная с некорректной проверкой пакетов данных в реализации UDP-протокола и используемая для подмены IP-адресов. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, вызвать отказ в обслуживании (DoS) целевой системы или даже всей сети. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление (подробнее). Об уязвимости:В системах Broadcom, Honeywell, Microsoft и MikroTik обнаружена уязвимость, связанная с некорректной проверкой пакетов, а именно с подменой IP-адреса с целью обмена сообщениями прикладного уровня между двумя серверами. Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, вызвать отказ в обслуживании (DoS) целевой системы или даже всей сети. Исправление:Всем пользователям рекомендуется как можно скорее провести обновление (подробнее). Уязвимость представляет собой использование недостатков в системе безопасности, позволяющие злоумышленнику буквально в два действия вскрыть несколько моделей RFID-замков марки Saflok производителя Dormakaba. Все начинается с получения любой карты-ключа в отеле, далее считывается определенный код с этой карты с помощью RFID-устройства с функциями считывания и записи, и следом записываются две собственные карты-ключа. При нажатии этими двумя картами на замок: первая перезаписывает определенную часть данных замка, а вторая открывает его. Компания Dormakaba начала замену и модернизацию уязвимых замков в отелях еще в ноябре 2023 года, но пока к марту 2024 года более 64% умных замков так и остаются уязвимыми по причине затяжного перевыпуска всех карт и обновления кодирующих устройств.",
    "125": "Мы в Ivinco помогаем нашим клиентам строить, развивать и поддерживать инфраструктуру. C некоторыми из них мы работаем уже более 10 лет, с другими только начинаем. Все это естественным образом предполагает, во-первых, гетерогенную среду для работы и, во-вторых, соседство легаси и современных систем и подходов. И поскольку поддержка инфраструктуры само собой подразумевает ее мониторинг, то мы обязаны следить за всем этим IT ландшафтом и оперативно реагировать на инциденты. Долгое время основным инструментом мониторинга у нас был Nagios. Те, кто имеет опыт работы с ним, знают, что это хороший инструмент, но его GUI абсолютно не функционален. Поэтому мы использовали nagios API от проекта Zorkian и самописный GUI. У нас были вопросы по производительности и к API, и к нашему собственному GUI, однако в целом нам этого хватало. Но по мере роста количества проектов добавлялись новые системы мониторинга: Zabbix, Prometheus. А поскольку мы предоставляем услугу по поддержке 24/7, то нам крайне важно, чтобы дежурный инженер получал актуальную информацию о событиях с разных систем из разных проектов на одном экране. Так мы пришли к пониманию, что нам нужен алерт менеджер, который способен агрегировать  алерты из разных инструментов мониторинга. Мы довольно четко понимали требования, которые выдвигали к алерт менеджеру. Во-первых, самое главное: он должен поддерживать одновременную работу с несколькими системами мониторинга, как минимум с Nagios, Prometheus, Zabbix. Во-вторых, он должен быть достаточно простым в развертывании и обслуживании. У нас нет ресурсов, чтобы выделить целую команду на систему мониторинга и управления алертами. В-третьих, графический интерфейс должен быть минималистичным и в то же время максимально информативным. На экране должно помещаться достаточное количество алертов, а их статус, уровень severity и основная информация должны восприниматься с первого взгляда. Должен обеспечиваться быстрый доступ к более подробной информации, а интерфейс не должен быть перегружен малозначимыми данными. Кроме того обязателен механизм группировки алертов по разным признакам и возможность  действий над группой алертов, таких как назначение ответственного инженера, постановка в список игнорирования, удаление и др. Должна присутствовать интеграция с разными системами оповещения (Slack, Telegram, звонки на мобильные номера). Большим бонусом будет наличие механизма хранения исторических данных и возможность получения статистики по алертам. Ну и при всем при этом желательно, чтобы это было open source решение, ну или хотя бы продукт с небольшой ценой. В первую очередь мы, естественно, начали исследовать существующие системы, которые могли бы удовлетворить всем нашим требованиям.  Мы рассмотрели следующие продукты: AlertOps, Opsgenie, BigPanda, Freshworks Freshastatus, Grafana, Liongard, OnPage, PagerDuty, Splunk-On-Call, xMatters. Я не буду здесь подробно разбирать по критериям каждый из этих продуктов. Но по ряду причин нам не подошел ни один из них. И вовсе не потому, что они недостаточно хороши и мы решили, что способны сделать что-то лучше. Это все зрелые продукты, над которыми работают большие команды. Но по тем или иным причинам нам они не подошли. Где-то хромала интеграция с разными системами, какие-то инструменты в целом решают другие проблемы. Интерфейс некоторых из них не удовлетворял нашим требованиям в части минималистичности и информативности. Большую часть из этих инструментов довольно тяжело интегрировать в уже действующую инфраструктуру без внесения правок в системы мониторинга и существующие алерты. Можно еще придумать какие-то причины, но в целом это все сравнение выглядело так, что все большие продукты - это контейнеровозы, которые бороздят просторы мирового океана и обслуживаются сотней членов экипажа. Нам же нужна лодка. Просто лодка. Не найдя подходящей лодки, мы решили собрать свою собственную. Так родился проект Tenis — Team Event Notification and Interoperability System. Вообще-то, поскольку мы планировали создавать его в виде подключаемых модулей, название было Pluggable Event Notification and Interoperability System, но аббревиатура получалась совсем неприличной. Tenis представляет из себя несколько элементов. Ядро системы — бэкенд модуль. Это Python приложение, которое в качестве хранилища использует MongoDB. Он управляет алертами, принимая их от источников, сохраняя в базе и передавая в веб-интерфейс пользователей. Также здесь сосредоточена логика по эскалации событий до непосредственного оповещения инженеров через мессенджеры или голосовой звонок. Алерты собираются плагинами. На сегодняшний момент в разработке находятся плагины под Nagios, Prometheus и Zabbix. Это небольшие самостоятельные приложения, которые получают алерты от систем мониторинга, преобразовывают их под единый формат и отправляют на бэкенд. Они могут располагаться отдельно от бэкенда, а их количество не ограничено. Таким образом мы решили вопрос с тем, чтобы собирать алерты из разных проектов, разных компаний в одну панель мониторинга. Также с помощью плагинов осуществляется обратное взаимодействие с системами мониторинга, если нужно выполнить, например, повторный запуск чека или получить новое значение метрики. Согласно идеологии приложения, которую мы в него вкладываем, мы стремимся к тому, чтобы модули не зависели от нашего бэкенда в технологическом плане. Коммуникация между ними осуществляется посредством API. И, потенциально, пользователи должны иметь возможность писать свои собственные плагины и свободно подключать их к системе.  Так, например, наш плагин для связи с Nagios написан на Python, а для работы с Prometheus – на Go. Веб-интерфейс для пользователей предоставляет react приложение, которое получает информацию об алертах от бэкенда по вэб-сокету, обеспечивая таким образом отображение информации в режиме реального времени для всех пользователей. Технологический стек мы выбирали по двум критериям – минимальная достаточность и уровень компетенций. Поскольку в своей ежедневной практике мы чаще всего работает с Python и Go, то именно эти языки и были выбраны для реализации проекта. Был еще вариант написать все на Bash, но вроде бы как принцип гуманизма уже окончательно утвердился во всех  самых распространенных идеологиях, поэтому от такого варианта мы отказались сразу. Объекты алертов должны быть довольно гибкими. Мы должны иметь возможность модифицировать их и подстраивать под свою среду. Все таки мы разрабатываем максимально гибкую и кастомизируемую систему. Кроме того, пока что не предполагается большой связности их с другими сущностями. Именно поэтому выбор пал на MongoDB в качестве хранилища данных. Проект пока что находится на стадии активной разработки, и возможно в будущем будут внесены какие-то изменения в стек. Но на сегодняшний день он нас полностью устраивает. Во время тестирования мы давали нагрузку на систему в 30 тыс. алертов одномоментно, которую она прекрасно переварила как на стороне бэкенда, так и на стороне пользовательского интерфейса. Руководствуясь принципом минималистичности интерфейса, практически весь главный экран занимают алерты. Дополнительно представлена информация об общем количестве горящих алертов и дежурном инженере, а также набор кнопок, скрывающих дополнительный функционал. Алерты визуально с помощью цвета отображают уровень severity, что помогает моментально оценивать обстановку, и содержат только необходимый минимум информации: проект, к которому относится конкретный алерт, имя хоста или instance, на котором он сработал, ответственного инженера, если он назначен, имя алерта, время срабатывания и краткое описание сути проблемы. Нам было важно, чтобы алерты могли группироваться. Первоначально они объединяются в группы по имени хоста или по имени алерта. Но также существует возможность самостоятельного объединения в группы по любому признаку, например, по имени задачи. Для компактности отображения группы могут схлопываться, предоставляя информацию только о количестве алертов в группе, признаку группировки ( в данном случае  - по имени хоста) и сообщению. При необходимости можно развернуть каждый конкретный алерт и посмотреть его подробности. В них, помимо информации, указанной в основном теле, существует поле комментария, в который можно внести какую-то дополнительную информацию, и кастомные поля, необходимые для решения этого алерта. В данном примере указаны ссылки на документ в wiki, который содержит инструкции по устранению проблемы, и ссылка на графики, которые отображают затронутую метрику. Помимо основного экрана также реализованы вкладки с историей, на которой можно увидеть состояние панели мониторинга в заданное время, и вкладка со статистикой, где указаны общее время активных алертов, сроки реагирования и решения и др. Таким образом, с помощью проекта Tenis мы надеемся сделать работу по мониторингу и реагированию на инциденты максимально простой и удобной. Это очень простой, минималистичный инструмент, который полностью покрывает наши потребности. На данный момент проект находится на ранней стадии разработки и готовится к первому этапу альфа-тестирования. После того, как Tenis будет испытан в реальных рабочих процессах, будет собрана и обработана обратная связь от наших инженеров, мы планируем выставить его в Open Source. Кому, кроме нас, он может пригодиться? Нам кажется, что это будет прекрасный инструмент для небольших команд поддержки, для команд, которые обслуживают несколько независимых друг от друга инфраструктур. Для тех, кому не нужно 90% функционала, предлагаемого большими продуктами. Наверное, кто-то скажет, что мы изобретаем собственный велосипед. И, возможно, будет прав. Но в конце концов мы можем себе позволить его сделать, и нам это нравится. К тому же, велосипед - не такое уж и плохое средство для передвижения в случае, если вам нужен именно он. Ну или лодка.",
    "126": "В процессе расследования инцидентов в сетевой области традиционно применяют такие инструменты как Wireshark, Zeek, Suricata. Каждый из указанных инструментов обладает своими достоинствами и недостатками, соответственно было бы целесообразно использовать их в связке из единого интерфейса. Такую возможность предоставляет анализатор трафика ZUI (Brim), о котором пойдет речь в данной статье. Wireshark предоставляет мощнейший язык фильтров, поддержку огромного количества протоколов и возможность проанализировать каждый заголовок сетевого пакета. Из недостатков стоит отметить низкую производительность и отсутствие возможности отображения метаданных сетевых сессий. Zeek обладает мощным DPI, возможностью анализировать протоколы Windows (SMB, DCE RPC, NTLM, Kerberos), предоставляет обширные метаданные сессий, обеспечивает поддержку обнаружения сложных сетевых атак за счет собственного скриптового языка. Из недостатков — отсутствие удобного графического интерфейса. Suricata прекрасно справляется с задачей обнаружения атак сигнатурным способом, обладает высокой производительностью, но по части DPI, возможностям разбора сетевых протоколов и обнаружения сложных атак ощутимо уступает Zeek. ZUI несет под капотом движки Zeek и Suricata и в отличие от Wireshark обладает высокой производительностью, т.к. работает на уровне сетевых сессий, а не отдельных пакетов данных. Производительность обеспечивается также собственной дата моделью, заточенной под высокую скорость обработки данных. Рассмотрим возможности ZUI на примере трафика вредоносного семпла Formbook. Первое, что стоит отметить, — это удобство и быстрота выполнения агрегированных запросов, сбора статистической информации, которые важны для первичного анализа трафика. Разработчик подготовил TOP наиболее частых и полезных запросов в виде json файла, который легко импортируется в интерфейсе ZUI методом drug and drop. Выделим некоторые из них и рассмотрим в дальнейшем на примере трафика с вредоносным семплом. Сформируем перечень уникальных DNS запросов в порядке убывания количества. Выдает список уникальных DNS запросов в порядке убывания количества. На скриншоте выше видим перечень доменов, которые были зафиксированы в трафике — как увидим в дальнейшем, большая часть из них вредоносные и связаны с C&C активностью. Далее выведем статистику переданных и полученных байт по IP адресам. Из данной статистики видно, что из внутренних адресов был задействован единственный IP адрес 10.7.11.101. Больше всего данных было получено с внешнего IP 104.21.18.206. Отфильтровав вывод по данному IP адресу, видим, что на нем припаркован домен kyliansuperm92139124.shop. Обратившись к Virustotal из контекстного меню, заключаем, что данный домен вредоносный и вероятно принадлежит вирусному семейству Formbook. Сразу бросается в глаза передача exe файла (md5 хэш c0e88be9b83acf26d8269f07fe927f5c) на фоне многочисленных plain-text, html файлов и x509 сертификатов. Из контекстного меню вызываем анализ данного exe посредством virustotal и убеждаемся, что это троян formbook. Таким, образом мы зафиксировали факт загрузки трояна. Теперь необходимо понять, в результате чего это произошло. Проанализируем два файла, которые по времени предшествовали загрузке трояна. Снова призываем на помощь virustotal и видим, что первый из них (md5 хэш de36330f5ecc55cc90226af3db4a8992) — эксплойт CVE-2017-11882 в формате RTF. Это критическая RCE уязвимость в Microsoft Office, которая по сей день активно эксплуатируется «в дикой природе» в фишинговых атаках. Эксплуатация данной уязвимости приводит к загрузке OLE объекта в виде вредоносного hta файла с powershell содержимым, в чем мы можем убедиться, проанализировав следующий файл (md5 хэш d78521f4011c505c5d57d1ef39221969). Выполнив запрос md5==\"d78521f4011c505c5d57d1ef39221969\" и нажав на иконку загрузки пакетов, мы можем провалиться в Wireshark, открыв соответствующий pcap c содержимым hta файла, что является еще одной удобной особенностью ZUI. Видно, что внутри содержимого hta файла находится закодированный powershell код, который, судя по отчету virustotal и алертам Suricata, является загрузчиком первой ступени трояна Formbook (файл win.exe). Для пущей наглядности мы можем отфильтровать вывод по HTTP трафику запросом http | method!=null | fuse, из которого подтверждаются сделанные нами выводы относительно цепочки компрометации жертвы: запуск эксплойта  CVE-2017-11882 –> загрузка вредоносного hta пейлода с powershell кодом —> загрузка первой ступени Formbook трояна в виде exe файла. Далее мы наблюдаем многочисленные запросы к C&C доменам с отличительным паттерном /qm18 в URL запроса. 1. 2023-07-11T16:24:54 Запуск rtf файла с эксплойтом CVE-2017-118822. 2023-07-11T16:24:56 Загрузка вредоносного hta пейлода IE_NET.hta с powershell кодом3. 2023-07-11T16:31:53  Загрузка и запуск первой ступени трояна Formbook в виде exe файла win.exe4. 2023-07-11T16:32:09  Загрузка и запуск тела трояна Formbook с вредоносного домена kyliansuperm92139124.shop5. 2023-07-11T16:35:19  - 2023-07-11T16:58:58 Многочисленные C&C отстуки к вредоносным доменам Formbook Продемонстрируем также, какую огромную пользу приносит интеграция данного инструмента с IDS Suricata. Выполнив запрос с отображением списка алертов Suricata с группировкой по количеству, мы снова находим подтверждение ранее сделанных нами выводов относительно картины заражения трояном Formbook, что наглядно видно из перечня алертов IDS. За счет высокой производительности, достигаемой вследствие работы с метаданными сетевых сессий вместо отдельно взятых пакетов, ZUI продемонстрировал себя как эффективный инструмент сетевой форензики, который способен покрыть многие слепые зоны Wireshark. В первую очередь это достигается интеграцией с Zeek, обладающим мощнейшим DPI, а также связкой с аналитическими сервисами (Virustotal, whois). Приятным бонусом идет интеграция с Suricata, что позволяет обнаруживать атаки и аномалии из единого интерфейса.",
    "127": "Сокеты, как и процессоры, история совсем не вечная. Поэтому с течением времени и те, и другие подвергаются определенным изменениям. Одним из таковых преобразований должен стать запуск нового сокета Intel под кодовым названием LGA 1851. Он обещает стать основой для следующего поколения высокопроизводительных процессоров Intel Arrow Lake, преемников нынешних Raptor Lake, и, вероятно, даже Meteor Lake. По слухам, Intel не планировала выпускать процессоры на базе архитектуры Meteor Lake для настольных компьютеров, ориентируясь при их создании в первую очередь на потребности ноутбуков. Поэтому сокет LGA 1851 должен был стать вотчиной процессоров Arrow Lake, чей релиз запланирован на 2024 год. Но, судя по всему, компания изменила свое решение, о чем косвенно свидетельствуют материалы IBASE, производителя материнских плат. Он уже настроил производственные линии для выпуска МП с сокетом LGA 1851 и даже опубликовал кое-какие спецификации, уточнив, в частности, что будущая плата будет совместима с процессорами Meteor Lake-PS серии Core Ultra. Безусловно, это может быть ошибкой, учитывая, что Meteor Lake должны поставляться распаянными на материнской плате со встроенной оперативкой, но чем черт не шутит. В конце концов, Intel может просто сделать отдельную версию для десктопных платформ, и ничего ей за это не будет. Платформа LGA 1851 разрабатывалась, чтобы удовлетворить растущие потребности современных процессоров, которые становятся все более и более производительными. Это необходимо для запуска новых игр, обучения искусственного интеллекта, обеспечения работы тяжелых приложений и выполнения других ресурсоемких задач. Усовершенствованный графический процессор Intel Xe 2, который станет основой как для интегрированного, так и для выделенного GPU Intel. Новые графические ядра обещают прирост быстродействия в играх и приложениях с аппаратным ускорением. Встроенный аппаратный акселератор AI Engine для ускорения задач машинного обучения и работы с искусственным интеллектом. Чип Intel AI Engine призван обеспечить высокую скорость обработки нейросетевых вычислений. Интеграция интерфейса PCIe 5.0 с поддержкой 20 линий, который обеспечит увеличенную пропускную способность по сравнению с PCIe 4.0 и PCIe 5.0x16.. Одним из ключевых отличий LGA 1851 от предшественника в лице LGA 1700 является увеличенное число контактов, которых, как следует из кодового имени, теперь будет 1851 вместо 1700. Дополнительные полтораста пинов позволят нарастить пропускную способность при сохранении высокой энергоэффективности. Несмотря на запланированное увеличение контактов, само посадочное гнездо LGA 1851 больше, по сравнению с LGA 1700, практически не станет. Сохранение прежних размеров позволит использовать в паре с процессорами нового поколения кулеры, ориентированные на LGA 1700. Таким образом пользователям будет доступна возможность менее затратного апгрейда своей компьютерной сборки, пусть и не без оговорок. В частности, гнездо LGA 1851 будет лишено поддержки оперативной памяти DDR4, как LGA 1700, и сможет принять в сборку только более новый стандарт DDR5. К счастью, в последнее время стоимость более современной оперативки практически сравнялась с предыдущим поколением. Таким образом вы можете купить более свежее решение с большим потенциалом для разгона по той же, а иногда и более низкой цене. Несмотря на ряд новшеств LGA 1851, Intel прислушалась к пожеланиям пользователей о сохранении совместимости. Ожидается, что первые системные платы на базе LGA 1851 будут использовать знакомые чипсеты Intel 700-й серии, такие как Z790, но к их числу добавятся решения на базе 800-й серии, которая предложит новые технические возможности. Переход на новый сокет LGA 1851 открывает путь к повышенной производительности и новым технологиям. В сочетании с сохранением обратной совместимости накопителей и оперативной памяти, это должно сделать апгрейд системы более плавным и менее затратным для многих пользователей. В то же время увеличенное количество контактов и площадь LGA 1851 обеспечат дополнительный потенциал для дальнейших улучшений в будущем. Предполагается, что сокет LGA 1851 будет сохранять свою актуальность на протяжении следующих 3 лет, как минимум до 2026 года включительно. Кому-то, вероятно, хотелось бы, чтобы этот срок был еще дольше. Но практика показывает, что улучшение процессоров происходит слишком стремительно, и зацикливаться на одной единственной платформе для производителей может быть чревато негативными последствиями.",
    "128": "Привет! Меня зовут Олег Рябов, я главный эксперт Управления исследований и разработок новых решений компании «Ростелеком-ЦОД» и автор программы и методики испытаний (ПМИ) серверов. В этой статье расскажу, как мы проводим тестирование серверов и какие утилиты и методы используем. В 2020 году перед нами поставили задачу выбрать процессор для типового вычислительного узла, который будет использоваться в 95% наших проектов. На тот момент еще никто в мире не смог разработать единую метрику производительности процессоров. Метрика производительности сетевого оборудования — это PPS (Packet Per Second), например, а производительность СХД измеряется в IOPS (Input/Output Operations Per Second). А у процессора нет единицы измерения, с помощью которой можно измерить производительность и сопоставить результаты тестов. Мы начали с того, что выбрали утилиты VMmark и Geekbench. Естественно, они выдают результаты в своих показателях, и эти показатели никак не соотнести друг с другом. Поэтому трудно разработать единую метрику производительности на основе получаемых данных, в этом вся загвоздка. Тем не менее мы продолжали набирать базу результатов тестирования процессоров и серверов и тогда же написали первую версию ПМИ серверов. В конце 2021 года мы планировали провести тестирование процессоров Intel Xeon Scalable 3rd Gen и для этого обратились в Intel. Рассказали им, как проводим испытания, показали свою ПМИ. Чуть позже представители Intel пришли к нам с предложением вместе разработать единую метрику производительности процессоров, так как им самим этого сделать не удалось. Мы успели протестировать две модели процессоров на их серверах, но не смогли завершить проект, потому что случился 2022 год и вскоре Intel ушел из России. В рамках импортозамещения приоритет получила задача испытания отечественных серверов. Сейчас перед нами снова поставлена задача выбрать процессор для типовых вычислительных узлов и разработать единую метрику производительности процессоров. Но теперь мы будем тестировать их уже на отечественных серверах. Мы берем на испытание не менее двух серверов идентичной конфигурации, чтобы перепроверить результаты, если, например, при нескольких запусках одного и того же теста будет большая статистическая погрешность. применимость в существующих и перспективных продуктивных инсталляциях в инфраструктуре нашей компании. Обязательно оцениваем взаимодействие с поставщиком или производителем серверов в ходе испытаний и его реакции на выявленные ошибки и недоработки. Собираем и подключаем стенд: устанавливаем серверы в стойки и подключаем их в сеть передачи данных (LAN), сеть хранения данных (SAN) и сеть управления (management). На рисунке представлены подключения в рамках тестового стенда и дополнительное оборудование для проведения тестирования. включаем Hyperthreading (SMT), если в проекте нет требования отключить его. Важно: обновленные версии микрокода BMC, BIOS и других компонентов сервера не меняем до завершения испытаний. Систему запускаем в режиме UEFI, но в виде исключений, например, когда не поддерживается PCIe-карта или какая-либо функциональность, используем режимы Legacy или DUAL (смешанный режим). Мы можем проводить испытания как на Bare Metal, так и в ВМ для задач виртуализации. Используем ОС семейства Linux или Windows, гипервизоры VMware ESXi или другие, согласованные с заказчиком испытаний. Версия ОС, на которой выполняются тесты, должна быть сертифицирована производителем серверов или указана заказчиком. Использование как статического, так и динамического (DHCP) режима конфигурации сетевого подключения интерфейса управления (BMC). Взаимодействие с интерфейсом управления по протоколам IPMI и SNMP v3. Взаимодействие c интерфейсом управления посредством Redfish API. о состоянии аппаратных компонентов и журналов событий, аудита и обслуживания. о состоянии аппаратных компонентов и журналов событий, аудита и обслуживания. информации о текущих показаниях датчиков температуры, напряжения, скорости вращения вентиляторов, энергопотребления и пр.; информации и возможности создания дисковых томов с использованием аппаратного и встроенного программного RAID-контроллера. Отправка сообщений о событиях с использованием SMTP, syslog и SNMP Trap. Доменная аутентификация и авторизация посредством LDAP, Active Directory, RADIUS, ролевой доступ. Генерация нового или добавление существующего SSL-сертификата. Создание новых пользователей с определенными правами доступа (администратор, оператор, только чтение). Создание новых прав доступа и назначение их пользователям. Использование HTML KVM-консоли или Java-консоли для установки и управления ОС. Подключение через консоль образов оптических дисков. Подключение образов жестких дисков (img) и проброс USB-накопителей в интерфейсе управления. Обновление версии микрокода BMC без влияния на работу ОС. Разделение предопределенных сетевых портов для доступа в интерфейс управления и в ОС (NCSI). Полноценный Command Line Interface, по возможностям настройки и управления не уступающий интерфейсу управления. Автоматическое открытие сервисных заявок (call home) в технической поддержке производителя. Экспорт и импорт конфигурации интерфейса управления. Возможность настройки сетевых параметров интерфейса управления в BIOS Legacy или UEFI и создания пользователей для доступа в интерфейс управления. Наличие преднастроенных профилей производительности в BIOS Legacy или UEFI. Создание дисковых томов с помощью утилиты RAID-контроллера в BIOS Legacy или в режиме UEFI. Наличие у производителя серверов MIB-файлов. Функциональность серверов мы можем протестировать самостоятельно, но проверять каждый параметр слишком долго, поэтому часть характеристик мы уточняем у техподдержки производителя или сверяемся с технической документацией. В зависимости от того, выполняем ли мы тестирование отдельного компонента сервера (CPU, RAM, сеть и др.) или сервера в целом, есть свой набор тестов и утилит. Для сравнения полученных результатов тестирования мы запускаем утилиты с одинаковым набором параметров. Полученные результаты производительности серверов — это база для дальнейших исследований по допустимой нагрузке на инфраструктуру и возможной переподписке ресурсов, а также для выбора предпочтительных конфигураций серверов. У нас есть эталон — определенная модель сервера с процессорами Intel Xeon Gold 6254, которая подходит для использования в нашей инфраструктуре. Все результаты нагрузочного тестирования серверов должны быть не хуже результатов эталона. Дополнительно проводим сравнение результатов тестируемых серверов с результатами, которые опубликованы на официальных сайтах используемых утилит тестирования. PassMark CPU. производительность платформы «1С:Предприятие». Утилита предназначена для тестирования производительности серверов в качестве серверов виртуализации под управлением VMware vSphere. Показатели производительности выражаются в баллах и в количестве тайлов. Инфраструктуру для проведения тестирования подготавливаем по документу VMmark Users Guide. Один тайл (tile) включает 20 виртуальных машин различного объема в части вычислительных ресурсов и назначения, чтобы имитировать рабочие нагрузки на платформу виртуализации. Нагрузку масштабируем путем инициализации необходимого количества тайлов. около 180 ГБ RAM. полоса пропускания — 600–650 Мбит/с, подключение — от 1 Гбит/с. Мы оцениваем производительность по каждому тайлу и общую производительность, а также производительность инфраструктуры по таким операциям, как развертывание виртуальных машин, миграция виртуальных машин между серверами и по хранилищу и автоматическая балансировка нагрузки. не менее двух идентичных по конфигурации серверов, на которые устанавливаем гипервизор VMware ESXi версии 7.0 или новее; систему хранения данных или сервер в виде СХД. Во втором случае презентуются блочные диски с использованием iSCSI TGT. Через NFS тест работает некорректно. При тестировании рекомендуем не изменять конфигурацию СХД. Объем свободного дискового пространства на СХД — не менее 900 ГБ на один тайл + необходимый запас 20%. — При использовании СХД дисковое пространство предоставляем одним LUN (дисковое пространство) всем серверам. образ теста VMmark, из которого мы клонируем prime client и настраиваем его дальше по VMmark Users Guide. LUN с СХД презентуется серверам платформы виртуализации по протоколу iSCSI или Fibre Channel так, чтобы LUN был доступен по двум путям или более, при этом используется политика round-robin. При использовании сервера в виде СХД и протокола iSCSI, если невозможно обеспечить два независимых пути или более, устанавливаем два IP-адреса в одной подсети на сервере iSCSI. Установка первого тайла занимает около 12 часов, так как на одной из ВМ создается база данных. Последующие тайлы копируют образ первого, поэтому их установка занимает не более 1 часа. Тестирование проводится с постепенным увеличением количества тайлов до тех пор, пока показатели растут, а вычислительные ресурсы в системе еще доступны для использования. Важно: при тестировании в VMware vCenter необходимо обращать внимание на показатели таких параметров, как CPU Ready, CPU Usage, CPU Utilization, Memory Active, Memory Consumed. VMmark3_Score — общая оценка производительности системы, которая рассчитывается как сумма VMmark3_Applications_Score и VMmark3_Infrastructure_Score, причем 80% рабочей нагрузки приходится на работу приложений, а остальные 20% — на инфраструктурные операции. После тестирования формируется каталог конфигурационных файлов с информацией о типах нагрузок и работе утилиты с общими результатами и графиками. Если есть хотя бы один красный график, как показано на рисунке, значит серверы не справляются с рабочей нагрузкой, что говорит о нехватке свободных вычислительных ресурсов. Следовательно, нет необходимости в продолжении дальнейшего тестирования данной утилитой. Затем мы анализируем производительность платформы виртуализации при увеличении рабочей нагрузки на нее, выраженной в количестве тайлов, и сравниваем результат с эталонными результатами сервера с процессорами Intel Xeon Gold 6254 (на один сервер 2 тайла, AppScore — 2.18) и с верифицированными результатами на сайте VMware. Эту утилиту используем для тестирования производительности процессоров и оперативной памяти серверов. Она позволяет найти узкие места подсистемы памяти, NUMA нод, Hyperthreading/SMT и пр. на простых операциях, которые легко параллелятся. На тестируемых серверах устанавливаем совместимую ОС, например Ubuntu Server 20.04, Ubuntu Server 22.04 или новее. Утилиту Sysbench загружаем и устанавливаем из репозитория ОС или из другого доверенного источника. При тестировании производительности процессоров в серверах вариативность теста обеспечивается за счет использования параметра cpu-max-prime с различными значениями (10 000, 20 000, 40 000). Этот параметр определяет, до какого значения будет идти вычисление простых чисел. Вычисления происходят с использованием 64-битных целых чисел. Производительность процессоров выражается в количестве событий (event) в секунду. Для параметра cpu-max-prime подставляем одно из трех значений (10 000, 20 000 или 40 000) а для параметра num-threads подставляем любое количество процессорных потоков, которое не должно превышать общего количества потоков в тестируемом сервере. При тестировании производительности (скорости доступа) оперативной памяти в серверах как при операциях чтения, так и при операциях записи вариативность теста обеспечивается за счет использования параметра memory-block-size с различными значениями (4, 8, 16, 32, 64, 128 ГБ). Этот параметр означает размер блока оперативной памяти. Производительность оперативной памяти выражается в пропускной способности (МиБ/с) при операциях чтения и записи. Здесь для параметра memory-block-size подставляем одно из шести значений (4G, 8G, 16G, 32G, 64G или 128G), для параметра memory-total-size подставляем значение, равное объему оперативной памяти в сервере, а для параметра threads — любое значение количества потоков, которое при умножении на значение параметра memory-block-size не превышает объема оперативной памяти сервера. Тестирование производительности оперативной памяти проводим на локальных блоках памяти, когда каждый поток взаимодействует со своим выделенным блоком памяти, поэтому в команде запуска Sysbench используем параметр memory-scope=local. По результатам тестирования производительности процессоров и оперативной памяти анализируем показатели производительности и описываем зависимость производительности от используемого количества потоков и размера блока памяти. Затем сравниваем результаты с эталонными. Используем для нагрузочного и стресс-тестирования компонентов серверов: процессоров, кэш-процессоров, оперативной памяти и дисковой подсистемы. На тестируемых серверах устанавливаем совместимую ОС. Утилиту Stress-ng загружаем из репозитория ОС или из другого доверенного источника. Запускаем нагрузочное и стресс-тестирование на 48 часов, на протяжении которых посредством контроллера BMC контролируем доступность серверов и показатели датчиков температуры на предмет превышения пороговых значений. Если температура превысит критическое значение, появится уведомление и отметка в системном журнале. Для параметра vm-bytes подставляем значение в ГБ, равное объему оперативной памяти сервера. Основным показателем производительности является параметр bogo ops/s (real time) — количество фиктивных операций в секунду. Значение данного параметра рассчитывается для каждого компонента сервера, включенного в тестирование. Также утилита Stress-ng в ходе тестирования фиксирует показатели температуры, которые включаем в результат тестирования в виде минимального и максимального значений. По результатам тестирования мы анализируем работу серверов под 100%-й нагрузкой или близкой к таковой. Сравниваем полученные в утилите Stress-ng и зафиксированные в BMC серверов показатели температуры с критическими значениями температуры датчиков в BMC. Если во время тестирования серверы работали безошибочно, а максимальные температуры, согласно датчикам, были на 10 ℃ ниже критического уровня температур, то тест считается успешно завершенным. Включает в себя несколько тестов, характеризующих работу приложений с различной нагрузкой. Эту утилиту используем для тестирования производительности серверов как в одноядерном (Single-Core Performance), так и в многоядерном режиме (Multi-Core Performance). На тестируемых серверах устанавливаем совместимую ОС. Загружаем с официального сайта или из другого доверенного источника утилиту Geekbench. В тестировании используем версии 5.х и 6.х. Важно: в утилите Geekbench 6.х тесты модернизированы и их меньше, чем в Geekbench 5.х, что приводит к разным результатам тестирования, которые невозможно сравнить между собой. Для запуска Geekbench необходимо распаковать архивы tar.gz и воспользоваться файлом geekbench5 или geekbench6. Результат тестирования будет представлен виде баллов по каждому тесту, входящему в утилиту, для обоих режимов (Single-Core Performance, Multi-Core Performance), и на основе полученных баллов рассчитывается общая оценка производительности серверов. Результаты тестирования мы загружаем на официальный сайт Geekbench и сравниваем их с опубликованными результатами серверов с такими же процессорами и результатами нашего эталона. Позволяет генерировать TCP- и UDP-трафик и используется для тестирования пропускной способности сетевых интерфейсов серверов. На тестируемых серверах устанавливаем совместимую ОС и загружаем утилиту Iperf3 из репозитория ОС или из другого доверенного источника. Если мы используем на серверах более одного сетевого интерфейса, агрегируем несколько сетевых интерфейсов в один логический сетевой интерфейс с использованием протокола LACP стандарта IEEE 802.3ad. Это нужно для повышения пропускной способности и отказоустойчивости сети передачи данных. Для агрегированного сетевого интерфейса применяем режим работы mode=4 (802.3ad) с политикой хэширования на уровне layer3+4. Дополнительно на коммутаторах, куда подключены порты серверов, выполняем настройку по агрегации сетевых интерфейсов в один логический интерфейс. Утилита Iperf3 работает в один поток, что может отразиться на способности одного ядра процессора нагрузить сеть, поэтому рекомендуем разрешить Jumbo frames и увеличить MTU до максимально возможного. В случае если загрузка одного ядра процессора равна 100%, а пропускная способность сетевых интерфейсов серверов не достигла максимальных результатов, тогда надо увеличить количество потоков с трафиком, генерируемым клиентом Iperf3. Тестируем одновременно на двух серверах в течение 24 часов, с периодической проверкой доступности серверов. На одном сервере утилита Iperf3 запускается с ролью «сервер», на другом — с ролью «клиент». Для параметра -P подбираем такое значение количества потоков, при котором пропускная способность и процент утилизации сетевых интерфейсов были бы максимальными. По результатам тестирования проводим анализ полученных показателей пропускной способности сетевых интерфейсов серверов и рассчитываем процент утилизации сетевых интерфейсов. Тест пройден успешно, если процент утилизации больше 90 от максимального показателя в 25 Гбит/с. Эта утилита — реализация высокопроизводительного эталонного теста High Performance Linpack специалистами Intel для процессоров Intel, поэтому ее можно использовать только на серверах с процессорами Intel. Мы применяем ее для тестирования производительности серверов при решении случайных систем линейных уравнений. На тестируемых серверах устанавливаем совместимую ОС. Для установки и запуска утилиты с официального сайта Intel загружаем и устанавливаем пакеты OneAPI Base Toolkit 2022.1.2.146 и OneAPI HPC Toolkit 2022.1.2.117 или более новых версий. Пакеты содержат библиотеки, необходимые для запуска утилиты. ~$ . /opt/intel/oneapi/setvarsh.sh Для проведения тестирования правим файл настроек HPL.dat в директории /opt/intel/oneapi/mkl/latest/benchmarks/mp_linpack (директория по умолчанию). Изменяем значения следующих параметров: Ns — размерность матрицы. Подбираем такое число, при котором утилизация оперативной памяти при работе утилиты Intel MP Linpack составляет 75–80%. NBs — размер блока. Выставляем значение 384. Ps и Qs — распределение матрицы между процессорами. Количество процессоров должно быть не меньше произведения Ps и Qs. Например, если в сервере два процессора, тогда для параметра Ps выставляем значение 1, а для параметра Qs — значение 2. Остальные параметры в файле HPL.dat оставляем без изменения. Тестирование может продолжаться 2–3 часа в зависимости от установленных процессоров и оперативной памяти в серверах, а также от размера матрицы. Полученный результат выражается в GFLOPS, что означает количество операций с плавающей запятой в секунду, выполняемых тестируемым сервером. Результаты тестирования сравниваем с эталоном. Тест Гилева оценивает применимость серверов и производительность платформы «1С:Предприятие» на этих серверах. Тест состоит из двух частей, которые не зависят друг от друга и запускаются по отдельности. Первая часть теста (TPC) — однопоточный тест, оценивает производительность выполнения операций в один поток, что характерно для платформы «1С:Предприятие». Результатом теста является столбчатая диаграмма, на которой указан результат теста в виде количества баллов и результаты, соответствующие оценкам «плохо», «удовлетворительно», «хорошо» и «замечательно». Вторая часть теста (G1C) — многопоточный тест, позволяет оценить скорость записи на диски при одновременном обращении нескольких запросов к базе данных. Результатами второй части являются: рекомендуемое количество пользователей. Для выполнения теста Гилева используем ранее протестированную платформу виртуализации VMware, где установлена и настроена утилита VMware VMmark, необходимая для создания определенного уровня нагрузки на серверы. На платформе виртуализации VMware создаем одну виртуальную машину с ресурсами 8 vCPU, 64 ГБ RAM и один виртуальный диск объемом 300 ГБ. В эту ВМ устанавливаем ОС Windows Server 2016, СУБД MS SQL 2019 и платформу «1С:Предприятие» версии 8.3. В «1С:Предприятие» создаем новую тестовую базу, куда загружаем скачанную базу Гилева. для сервера виртуализации VMware режим работы выставить в значение High Performance. Для запуска теста Гилева необходимо открыть ранее загруженную базу в режиме «1С:Предприятие» и нажать кнопку «Выполнить тест». Выполнение каждой из двух частей теста длится около 3–5 минут. Если на ВМ есть выход в интернет, то по завершении теста результат можно сравнить с результатами других проведенных тестов. запуск теста Гилева при CPU Usage ≈ 75% на том сервере, где расположена ВМ с «1С:Предприятие». Получив результаты тестирования, анализируем нагрузку на серверы. Сравниваем полученные результаты с эталоном и определяем применимость этих серверов в конкретной конфигурации для использования платформы «1С:Предприятие». Набор тестов PassMark PerformanceTest Linux оценивает производительность сервера под различными типами нагрузки. Состоит из следующих тестов: Extended Instructions Test (SSE) — тест встроенных инструкций процессора. Тестирование выполняется около 4–5 минут, результат представляется в виде баллов, характеризующих производительность CPU и RAM. Чем больше баллов, тем лучше результат. Полученный результат тестирования можно выгрузить на официальный сайт Passmark, чтобы сохранить и получить к нему доступ в любой момент. На этом этапе проверяем устойчивость работы серверов в случае отказа какого-либо компонента сервера и определяем поведение отработки такого отказа. Тесты выполняем под фоновой нагрузкой, которую генерируют такие утилиты, как stress-ng и fio. Если один из тестов отказоустойчивости не пройден, а производитель не предоставил нам патч, который исправил бы ошибку, то мы завершаем испытания и признаем сервер неприменимым в инфраструктуре компании. Перед проведением тестирования для получения эталонных результатов запускаем утилиту Stress-ng на 2 часа с параметрами, которые были описаны в разделе «Утилита Stress-ng». Фиксируем результаты для последующего сравнения. Для создания нагрузки на сервер при выполнении тестирования повторно запускаем утилиту Stress-ng на 2 часа с параметрами, описанными в разделе «Утилита Stress-ng». От блока питания PSU1 отключаем кабель питания и наблюдаем за BMC сервера с использованием IPMI на предмет появления уведомлений об ошибках и создания записей в системном журнале. Блок питания PSU1 извлекаем из сервера и наблюдаем за BMC сервера, как описано в пункте 3. По истечении 2 часов работы сервера под нагрузкой на одном блоке питания PSU2 фиксируем полученные результаты работы утилиты Stress-ng, чтобы сравнить с результатами, полученными при выполнении пункта 1. Устанавливаем обратно блок питания PSU1. Наблюдаем за BMC сервера, как описано в пункте 3. К блоку питания PSU1 подключаем кабель питания и наблюдаем за BMC сервера, как описано в пункте 3. Повторно выполняем пункты со 2 по 7 включительно для блока питания PSU2. Во время работы под нагрузкой сервер доступен по ssh. При отключении от блока питания в BMC появилось уведомление, в системном журнале создалась запись о событии, датчики зафиксировали отсутствие напряжения на блоке питания. При извлечении блока питания из сервера в BMC создалось уведомление, определяется только один блок питания. При возврате блока питания в сервер и подключении кабеля питания в BMC определяются все блоки питания, все датчики с нормальными значениями, в системном журнале создалась запись о наличии напряжения на блоке питания. Результаты утилиты Stress-ng при работе сервера под нагрузкой и на одном блоке питания не отличаются в меньшую сторону больше чем на 10% по сравнению с результатами, полученными при работе сервера под нагрузкой на двух блоках питания. На свободных накопителях в сервере создаем дисковое хранилище с уровнем резервирования RAID1, RAID6 или RAID10 через BMC, BIOS или с использованием, например, утилиты StorCLI. Тестирование проводим для каждого уровня резервирования дискового хранилища. При возможности один из свободных накопителей настраиваем как резервный (Global Hot Spare). Запускаем утилиту FIO с профилем нагрузки: 40% случайных операций записи и 60% случайных операций чтения блоком 8 КБ, 16 потоков. Фиксируем производительность в IOPS на чтение и запись. Извлекаем из сервера один из накопителей. До начала перестроения дискового хранилища снова фиксируем производительность в IOPS на чтение и запись. Наблюдаем за BMC сервера на предмет появления уведомлений и создания записей в системном журнале. Контролируем статус и процесс перестроения (rebuild) дискового хранилища, если есть свободный резервный накопитель в сервере (Global Hot Spare). Фиксируем производительность в IOPS на чтение и запись во время перестроения дискового хранилища. При использовании дискового хранилища с уровнем резервирования RAID6 или RAID10 извлекаем второй накопитель из сервера. Фиксируем производительность в IOPS на чтение и запись. По завершении перестроения дискового хранилища все ранее извлеченные накопители устанавливаем обратно. Отслеживаем статус всех накопителей, а также наблюдаем за BMC сервера, как описано в пункте 2. При необходимости через BMC или с помощью утилиты StorCLI в ОС удаляем стороннюю конфигурацию на накопителях и меняем их статусы. По завершении второго перестроения дискового хранилища отслеживаем статус и состав дискового хранилища, а также статус самих накопителей. На дисковом хранилище создаем один раздел максимального размера и файловую систему ext4, которую подключаем к директории test1.. Запускаем копирование большого файла (несколько десятков ГБ) в директорию test1. Извлекаем из сервера один из накопителей. Отслеживаем статус дискового хранилища и наблюдаем за BMC сервера на предмет появления уведомлений и создания записей в системном журнале. Контролируем процесс перестроения дискового хранилища, если есть свободный накопитель в сервере (Global Hot Spare), и отслеживаем скорость копирования файла. Если используем дисковое хранилище с уровнем резервирования RAID6 или RAID10, извлекаем второй накопитель из сервера. После перестроения дискового хранилища и окончания копирования файла устанавливаем накопители обратно. Отслеживаем статус всех накопителей, а также наблюдаем за BMC сервера, как описано в пункте 3. Вычисляем и сравниваем контрольные суммы файла-источника и скопированного файла. При необходимости через BMC или с помощью утилиты StorCLI в ОС удаляем стороннюю конфигурацию на установленных накопителях с последующим изменением статусов установленных накопителей. После второго перестроения дискового хранилища отслеживаем статус и состав дискового хранилища, а также статус самих накопителей. Работа утилиты FIO не прерывалась во время тестирования, и дисковое хранилище было постоянно доступно на чтение и запись. Сработала автоматическая замена извлеченного накопителя на резервный (Global Hot Spare). Дисковое хранилище успешно выполнило все перестроения. Возврат к начальной конфигурации дискового хранилища (состав и статус накопителей) происходит без перезагрузки сервера. Файловая система на дисковом хранилище была доступна на протяжении всего времени тестирования. Копирование файла завершилось без прерываний. Сработала автоматическая замена извлеченного накопителя на резервный (Global Hot Spare). Дисковое хранилище успешно выполнило все перестроения. Контрольные суммы файла-источника и скопированного файла остались одинаковыми. Конфигурация дискового хранилища (состав и статус накопителей) вернулась в начальное состояние без перезагрузки сервера. На первом сервере запускаем утилиту iperf3 с ролью «сервер» и запускаем ping до второго сервера. Запускаем копирование большого файла (несколько десятков ГБ) на второй сервер. На втором сервере запускаем утилиту iperf3 с ролью «клиент» не меньше, чем на 1 час. На коммутаторе отключаем один из портов, куда подключены порты второго сервера. Проверяем статус портов на коммутаторе. Фиксируем снижение пропускной способности, исходя из работы утилиты iperf3 на стороне «сервера» и «клиента», контролируем процесс копирования файла и работы ping. На коммутаторе включаем ранее отключенный порт. Проверяем статус портов на коммутаторе. По выводимым результатам работы утилиты iperf3 на стороне «сервера» и «клиента» фиксируем повышение пропускной способности до исходных показателей, контролируем процесс копирования файла и работы ping. По окончании копирования файла вычисляем и сравниваем контрольные суммы файла-источника и скопированного файла. Агрегированные сетевые интерфейсы на серверах и на коммутаторе настроены, сетевая связанность между серверами присутствует. При отключении порта на коммутаторе пропускная способность снижается не более чем на 55%. Прерывания при копировании файла и работе ping при отключении порта на коммутаторе отсутствовали. Пропускная способность при включении порта на коммутаторе повысилась до исходных показателей. Контрольные суммы файла-источника и скопированного файла остались одинаковыми. Проверяем, что серверу с СХД презентован LUN через два Fibre Channel коммутатора и доступен не менее чем по двум путям. Проверяем, что в конфигурационном файле /etc/multipath.conf прописаны настройки многопутевого доступа LUN (multipath) для повышения уровня отказоустойчивости в случае аварийных ситуаций в сети хранения данных. Командой multipath-II проверяем, что LUN доступен как единое дисковое хранилище не менее чем по двум путям. Проверяем статус всех путей доступа LUN (должны быть active, ready, running). Запускаем утилиту FIO с профилем нагрузки: 40% случайных операций записи и 60% случайных операций чтения блоком 8 КБ, 16 потоков. Фиксируем производительность в IOPS на чтение и запись; Последовательно отключаем все порты на одном Fibre Channel коммутаторе, куда подключены порты сервера, так, чтобы дисковое хранилище было доступно через порты другого Fibre Channel коммутатора. При отключении каждого порта контролируем производительность на чтение и запись, фиксируем статусы и количество доступных путей дискового хранилища. Последовательно включаем все ранее отключенные порты на Fibre Channel коммутаторе. Контролируем восстановление производительности на чтение и запись, фиксируем статусы и количество доступных путей дискового хранилища. На дисковом хранилище создаем один раздел максимального размера, создаем файловую систему ext4 и подключаем к директории test1. Запускаем копирование большого файла (несколько десятков ГБ) в директорию test1. На одном из Fibre Channel коммутаторов последовательно отключаем все порты, к которым подключены порты сервера, так, чтобы дисковое хранилище было доступно через порты другого Fibre Channel коммутатора. Контролируем процесс копирования файла, фиксируем статусы и количество доступных путей дискового хранилища. По окончании копирования файла вычисляем и сравниваем контрольные суммы файла-источника и скопированного файла. Последовательно включаем все ранее отключенные порты на Fibre Channel коммутаторе. Фиксируем статусы и количество доступных путей дискового хранилища. Работа утилиты FIO не прерывалась на всем протяжении тестирования, и дисковое хранилище было постоянно доступно на чтение и запись. Производительность снизилась не более чем на 55% относительно исходных показателей при доступности хранилища через порты одного Fibre Channel коммутатора (измеряем производительность в течение 15 минут после отключения портов на одном коммутаторе). Производительность восстановилась до исходных показателей по мере включения ранее отключенных портов на Fibre Channel коммутаторе. Файловая система на дисковом хранилище была доступна на всем протяжении тестирования. Копирование файла успешно завершилось без прерываний. Контрольные суммы файла источника и скопированного файла остались одинаковыми. Тестирование на совместимость серверов с модулями доверенной загрузки (МДЗ) нужно для определения возможности использования модулей для реализации мер защиты УПД.17 и УД.3. При тестировании мы должны применять МДЗ не менее трех производителей. Если один из тестов совместимости МДЗ пройден неуспешно, то переходим к тестированию совместимости сервера с МДЗ другого производителя. Однако мы можем провести повторное тестирование, если производитель выпустит исправление для своего МДЗ. Если сервер не прошел тест совместимости ни с одним МДЗ ни одного из производителей, то сервер признается неприменимым в нашей инфраструктуре. В таблице приведен список функциональных требований МДЗ, подлежащих проверке при выполнении тестирования. 1. Выполняем попытку загрузки ОС с внешнего носителя информации без авторизации администратора СЗИ. 2. Выполняем попытку входа в BIOS для смены порядка загрузки ОС без авторизации администратора СЗИ. 3. Выполняем попыткузагрузки ОС с помощью вызова меню вариантов загрузки без авторизации администратораСЗИ. 1. Неуспешная попытка загрузки с внешнего носителя. 2. Неуспешная попытка входа в BIOS без авторизации администратора СЗИ. 3. Блокируется попыткавывода меню вариантов загрузки или блокируется ПК при попытке загрузки сдругого носителя. Выполняем вход с использованием разных идентификаторов. В журнале МДЗ регистрируются события входа пользователей с идентификаторами в ИС. 1. Выполняем в МДЗ настройку контроля целостности в соответствии с инструкцией к ПАК. 2. После загрузки ОСпроизводим изменение любого файла, поставленного на контроль, и перезагружаемсервер. В результате нарушения целостности МДЗ блокирует загрузку и вход в систему до входа администратора. Выполняем попытку входав BIOS и изменения порядка загрузки без авторизации идентификатором администратора. В результате попытки изменения параметров BIOS происходит блокировка сервера до авторизации администратора или возврат настроек к предыдущим значениям. Мы описываем способы взаимодействия с производителем на начальном этапе, при согласовании конфигурации тестовых серверов и организации их доставки в ЦОД, и на этапе тестирования. Делаем отдельную пометку, если поставщик выделил нам технического специалиста, у которого можно спросить, например, о рекомендуемых настройках, выявленных ошибках или некорректном поведении серверов. Оцениваем скорость ответов технического специалиста, их качество и полноту. Отмечаем, насколько содержательна техническая документация по серверам и доступна ли она для скачивания, а также оперативность предоставления документации, если ее нет в публичном доступе. Серверы с заведомо более производительными процессорами (больше ядер, выше тактовая частота ядер, больше объем кэш-памяти) по сравнению с эталонным результатом сервера с процессорами Intel Xeon Gold 6254 получают такой же или меньший результат при нагрузочном тестировании во всех используемых утилитах. Полученный результат производительности тестируемых серверов при нагрузочном тестировании меньше результатов, которые размещены на официальных сайтах используемых утилит для серверов с такими же процессорами на 20% или более. Любой из тестов отказоустойчивости пройден неуспешно. В части критических функциональных возможностей серверов стоп-факторы применимости указаны в начале статьи в списке проверяемых функциональных возможностей серверов. Для восстановления работы серверов необходимо обесточивание серверов на любой промежуток времени. Критические замечания (зависание BMC, спонтанные перезагрузки сервера в процессе тестирования, отсутствие уведомлений об ошибках и другие, критически влияющие на эксплуатацию серверов). Серверы несовместимы ни с одним модулем доверенной загрузки. После завершения всех тестов составляем подробный отчет с полученными результатами, отмечаем выявленные ошибки и нештатные ситуации в работе серверов в процессе их тестирования и делаем вывод о пригодности рассматриваемой модели сервера в продуктивной среде. Результаты представляем на внутреннем архитектурном комитете, который принимает решение о закупке нового оборудования. Поскольку сейчас мы вынуждены срочно искать и тестировать отечественное железо, ПМИ будет постепенно меняться и дополняться. Сейчас мы планируем добавить утилиту по проведению испытаний производительности баз данных и выбрать утилиту для тестирования кластера из пары серверов на базе отечественных или опенстековых систем виртуализации. Интересно, какие утилиты и методы определения производительности серверов для конкретных задач и какую последующую оценку применимости серверов в инфраструктуре используют наши коллеги в других компаниях. Буду рад ответить на вопросы, которые не затронуты в статье. Особенно интересно узнать, приходилось ли кому-то заниматься разработкой единой метрики производительности процессоров и каковы ваши успехи в этом направлении.",
    "129": "с большим удовольствием я вступаю в ваши ряды, чтобы поделиться собственными разработками в области оптимизации, и для начала несколько слов о себе. Моим образованием является инженерное дело в области автоматизации производств, и параллельно на протяжении долгого времени я занимаюсь программированием и разработкой торговых стратегий. Мой язык общения с финансовыми рынками - MQL5. Однако я всегда ощущал влечение к оптимизации. И вот, благодаря упорству и настойчивости, все же я смог адаптировать один из таких алгоритмов для торговли еще много лет назад :) Это было настоящим открытием для меня, и я был восхищен тем, как алгоритмы могут влиять на результаты в финансовой сфере. Мои текущие исследования сосредоточены на метаэвристических популяционных алгоритмах оптимизации. Я прилагаю усилия для разработки собственных уникальных методов и каждый день погружаясь в этот творческий процесс. Давайте вместе разделим наши идеи, обсудим новые тенденции и воплотим в жизнь самые смелые проекты. Ведь именно объединение интеллектов и творческих усилий позволяет нам достичь великих результатов. Поехали… Многопопуляционные и многороевые алгоритмы оптимизации представляют собой мощные инструменты, основанные на использовании нескольких независимых популяций для решения задач оптимизации. Эти алгоритмы работают параллельно, обмениваясь информацией об оптимальных решениях и исследуя разные области пространства параметров. В данной статье рассмотрим многопопуляционный алгоритм эволюции социальных групп ESG (Evolution of Social Groups) собственной разработки, который был написан мной буквально на одном дыхании за пару часов, и изучим принципы его работы. Многопопуляционные алгоритмы оптимизации представляют собой методы, в которых несколько независимых групп агентов работают над решением одной и той же задачи оптимизации. Каждая группа агентов функционирует независимо от остальных групп и проводит свои поисковые операции в пространстве параметров. Такой подход позволяет исследовать различные области пространства одновременно и находить оптимальные решения. В отличие от «моно» - популяционных алгоритмов, где все агенты работают в одной популяции, многопопуляционные алгоритмы предоставляют большую гибкость. Популяции (группы). Группы агентов сотрудничают и обмениваются опытом о лучших решениях. Коллективное движение. Частицы внутри групп совместно перемещаются в пространстве параметров. Локальный и глобальный опыт. Группы сохраняют лучшие решения (локальные внутри группы и глобальные). Эволюция и обмен опытом. Алгоритм проходит через итерации, обновляя группы согласно улучшенным решениям и обмениваясь опытом. Рассмотрим особенности поисковой стратегии ESG. В группе частиц, называемой \"социальной группой\", существует определенная центральная модель поведения. Частицы могут отклоняться от центра согласно закону распределения. Более приспособленная модель поведения становится новым центром группы, таким образом, группа перемещается в поисках стабильной модели поведения. Это многопопуляционный алгоритм, в котором моделируется поведение членов группы на низком уровне и глобальное поведение групп на высоком уровне. Группы могут останавливаться в развитии и застревать в локальных экстремумах, этой проблемой страдают большинство метаэвристических алгоритмов. Для избежания застревания используется тактика \"расширения сферы влияния социальной группы\". Границы группы расширяются, чтобы открыть новые области поиска и разнообразить популяцию членов в группе, что помогает избежать застревания в локальных экстремумах. При нахождении нового решения границы сокращаются, побуждая группу к уточнению решения. Однако простое расширение зоны влияния при фиксированном текущем положении центра группы может быть неэффективными. В таких случаях возникает необходимость передвинуть центр группы в новое положение. Для реализации стимула к перемещению центра групп достаточно обменяться успешным опытом между группами, поэтому частицы способны заимствовать идеи у центров других групп. Влияние новых идей может привести группу как к новым открытиям и улучшению положения в пространстве решений, так и ухудшить. Модель поведения социальных групп представлена на рисунке. На схеме выше расширение группы происходит в случае отсутствия прогресса, сужение - в случае улучшения решения, заимствование \"лучших идей\" (координат) от соседних групп \"Bt\" (best of team) частицами \"p0\". Поместить случайным образом центры групп в пространстве поиска. Разместить частицы групп вокруг соответствующих центров с заданным распределением. Рассчитать значения приспособленности частиц. Обновить после п.3 глобальное решение. Обновить затем центр каждой группы. Расширить границы групп в случае отсутствия \tулучшения положения центра и уменьшить, если удалось улучшить положение. Разместить частицы групп вокруг соответствующих центров с заданным распределением. Добавить информацию из центров \"чужих групп\" в одну частицу каждой группы (частица получает набор координат из чужих групп, выбранных случайным образом). Рассчитать значения фитнес-функции частиц. Повторить с п.4 до выполнения критерия останова. 1. \"Инициализация\": Создаются структуры \"S_Group\" и \"S_Agent\" для представления групп и агентов соответственно. Класс \"C_AO_ESG\" инициализирует эти структуры и параметры алгоритма. 2. \"Цикл оптимизации\": В цикле, который продолжается до выполнения критерия остановки, выполняются следующие шаги: \"Вычисление фитнес-функции\": Для каждого агента вычисляется значение фитнес-функции. \"Обновление групп\": Группы обновляются на основе текущих позиций агентов. \"Обновление позиций\": Позиции агентов обновляются в зависимости от их группы. 3. \"Возврат лучшего решения\": После завершения цикла оптимизации возвращается лучшее найденное решение. 1. Init. Этот метод инициализирует параметры алгоритма и начальную популяцию. Он также разделяет популяцию на группы. 2. Moving. Метод отвечает за первоначальное размещение групп в пространстве поиска и агентов в группах. 3. Revision. Здесь обновляются позиции агентов, если не произошло улучшения, радиус группы увеличивается, наоборот- уменьшается. Этот метод так же пересматривает позиции агентов и обновляет лучшее найденное решение, если агент находится вне границ, его позиция корректируется. 4. SeInDiSp. Метод используется для обеспечения того, чтобы значение находилось в заданном диапазоне. Если значение выходит за границы диапазона, оно корректируется до ближайшей границы с соблюдением шага поиска. 5. RNDfromCI. Этот метод генерирует случайное число в заданном диапазоне. 6. Scale. Этот метод масштабирует входное значение из одного диапазона в другой. 7. PowerDistribution. Этот метод используется для генерации новых позиций агентов с использованием степенного распределения. Ниже код метода Revision, а весь код на C# с примером использования можно найти здесь. Кто дочитал до конца, тому будет интересно, что алгоритм тестируется на тестовой функции Растригина, причем в коде легко можно заменить функцию на любую другую при желании, она не является частью класса. Такая архитектура алгоритма позволяет реализовывать любые пользовательские приложения, где требуется оптимизация. Алгоритм \"Evolution of Social Groups\" (ESG) - это эффективный метод оптимизации, основанный на взаимодействии групп. Он адаптивен, разнообразен и способен находить оптимальные решения в различных задачах. ESG может быть применен в областях, где требуется оптимизация параметров, таких как машинное обучение, оптимальное управление и комбинаторная оптимизация. Архитектура ESG позволяет легко внедрять различные методы оптимизации, объединяя их преимущества. ESG - это гибкое и самодостаточное решение для сложных задач. Простая архитектура и высокая переносимость на другие языки программирования. Высокая сходимость на сложных задачах. Чрезвычайно быстрый алгоритм с низкими вычислительными требованиями.",
    "130": "Технологическая индустрия, СМИ, якобы независимые эксперты, как правило, рекомендуют покупать самые новые, самые продвинутые гаджеты: смартфоны, телевизоры, ноутбуки. А что делать, если обновиться нужно, а денег на топовое устройство нет? Ну, или просто не хочется тратить слишком много. Брать новое дешёвое устройство? Такой себе вариант. Вполне вероятно, что хороший подержанный ноутбук сможет закрыть ваши задачи лучше. Если его немного подшаманить. Ноутбуки ThinkPad повсеместно считаются отличным вариантом подержанного устройства. Изучив доступные модели, их возможности и свой бюджет, я решил купить Lenovo ThinkPad T450s примерно за 150 евро (~15 тыс. рублей). Почему именно эта модель? Я не люблю большие ноутбуки, мне не хотелось ничего крупнее 14 дюймов. А поскольку я немного сноб в отношении количества пикселей, то и разрешение 1920×1080 не подлежало обсуждению. Поскольку у меня уже есть Dell XPS 13 с процессором Core i7 8-го поколения, я решил, что переход на 3-4 поколения вниз даст мне хотя бы какую-то разницу в производительности. SSD, очевидно, был необходим, как и возможности расширения, а объём ОЗУ не имел особого значения. T450s  оказался отличным вариантом. У него 14-дюймовый IPS-дисплей с разрешением 1920×1080 (есть модели с более низким разрешением, проверяйте этот момент), Core i5-5300U с 2 ядрами и 4 потоками с базовой частотой 2,30 ГГц и максимальной повышенной частотой 2,90 ГГц, графикой Intel HD 5500, SSD SATA ёмкостью 128 ГБ и 4 ГБ оперативной памяти. Поскольку 4 ГБ сегодня маловато, я сразу же заказал дополнительный модуль SO-DIMM на 8 ГБ за 35 евро (~3,5 тыс. рублей). Таким образом, общая стоимость машины составила 185 евро (18,5 тысяч рублей), что я посчитал приемлемым. На ноутбуке ещё и лицензионная Windows была, тоже экономия. Я не хочу превращать эту статью в подробный обзор ноутбука 2015 года, просто коротко опишу, каково пользоваться этой машиной сегодня. Крышка дисплея сделана из армированного углеродом пластика, а всё остальное — из магния. Чувствуется, что этот ноутбук немолод, так как он выглядит более громоздким, чем XPS 13 9370 или крошечный Chuwi MiniBook X (2023). Он не кажется плохим, дешёвым или ещё каким-то — просто он не такой крепкий, как современные модели. Зато у него есть целый ряд портов, с которыми можно работать, что очень круто. На левой стороне: слот для смарт-карт, USB 3.0, mini DisplayPort, ещё один USB 3.0 и разъём питания. На правой стороне: разъём для наушников, слот для карт памяти SD, ещё один порт USB 3.0, разъем Ethernet и порт VGA. На нижней части ноутбука находится док-порт для подключения к различным док-станциям с дополнительными портами и разъёмами. Внутри есть свободный слот M.2 (маленький, 2242). Побаловавшись некоторое время с различными операционными системами и дистрибутивами, я установил свой дистрибутив Fedora вместо предустановленной Windows. Но для разнообразия выбрал Xfce, а не привычный KDE. ThinkPad, как правило, хорошо поддерживают Linux, и T450s не стал исключением. Всё, что я смог протестировать — за исключением устройства чтения смарт-карт, поскольку у меня нет смарт-карты  — работает из коробки. Ничего не требовало ручной настройки для правильной работы. Все, от жестов трекпада до маленького индикатора ThinkLight на крышке, работало идеально, без необходимости искать драйверы и прочую ерунду, что характерно для Windows. Это нормальное явление для большинства ноутбуков с Linux, но приятно видеть, что оно применимо и к этой модели. Работа с T450s была... Непринужденной. Приложения открываются быстро, никаких задержек или лагов. Несмотря на наличие всего 2 ядер и 4 потоков, а также устаревшего интегрированного GPU, я не чувствовал, что мне чего-то не хватает при просмотре веб-страниц, написании и переводе текстов, просмотре видео и подобных задачах. Это не ультрамощный ноутбук для видеомонтажа, игр, компиляции кода или чего-то ещё, но для всего остального он подходит отлично. Разумеется, я кое-что сделал, чтобы машинка стала более приятной в использовании. Первым делом снял радиатор, очистил его от старой термопасты и нанёс свежую. Затем я занялся управлением вентиляторами и установил zcfan, демон управления вентиляторами Linux для ThinkPad, используя его настройки по умолчанию, и создал службу systemd, чтобы она запускалась автоматически. Кроме того, я проверил, какие кодеки поддерживает Intel HD 5500, а также какие из них Firefox считает рабочими, а затем использовал enhanced-h264ify для отключения VP9 на YouTube, поскольку HD 5500 не может аппаратно ускорить декодирование VP9. В итоге вентилятор практически никогда не включается. Пожалуй, это единственный вентилятор, который мне нравится. Как бы я ни был доволен ThinkPad, у него есть несколько серьёзных недостатков, типичных для б/у ноутбука. Батарея еле держит заряд, на дисплее есть красное пятно, клавиатура использует шведскую раскладку вместо привычной QWERTY, а ещё у него нет подсветки. И вот здесь мы сталкиваемся с истинной причиной, по которой ThinkPad называют лучшим вариантом подержанного ноутбука: доступность запчастей для этой линейки исключительна. У T450s интересная конфигурация аккумулятора: есть место для внутренней батареи, и ещё для внешней. Маленькая внутренняя батарея стоит около 40 евро (~4000 рублей), а внешние батареи от 60 до 80 евро (~6-8 тыс. рублей). Таким образом можно получить серьёзную ёмкость, особенно если вы купите две или даже больше внешних батарей. Lenovo утверждает, что вы сможете достичь 20,7 часов автономной работы, если объедините внутреннюю батарею с самой большой внешней. В моей модели внутренний отсек для аккумулятора частично занят считывателем смарт-карт, поэтому, чтобы вставить внутренний аккумулятор, мне придётся снять его и найти способ закрыть слот для смарт-карт. Замена внешнего аккумулятора, очевидно, намного проще, так как он вставляется прямо в устройство без необходимости открывать его. Ещё один интересный факт: при желании вы можете установить второй твердотельный накопитель формата M.2, используя разъём, освободившийся после удаления считывателя смарт-карт. Однако обратите внимание, что для этого потребуется плата адаптера (FRU 04X3827) и ленточный кабель (FRU 04X3987), которые обойдутся вам примерно в 20 евро (2000 рублей) каждый. Если место для хранения данных для вас важнее ёмкости аккумулятора, это может стать отличным решением. Замена клавиатуры также возможна и не представляет особой сложности. Новая обойдётся вам не более чем в 40 евро (~4000 рублей), а доступных вариантов — тьма. Ещё я могу заменить дисплей, и, похоже, это довольно простое дело. Красноватое пятно на моем дисплее заметно только на очень темном участке экрана, но оно всё равно беспокоит меня. Новый 1080p-дисплей для T450s стоит около 60 евро (~6000 рублей). Что именно я буду менять в ноутбуке? Пока не решил. Даже если я куплю две батареи, новую клавиатуру и новый дисплей, чтобы вернуть ноутбуку почти первозданный вид, я потрачу всего 180 евро (~18 тыс. рублей), что не так уж много, учитывая, что взамен я получу: тонну времени автономной работы, клавиатуру с подсветкой и правильной раскладкой, а также приятный глазу дисплей. Однако даже без всех этих улучшений и исправлений T450s является отличным б/у ноутбуком, способным удовлетворить потребности большинства пользователей. Это отличная покупка для детей, но даже в качестве основного ноутбука для более требовательных пользователей он станет хорошей рабочей лошадкой. Так что не гонитесь за новинками. Иногда выгоднее и удобнее немного проапгрейдить подержанное устройство.",
    "131": "В документации по UIKit компании Apple можно найти объяснение, что структура приложений основана на шаблоне проектирования Model-View-Controller (MVC). В материалах Apple по SwiftUI объяснений и даже просто ссылок на паттерны проектирования, похоже, нет. Попробуем сначала разобраться почему. Далее рассмотрим логичные и простые решения для построения как отдельных компонентов, так и уровень приложения с использованием состояний и property wrappers; подход, который логично обозначить как State-Model-View. Тема использования шаблонов (паттернов) для архитектуры приложений считается сложной. Начинающие разработчики с ней мучаются много и долго. Сложна она скорее не тем, что непонятны сами идеи паттернов проектирования с акронимами, возникшими чаще задолго до мобильных приложений, а тем что изобретатели придумывают некоторые новые абстрактные сущности вместе с изощренными методами как их связать друг с другом. Та-да, Сoupling здесь передает нам большой привет! :) Представьте уважаемого джуна, который не может ухватить практическую целесообразность применения тех или иных паттернов только потому что именно об этом  обычно не говорится с примерами кода, позволяющими понять почему с паттерном жить лучше, чем без него. Дело ограничивается абстрактными тезисами которые читателю нужно принять на веру, например, что данный [подставьте свой любимый] шаблон “решает проблему сборки”, “облегчает процесс разработки сложных приложений”, “позволяет отделить логические части проекта на разные объекты” и даже страшилки, что “упрощенный подход к архитектуре непременно сыграет злую шутку”. Не будем спорить с уважаемыми авторами и примем эти тезисы на веру. Поищем, что говорит сама Apple про архитектуру для SwiftUI - приложений. Хм, похоже, почти ничего. Впрочем, на форуме Swift-разработчиков легко найти юмористическую презентацию Stop using MVVM for SwiftUI. В ней говорится, что не нужно заниматься овер-инжинирингом, а лучше использовать простые решения в логике SwiftUI вместо искусственно усложненных. Всё это со сравнениями фрагментов кода. Попутно отметим, что архитектура там в шутку названа как MV без С, то есть Model-View без Controller. Очень ценны и обширные комментарии к посту с различными точками зрения и примерами. Людям не интересно обсуждать простые и понятные вещи, тема использования паттернов для проектирования кода действительно сложна. Всё же, кажется, что любое, даже совсем простое приложение на SwiftUI не очень удачно описывается как MV, так как состоит не только из модели данных и вьюшек для их визуализации. Не хватает клея, который их соединяет. Вернемся к этой теме чуть позже. SwiftUI uses a declarative syntax, so you can simply state (state подчеркнуто мной) what your user interface should do. Рассмотрим простой пример. Здесь разработчик разместил в горизонтальном контейнере HStack два видимых элемента (картинку и текст) и один невидимый экземпляр Spacer, который толкает другие компоненты влево. Если этот Spacer разместить выше post.image, то элементы сместятся к правому краю. Если перевернуть экран симулятора или физического устройства  с portrait на landscape, то все компоненты корректно переместятся. Если включить в iOS dark mode, то цвета фона и текста автоматически переключатся. На разных устройствах, в том числе  iPad визуал будет масштабирован по другому, с учетом другого разрешения и соотношения сторон экрана. То есть здесь разработчик декларировал принцип компоновки этого экрана с необходимыми компонентами, задал, где надо ограничения, a SwiftUI автоматически построил визуальный интерфейс. И надо признать, что SwiftUI в абсолютном большинстве случае чисто и корректно делает всю императивную работу по визуализации экрана. Эх, маркетологи - это не инженеры. Свою работу знают, поэтому их нарративы легко приживаются в умах разработчиков. Можно ли назвать “императивными” методы из UIKit, когда программисты используют, например, вот такие описания: Здесь по сути задается описание ограничений системы линейных уравнений для их решения внутри движка без участия программиста.  Этот пример - сама квинтэссенция декларативности. Впрочем, если посмотреть первоисточники из WWDC 2019, где много говорилось именно про декларативность, то всё же дилемма “императивный UIKit vs декларативный SwiftUI” скорее предложена разными последующими комментаторами. Таковы паттерны мышления людей - везде искать и находить  дихотомии, которые маркетологи используют в своих целях. А тогда какую концепцию разработки реализует UIKit? Для разработчика из 90, недавно освоившим ООП ответ был бы вполне очевидным - это архитектура, управляемая событиями (event-driven), тесно связанная с объектно-ориентированным подходом . Да, действительно, вот довольно старое, но актуальное описание от Apple. Не отсюда ли в числе прочего следует скорее надуманная, чем реальная проблема “massive view controllers”? Есть простые методы решения этой проблемы. Но вернемся к SwiftUI. Концепция логичной архитектуры для SwiftUI лежит на поверхности. Для визуализации интерфейса через views особое значение имеют переменные, которые имеют смысл состояний и в режиме run time имеют функцию триггеров. Для views количеством более двух можно использовать перечисление enum и конструкцию switch. Пример кода - в конце статьи. Обратим здесь внимание на принципиальную особенность метода передачи данных через binding в SwiftUI: мы никак не извещаем корневой view об изменении состояния переменной quizzing, показывающей находимся ли мы в состоянии квиза и не принимаем никаких действий по скрытию экрана, когда тест уже завершён. Мы в нужный момент просто меняем state-переменную. Далее движок автоматически передает изменение состояния в корневой view и таким образом изменение состояния триггера полностью меняет вид экрана с квиза на его результат без каких либо иных императивных предписаний и условий. Здесь код содержит две переменных, от состояния которых зависит, отображается ли алерт на экране, а после ответа “Да” в иерархию отображения добавляется и новый экземпляр SheetView. Модификаторы .alert и .sheet не обязательно подстраивать к кнопке, они могут быть размещены при любых views в локальном scope свойства body. По сути эта архитектура крутится вокруг State, дата Model (в этих примерах с использованием SwiftData) и композиции Views. Всем нравится конструкторы лего и конструировать по такому принципу в SwiftUI легко и приятно, если освоить передачу данных модели через параметры инициализаторов одним из способов, как это было продемонстрировано в примерах выше. В SwiftUI легко сделать маленький компонентный View из двух - трех элементов (как на первом сниппете), далее поместить его в один или несколько более сложных узлов и так далее, пока в умелых руках конструктора из элементарных кирпичиков не соберется приложение в целом. Конечно не всё так просто, но все же принцип лучше использовать  такой. Каждая диаграмма сделана отдельным компонентом, все вместе помещены в вертикальный контейнер VStack и каждому компоненту передана одна версия правды - модели данных из SwiftData. Отметим ясность и простоту компонента StatisticsView на экране выше - вся сложность декомпозирована и скрыта в невидимых здесь деталях. Модификатор .frame(height: 260) для верхней диаграммы круговой диаграммы вносит точное указание по высоте. Без него бублик сверху будет визуально казаться непропорционально мелким. Если бы диаграмм было только две, а не три, то это ограничение тогда было бы излишним. Поэтому не следует frame включать внутрь компонента SectorMarkView. Посмотрим, кстати на этот компонент детальнее: Ничего особенного, SwiftUI в отсутствие других соперников за место на экране растянул диаграмму на весь экран. Отметим модификатор .groupBoxed - его нет в стандартной библиотеке модификаторов, это кастомный элемент. Вот он: GroupBoxWrapper не делает ничего супер-сложного, просто упаковывает наш content view-источник внутрь контейнера GroupBox, немного упрощая визуальное представление кода, но здесь важен еще один принцип конструирования визуальных компонентов SwiftUI: мы использовали протокол ViewModifier и extension для протокола View, чтобы добавить еще один кастомный компонент лего в библиотечку стандартных узлов. В SwiftUI на удивление легко сделать приложение, напрямую управляемое состояниями. Для примера: [бесплатное] приложение Swift-Way в App Store - симулятор набора разработчиков Swift в A Dream Company. Приложение имеет простой интерфейс, но сложную логику и сценарии, включает в себя специальный скриптовый язык с интерпретатором для генерации диалогов интервью на “живом” экране, модель компетенции программистов. Первое, мы видим, что в зависимости от текущего значения в переменной appState приложение с помощью конструкции switch отображает один из трех case с Views. Самый простой - это MainView с таб-баром, который сейчас виден на темном экране симулятора справа. Это SwiftUI компонент, поэтому переменная состояния передана туда через обыкновенный binding. Два других - экран интервью и экран casual-игры - это другой фреймворк эпохи UIKit (SpriteKit), поэтому изменение состояния здесь производится через извещение центра нотификаций в замыкании. В любых случаях, когда текущий режим отображения завершает свою работу, он изменяет или сигнализирует об изменении состояния приложения и корневой ContentView в этом случае актуализирует нужный view в своей иерархии. Это простая и удобная схема для разработки общей архитектуры приложения. Настолько простая, что даже сложно это назвать “паттерном проектирования”. Впрочем, здесь нужно добавить, что если приложение имеет сложную бизнес-логику с этапами, то нужно различать состояние интерфейса от состояния истории (story). В этом примере имеется еще дополнительное состояние (переменная situation) которая отслеживает общий статус воронки набора нового программиста в команду разработки. Статус может иметь одно начальное состояние (получено приглашение от компании к серии интервью) и несколько конечных, в том числе приятный оффер и между ними различные переходные статусы. В зависимости от статуса истории, а также временных событий (интервью проходят в режиме real time) формируются совершенно различные диалоги с использованием одного визуального компонента. Не нужно усложнять вещи паттернами с непроверенной ценностью - практично использовать SwiftUI так, “как оно есть”.",
    "132": "Всем привет! Меня зовут Егор, я стажёр backend-разработчик в зарплатном проекте Росбанка (он же Payroll). В этой статье я расскажу про путь становления от «зеленого» стажера до боевой единицы в команде: через что мне пришлось пройти, с какими трудностями я столкнулся и как прокачал свои скилы. Год назад я закончил курсы по разработке на Java. Курсы были неплохими, но проблема в том, что на сегодняшний день нельзя просто закончить курсы или посмотреть 5-6 видео по программированию, чтобы считать себя крутым джуном. Тем не менее, курсы дали мне неплохую базу: Java Core, Spring Framework, Spring Boot, JUnit, Spring JPA, PostgreSQL, работа с Рostman, Docker и некоторые другие технологии. Но с точки зрения объёма материала и более глубокого изучения технологий курсы, конечно же, не дают те знания, которыми может и должен обладать младший специалист, всё приходилось добирать самому. Закончив курсы, я начал повторять базу и параллельно искал стажировку. Получал отказы от компаний, но не сдавался. И однажды мой знакомый из Росбанка закинул моё резюме. Меня пригласили на собеседование, где присутствовали product manager, техлид команды и мой потенциальный ментор (опытный java разработчик). Были стандартные вопросы про Java Core, Spring, пару вопросов про базы данных, REST и небольшая задачка. Отвечал я неплохо, как мне казалось. Но если бы я посмотрел на эти ответы сейчас, было бы очень стыдно. Спустя 2 дня после собеседования мне позвонили и сказали, что готовы взять меня на стажировку, однако надо подтянуть базу, ибо она сильно хромала. Отдельно хочу отметить то, что меня похвалили за soft skills, и я считаю, что это тоже важные навыки наравне с hard skills. Прежде всего, хочу выразить огромную благодарность моим наставникам за то, что давали дельные советы, поддерживали и направляли меня к изучению нужных технологий. Как я уже сказал ранее, для начала мне надо было подтянуть базу, и этим я занимался с моими наставниками около 2 месяцев. По началу мы созванивались раз в неделю, где общались на определенные темы и решали задачи. Сначала ментор спрашивал, что я изучил и какие были вопросы по задаче. Далее разбирали задачу и обсуждали то, что надо подготовить к следующей встрече. Например, мы обсуждали Java Core, чем сравнение объектов через equals отличается от «==», как работать с дженериками и многое другое. После первой же встречи с техлидом и ментором в рамках стажировки моё восприятие жизни разработчика изменилось. Я действительно осознал, что важно постоянство. Каждый день я повторял основы Java, читал различные статьи про Spring, PostgreSQL и так далее. Спустя пару встреч мы решили, что мне нужно написать пет-проект: маленькое Spring Boot приложение с парой rest-ручек, где также используется PostgreSQL, и выполнить миграцию БД с помощью Flyway. Повторив основы, я столкнулся с новыми определениями в рамках миграций БД. Ранее с этим я никогда не работал. Казалось, что помощь сеньоров тоже является обязательной частью стажировки. Но со временем я понял, что суть стажировки не в том, чтобы тебе дали всё готовое на блюдечке или сказали что, куда и как написать, а в том, чтобы стажёр самостоятельно изучил материал, сам работал с проектом. Конечно, здорово, когда ты можешь обращаться за советом к опытным разработчикам, чтобы они тебе помогли и чуть ли не все тебе рассказали (чего греха таить, я сам пару раз так делал). Но это так не работает и не должно работать. Здесь важна самостоятельность, ведь не может быть так, чтобы какой-нибудь middle-разработчик приходил к сеньору и просил полностью за него сделать задачу, объяснить, как писать код и тд. И вот когда стажёр начинает самостоятельно изучать новые технологии, читает документацию и различные статьи, он начинает расти. После того как я закрепил основы, изучил пару новых технологий, меня подключили к команде. Теперь я должен был каждый день созваниваться с командой на дейли и грумингах, чтобы обсуждать различные задачи, синхронизировать работу всей команды и оценить новые задачи. В первые же дни я получил свою задачу. Нужно было прогнать Flyway-скрипт. Задача была несложная. Мне ее дали, чтоб я понял, как устроены процессы в команде, как работать с общим репозиторием и т. д. Ощущения после выполнения такой маленькой задачки были прекрасны: начинаешь думать, что приносишь команде пользу, и чувствуешь, что стал лучше. Конечно, это все так, но только отчасти. Для новых задач нужно изучать новые технологии, общаться с аналитиками, анализировать и оценивать задачи. Поначалу было тяжело: пока разберешься с архитектурой приложения, пока изучишь технологию и созвонишься с аналитиком, уже пройдет неделя, а это половина спринта. Надо бы задачи уже выполнять. В такие моменты начинаешь загонять себя, думаешь, что уже не так эффективен, мешаешь команде и вообще делаешь недостаточно. И тут на помощь приходит, как неудивительно, сама команда. Например, когда мне дали одну из моих первых задач, я ее реализовал и влил в общую ветку. Спустя пару часов мне пишет тестировщик и говорит, что есть баг в этой задаче. У меня была небольшая паника из-за непонимания, как действовать далее. Мы созвонились, начали обсуждать процесс, и я ничего не понял. Мы решили подключить к встрече одного из аналитиков. Я предвкушал, что будет некое осуждение с их стороны из-за того, что я не до конца разобрался в задаче и не понял, что от меня требуют. Но нет, коллеги указали мне, в каком месте может быть ошибка, каким по итогу должен быть процесс и, конечно же, поддержали. Здесь все сплочены общей целью – делать наш продукт лучше. Это значит, что нужно помогать друг другу. Опытные специалисты понимают, что у стажеров ещё нет нужного бэкграунда и знаний для определенных задач. Однако никто тебя не обвиняет в этом, а, наоборот, стараются помочь тебе. Работа с продуктом – это уже не пет-проект. Тут большая и сложная архитектура приложения, которую нужно понять. Менторы сразу сказали, с чего мне начать. Так как на дворе 2024 год, кругом микросервисы, то надо ознакомиться с принципами их разработки. Для Java-разработчика основой в разработке микросервисов служат Spring, Kafka, Docker и многое другое. Полтора месяца я изучал Spring Cloud и его плюшки (Spring Cloud Gateway, Spring Cloud Config, Spring Cloud Security, Spring Cloud OpenFeign и другое) для понимания работы наших микросервисов. Также не стоит забывать про Spring Security – тоже важная технология, без знаний которой сложно представить себе современную разработку на Java. Нельзя не упомянуть про умение работать с Docker, Kafka, Openshift, Git и Swagger. Понимание основ этих технологий и работа с ними делают из стажера уже полноценного разработчика, который сможет реализовать большинство задач. С каждой новой задачей ты практикуешь работу с этими инструментами, что бывает тяжело, но безумно интересно, и порой приходится отстаивать свои решения. Например, мне дали задачу по выполнению миграции БД, когда я только вливался в команду. Мне надо было в своей локальной ветке прогнать скрипт, который добавлял новое поле в общую БД на тестовом стенде, но параллельно с этим, кто-то из разработчиков влил свои изменения в общую ветку. И образовалась проблема: проект требовал от меня выполнения той миграции, которой у меня пока не было. Я долго думал, что проблема в локальных настройках Flyway, что не до конца разобрался с технологией, однако надо было всего лишь добавить изменения из общей ветки к себе (сделать rebase, проще говоря). Так потихоньку и появляется ценность тебя как разработчика в команде. Конечно же, нельзя останавливаться на достигнутом. Надо больше погружаться в проект, брать больше важных задач, изучать новые технологии. Проще говоря, надо развиваться. И, к счастью, у стажеров всё для этого есть: И, конечно же, прекрасная команда, которая поможет в трудную минуту. Возможностей море. Самое главное – не бояться и идти только вперёд! Интересно было бы узнать, как вы проходили свой путь из стажера в джуны и так далее. С радостью обсужу это в комментариях.",
    "133": "Российский рынок ERP-систем сильно изменился за последние два года. За это время страну покинули крупнейшие западные вендоры — Oracle, SAP, Microsoft. Многие компании, которые пользовались иностранными продуктами, остались в состоянии неопределенности. Вроде бы нужно срочно мигрировать на российское ПО или что-то решать с поддержкой текущих систем, которой теперь нет. Но при этом есть вероятность, что иностранные вендоры вернутся и деньги будут потрачены зря. На фоне всех сомнений и событий изменилась жизнь многих SAP-консультантов, ABAP-разработчиков и других ИТ-специалистов. Еще недавно они занимались поддержкой и развитием иностранных продуктов на своих и чужих предприятиях. А теперь их работодателям интереснее люди с компетенциями по 1С. Но давайте обо всем по порядку. Много лет ведущими игроками российского ERP-рынка были немецкая SAP, американская Microsoft и российская 1С. К первой пятерке вендоров также относили Oracle (США) и «Галактику» (РФ). Если верить аналитикам «Эдит про», в 2021 году 60% рынка занимали иностранные компании, на долю 1С тогда приходилось 35%, а остальные российские вендоры делили между собой оставшиеся 5%. Весной 2022-го расклад стал меняться. Зарубежные вендоры перестали продлевать лицензии и поддерживать свои решения в российских компаниях. Сначала с резидентами РФ прекратила операции Oracle (март 2022-го). Потом о таком же решении заявила Microsoft, которая планировала прекратить обслуживание российских компаний через полтора года после начала СВО (октябрь 2023-го). Хотя я слышал от коллег, что все ПО продолжает работать до сих пор. Последние угрозы Microsoft разослала клиентам в марте 2024-го, в списке облачных продуктов на отключение также была ERP-система Dynamics 365. История с SAP оказалась более увлекательной. В СМИ несколько раз сообщалось о намерениях покинуть Россию. Первая новость появилась уже спустя месяц после начала спецоперации (апрель 2022-го). Однако официальная поддержка систем SAP в России завершилась намного позже (декабрь 2023-го). Причем пользоваться облачными решениями по подписке можно было вплоть до 20 марта 2024-го. Более того, недавно выяснилось, что немецкий вендор ушел из России не полностью. В SAP решили сделать исключение для российских представительств европейских корпораций и дипломатических ведомств. В итоге, по данным АНО НЦК, к концу 2023-го отечественные продукты ERP стали занимать 55% рынка. Из них более 80% пришлось на 1С. Западные же платформы поделили между собой 45% рынка — на 15% меньше, чем в 2021 году. Иногда ничем. Такое часто происходит в компаниях, которые не обязаны переходить на отечественное ПО до 2025 года. Системы SAP и Oracle не обновляются, но их поддержкой занимаются штатные специалисты или сторонние сервисные организации. С одной стороны, ничего критичного в таком сценарии нет. Системы работают, никакие вложения в новое ПО не требуются. С другой — без новых патчей остаются незакрытыми уязвимости и угрозы информационной безопасности. Но, видимо, этот риск для многих таких компаний не сильно страшен. Другой популярный сценарий: компании фактически продолжают использовать тот же SAP без поддержки и обновлений, но параллельно внедряют 1С или другое отечественное решение. Так они выполняют требования законодательства, по импортозамещению (в основном на бумаге, да) например. А еще это неплохой способ растянуть миграцию модулей, которая по факту часто превращается во внедрение системы с нуля и реорганизацию бизнес-процессов. Тем не менее уход иностранных вендоров — это факт. На фоне новостного шума многие компании стали активнее изучать и тестировать российское ПО. Первым пунктом в этом списке стоит платформа 1С: Предприятие, которая много лет остается наиболее популярным ERP-продуктом в России и СНГ. Второе место делят между собой другие аналоги от отечественных разработчиков, многие из которых давно известны на рынке. Это такие ERP-системы, как «Галактика», «Парус» и «Компас», заточенные под конкретные отрасли или бизнес-процессы. В Рунете о реальном использовании опенсорсных ERP-систем пишут редко, но в принципе такие продукты тоже есть. Многие из них написаны на Java, работают как веб-приложения и ориентированы в основном на малый и средний бизнес, который не готов оплачивать лицензии и поддержку проприетарных систем. Примеры: Apache OFBiz, Odoo, Dolibarr, ERPNext. Пользователи часто отмечают, что российские ERP-системы не являются достойной заменой западным. Отличия действительно есть. Причин, почему клиенты не спешат отказываться от иностранных ERP, много. И я думаю, что чаще всего срабатывает психологический фактор: когда вкладываешь десятки миллионов в ПО и пользователи наконец-то привыкли к нему, отказаться не так уж просто. Да и выстраивать «бесшовные» процессы, которые были в том же SAP, с российскими продуктами часто — задачка со звездочкой. Кроме того, у SAP есть цельноядерная структура и собственная СУБД HANA, которая обеспечивает высокую производительность модулей. В случае с 1С для работы с большими объемами данных нужно подключать сторонние СУБД. В том числе от поставщиков Microsoft и Oracle, которые объявили бойкот российским клиентам (хотя всегда есть альтернативы без санкций типа PostgreSQL, например). Но это лишь один аргумент, который я слышал на практике. На самом деле их множество, и эта тема тянет на отдельную статью. Тем не менее у российских ERP-систем есть как минимум два очевидных преимущества. Первое — они изначально адаптированы к специфике российского бизнеса и законодательства. Это заметно и по функциональности систем, и по поддержке клиентов. Вендоры погружены в бизнес-среду клиента, поэтому могут оказать качественную помощь в решении «чисто-российских» проблем. Второе преимущество — цена. Стоимость лицензий и обслуживания отечественных ERP-систем и раньше была ниже, чем у международных компаний. Именно поэтому продукт от 1С всегда был востребованным в малом и среднем бизнесе, где обычно стремятся сократить затраты на внедрение и поддержку ИТ-продуктов. Сейчас, с учетом нового курса валют, разница в цене стала еще больше. Помимо этого, у российских решений есть мобильная платформа. В случае с западными ERP, конечно, тоже доступны частные решения для смартфонов. Но чтобы их получить, потребуются дополнительные вложения. Изменения на рынке привели к трансформации ИТ-рекрутинга. Теперь компании меньше нуждаются в специалистах с компетенциями по западным ERP-системам. И хотя сейчас они пока нужны работодателям, которые только переходят на новое ПО или остаются на старом, множество сотрудников все же опасаются увольнения. Изменились и доходы. Раньше специалисты по SAP по Oracle получали более высокую зарплату в сравнении с теми, кто специализировался на российских решениях. Для сравнения: в 2021 году, согласно отчету Global Career, в среднем в месяц ABAP-разработчик получал 200—250 тыс. рублей. Сейчас на hh.ru открытые офферы в основном находятся в пределах 115—150 тыс. рублей. Более того, открытых вакансий ABAP-разработчиков на сайте сейчас не больше 60. Хотя еще в ноябре 2023-го их было 260+, а пару лет назад — около тысячи (инфа из предыдущих обсуждений темы на Хабре). Многие SAP-специалисты либо покинули страну и работают в иностранных компаниях, либо занялись другими задачами. Большинство пошли по наиболее очевидному пути — стали изучать 1С. И это логично: специалисты с компетенциями по российским ERP-решениям теперь могут получать больше. Для сравнения: в 2021 году, по данным ГородРабот.Ру, зарплата составляла 89 тыс. рублей. К марту 2024-го она выросла до 136 тыс. рублей. Но все это  — с одной стороны. С другой — есть логичная вероятность, что через несколько лет специалисты по SAP и Oracle будут уникальными кадрами. И их ценность, наоборот, может вырасти. В первую очередь среди тех компаний, которые продолжат использовать западные системы и самостоятельно дорабатывать их под свои нужды. Также спрос на такие кадры может сохраняться благодаря сервисным компаниям. Они по-прежнему продолжают поддерживать зарубежные продукты, хоть и собственными силами. И пока ситуация выглядит так, что без официальной поддержки вендоров спрос на их услуги может только вырасти. Перспективы рынка предсказать непросто. Многое зависит от того, когда именно западные вендоры вернутся в Россию и произойдет ли это вообще. Пока же я могу предположить, что отечественные ERP-системы будут развиваться минимум в трех направлениях. В ближайшие пару лет разработчики начнут вкладывать больше средств: в облачные решения: клиенты все чаще подключают виртуальную версию 1С по подписке. Таким образом они обходятся без капитальных вложений, могут работать с новыми продуктами в режиме тестирования и использовать их как подстраховку на время, пока западные вендоры не вернутся в Россию. Главная задача отечественных разработчиков здесь — предложить достойную альтернативу, к которой компании успеют привыкнуть и начнут доверять. в ускорение внедрения новых решений. Здесь все очевидно: длительная и сложная миграция в текущих реалиях — проблема для любого бизнеса. Если ее не решать, то организациям будет проще поддерживать иностранные ERP-системы собственными силами или подключать для этого сервисные компании. в разработку специализированных модулей для различных отраслей. Пока отечественные решения по этому показателю отстают от западных. Уточню: все описанное выше — только мое мнение, которое сформировалось на основе нескольких лет работы с ERP-решениями (в основном российскими). Возможно, у вас больше опыта и вы видите текущую ситуацию и перспективы иначе. Особенно если занимаетесь развитием SAP или других иностранных продуктов в компаниях. Если это так, поделитесь мыслями в комментариях. Мне интересно, что вы думаете о будущем и ситуации с рынком ERP вообще.",
    "134": "МТС стала одной из самых популярных цифровых экосистем России. Чтобы этого достичь, мы прошли огонь, воду и цифровую трансформацию. У нас бы точно не вышло реализовать её без квалифицированных людей, горящих своим делом и готовых двигать технический прогресс вперёд. Если хорошего разработчика найти можно, достаточного количества архитекторов на рынке просто не оказалось. И мы решили, что вырастим их сами. Так и появилась команда школы архитекторов МТС. Сегодня расскажем, чем архитектор отличается от разработчика или аналитика, чему мы обучаем в своей школе и как вообще дошли до жизни такой. По нашему опыту, главная сложность при переходе из разработчика в архитекторы — наличие очень хороших профессиональных навыков и нехватка того, что мы называем архитектурным мышлением, или майндсетом. Про то, что это и чем отличается от мышления лида или просто разработчика, расскажу чуть-чуть дальше. А теперь — о том, почему мы решили, что нужно учить архитекторов: и нынешних, и потенциальных. Во-первых, наша школа упорядочивает мышление тех, кто уже работает архитектором в каком-то проекте. И именно на майндсет, а не на технологии мы в итоге сместили акцент. Во-вторых, архитектор — кадр редкий и дорогой. Порой проще вырастить его внутри компании, чем искать за её пределами. У нас довольно низкая конверсия из собеседований на архитекторов решений в сотрудников. В-третьих, обучение разработчиков и аналитиков (они частые наши ученики) помогает подчеркнуть роль и задачи архитектора в продуктовой команде. Коллеги начинают понимать, где зона ответственности архитектора и какую пользу приносят решения, проработанные на раннем этапе. А ещё студенты учатся правильно писать документацию, аргументированно отстаивать свои решения. В итоге всем становится проще понимать друг друга, возникает меньше путаницы. Четвёртая причина: на небольших проектах технический руководитель или другой человек может выполнять задачи архитектора. Но по каким-то вопросам, например по corner-case, они всё равно идут в Центр практик архитектуры. И, если человек прошёл обучение у нас, объяснить ему, что и как делать с проектом дальше, намного проще. По моему опыту, часто разработчики смотрят на задачи не как на потребности бизнеса, а как на техническую головоломку, что порой приводит к потере ориентации на цель. Разработчик, как человек, работающий с технологиями, сразу думает о, собственно, технологиях. «Вот я здесь возьму java, тут будет spring boot, а вон там — микросервис». Причина этого в том, что у них совершенно другие задачи и функции по сравнению с архитектором. Архитектора же этот подход может привести к нерациональным решениям. Сначала он должен понять задачу, оценить потребности бизнеса, пожелания заказчика, возможности. И уже потом думать о технологиях. Больше общаться с бизнесом и продактом. Стараться понять, как развивается продукт, почему именно так, каким он должен стать. Задаваться вопросом «зачем?». А точнее — «зачем мы это делаем?». Пока архитектор не понимает, зачем делается система, что-то меняется и дорабатывается, он не может принять оптимальное решение. Ведь он не будет понимать, что нужно бизнесу. Смотреть на решения technology agnostic — сперва решение, а потом технологии, а не наоборот. Бизнес первичен — и это понимание становится важным шагом на пути из разработчика в архитекторы. А что ещё будет полезно? Несколько личностных качеств. Во-первых, будущий архитектор должен уметь донести свою мысль до кого угодно. И не бояться конструктивно дискутировать. Не всегда его решения будут принимать с восторгом. Случаются моменты, когда то, чего хочет заказчик, — это не то, что ему нужно на самом деле. И умение общаться, налаживать контакт, отстаивать своё мнение очень важно. Во-вторых, должна быть огромная страсть к изучению технологий. Без этого ничего не выйдет. А если говорить о hard skills, то идеальный архитектор — это специалист уровня senior+. Он не просто недавно вырос в грейде, но уже успел поработать и прокачаться в своей прошлой профессиональной роли. Сначала мы фокусировались на основных архитектурных практиках. В первых потоках достаточно много занятий посвящалось проектированию архитектуры какой-либо системы, был большой упор на технологии. Студенты даже разворачивали и писали приложения. Курс длился около 2 месяцев, 8 занятий по одному в неделю. Мы встречались со студентами по понедельникам онлайн, отвечали на вопросы и давали домашние задания. Тогда ещё не было сквозных домашек — каждый лектор придумывал свои д/з, и он же их проверял. У этого были свои минусы: проверь-ка 30–40 работ и дай по всем обратную связь. И, что называется, без отрыва от производства — твои повседневные рабочие задачи никто не отменял. Изначально был свободный отбор в школу архитекторов. Мы придумали тестовые задания, в том числе с вопросами о том, зачем человек вообще хочет к нам идти. Затем собрали заявки от всех желающих, отсекли лучших. Да-да, здесь нет опечатки. Предполагалось, что, если человек идеально выполнил входное тестовое задание, он уже готов к профессии без нашей помощи. Дальше изучали мотивацию и набирали группу студентов. Ребятам, которым «ну просто интересно», тоже отказывали: ресурса было маловато, а мы оценивали эффективность обучения для компании. Так прошло 3 потока, но казалось, что рассказываем мы всё равно не о том. Настало время перемен. К школе архитекторов МТС я присоединился как лектор на третьем потоке. Дмитрий Дзюба, создатель школы, попросил меня подготовить модуль по Domain-Driven Design. Уже позже я стал лидером гильдии архитекторов и получил школу «в наследство». До МТС у меня был опыт обучения архитекторов в моей предыдущей компании. Там уже были готовые материалы для школы от моих коллег. Я запускал обучение сначала в московском офисе, а потом во всех российских филиалах компании. Координировал, подбирал педагогов, сам читал лекции, проверял выполнение домашних и выпускных работ. Этот опыт очень мне помог. Мы изменили подход к набору: сейчас рекомендации приходят от главных архитекторов кластеров. Они формируют списки людей, которые действительно будут выполнять архитектурные функции, и пишут обоснования. Так мы получаем более мотивированных студентов — им наши лекции точно принесут пользу и помогут решать рабочие задачи. Важный момент: к знаниям тоже нужно быть готовыми. Бывает, что люди говорят: «Я хотел технологии изучать, а не теорию». Хотя наш курс именно про подходы к работе. И мы уже обсуждаем, как менять процесс набора и информирования будущих студентов. Нам важно формировать у них правильные ожидания и получать более мотивированных на учёбу людей. Ещё мы переработали курс и сместили акцент с конкретных технологий на майндсет. Развитие технологических навыков мы вынесли в отдельные мини-курсы. А в новую программу добавили две ключевые лекции, с которых, в принципе, надо начинать обучение. Первая лекция — про то, что такое архитектура и в чём её ценность, кто такой архитектор, какую задачу он решает. Разбираем, чем он отличается от аналитика, где его зона ответственности. Смотрим, как архитектор работает в условиях быстро меняющихся требований и технологий. Вторая лекция от другой нашей коллеги — Анны Вахтёровой — про бизнес-архитектуру. Очень многие её упускают из виду, хотя она лежит в основе проработки архитектурного решения. Именно с этой темы часто начинается проектирование любой системы. И нередки случаи, когда проект проваливался из-за того, что архитектор не понимал задачу с точки зрения бизнес-архитектуры. Следующим шагом мы добавили тему про качество архитектуры и тактики его достижения. Потому что можно решать одну и ту же задачу разными способами, среди которых нет однозначно хороших или плохих. Есть наиболее и наименее подходящие под текущие потребности. Как работать с архитектурными компромиссами, принимать решения, на что обращать внимание и как идентифицировать архитектурно значимые требования — вот что важно знать архитектору. Ещё одно важное изменение: лекции мы записали заранее и добавили дополнительные встречи с вопросами-ответами и практикой. Их проводят либо сами преподаватели, либо менторы курса. Состав менторов мы расширили за счёт коллег, готовых делиться знаниями. За каждым студентом закрепляется свой ментор на весь курс. Это помогло нам немного разгрузить лекторов и брать на обучение больше людей в одну группу. Темп мы тоже сбавили: слишком много студентов раньше бросало занятия из-за нехватки времени на учёбу и домашние задания. «Один CTO — мой студент недавно защитил свой проект перед техдепартаментом, — рассказывает ментор школы архитекторов Ирина Городилова. — И в этом ему помогло понимание принципов архитектуры, умение сравнивать разные решения и описывать их. Нынешняя школа архитекторов показывает, что эта профессия в первую очередь про ответственность, принятие решений, мышление. Студенты учатся использовать аспекты бизнес-архитектуры, выделять требования к системе, понимают стоимость разработки. А ещё они видят, как архитектура продукта помогает ему либо развиваться, масштабироваться, либо угасать». Мы уже сейчас можем сказать, что в среднем качество сотрудников растёт. Пока от некоторых проектов приходят 1–2 человека на поток, от других — 5–6. И работа нам предстоит долгая и кропотливая. Но она того, безусловно, стоит. Выпускать минимум 80% от поступивших против нынешних 66%. В этом помогут смена формата обучения с онлайн на запись, уменьшение темпа курса и набор более мотивированных учащихся. Собирать больше обратной связи от выпускников и централизованно отслеживать их судьбу и проекты. Доработать систему рекомендаций. Не все архитекторы кластеров ещё идеально пользуются таким инструментом получения сотрудников, как школа. Нам нужно объяснить, кому можно рекомендовать наше обучение, а кто ещё не готов к курсу. Расширить менторский состав, а значит, набирать больше студентов. Выйти на внешний рынок. Это повысит качество IT-специалистов в целом на рынке, а значит, и внутри компании тоже.",
    "135": "Универсальные типы в python являются незаменимым инструментом, который позволяет выявлять множество ошибок на моменте написания кода, а также делает код чище и элегантнее. Меня зовут Саша, и в своей работе часто сталкиваюсь с ситуациями, когда нужно создавать классы, работающие с различными типами, и при этом избегать дублирование кода, а также получать актуальные подсказки от type checker'а. В этой статье я рассмотрю различные примеры использования универсальных типов и постараюсь доступно описать, в чем разница между инвариантностью, ковариантностью и контравариантностью. Начнем с самого простого. Предположим, что у нас есть несколько типов документов: обычный и его расширение - складской. Ещё у нас есть реестр, который умеет работать с документами различных типов. Предположим, что есть некие методы или функции, которые хотят работать с нашим реестром документов. Нам не принципиально, что именно будет делать функция, назовем ее operation, якобы она выполняет любую операцию над реестром. В данной реализации type checker не нашел никаких ошибок. Рассмотрим вариант, в котором будем передавать реестр в функцию, умеющую работать со всеми подтипами документов реестра, содержащего только складские документы WarehouseDocument. Type checker сразу говорит нам, о том, что тип DocumentsRegistry[WarehouseDocument] не совместим с типом DocumentsRegistry[Document]. Ошибка выглядит странно, ведь тип WarehouseDocument является производным от типа Document, следовательно обязан реализовать все методы, атрибуты родительского типа и принцип подстановки Лисков не нарушается. Для того, чтобы разобраться почему type checker выдает ошибку, обратим внимание на следующую строку: Type parameter \"DocumentT@DocumentsRegistry\" is invariant, but \"WarehouseDocument\" is not the same as \"Document\". Она говорит о том, что универсальный тип DocumentT является инвариантом, но WarehouseDocument - не тоже самое, что Document. Для более корректного объяснения данной ошибки нужно понять, что такое инвариант и какие еще бывают отношения типов, а также немного погрузиться в теорию множеств. Для начала определимся с понятием \"тип\". Для этого проведем небольшое исследование и соберем несколько определений в различных источниках. Тип данных (тип) - множество значений и операций над этими значениями (IEEE Std 1320.2-1998). Тип данных - категоризация абстрактного множества возможных значений, характеристик и набор операций для некоторого атрибута (IEEE Std 1320.2-1998) Во всех определениях есть ключевое слово - множество, следовательно, тип - это тоже множество. Но как же так, во втором определении речь идет не о множестве, а о классе данных? Давайте посмотрим на определение слова \"класс\". Класс — термин, употребляемый в теории множеств для обозначения произвольных совокупностей множеств, обладающих каким-либо определённым свойством или признаком. И снова мы приходим к понятию множества. В рамках этой статьи будем думать о типе в программировании как о множестве, хотя в действительности эта тема достаточно спорная и по ней есть очень интересная статья. Теперь проведем параллель между наследованием/обобщением типов и множествами. Для множеств наследование будет эквивалентно выделению в множестве подмножества, а обобщению - переход от множества к надмножеству. Рассмотрим на простом примере. Для упрощения восприятия информации будем подглядывать в картинку, на которой изображены множества чисел и их отношения. По изображению видно, что рациональные числа являются обобщением целых чисел, так как множество рациональных чисел содержит все целые числа и дроби. И наоборот, натуральные числа являются уточнением целых чисел, так как множество целых чисел содержит все натуральные числа, а также 0 и отрицательные целые числа. Изобразим это в коде. Теперь можно смело переходить к видам отношений типов. Инвариантность - самое простое отношение типов, оно говорит о том, что тип A является типом B. Если рассматривать с точки зрения множеств, то множество A инвариантно множеству B, когда множество A полностью соответствует множеству B. То есть оба множества состоят из одних и тех же элементов. В нашем примере с документами, каждый складской документ WarehouseDocument является документом Document, но не каждый документ Document является складским документом WarehouseDocument. Следовательно, тип Document не является инвариантом по отношению к типу WarehouseDocument, о чем и говорит ошибка type checker'а. Ковариантность - отношение типов, при котором сохраняется иерархия типов в сторону уточнения, то есть все производные типы считаются совместимы с базовым типом. Рассмотрим на примере множеств. Для достижения ковариантного отношения множество B должно быть подмножеством для множества A. То есть множество A должно содержать все элементы из множества B. В примере с документами тип Document, определяющий все множество возможных документов, содержит в себе все элементы из множества определяемого типом WarehouseDocument. Это следует из того, что каждый складской документ WarehouseDocument является документом Document, но не каждый документ Document является складским документом WarehouseDocument. Ковариантность - это как раз то, что мы бы хотели видеть в примере из начала статьи. Давайте исправим ошибку и укажем, что наш универсальный тип является ковариантным. Для этого достаточно при создании переменной типа указать ключевой аргумент covariant=True. Теперь ошибки нет и type checker работает так, как мы от него ожидаем. Остался еще один вид отношения типов - контравариантность, он похож на ковариантность, но работает в обратную сторону. Иными словами, все обобщенные типы являются совместимыми с дочерним типом. Снова рассмотрим пример на множествах. Для контравариантности множество B должно быть надмножеством для множества A. Другим словами, множество B должно содержать все элементы из множества A. Говоря об универсальных типах, нельзя не упомянуть про возможность ограничивать переменные типов. Существует 2 варианта ограничения: через ключевой аргумент bound и через позиционные аргументы в переменной типа. Вернемся к нашему примеру с документами и рассмотрим один из вариантов использования ограничений. На этот раз у нас будет два типа документов: обычный базовый документ Document из прошлого примера и документ-черновик DocumentDraft. Каждый базовый тип образует свою иерархию наследования. В первом примере укажем ограничения через позиционные аргументы для переменной типа. Далее создадим реестр документов, который будет служить контейнером. Напишем функцию, которая будет принимать два реестра документов и возвращать новый реестр документов. Что будет делать функция - не важно. Главное - описать ее сигнатуру. Вызовем функцию, передав в нее экземпляры типа DocumentsRegistry, указав одинаковый универсальный тип. Данный код проходит проверку типов. Теперь попробуем дженерик в одном из аргументов заменить на тип WarehouseDocumentDraft. Из ошибки видно, что сначала type checker нашел базовые типы, а далее указал нам на то, что Document и DocumentDraft несовместимы. В данном случае type checker выбирает один из типов, указанных в позиционных аргументах переменной типа, затем пытается разрешить все типы аргументов функции от выбранного. Теперь переопределим переменную типа, используя указание ограничения через позиционный аргумент bound. Теперь код, который до этого выдавал ошибку при проверке типов, ошибок не выдает. В данном случает type checker уже разрешает типы от Union[Document, DocumentDraft], так как и тип Document, и тип DocumentDraft входят в множество Union[Document, DocumentDraft], проверка type checker'а проходит успешно. Таким образом, два варианта ограничения переменных типа позволяют писать более гибкий код. Разные варианты ограничения подходят под разные сценарии использования. Также стоит упомянуть, что переменные типа могут быть либо связанными (с использованием bound), либо ограниченными (с использованием позиционных аргументов), но не могут быть одновременно связанными и ограниченными. Тоже самое, как ни странно, относится и к отношению типов. Переменная типа может быть либо ковариантной, либо контрвариантной, но не может быть одновременно и ковариантной, и контрвариантной. Надеюсь, статья была для вас полезна, и вы узнали что-то новое.",
    "136": "Бизнес аналитик плотно работает с заинтересованными лицами на проекте и в курсе целей проекта и способов определения его успеха. Довольно часто важные для проекта люди могут быть перегружены работой или не настолько заинтересованы в нём, на сколько мы бы этого хотели. В этой статье я бы хотел расписать способы заинтересовать их в проекте с целью получения от них нужной информации и поддержки, необходимой для успешного завершения инициативы. #1 Информируйте. Самая простая вещь, с которой можно начать - это информировать о событиях на проекте. Приглашайте на ежедневные планёрки и другие созвоны, включайте их в копию важных переписок на проекте. #2 Будьте ведущим на встречах. Всегда показывайте, что готовитесь к встречам. Например, отправляйте список того, что бы вы хотели обсудить всем участникам до встречи и составляйте подробное описание встречи, а также давайте ей ёмкое название. Никогда не опаздывайте на созвоны/встречи, а лучше даже приходите/подключайтесь слегка заранее. #3 Идите навстречу. Если нужный Вам человек обычно занят, будьте готовы встретиться с ним и вне рабочего времени, если это единственный шанс застать его, чтобы обсудить важные детали по проекту. Такая открытость покажет им, что Вы сами заинтересованы в проекте. #4 Давайте им принимать решения. Если нужный Вам человек отстраняется от проекта, то есть вероятность, что это происходит по той причине, что он чувствует свою необязательность в данной активности, так что дайте им почувствовать, что их мнение действительно важно. Задавайте им вопросы, предлагая варианты реализации и просите из выбрать лучших вариант, который потом пойдёт в реализацию. Пишите и высылайте итоги встреч отдельным письмом всем участникам. #5 Доносите ценность. Каждый проект был начат, потому что он нацелен принести ценность компании. Одна из основных задач бизнес-аналитика понимать эту ценность и доносить её до всех связанных с проектом лиц. Правильное объяснение ценности проекта как для компании, так и для каждого отдельного человека может стать сильным инструментом, чтобы заинтересовывать людей помогать реализовывать проект. Ведь, люди могут не знать или попросту забыть, какую глобальную ценность приносит проект или та или иная задача. #6 Стройте доверенные отношения. Все мы в первую очередь люди, и у нас есть жизнь помимо работы. В идеале, личная жизнь не должна влиять на профессиональную, но такое бывает только в книжках и лозунгах, так что важно построить с важными людьми доверительные отношения, чтобы понимать их скрытые мотивы и быть готовым к их реакциям на те или иные ситуации, и чтобы понимать, как и когда можно к ним обращаться. Этому может помочь, например, поддерживание приветственных разговорах о погоде или планах на выходные в начале встреч. #7 Обучайте. Необходимые для проекта люди могут быть неактивными просто по той причине, что они не знают, что нужно для разработки ПО, и поэтому предполагать, что их участие не нужно. Рассказывайте им, почему вам нужна та или иная информация - это поднимет их заинтересованность в разговорах с Вами. #8 Презентуйте информацию просто и доступно. Старайтесь преподносить информацию максимально простыми словами, которые точно будут понятны слушателю. Важно также использовать визуальные элементы, если это возможно, так как их проще понять и усвоить большинству людей. Используйте для презентации знакомые им шаблоны и типы диаграмм. Если Вы не знаете, что им знакомо, хорошо будет прибегнуть к стандартам в диаграммах, например, к BPMN, UML или другим. Чем выше уровнь абстракции вы выберите для презентации, тем большее количество людей сможет её понять и усвоить информацию. #9 Организуйте доступность информации. Предоставьте всем связанным с проектом людям место, где они смогут получить всю информацию по проекту в любой момент времени, и поддерживайте информацию актуальной. #10 Найдите нужных людей. Проведите анализ заинтересованных лиц заранее - узнайте их влияние на проект, временную зону, в которой они работают, их руководителей и даже руководителей их руководителей. Соберите как можно больше информации о каждом участнике, чтобы построить правильный формат общения. #11 Будьте готовы эскалировать. Если нужный вам для проекта человек не отвечает слишком долго, всегда можно задать вопрос его руководителю или руководителю его руководителя. Но с такими коммуникациями нужно быть осторожным - каждый такой запрос ваше должен иметь реальное обоснование, должен быть сформирован максимально ясно и объективно, спокойным деловым языком. Таким образом вы обезопасите себя от возможного недовольства. #12 Разработайте план коммуникаций. Лучших способ обезопасить себя от возможного недовольства при эскалации - это составить план коммуникаций заранее, где определить ожидания по времени ответа каждого из участников, и обозначить точки и сроки эскалации. Это позволит избежать неприятных сюрпризов для всех участников процесса. #13 Обратиться к своему менеджеру тоже вариант. Хотя начиная с уровня старшего специалиста, от Вас ожидают максимальную самостоятельность, иногда эскалации, описанные выше не помогают, и тогда всегда есть вариант попросить вашего менеджера помочь Вам своими связями и влиянием, так как велика вероятность, что Ваш руководитель имеет доступ к людям выше уровнем. #14 Будьте гибкими. Также, нужно уметь подстраиваться под собеседника и выбирать тот стиль общения, который лучшим способом работает конкретно с этим человеком. Не существует серебряной пули ни в одном из процессов, особенно когда это касается общения с людьми. В английском языке есть слово - stakeholders, обозначающее заинтересованных лиц на том или ином проекте или активности. К сожалению, я не смог найти полноценного перевода этого термина на русских язык одним словом и с точно таким же значением. Поэтому в этой главе бы хотел начать его использовать как есть, надеюсь, мой читатель меня простит. Забавно, но когда я в первый раз заговорил с моей женой об этом термине, она была уверена, что stakeholders относится только к держателям акций, но на самом деле этот термин используется гораздо чаще. Stakeholder - это любой человек или даже юридический орган, который может повлиять на компанию, продукт или проект. Их можно разделить на внешних и внутренних. Владельцы акций компании. Кредиторы. #1 Выявить и упорядочить. Для начала, надо выписать список стратегических целей компании, в которой Вы как бизнес аналитик делаете проект или продукт. Затем, составить список stakeholders, доступных вам вместе с их сферами влияния. Потом подготовить список процессов, задач и проектов, за которые Вы несёте ответственность. Как финальный этап, совместить эти списки, чтобы определить, кто из людей вокруг Вас чем можем Вам помочь. #2 Охарактеризуйте заинтересованных лиц. Категоризировать и охарактеризовать заинтересованных лиц можно по следующим признакам: Какая информация им интересна. #3 Поймите их мотивы. Узнайте их амбиции на проекте и как каждый stakeholder выигрывает от успеха проекта или продукта. Поможет задавать себе вопросы: #4 Подстраивайтесь. Не пробуйте применить одну и ту же манеру общения ко всем заинтересованным лицам. Используйте информацию, которую удалось собрать о каждом stakeholder включая их свободное время, предпочитаемые способы общения для построения индивидуальной манеры общения. Не обязательно давать всем заинтересованным лицам весь объем информации в одной и той же форме, так как у них могут быть разные ожидания и потребности относительно получаемой информации. #5 Ценность. Старайтесь сделать каждое взаимодействие с Вами ценным для stakeholders. Не перегружайте их информацией - это может привести к их желанию быть подальше от проекта. В то же время, не следует давать им слишком мало информации - недостаток может либо привести к их неадекватному вмешательству в проект, либо к дистанциированию и нежеланию помочь там, где они могли бы. Не копите слишком много информации - всегда лучше синхронизировать их ожиданиями итеративно. #6 Установите расписание. Лучше всего договориться о взаимно удобном и соответствующем нуждам проекта расписании общения как можно раньше, и придерживать этой периодичности. Вполне вероятно, что это расписание будет разным для разных заинтересованных лиц. Например, для внешних stakeholders как правильно периодичность общения необходима ниже, чем для внутренних. #7 Устанавливайте сроки. Всегда обозначайте сроки, когда вам нужно что-то получить от заинтересованных лиц. Даже если вы не знаете их точно, обозначьте хоть что-то, от чего можно будет отталкиваться, иначе задача рискует быть не сделанной, пока не будет уже слишком поздно. Также, это облегчит планирование для людей, с которыми Вы взаимодействуете. #8 Запрашивайте обратную связь. Следите за реакцией stakeholders во время общения с вами, и подстраивайте свои процессы и стиль общения в зависимости от этой реакции, чтобы сделать работу максимально эффективной и долговременной. Кроме того, нет ничего плохого в том, чтобы спросить людей вокруг Вас том, как им с Вами работается. Это покажет вашу заинтересованность и открытость, что всегда распологает к себе. #9 Встречайтесь один на один. Часто кажется, что проще собрать всех в одной комнате или на одном созвоне, чтобы обсудить все вопросы, но такие встречи в большом составе далеко не всегда эффективны. Встречи один на один помогают построить более доверительное общение, и получить более точную и развёрнутую информацию. #10 Эскалируйте. Заранее узнайте, кому Вы можете эскалировать проблему, если не получается решить что-то самому. Согласуйте процесс эскалации со всеми участвующими в нём заинтересованными лицами, чтобы избежать неприятных сюрпризов. Стройте эскалацию на фактах и объективных доводах. Не редкость, что заинтересованные лица на проектах могут не соглашаться о чём-либо, как что-то должно быть разработано или в каком порядке. По моему опыту бизнес аналитик может прибегнуть к следующим стратегиям в таких случаях: #1 Сначала один на один. Если собрать не согласных stakeholders в одной комнате, то это может только усугубить спор, перевести его в эмоциональное русло, как итог замедлить решение вопроса. К такой встрече Вам, как бизнес аналитику, нужно быть хорошо подготовленным, чтобы быть в состоянии увести её в продуктивное русло. Для этого следует встретиться со всеми сторонами сначала один на один, и максимально подробно изучить позицию каждого заинтересованного лица. #2 Миссия компании. Миссия и цели компании могут стать универсальным доводом в споре о правильном решении, ведь, все спорящие стороны работают в этой одной компании или проекте. Изучите миссию и цели, чтобы быть готовыми использовать их для решения вопроса и поиска компромисса. #3 Мы все здесь, чтобы решить проблему. Во время спора заинтересованных лиц, попросите каждую сторону раскрыть свою позицию, задавая открытые вопросы. Делайте выводы на основании цели проекта или миссии компании, напоминая всем о них. #4 Уважайте каждое мнение. Никогда не позволяйте никому почувствовать, что Вы игнорируете или Вам не интересна чья-либо позиция. Даже самые неочевидные идеи или споры могут привести к лучшему продукту. #5 Спрашивайте, не убеждайте. Всегда есть соблазн навести порядок жесткой рукой, полагая, что Вы сами знаете, как лучше, но действительно хорошие вещи делаются группами людей с разным опытом и взглядами. Во время спора мы часто склонны просто к попыткам убедить человека, но вместо этого гораздо эффективнее это получиться путем задачи правильных вопросов. Например, можно спрашивать, как определённое решение скажется на репутации и долгосрочных целях компании. Работа с людьми всегда требует индивидуального подхода, так что я не удивлюсь, если моих советов будет мало для решения той или иной ситуации, но я надеюсь, что хотя бы один пункт вам поможет.",
    "137": "Привет, Хабр! Меня зовут Антон, работаю бизнес-аналитиком в компании GlowByte. Некоторое время назад я проводил любительские исследования инфополя и его влияния на человека, а теперь хочу поделиться своими наблюдениями. Если вы никогда не интересовались этой темой, то вот вам простая мысль для затравки: медиасфрера – это такой же элемент окружающей среды, как солнце, воздух и вода. Они, как известно, наши лучшие друзья, правда, есть одно но: если вы умеете загорать, плавать и вообще вести себя на природе. В современных реалиях получить по голове пропагандой гораздо легче и доступнее, чем получить солнечный удар. Ежедневно нас окружают миллионы байт информации. Утро начинается не с кофе, а с прочтения многочисленных уведомлений от сервисов и социальных сетей. Едва взяв в руки телефон, открыв интернет-браузер, включив телевизор, мы погружаемся в пучину контента и зачастую уже не можем вернуться к своим мыслям. Не всем хватает воли для того, чтобы отложить свой контентоприёмник в сторону и вернуться из завораживающих глубин в этот обычный мир. Из-за того, что разнообразного медиа становится всё больше, а способы доставки постоянно удешевляются, объём потребляемой информации растёт, а мы незаметно для себя перегружаем психику, которая не может эволюционировать со скоростью появления новой модели iPhone. Тем более эволюция не готовила нас к тому, что поступающая информация будет настолько технологичной. Мы смотрим картиночки, тщательно подобранные алгоритмом исходя из анализа наших предпочтений. Читаем новости, снабжённые цепляющим заголовком, профессионально упакованные согласно редакционной политике. Мы смотрим видео, специально смонтированное так, чтобы удерживать внимание заданное количество секунд.В HD-качестве, на любой платформе, в любую секунду, безлимитно. Изобилие, доступность и технологичность контента не оставляют пользователям шансов удержаться от его безудержного потребления. На это накладывается тот факт, что никто не учит нас работе с информацией, техникам её восприятия и анализа. Как следствие, общество практически беззащитно перед глобальной машиной контента. Проблема усугубляется ещё и фактором инфляции информации: зачастую потребляемый контент теряет свою актуальность уже через мгновение. Это похоже на золотой песок, который обращается в золу от первого прикосновения. Мы тратим ресурсы и время на пролистывание бесконечной ленты, но что осталось за верхней границей экрана - уже не важно. Мы превращаем время своей жизни в трафик. В совокупности эти факторы приводят к целому спектру проблем. Выберите по вкусу: ослабление функций мышления, утомляемость, рассеянность внимания, гиподинамия, выгорание, и это только вершина айсберга. Подробнее об информационной перегрузке можно почитать здесь и здесь. Несколько лет я изучал бушующую инфопучину современности и пришёл к четырём стратегиям “выживания” в ней. О них и расскажу далее. Не делать ничего, погружаясь в пучину информации, и нестись по её течению. «Сёрфить», переключая каналы информации, как каналы на пульте телевизора. Подписаться на случайные сообщества в социальных сетях и каналы в Telegram, которые вызывают интерес в моменте. Листать запрещённый на территории Российской Федерации Instagram, пока ужинать не позовут, и доверять трендам YouTube. возможность поддержать разговор на любую актуальную тему из интернета. иллюзия информированности – ситуации, когда само событие, как факт, скрывается под слоями информационного шлака после прочтения множества интерпретаций из множества источников. При этом возникает ощущение, что, раз информации много, значит и знаний о событии тоже много. Это не так. Как и в случае с едой, переизбыток приводит к тому, что потребляемое не успевает ни перевариться, ни усвоиться, нанося вред потребителю. Иллюзия информированности может сыграть злую шутку: неадекватное восприятие действительности рано или поздно заканчивается конфликтом. В лучшем случае – конфликтом с этой самой действительностью. Чётко разделить входящий поток информации на ту, которая подлежит потреблению, и ту, которая должна пройти мимо. Ограничить себя от ряда источников и тем. Не вступать в споры без железобетонного понимания контекста. Определить часы тишины, в которые потребление информации должно быть сведено к нулю. Читать художественную литературу вместо актуального контента. Например, Чехова или Пришвина. Они давно уже про всё написали. снижается риск попасться на случайный фейк, потому что основная масса фейков отсеется вместе с источниками – “фейкомётами”. выборочное восприятие информации приводит к тому, что формируется набор «проверенных», привычных источников. Своеобразная информационная гавань, защищённая от иных течений. С повышением уровня доверия к таким источникам снижается и уровень критического восприятия их контента. Так можно авторитетно поверить в любой фейк,  главное, чтобы он был размещён в «правильном» источнике; слабая способность к восприятию контента источников, не относимых к группе «доверенных». Предвзятость и узость в суждениях, нежелание рассмотреть картину события шире; снижение способности к идентификации фейка из областей, попавших в «слепые зоны»: те темы новостей, восприятие которых было умышленно ограничено. Легко поверить в обман там, где знаний нет. Следует добавить, что “информационная диета” может быть вольной или невольной. Если субъект не имеет возможности пользоваться многими источниками в силу ограничений, это тоже можно назвать вынужденной диетой. Тем не менее не употреблять тот контент, который без последствий можно не употреблять, знать и соблюдать свою дневную меру потребления – уже дорогого стоит. *- говоря на чистоту, полная изоляция в современных условиях практически невозможна. Отключить смартфон на выходные, отключить все уведомления. Использовать смартфон как... телефон. Или вовсе завести кнопочный телефон (кстати, интересный опыт). Не включать телевизор. Читать соцсети не более часа в день или совсем не читать и не регистрироваться в них. Как минимум, не пользоваться теми соцсетями, в которых нет необходимости. Пользоваться привилегией живого общения. снижение зависимости от гаджетов. снижение навыка восприятия и критического анализа информации. Как результат – подверженность быстрой ангажированности на выходе из «изоляции». Иными словами – легковерность; «информационный шок» при попадании в агрессивную информационную среду после периода изоляции. Мечта-мем – уединённый домик без интернета и телевизора. Так ли она хороша, учитывая, что всё равно придётся вернуться? Ведь полная изоляция от поступающей информации давно технически невозможна (экстремальные варианты вроде отшельничества в сибирской тайге не берём). С другой стороны – вспомните хотя бы пару актуальных новостей, будораживших ваше сознание пару месяцев назад? Какова их ценность сейчас? Изучать информацию, алгоритмы и законы, по которым она работает. В меру ограничивать себя, тренировать критическое мышление и навыки восприятия информации. Анализировать своё восприятие. Проверять источники и достоверность информации. Верить только фактам, а факты тщательно проверять. Использовать силу (фактов). Есть несколько способов проверять информацию. Сообщите в комментариях свои способы, и если эта тема зайдёт, посвятим ей отдельный пост. растущая возможность сравнительно чёткого понимания происходящего. необходимость постоянно быть в курсе событий. Иначе можно пропустить значимое событие, сформировать неверные суждения, сделать ошибочные выводы (это неизбежно, но нужно стремиться этого не допускать); загруженность сознания постоянным анализом событий, происходящих в информационном поле, и анализом потребляемой информации. Это самая сложная стратегия, дающая наибольший профит. Путь джедая труден, но он вознаграждается высшей наградой – адекватной картиной мира. Истинным просветлением, если хотите. Остаётся добавить, что перечисленные стратегии сложно применить по отдельности. Скорее всего, задачей каждого является не выбор, а индивидуальное осмысление и компиляция стратегий (или же их элементов), дающая наибольший эффект. При этом в качестве критериев успеха можно выбрать три вектора: способность ощущать себя счастливым. Мерой ценности контента может служить ваше личное продвижение по выбранному пути развития (конечно, если он у вас есть). То направление, в котором вы достигаете мастерства.Мерой адекватности восприятия мира может служить количество неожиданных откровений и внезапных открытий на вашем жизненном пути. Ну, а как понять в чём ваше  счастье, вы и сами знаете. В конце концов не к нему ли мы стремимся?Буду рад, если мои мысли найдут отклик. Если есть чем поделиться (о чём подискутировать) после прочтения – добро пожаловать в комментарии!",
    "138": "Прошёл ровно год с момента релиза модели Kandinsky 2.1 — именно эта модель принесла известность нашей исследовательской группе Sber AI Research и дала толчок развитию всей линейки моделей Kandinsky. В честь этой даты мы выпускаем новую версию модели Kandinsky 3.1, о которой я расскажу подробнее в этой статье. В 2023 году мы все наблюдали очень большой прогресс в области исследования языковых, мультимодальных и генеративных моделей. Этот прогресс коснулся и привычной нам модальности изображений (Kandinsky 2.1, 2.2, 3.0, Stable Diffusion XL, IF, Шедеврум, MJv6 и др.), и модальности текстов (ChatGPT, GPT-4, LLaMA, Falcon, GigaChat и др.), и аудио (VALL-E, MusicLM и др.), и 3D (Magic3D и др.), и даже модальности видео (Kandinsky Video, Gen-2, CogVideo и др.). В 2024 всё движется вперёд ещё более впечатляющими темпами: выходят на абсолютно новый уровень модели генерации изображений (SD3, SD3-Turbo), видео (Sora), музыки и целых песен (Suno) и т. д. При этом все основные игроки стараются равномерно развиваться и повышать качество синтеза. Текстовые чат‑боты научились взаимодействовать с внешними системами посредством плагинов, синтез изображений вышел на уровень фотореалистичных генераций, длина генерируемых видео постепенно увеличивается с сохранением сюжетной связности между кадрами (вполть до 1 минуты у Sora). И такой прогресс обусловлен уже не только наращиванием вычислительных мощностей, но и большим числом неординарных архитектурных решений, которые позволяют добиваться лучшего качества, а также сложными технологиями инженерии данных, обеспечивающими создание огромных и в то же время очень качественных датасетов для обучения моделей. В ноябре прошлого года на конференции AI Journey наша команда представила text-to-image модель нового поколения Kandinsky 3.0. В целом, пересмотрев архитектуру и взяв более мощный текстовый энкодер (по сравнению с семейством моделей Kandinsky 2.X), нам удалось добиться значительного роста в качестве изображений с точки зрения реалистичности и детализации, улучшить понимание текста и побить качество модели SDXL на side-by-side-сравнении с точки зрения человеческих предпочтений. Это является наиболее показательной и справедливой метрикой качества в задачах генерации любой модальности (хотя у этой методологии тоже есть свои минусы). Подробнее о модели Kandinsky 3.0 можно прочитать в этой статье. Одновременно с этим на базе этой модели мы выпустили первую российскую модель генерации видео по тексту Kandinsky Video, о которой можно больше узнать здесь. Сегодня мы представляем модель Kandinsky 3.1 — идейное продолжение модели Kandinsky 3.0, которую мы улучшили и обогатили набором различных полезных функций и режимов, предоставляющих пользователям больше возможностей полноценно использовать всю силу нашей новой модели. В Kandinsky 3.1 входит сразу несколько улучшений, о каждом из которых я расскажу подробнее в статье: дистилляция по числу шагов диффузии (Kandinsky 3.0 Flash); IP-Adapter, который позволяет дополнительно (помимо текста) обуславливаться на изображение (за счёт этого удалось вернуть режимы смешивания изображений, изображения и текста, которые были в версиях Kandinsky 2.X, но работали там за счёт наличия в этих моделях специального блока image prior); ControlNet — механика, реализующая способы дополнительного обуславливания (контроля за генерацией) на основе canny edges, depth maps и т. д.; SuperRes — специальная диффузионная модель, повышающая разрешение изображения (так, в Kandinsky 3.1 теперь можно генерировать 4K изображения). Кроме этого, мы обучили маленькую версию модели Kandinsky 3.0 Small (1B), с которой более просто и удобно экспериментировать. Серьёзной проблемой модели Kandinsky 3.0, как и всех диффузионных моделей, была скорость генерации. Для получения одного изображения необходимо было пройти 50 шагов в обратном процессе диффузии, то есть 50 раз пропустить данные через U-Net с размером батча 2 для classifier free guidance. Для решения этой проблемы мы использовали подход Adversarial Diffusion Distillation, впервые описанный в статье от Stability AI, но с рядом существенных модификаций: В случае использования предобученных пиксельных моделей в качестве дискриминатора возникла бы необходимость декодировать сгенерированную картинку при помощи MoVQ Decoder и пробрасывать через него градиенты, что привело бы к огромным затратам памяти. Данные затраты не позволили бы обучать модель в разрешении 1024 × 1024. Поэтому в качестве дискриминатора мы использовали замороженную downsample-часть U-Net от Kandinsky 3.0 с обучаемыми головами после каждого слоя понижения разрешения. Это связано с желанием сохранить возможность генерировать картинки в высоком разрешении. Мы добавили Cross Attention на текстовые эмбеддинги от FLAN-UL2 в головы дискриминатора вместо добавления текстового CLIP эмбеддинга. Это позволило улучшить понимание текста дистиллированной моделью. В качестве лосс-функции мы использовали Wasserstein Loss. В отличие от Hinge Loss он является ненасыщаемым, что позволяет избежать проблемы зануления градиентов на первых этапах обучения, когда дискриминатор оказывается сильнее генератора. Мы убрали регуляризацию в виде Distillation Loss, так как по нашим экспериментам она не оказывала существенного влияния на качество модели. Мы обнаружили, что довольно быстро генератор становится сильнее дискриминатора, что приводит к нестабильности обучения. Чтобы решить эту проблему, мы значительно увеличили learning rate у дискриминатора. У дискриминатора он равнялся 1e-3, в то время как у генератора — 1e-5. Для предотвращения расходимости мы также использовали gradient penalty, как и в оригинальной работе. Обучение происходило на «эстетичном» (отобранном руками) датасете размером 100K пар «текст-изображение», который является подсетом датасета для претрейна Kandinsky 3.0. В результате этого подхода получилось ускорить Kandinsky 3.0 почти в 20 раз, сделав возможной генерацию изображения всего за 4 прохода через U-Net. Также на скорость повлиял тот факт, что теперь нет необходимости использовать classifier free guidance. Kandinsky 3.0 из диффузионной модели по факту превратился в GAN (Kandinsky 3.0 Flash), обученный с хорошей начальной инициализацией весов после претрейна. Однако для серьёзного ускорения пришлось пожертвовать качеством понимания текста, что показывают результаты side-by-side (SBS) сравнения. SBS проводится на фиксированной корзине запросов из 2100 промптов (100 промптов по каждой из 21 категорий). Каждая генерация оценивается по визуальному качеству (какое из двух изображений вам больше нравится) и по соответствию тексту (какое из двух изображений лучше соответствует запросу). Про методологию side-by-side (SBS) сравнения можно подробнее прочитать в статье Kandinsky 3.0. Приведём примеры изображений, сгенерированных моделью Kandinsky 3.0 Flash. Если вы пользовались прошлыми версиями Kandinsky или другими моделями генерации изображений по тексту, то вы скорее всего замечали, что чем подробнее написан запрос, тем красивее и детальнее получается картинка. Это происходит из-за того, что в обучающей выборке чаще встречались достаточно подробные описания изображений. Однако в реальности большинство запросов пользователей очень короткие и содержат мало подробностей об объектах генерации (по понятной причине — не у всех пользователей есть время долго подбирать нужный им промпт). Для решения этой проблемы в Kandinsky 3.1 была встроена опция бьютификации запроса — способ улучшения запроса (добавление в него деталей) пользователя с помощью большой языковой модели (LLM). Бьютификация работает очень просто: на вход языковой модели подаётся инструкция с просьбой улучшить запрос, а далее ответ модели подается на вход Kandinsky для генерации. ### System:\\nYou are a prompt engineer. Your mission is to expand prompts written by user. You should provide the best prompt for text to image generation in English. \\n### User:\\n{prompt}\\n### Assistant:\\n Тут {prompt} — это запрос, который написал пользователь. Примеры генераций на один и тот же запрос, но с бьютификацией и без неё представлены ниже (с помощью модели Kandinsky 3.0). Также мы провели side-by-side сравнение качества генераций, сделанных Kandinsky с использованием функции бьютификации запроса и без него. Мы провели тестирование и для Kandinsky 3.0, и для Kandinsky 3.1, чтобы оценить, как сильно влияет языковая модель на генерируемые изображения. Сравнение Kandinsky 3.1 (Flash + улучшение промпта) с предыдущими версиями: Сравнение Kandinsky 3.1 (Flash + улучшение промпта) с другими моделями: Также интересно проследить эволюцию моделей Kandinsky, начиная с Kandinsky 2.1: В отличие от модели Kandinsky 3.0, в версию Kandinsky 3.1 мы внедрили функционал генерации изображения не только с помощью текстового запроса, но и/или с помощью визуальной подсказки в виде подаваемого на вход изображения. Это позволяет редактировать уже имеющееся изображение, изменять его стиль и добавлять к нему новые объекты. Для этого мы использовали IP-Adapter — подход, продемонстрировавший хорошие результаты в сравнении с традиционным дообучением. Для реализации IP-Adapter-а на основе нашей базовой модели генерации и её имплементации в библиотеке diffusers мы использовали адаптеры внимания. В качестве энкодера изображений был взят ViT-L/14 (Visual Transformer, дообученный в пайплайне CLIP). C его помощью мы получаем эмбеддинги размера batch_size x 768, которые потом, посредством линейного слоя, преобразуются в тензоры размером batch_size × 4 × 4096. Добавив пару новых слоев для key и value изображений в механизме cross attention, мы складываем выход обычного текстового cross attention с выходом cross attention для изображений. Обучение производилось на датасете COYO-700M с размером батча 288 в течение 800K итераций. Вариация изображения. Чтобы сделать вариацию изображения, мы просто считаем эмбеддинги изображения с помощью CLIP-ViT-L/14 и подаём их в модель. Смешивание изображений. Здесь мы считаем эмбеддинги для каждого изображения и складываем их с заданными весами, после чего результат подаётся в модель. Смешивание изображения и текста. Мы считаем эмбеддинги изображения и подаём их в модель вместе с текстом, поскольку мы сохранили стандартный cross attention на текст. Мы обнаружили, что подход на основе IP-адаптера не сохраняет форму объектов на изображении, поэтому мы решили обучить ControlNet в дополнение к нашей модели генерации для консистентного изменения внешнего вида изображения, сохраняющего больше информации в сравнении с исходным. В качестве модели для получения границ на изображении, подающихся на вход ControlNet, мы использовали HED detector. Обучение длилось 5000 итераций на датасете COYO 700m на 8 GPU Tesla A100 с размером батча 512. В Kandinsky 3.1 Inpainting мы сфокусировались на улучшении качества модели генерации объектов. В Kandinsky 3.0 мы учили модель восстанавливать изображение по его исходному описанию — из-за этого модель очень хорошо восстанавливает исходное изображение, но когда дело доходит до создания другого объекта на месте старого, то модель может не справиться. Один из способов исправления этой проблемы — дообучение на масках из датасетов для задачи object detection или segmentation (например, Paint by Example или SmartBrush). При использовании bounding box масок модель учится генерировать изображения чётко по текстовому запросу, а не по описанию целой картинки. Таким образом, качество модели при использовании её людьми растёт —именно в таком формате модель Inpainting и используется на инференсе. Чтобы модель не разучилась делать и «классический» inpainting (по полным описаниям изображения), мы сбалансировали наши обучающие сеты — 50% масок приходят с bounding boxes, а оставшиеся 50% выбираются случайным образом, как было раньше в Kandinsky 3.0 Inpainting. Поскольку в качестве текстовых запросов мы используем только имена классов, то модель может разучиться генерировать изображения по длинным запросам. Поэтому мы решили доразметить наш датасет с помощью LLaVA 1.5. Для этого, после выбора bounding box, который мы использовали в качестве маски, мы подавали crop-изображения в LLaVA, чтобы получить текстовое описание этого кусочка изображения. Далее это текстовое описание использовалось как текстовый запрос. Также мы провели сравнение с другими моделями, чтобы «оцифровать» качество нашего нового метода inpainting-а. Для этого мы взяли датасет COCO, случайным образом выбрали из него 1000 изображений и по одному объекту с каждого изображения, который мы перегенерировали. Далее на полученных картинках мы прогнали модель, обученную на датасете детекции YOLO-X, и посчитали её метрики качества детекции. Если детектор, обученный на реальных изображениях, сможет задетектировать сгенерированный объект, то можно сделать вывод, что объект сгенерирован достаточно естественно. Ниже приведены метрики: Kandinsky 3.0 (Ours) Kandinsky 3.1 (Ours) Из таблицы следует, что наша модель отлично справляется с дорисовыванием больших объектов (да и в целом её качество при добавлении любых объектов очень хорошее). В новой версии Kandinsky 3.1 появилась возможность получать генерации изображений в разрешении 4K. Для этого была обучена диффузионная модель повышения разрешения KandiSuperRes. За основу была взята архитектура Kandinsky 3.0, но с некоторыми модификациями. Вместо латентной диффузии была реализована пиксельная диффузия, чтобы исключить потерю качества при кодировании и декодировании картинки автоэнкодером. Помимо этого, в ходе экспериментов было выявлено, что для данной задачи пиксельная диффузия сходится быстрее и лучше, чем латентная. Вместо обычного U-Net был реализован EfficientUnet подобно подходу, описанному в статье Imagen. В сравнении с U-Net Kandinsky 3.0, EfficientUnet потребляет меньше памяти и также имеет лучшую сходимость. Отличие в том, что вместо 3 Residual блоков на каждом понижении разрешения в EfficientUnet используется большее количество блоков на низких разрешениях и меньшее количество блоков на высоких. Также изменяется порядок выполнения свёртки и downsampling/upsampling-операций относительно исходного U-Net. Кроме этого, мы убрали обусловливание на текстовый промпт, так как он не вносит вклад в генерации в высоком разрешении 4К. В итоге EfficientUnet KandiSuperRes содержит 413M параметров. Во время обучения EfficientUnet предсказывает не уровень шума в данный момент времени t, как это обычно принято при обучении диффузионных моделей, а x_0 (то есть исходную картинку), что позволило избежать проблем, связанных с изменением цвета сгенерированной SR картинки. Обучение осуществлялось в 2 этапа: сначала модель училась на датасете LAION на 32 A100 в течение 1,57М шагов с размером батча 2 на разрешение 256 → 1024. Затем модель училась на «эстетичных» сетах высокого разрешения, используемых при обучении Kandinsky 3.0, в течение 1,5М шагов. На втором этапе обучения было добавлено JPEG-сжатие подобной той схеме, которая описана в статье Real-ESRGAN. Модель KandiSuperRes позволяет работать с изображениями различного разрешения, однако основной целью являются генерации в высоком разрешении 4K. Так как модель KandiSuperRes обучалась задаче увеличивать разрешение с 256 до 1024, а на больших разрешениях обучать не было возможности из-за переполнения памяти A100, для генерации в 4К использовался алгоритм MultiDiffusion, позволяющий создавать панорамы. Суть алгоритма заключается в том, что изначально изображение делится на перекрывающиеся патчи, и затем на каждом шаге диффузии удаляется шум, а значения пикселей/латентов перекрывающихся областей усредняются. И таким образом, пройдя все шаги диффузии, мы получаем бесшовное изображение любого разрешения. В итоге на инференсе модель KandiSuperRes работает за 5 шагов, используя DPMSolverMultistepScheduler. Время инференса для генерации изображения в 4К занимает 13 секунд, а в 1К — 0,5 секунды. В таблице ниже приведено сравнение KandiSuperRes с моделями Real-ESRGAN и Stable Diffusion x4 Upscaler по метрикам FID, SSIM, PSNR и L1 на датасетах Wikidata 5К, RealSR(V3) и Set14. Wikidata 5К содержит 5000 изображений, собранных из Википедии, в разрешении 1К. RealSR(V3) содержит 100 тестовых изображений в разрешениях 1K и 2K. Set14 содержит 14 изображений в низких разрешениях с JPEG-артефактами. В итоге модель KandiSuperRes показала наилучшие результаты. 9.96 24.48 0.73 0.0428 3.04 25.05 0.67 0.0435 0.89💥 28.52💥 0.81💥 0.0257💥 73.26 23.12 0.72 0.0610 47.79 24.85 0.67 0.0493 47.37💥 25.05💥 0.75💥 0.0462💥 115.94 22.88 0.62 0.0561 76.32 23.60 0.57 0.0520 61.00💥 25.70💥 0.70💥 0.0390💥 На рисунке 1 приведены примеры генераций моделей KandiSuperRes, Stable Diffusion и Real-ESRGAN в разрешение 1024. На рисунке 2 приведены примеры генераций KandiSuperRes в разрешение 4К. Код и веса KandiSuperRes выложены на Github и HuggingFace. Чтобы сделать нашу модель более доступной для запуска и дообучения при малых вычислительных ресурсах, мы решили обучить её маленькую версию (Kandinsky 3.0 Small). Для неё мы взяли U-Net, содержащий 1B параметров, и текстовый энкодер от FLAN-T5 XL. Обучение происходило в течение 1М итераций на датасете COYO-700M с размером батча 2048. Внизу представлены примеры генераций с помощью этой модели. В этой статье мы представили новую версию нашей диффузионной модели генерации изображений по тексту Kandinsky 3.1, которую мы сделали более эффективной с помощью современных методов дистилляции и дополнили новыми режимами работы, которых не было в версии Kandinsky 3.0, — возможностью редактирования изображений и переносом стиля. Дополнительно мы улучшили качество работы inpainting-а — метода повышения разрешения для перехода в пространство пикселей из пространства скрытых представлений. Кроме того, помимо доступа к основной большой модели, мы предоставили пользователям возможность работы с версиями нашей модели с меньшим числом параметров, что гораздо удобнее для локального запуска и дообучения при малых ресурсах для своих целей и задач. Можно сказать, что наше основное достижение — это то, что мы сделали творческую генерацию гораздо более удобной для пользователя, который теперь может полноценно использовать весь потенциал нашей модели. Как и всегда, мы ценим открытые исследования и охотно делимся ими с вами, чему вновь стала примером данная статья. Мы предоставляем доступ к нашей модели, её функциям, коду и обученным весам совершенно бесплатно для любого пользователя, стимулируя тем самым развитие области генеративного искусственного интеллекта. Однако, разумеется, следует помнить об ответственности каждого, кто решает использовать генерации изображений в своих целях. Мы решительно выступаем против использования нашей модели вне закона, в частности для дезинформации, оскорбления, разжигания ненависти, мошенничества и т. д. Мы уверены, что развитие искусственного интеллекта и непрерывная работа нашей команды направлены на мирное и благополучное созидание и улучшение качества труда и жизни наших пользователей. Искусственный интеллект все больше входит в нашу жизнь и становится нашим полноценным помощником. Возможно, в ближайшее время модели генерации выйдут на совершенно иной уровень, комбинируя в себе возможности целых мультимедийных центров, оперирующих различными визуальными (и не только) модальностями. Наша команда активно вовлечена в подобные исследования, мы держим руку на пульсе, следим за самыми последними новинками и намерены продолжать радовать сообщество нашими достижениями. Шагайте и дальше в светлое и умное будущее за руку с технологиями и следите за нашими обновлениями! Скоро все желающие смогут протестировать новые возможности нейросети. Как и предыдущие версии, модель будет бесплатной и доступной на разных поверхностях. Модель Kandinsky 3.1 разработана командой Sber AI при партнёрской поддержке учёных из Института искусственного интеллекта AIRI на объединённых датасетах Sber AI и компании SberDevices. Коллектив авторов: Владимир Архипкин, Андрей Филатов, Вячеслав Васильев, Анастасия Мальцева, Игорь Павлов, Михаил Шойтов, Юлия Агафонова, Николай Герасименко, Анастасия Лысенко, Илья Рябов, Саид Азизов, Антон Букашкин, Елизавета Дахова, Татьяна Никулина, Сергей Марков, Андрей Кузнецов и руководитель научной группы Generative AI Денис Димитров. Igrek.log По всем возникающим вопросам и предложениям по развитию модели, добавлению новых возможностей и сотрудничеству в части использования наших разработок можно писать мне и Андрею. Пока что Kandinsky 3.1 доступен для ограниченного круга пользователей. При этом широкий доступ к модели и к её полному функционалу будет предоставлен в самое ближайшее время. fusionbrain.ai (доступна генерация по тексту и inpainting) Telegram-bot (доступна генерация по тексту, а также есть доступ к моделям линейки Kandinsky 2.X и их полному функционалу) rudalle.ru (доступна генерация по тексту) Project Page (description of Kandinsky 3.1 in English)",
    "139": "При отслеживании киберугроз мы, специалисты экспертного центра безопасности Positive Technologies, в очередной раз засекли ранее неизвестную APT-группу. Хакеры орудуют в России, Беларуси, Казахстане и Армении, а также в Средней Азии (Узбекистане, Кыргызстане и Таджикистане). По нашим данным, от их атак пострадали организации в государственном и финансовом секторах, в сфере образования и медицины. Всего было скомпрометировано около 870 учетных записей сотрудников. На этот раз нас удивил почерк группировки, который можно описать как «сложно не значит лучше». Киберзлодеи выделяются тем, что добиваются успеха, не прибегая к сложному инструментарию, сложным тактикам и техникам. Их основное оружие — примитивный стилер, написанный на Python. Вредоносная программа попадает к жертве при помощи старого доброго фишинга и отправляет украденные данные телеграм-ботам. Ознакомиться с полным отчетом, как обычно, можно в нашем блоге, а ниже вас ждет разбор вредоноса и немного спойлеров. Новую хакерскую группировку мы назвали Lazy Koala из-за простых техник нападения, а также из-за имени пользователя, управлявшего ботами, — Koala Joe. «Ленивая коала» занимается поеданием эвкалипта шпионажем и кражей данных. Во вредоносной кампании против ранее упомянутых организаций число скомпрометированных учеток достигло 867, из них уникальных — 321. Мы уведомили всех пострадавших и предполагаем, что судьба украденных данных — перепродажа или использование в последующих атаках на внутренние структуры компаний. Хотя точный вектор нападения нам определить не удалось, косвенные признаки указывают на фишинг. Киберпреступники умело втираются в доверие к получателям сообщений, убеждают их открыть вложение и запустить нужный файл в браузере. Оценить их мастерство позволяют найденные нами документы. Любопытно, что некоторые из них были специально написаны на национальном языке жертв. Анализ техник и инструментов в арсенале злоумышленников, также как анализ географического расположения пострадавших компаний, не показал связей с уже известными APT-группами. Визитная карточка Lazy Koala — новый, ранее никем не описанный стилер собственной разработки. Он настолько прост в исполнении, что по аналогии с группой был назван LazyStealer. Анализ и принцип его работы подробно описаны в исследовании. Если коротко: стилер похищает логины и пароли из браузеров и отправляет их телеграм-ботам. Так, в зафиксированных нами атаках LazyStealer охотился за учетными данными, которые сотрудники компаний сохраняли в Google Chrome. Выбор мессенджера для пересылки тоже неслучаен: с каждым годом Телеграм завоевывает все большую популярность у злоумышленников. Тем не менее у стилера довольно хорошая защита, которая позволяет уклоняться от выявления средствами безопасности и замедляет анализ для исследователя. Любопытно, что нам не удалось обнаружить механизм закрепления LazyStealer. Это может говорить о том, что он либо является частью цепочки атаки, либо создан, чтобы не оставлять следов. Если верен последний вариант, то атака «ленивой коалы» превращается в одноразовую — это играет на руку киберпреступникам, поскольку помогает лучше скрываться. Все встреченные нами образцы ВПО в качестве упаковщика используют PyInstaller, после снятия которого в главном скрипте весь код накрыт протектором Pyarmor. Для снятия протектора нам понадобился скрипт bypass.py, установленный модуль Pyarmor, а также файл pytransform.py из этого модуля. Этот файл нужно разместить в той же папке, что и целевой файл с расширением .ру. После снятия протектора в одном из вариантов образцов мы встретили скрипт. Все, что он делает, — это подключает Python-модули. Выделенные на рисунке модули, которые в файловой системе имеют названия hello.cp39-win_amd64.pyd и pdfbyte.cp39-win_amd64.pyd, являются библиотеками DLL, собранными с помощью Cython. Они будут запущены при их импорте. При компиляции через Cython от первоначального скрипта сохраняются только строковые и числовые константы, а вся логика (например, циклы, операции присвоения, передача аргументов) будет реализована нативно, поэтому получить исходный скрипт не представляется возможным (как в случае с PyInstaller или Pyarmor), но его можно воссоздать близко к оригиналу. Для этого возьмем pdfbyte.cp39-win_amd64.pyd. Вся логика будет происходить в единственной экспортируемой функции PyInit_pdfbyte. В этом случае постфикс pdfbyte обозначает имя модуля, скомпилированного из pdfbyte.pyx. В зависимости от названия модуля меняться будет и постфикс имени экспортируемой функции. Как показало наше расследование, на мушке у Lazy Koala могут оказаться компании из разных стран и различных секторов экономики. Хакерская группировка показывает, что для успешных атак совсем не обязательно применять сложные утилиты, тактики и техники. Все их нападения начинаются с хорошо продуманных фишинговых посланий: сотрудников хитростью убеждают открыть вредоносный файл. Для защиты от подобных атак, с одной стороны, будут эффективны специализированные средства, такие как система поведенческого анализа трафика (например, PT Network Attaсk Discovery) и сетевая песочница (например, PT Sandbox). С другой стороны, чтобы сотрудники компаний не попадались на удочку хакеров, их нужно обучать киберграмотности, регулярно проверять их навыки с помощью сервисов для симуляции фишинговых атак, а также вовремя информировать о новых схемах мошенничества. В нашем полном отчете приведен детальный разбор LazyStealer и его компонентов. Мы также выделили основные техники «ленивой коалы» по матрице MITRE ATT&CK и индикаторы компрометации, которые вы можете использовать для проверки своей инфраструктуры. Больше отчетов PT Expert Security Center, посвященных новым образцам ВПО, активности APT-группировок, техникам и инструментам хакеров, можно найти в блоге.",
    "140": "Наверное, многие из тех, кто сейчас работает в области IT каким-то образом сталкивались с историей программирования. Тогда вам, конечно же,  будет известно имя первой программистки — Ады Лавлейс. Возможно, если бы в XVII-XIX веках, образование для женщин в сфере естественных наук было более доступным, то это было бы не единственным именем. Странно, что мы называем XVIII в. эпохой Просвещения, но просвещение не коснулось женщин, потому что им было отказано в праве получать высшее и профессиональное образование. В этой статье я хочу рассказать о существующем «мужском перекосе» в данной отрасли, почему он существует и почему женщинам так сложно пробиться в эту сферу. ДИСКЛЕЙМЕР: Основной посыл статьи — это призыв к переменам, чтобы мы начали замечать проблемы женщин и самих женщин. А не в том, что якобы все мужчины плохие; разумеется, не все, как и не все женщины плохие или хорошие. Чем больше мужчин и женщин узнают о проблеме — тем больше людей обратят на нее внимание и, возможно, даже начнут что-то менять, хотя бы в своем окружении. По оценкам на 2022 год в отчете Stack OverFlow видно, что мужчин в этой сфере намного больше, чем женщин. Я решила привести в пример именно этот опрос, потому что все программистки и программисты пользуются этим сайтом. Итак, начнем издалека. Один из учителей информатики, выcтупая на курсах по повышению квалификации в Университете Карнеги-Меллона, восклицал: «Где девушки, любящие программирование? У меня в классе сколько угодно мальчиков, помешанных на компьютерах. Хотел бы я встретить такую девочку». Наверно, этот учитель не слышал об исследовании, проведенном Джейн Марголис в этом же университете на факультете Компьютерных наук в 1990-х гг. Профессорше удалось выяснить, что семьи более охотно покупали компьютеры мальчикам, чем девочкам, даже если последние были сильно увлечены компьютерами. Возможно, таких было мало просто за неимением компьютера? Но вернемся к вопросу выше, от этого учителя. Мое предположение такое: Девочкам внушили то, что они недостаточно умны и поэтому они настолько сомневаются в себе, что не хотят начинать изучать что-то из области естественных наук, а в последствии — и программирование. Если проанализировать школьные учебники, то можно заметить, что в них намного чаще рассказывают об ученых-мужчинах и довольно редко о женщинах (хотя женщин-ученых тоже достаточно). Из-за чего у девочек может сложиться впечатление, что они просто не способны на то, чтобы изучать что-то такое сложное, как программирование. Недавнее исследование, которое проводили в США среди детей 5 летнего возраста, эту теорию доказало. Девочки, как и мальчики, в возрасте 5 лет считают что женщины могут быть «очень-очень умными». Но после того, как дети начинают посещать занятия в школе, у девочек что-то меняется и они начинают сомневаться в умственных способностях своего пола. Если пятилетним девочкам предложить поиграть в игру для «очень-очень умных детей» они охотно соглашаются, в то время как шестилетние девочки — нет. Есть ли решение этой проблемы? Да! Достаточно добавить в школьные учебники женщин-ученых. Как показало исследование это действительно работает. Ведь если в книгах содержатся больше изображений женщин-ученых, школьницы преуспевают в естественных науках. Анализ учебников по истории в США, учебных пособий по языку(используемых в Германии, США, Испании, Австралии), курсов по политологии указывает на то, что в среднем количество упоминаний или фотографий женщин, относительно мужчин, в этих учебниках варьируется от 5,3% до 10,8%. Аналогичный показатель был и при анализе российских, армянских, пакистанских и южноафриканских учебников. Также, не стоит забывать, что девочки и не должны вести себя как мальчики. Почему поведение мальчиков, при наличии у них интереса, считается «показательным», «эталонным», «нормальным»? Если девочки тоже любят программирование, это еще не значит, что они будут вести точно так же, как мальчики. Не стоит ожидать от них «мальчишеского» поведения. Заниматься чем-то одним, как одержимая, и не спать ночи напролет — это не столько признак любви к этому делу, сколько признак некоторой ограниченности и, возможно, инфантильности. Я даже иногда сравниваю такую гиперфиксацию с чувством влюбленности — когда все мысли заняты одним человеком, но ведь это не здорОво. Ведь когда у вас начинаются нормальные отношения, чувства и гормоны приходят в норму и ваша жизнь начинает состоять не только из этого человека. И тут точно также. Помимо одного интереса могут быть еще и другие. Большинство девушек, которые выбирают поступать в университет на техническую специальность, заведомо знают, что их ждет предвзятое отношение, и будет литься поток стереотипов о том, что «это не женский факультет», что она не справится, что ее мозг работает по-другому (хотя доказано, что это не так) и поэтому тут ей не место. Даже если открыто говорят это не все, но многие всячески намекают. Не уверена, есть ли статьи, где рассказывают об агрессивном отношении мужчин к женщинам на технических факультетах, но есть похожее про спорт площадки. Мужчины были часто враждебны и агрессивны к женщинам, которые приходили заниматься на общие спортплощадки, не разделенные на женские и мужские. Такие стереотипы заведомо создают неправильное восприятие женщин мужчинами и их умственных способностей. Исследование, проведенное в 2016 году установило: студентки более объективно оценивают свои знания, чем студенты. Также помимо своих знаний, студентки также более объективно оценивали успеваемость своих сверстниц и сверстников. Юноши же, напротив, упорно считали, что представители их пола более способны, чем девушки, даже несмотря на то, что успеваемость последних была выше. Такая предвзятость приводит не только к тому, что студенты неправильно воспринимают своих товарок по учебе, но и преподаватели нередко необъективно относятся к оным.  Из личного примера: был случай, когда преподаватель удивлялся, что впервые видит столько девушек на физическом факультете (нас было 5 девушек в группе из 11 человек). Но где же, как не в университете, должна процветать меритократия, казалось бы? Меритократия (букв. «власть достойных», от лат. meritus «достойный» + др.-греч. κράτος «власть, правление») — принцип управления, согласно которому высшие (главные) руководящие должности должны занимать наиболее способные люди, независимо от их социального происхождения. Я бы предпочла называть это «миф о меритократии», как делает в своей книге «Невидимые женщины» авторка Кэролайн Криадо Перес. А все из-за одного опроса, который этот миф доказал. Респондентам было предложено оценить достоверность результатов двух исследований: вымышленного и реального. Реальное исследование указывало на наличие «мужского перекоса» в научной и исследовательской среде, а второе вымышленное — доказывало обратное. Мужчины (прежде всего специалисты в области STEM) оценили результаты второго исследования как заслуживающие доверие, а результаты первого, настоящего, — как ненаучные. STEM (англ. science, technology, engineering and mathematics — естественные науки, технология, инженерия и математика). Есть также ряд исследований, где научные статьи проходили двойное слепое рецензирование (автор не знает, кто рецензирует его статью, также как и рецензент не знает, чью работу он рецензирует). Результаты оказались следующими: статьи, которые писали женщины оценивали выше и публиковали чаще. Что в очередной раз развеивает миф о мужском интеллектуальном превосходстве. Тогда почему же такой перекос существует, если интеллектуальные способности женщин ничем не хуже мужских? Резюмируя: мы имеем мужской перекос на технических специальностях, потому что многие девушки верят в стереотипы о том, что они не способны это все изучать. А их веру в эти стереотипы подкрепляет миф о меритократии. В результате чего так мало девушек сейчас работают в IT, именно те, которые с самого начала мечтали об этом и стремились туда попасть. Словом «оплачиваемая» я хотела подчеркнуть также то, что в среднем во всем мире женщины выполняют 75% всей неоплачиваемой домашней работы. Ежедневно женщины тратят на нее от 3 до 6 часов, в то время как мужчины — в среднем от получаса до двух часов. Также можете почитать про исландскую «длинную пятницу» :). Некоторые могут сказать, что женщины этим занимаются, потому что меньше работают и меньше получают. Отнюдь, как показали исследования за последние 20 лет, выполнение женщинами большей части работы по дому не зависит от того, сколько она будет вносить денег в общий бюджет. Но несмотря на все это, женщины все равно могут похвастаться различными карьерными достижениями. Этот факт подтверждает одно явление, которое показало себя в 1970-1980-х гг., в Нью-Йоркском филармоническом оркестре. Внезапно, в начале 1970-го, количество женщин в оркестре сдвинулось с отметки 0% до 10%. А причиной всему было введение слепых прослушиваний. Во время таких прослушиваний наниматели отгорожены от исполнителей экраном. Таким образом, уже к концу 1980-х гг. доля женщин в оркестре приблизилась к 50%. На сегодня доля исполнительниц составляет около 45%. На сегодняшний день, исследования показывают, что чем больше работодатель верит в свою непредвзятость или в то, что они не сексисты, делает их отъявленными сексистами. Те мужчины (и женщины), которые искренне верят в свою объективность, выбирая из двух кандидатов с практически одинаковыми резюме —  один из которых мужчина, а другая — женщина, обычно выбирают мужчину. Возможно, одна из причин такого низкого количества женщин в IT связана и с этим. Но допустим, что вас все-таки взяли на работу в области STEM. Но и тут не все так сладко. Анализ эффективности работы персонала в американских высокотехнологичных компаниях показал, что женщин критикуют за поведение, считающимся нормальным для мужчины. Женщинам советуют вести себя менее агрессивно, в то время как, упоминая «агрессивность» в отзывах о мужчинах, им советуют наоборот «вести себя более агрессивно». Не просто так более 40% женщин увольняются из сферы IT через 10 лет после начала работы (у мужчин этот показатель — 17%). Как отмечает CTI (Center for Talent Innovation), женщины называют среди причин увольнения «безобразное поведение руководства» и «отсутствие карьерных перспектив». Также в статье упоминается, что женщины уходят из этой сферы, потому что им постоянно отказывают в повышении должности, а их проекты отклоняют. Правда, веет «мифом о меритократии»? Если после увольнения женщина попросит рекомендательное письмо с прежнего места работы, то, скорее всего, оно снизит шанс на поиск нового места, нежели повысит. В результате исследования было выявлено, что, при написании рекомендательного письма для женщины, работодатели чаще акцентируют внимание на коммуникативных навыках («доброта», «отзывчивость»), чем на карьерные стремления («амбициозность», «уверенность в себе»), а рекомендации с таким содержанием снижали шансы соискателей(особенно женщин) на получение должности. Этот факт, наверное, уже известен каждому и более подробно про него рассказывать я не буду. Например в России эта разница составляет 37,3% , в целом в мире этот разрыв составляет около 16%, по другим данным - 37,8%. К сожалению, эта тенденция остается неизменной с давних времен. Даже Гарвардские вычислительницы были наняты Пикерингом, потому что женщинам нужно было платить меньше, чем мужчинам на аналогичных позициях. Также, очень интересный факт, который обнаружили в США за последние 50 лет, что как только больше женщины начинают работать в какой-то отрасли, зарплата там падает, а эта отрасль становится менее престижной. Даже такие авторитетные компании, как, например, Google страдают от этого. Анализ оплаты труда в Google в 2017 г. выявил, что мужчинам в среднем платили в 6-7 раз больше, чем женщинам, занимающим те же должности. С тех пор компания неоднократно отказывалась предоставлять данные об оплате труда работников, заявляя, что никакого гендерного дисбаланса нет. Вопреки мифу о том, что гендерные квоты способствуют продвижению неквалифицированных женщин, было доказано обратное. Исследование было проведено Лондонской школой экономики, и выявлено, что эти квоты, наоборот, способствуют «избавлению от некомпетентных мужчин». Возможно, для компаний (читай как «для бизнеса») в IT и для женщин введение таких квот сказалось бы очень положительно. В статье специалистка по данным рассказывала, что онлайн-платформа для рекрутинга, использующаяся в сфере высоких технологий может больше, чем просто анализировать резюме. Gild анализирует «сетевую активность» соискателей и ранжирует их по показателю вовлеченности программистов в цифровое сообщество. Этот показатель можно измерить, узнав, сколько времени человек тратит на разработку и обмен данными на таких платформах как GitHub и Stack OverFlow. Но, помимо этого, Gild также позволяет оценить и набор стереотипных качеств, которые требуется работодателям. Например, одно из таких - посещение соискателей одного японского сайта для манги считается «убедительным свидетельством блестящего владения навыками программирования». Казалось бы, что тут такого? Но как мы знаем, женщины выполняют 75% всей неоплачиваемой работы по дому, а значит, скорее всего, у женщин попросту не будет времени часами зависать на подобных сайтах. И если учесть, что подобные сайты в основном посещают мужчины, заведомо настроенные против женщин, то, скорее всего, большинство женщин будут избегать подобных сайтов. Разработчики Gild непреднамеренно заложили в свой алгоритм этот «мужской перекос». На самом деле, довольно странно предполагать, что программирование это мужская работа. Как по мне, в принципе, довольно странно классифицировать профессии как «мужские» или «женские». Исключениями могут быть те случаи, где действительно половой признак имеет место быть, например, такая чисто женская профессия, как молочная кормилица, или чисто мужская — донор спермы. Но так уж сложилось, что в обществе часто оперируют этими понятиями, поэтому я тоже воспользуюсь им. Изначально программирование считалось именно женским занятием. В 1940-е и 1950-е программированием тоже занимались в основном женщины. В 1940-1960-х гг. существовала такая профессия, как «компьютер» и первыми «компьютерами» были женщины. Их нанимали, чтобы они делали сложные математические расчеты для военных и НАСА, (например, при работе над запуском Explorer 1 или когда началась программа «Меркурий»). Позже их заменили «машинные расчетчицы» — счетно-вычислительные машины. Но, даже после того, как девушек заменили машины, потребовались годы, прежде чем в эту отрасль пришли мужчины. Первыми программистками, разрабатывающими программы для ЭНИАК-а стали шесть девушек. ЭНИАК(Electronic Numerical Integrator and Computer) — это первый в мире электронный цифровой вычислитель общего назначения, который можно было перепрограммировать для решения широкого спектра задач. Даже журнал The Cosmopolitan в 1967 г. опубликовал статью «Девушки - компьютерщицы», призывающую женщин заниматься программированием. «Это так же просто, как приготовить обед» — рассказывала пионерка программирования Грейс Хоппер. — «Нужно все заранее продумать и распланировать к определенному времени. Программирование требует терпения и умения учитывать все детали. Женщины просто созданы для программирования». Несмотря на все вышесказанное, не стоит забывать еще и о том, что наш мир все равно становится лучше. Хочу рассказать о нескольких примерах. И я надеюсь, что их будет становится все больше. В ходе исследования, где анализировали данные с 1985 г. по 2016 г. все больше детей рисовали женщин, когда их просили нарисовать «ученого». Доля девочек, нарисовавших женщину-ученого возросла с 33% до 58%, а доля мальчиков с 2,4% до 13%. В компании заметили, что только что родившие женщины, увольняются в два раза чаще других категорий работников. Тогда Google увеличила продолжительность отпуска по беременности и родам с трех месяцев с частичным сохранением заработной платы до пяти месяцев с полным сохранением содержания. Таким образом текучесть «женских» кадров уменьшилась на 50%. Компания Google платит работникам пособия на детское питание первые три месяца после рождения ребенка. Также, организует предоставление различных услуг непосредственно на рабочем месте — например, можно сдать вещи в химчистку не выходя из офиса Всего пару десятков лет назад везде только и говорили о том, что программирование — это женская работа. Но теперь все чаще и чаще женщины(работающие в IT или учащиеся там) слышат, что это не женское занятие. А давайте сделаем так, чтобы наше поколение начало понемногу избавляться от этих гендерных стереотипов! Давайте сделаем эту отрасль более «безопасной» для женщин, свободной от предрассудков! Ведь все это в наших руках :) P.S. Сердечно благодарю своего мужа за поддержку при написании статьи и Максима Нахмедова за редактуру.",
    "141": "Любите детективы? Пройдите квест «В поисках пропавших ссылок»! Регистрируйтесь на сайте и попробуйте себя в роли сыщика: найдите на страницах Selectel спрятанные ссылки и первыми дойдите до финала. Выиграйте эксклюзивный мерч и промокод на сервисы Selectel.",
    "142": "Коротенькое ТЗ: создадим игру, в которой можно будет привычными способами (что-то вроде стандартного FPS контроллера) управлять перемещением игрока по поверхности ленты Мебиуса. Игру в 3D с управлением от первого или третьего лица можно создать на любом современном движке минимальными усилиями — на все ключевые компоненты есть готовые решения, остается их только подключить. В чем сложность использования готовых компонентов на ленте Мёбиуса? Если коротко, сложность в гравитационном поле. Мир большинства игр (конечно, мы рассматриваем игры с бегающим героем в 3D) — плоскоземельный мир с однородным гравитационным полем. Вектор силы земного притяжения не нуждается в серьезных расчетах. Взяли орт «минус Up» (по более-менее традиционным соглашениям о системах координат это {0; -1; 0}), умножили его на mg, готово. Гравитация в окрестностях ленты нуждается в более сложной модели. Прямое следствие плоскоземельности — возможность упростить 3D пространство. Вектор гравитации — это еще и вектор вертикали, задающий ось «курсового» вращения контроллера игрока (мышь влево-вправо). Очень удобно иметь эту ось постоянной, занимаясь лишь двумя углами Эйлера из трех. В нашем случае очевидно, что ось будет подвижной. Как вообще должно работать гравитационное поле на ленте Мёбиуса, чтобы по ней можно было ходить? Здесь возможны некоторые вариации, но более-менее логичным выглядит правило: направление местного результирующего вектора притяжения ленты не должно слишком отличаться от нормали к поверхности ленты в ее ближайшей точке. Это правило должно соблюдаться для тех участков, в которых мы собираемся обеспечивать привычную ходьбу. Вооружившись этим замыслом, перейдем к математическому описанию мира. Примем, что линия, разрезающая ленту Мёбиуса (далее - «лента») вдоль посередине, представляет собой окружность с центром в начале координат (далее — «большая окружность»). Она будет лежать в плоскости Oxy. Будем собирать ленту из тонких «брусков», нанизываемых своими серединами на большую окружность.  Расположим каждый из них вдоль оси Oz. Получаем сформированную из брусков цилиндрическую поверхность. Для получения ленты осталось закрутить ее. Каждый брусок получит индивидуальный поворот. Все они поворачиваются вокруг «своих» касательных к большой окружности, значение угла поворота определяется угловым положением бруска на ней, отсчитанным вдоль её дуги. Продвигаясь вдоль большой окружности на угол 2π радиан, мы должны повернуть ленту на π радиан, следовательно, соотношение углов будет 1 к 2. Продвижение по дуге большого круга с вызовами instantiateItem() реализовано в методе Start(). Передаваемое значение — угол дуги большой окружности, отсчитанный от оси Ox до позиции конкретного бруска. По этому значению в методе instantiateItem() будет определено, где установить новый брусок и как его повернуть. Наименее очевидной частью скрипта выглядит вызов res.transform.Rotate() внутри метода instantiateItem(). Первым аргументом передается трехмерный вектор оси поворота (в нашем случае это касательная к большой окружности), вторым — величина поворота в градусах. Очевидно, «плотность» ленты регулируется значением константы NUM_OF_ITEMS. Приемлемый внешний вид — вещь субъективная, но за отправную точку можно принять, что при радиусе большой окружности 20 метров можно сделать почти сплошную ленту из примерно 1200 довольно тонких «брусков». Было бы странно иметь привычную гравитацию и механику ходьбы после ухода с поверхности ленты в открытый космос. Определим для привычной физики пространство внутри тора, содержащего ленту. Радиус образующей его окружности равен половине ширины ленты, или, проще говоря, нам нужен минимальный «бублик», содержащий в себе ленту. За пределами этой фигуры гравитация будет принципиально другой, а движение перестанет быть управляемым. При таком подходе на удалении от краев ленты (при ходьбе около центральной части «тропинки») все работает стандартно. По мере приближения к ним нарушается безопасность выполнения прыжков. Можно нечаянно покинуть область действия гравитации ленты и улететь. Такое свойство ленты кажется естественным. Вполне вероятно, что аналогичный риск присутствует и на настоящих космических лентах для ходьбы. Выше сказано, что желательным направлением гравитации является нормаль к поверхности ленты возле игрока. Хранить данные — предварительно просчитанные значения вектора гравитации, в трехмерном массиве, где его индексы массива соответствуют движению вдоль трех осей. Зная текущую позицию, можно было бы взять значения из ближайшей соответствующей ячейки или из нескольких ячеек с последующей интерполяцией. Вычислять гравитацию по закону всемирного тяготения, приняв какое-то огромное значение плотности вещества для ленты, и разбив ее на небольшие притягивающие элементы. Вектор гравитации будет суммой притяжений всех элементов. Отметим, что при точном расчете правило действия гравитации по нормали выполняться не будет. Можно считать вектор ближайшей нормали при каждом обращении во время выполнения. Приведенный список не может претендовать на полноту, а сравнение достоинств разных методов с учетом их возможных модификаций и оптимизаций займет не одну статью. Ограничимся отражением факта, что для реализации был выбран третий путь. Логика приложения внешних сил помещена в статический класс WorldPhysics. Рассмотрим методы формирования гравитации. Определим, находимся ли мы внутри тора, где действует «обычная механика» ходьбы. Параметр position (позиция объекта), учитывая наше соглашение о расположении ленты, является радиус-вектором из центра большой окружности. Легко найти ближайшую к position точку большой окружности (проекция вектора position на Oxy, нормализация полученного вектора и умножение результата на радиус большой окружности). Далее остается проверить дистанцию между position и найденной точкой. Она сравнивается с радиусом образующей окружности разграничивающего тора, равной половине ширины ленты. На практике для большей эффективности вычислений будем сравнивать квадраты величин (имеем право, поскольку операнды неотрицательные). Научившись понимать, на ленте мы или в открытом космосе, приступим к вычислению местной вертикали «внутри бублика». Метод принимает позицию и возвращает единичный вектор направления местной вертикали. Последовательность вычислений включает уже знакомую проекцию на плоскость большой окружности, вычисление угла позиции в ней, отсчитанного от оси Ox через скалярное произведение векторов, определение вектора местной касательной к большой окружности (он же ось вращения местного элемента террейна, см. описание создания ленты выше), вращение нормализованного вектора проекции вокруг местной оси крутки террейна на половину позиционного угла (то есть на угол крутки террейна). Местная касательная к большой окружности определяется через векторное произведение векторов: этот вектор перпендикулярен как WORLD_AXIS (ось Oz), так и нормализованной проекции радиуса-вектора на Oxy projOnWorldPlane (касательная к окружности перпендикулярна радиусу в точке касания и перпендикулярна нормали к плоскости окружности). Поворот вектора на заданный угол вокруг заданной оси выполняется с помощью класса кватернионов Unity, имя AngleAxis() точно отражает суть метода. Теперь, имея позицию игрока, мы точно знаем, куда направлена сила тяжести. В коде проекта на GitHub есть также контроллер игрока с механикой ходьбы, бега, прыжков и полета в открытом космосе. Возможно, этим компонентам будут посвящены новые публикации.",
    "143": "Маркетинг — это понимание клиентов через анализ данных и психологию людей, производимых для того, чтобы понять клиентов, их потребности, притязания, страхи, возражения, чаяния и надежды (как покупателей/потребителей) с целью максимально эффективного удовлетворения их потребностей. Что в свою очередь приводит к развитию компании (общественного института) и получению прибыли ©. Другого определения сути маркетинга быть не может. Маркетинг ≠ реклама.Реклама, это часть маркетинга. А реклама БЕЗ маркетинга реклама — это брак.Сделать рекламу без исследования, значит не поучить и десятой части возможного эффекта. Ответ в определении сверху. Нельзя получить отдачу от подаркинга, дарилкинга, печати визиток и прочей ерунды, и от рекламы нельзя, когда нет понимания себя, подчёркиваю (!) СЕБЯ и клиента. Можно продать слона, воздух, холодильник эскимосам только зная потребности, цели, ценности клиента и прочие нюансы его поведения и принятия решений. Тот же холодильник эскимосам нужен, чтобы еда НЕ замерзала. Просто настроить рекламу, что-то там покрутить и состряпать можно, но эффекта не будет или он будет минимальный. Достаточно спросить: «Что покупает клиент?», чтобы выявить несостоятельность представлений о рынке и конкуренции, которыми обычно руководствуются руководители компаний в своих действиях. Данные вопросы как нельзя хорошо перекликаются с концепцией JTBD, кстати говоря. Изучение своего бизнеса и клиента — наше всё. И ваше. Есть один собственник (да и не один он, конечно :)), который считает, что его клиентам всё равно, что и как он продаёт, потому что они «тупые». Им, якобы, ничего кроме заграницы, девочек и денег не надо (эти клиенты из сферы В2В). А эксплуатанты этих клиентов работают на том, что дали, и голоса не подают. То есть, этот человек продаёт просто потому, что клиенты по привычке покупают. Так себе позиция. Даже не возникнет вопрос «А что будет, если перестанут покупать (у него или вообще)?». А будет снижение доходов и закрытие бизнеса. Вот цена непонимания себя и клиентов (и потенциальных клиентов). Я предложил спор данному товарищу — считаю, что он заблуждается. Что продавать можно и нужно через обучение, мастер-классы, event. Об обучении в другой статье напишу кратенько. «Хотели бы вы посетить наш бесплатный мастер-класс, в котором мы покажем, как пользоваться тем или иным оборудованием, продемонстрируем его возможности и преимущества перед аналогами?». И мне отвечали утвердительно.То есть, изначальное предположение руководства фирмы — ошибка. Ошибка, цена затрат на избежание которой — 5 минут.Скажу ещё больше. Мало понимать клиентов и потенциальных клиентов. Надо понимать ещё и НЕклиентов. Есть ещё один нюанс. Иногда (если не всегда) клиенту надо объяснить, как работает ваше решение его проблем, т.е. какие его (выявленные явные и скрытые) потребности удовлетворяет ваш продукт/услуга. Научить клиента, другими словами. А это целая работа. Это как в продажах — есть менеджеры, которые отгружают клиенту товар (это не менеджеры по продажам, это дилетанты), есть те, кто выясняет потребность, а есть такие продавцы, которые СОЗДАЮТ потребность. Так и в маркетинге — если вы нашли максимально эффективный способ облегчения жизни клиента, далеко не факт, что ему этот способ будет понятен с первого раза. Нужно создавать понимание своих потребностей. Прошу вас — исследуйте всё, что только можете. Подвергайте сомнению абсолютно всё. И да пребудет с вами сила!",
    "144": "К нам на сайт Альфа-Банка ежемесячно заходят миллионы посетителей. Часть из них уже готовы оформить новый продукт (мэтч с первого клика), а часть идёт сравнивать предложения на сайты других банков. Дизайн важен для продажи продукта и донесения его ценности пользователю. Но как визуально выделиться и подчеркнуть уникальность предложений, если за несколько лет дизайн раскопировали конкуренты? Это и стало одной из причин, почему мы решили изменить дизайн сайта в конце 2022 года. Стоит сказать, что это не редизайн в его полном смысле, когда изменения затрагивают архитектуру сайта, а скорее рестайлинг или рефакторинг. Но редизайн звучит круче, поэтому оставим так. 80% пользователей заходят на сайт со смартфонов. Поэтому мы решили, что mobile first — это про нас, и речь далее пойдёт про редизайн мобильной версии. Основная задача главной страницы — навигировать пользователей в разделы продуктов, чтобы каждый смог без труда найти то, что ищет. Нам было важно, чтобы количество переходов на продуктовые страницы как минимум не упало, а лучше — чтобы выросло. Я поставил себе личную цель — сделать интерфейс настолько простым и интуитивно понятным, чтобы пользователь находил нужный продукт в пару кликов. Помимо вау-эффекта от редизайна мне было важно улучшить пользовательский опыт и вылечить текущие боли. Я понимал, что работаю не только над главной страницей, а над внешним видом всего будущего сайта. Когда работаешь в большой корпорации, важно предупредить всех заинтересованных, что и зачем ты делаешь. Тогда никто в середине проекта не уберёт твою работу в стол, если в соседнем отделе кто-то уже ведёт параллельный процесс. Но пусть не только знают, чем я буду заниматься и зачем, а ещё и участвуют. Поэтому первым делом мы сформировали рабочую группу из дизайнера (меня), продакта главной страницы, стейкхолдеров и лидов сайта и маркетинга. Так мы могли договориться об ожиданиях, собраться на еженедельные сессии дизайн-критики, повысить прозрачность процесса и вовлечённость коллег. Главное — договориться, чтобы участники не перетягивали одеяло из-за личных интересов, а обсуждали продуманное дизайнером решение. Бонусом я мог учитывать требования бизнеса для дальнейших итераций, чтобы к концу работы не вылезли случайные хотелки, которые сломают концепт. Разработка продукта всегда начинается с Discovery. Вот и я начал с анализа страницы и текущих метрик. Я посмотрел карту кликов и скролла, выделил костяк целевой аудитории, изучил поисковые запросы, чтобы понять, откуда и зачем пользователи приходят к нам на сайт. Результаты не впечатляли: конверсия на главном баннере составляла всего 1% от общего числа посетителей, переходы в раздел для бизнеса < 0,5%, клики по карточкам акционных предложений < 0,2% по каждой, а до середины страницы добирались самые стойкие. Я выделил самые популярные пользовательские сценарии, чтобы учесть и сохранить их в редизайне, а пользователю не пришлось знакомиться с интерфейсом заново, когда он вновь зайдёт на сайт. Затем я собрал визуальный анализ конкурентов, потому что, как говорится, их надо знать в лицо. Гуглим «Топ-10 банков», открываем Excel и анализируем все главные и не только страницы по разным параметрам: от структуры страниц и компонентов до анимации и стилей креативов. Выделяем плюсы и минусы, понимаем, что стоит повторить, а где есть возможности отстроиться. Так я узнал, что слайдер в главном баннере используют 9 из 11 конкурентов, и в моём решении его точно быть не должно. А ещё никто не использует анимацию, кроме слайдеров, и это может стать нашей точкой роста. Я не стал сразу накидывать дизайн-концепт, а предварительно сформировал гипотезы по улучшению интерфейса, основываясь на поведении пользователей. Ещё до стадии прототипирования я знал, на что обратить особое внимание, какая будет структура и какую проблему закроет каждый блок на странице. Например, по картам скролла было видно, где отваливаются пользователи и какие блоки стоит изменить, а из анализа структуры страниц конкурентов понял, какие элементы наиболее популярны и привычны для посетителей, и их стоит сохранить у нас. Когда ещё нет понимания, каким должен быть дизайн, стоит начать с поиска основной идеи. Для быстрой генерации вариантов лучше всего подходят низкоуровневые прототипы или «серые квадраты», как я их называю. Первое впечатление о сайте очень важно для пользователей, привлекательный интерфейс цепляет, побуждая пользователей к дальнейшему изучению. Поэтому работу над прототипом я начал с первого экрана. Создать вау-эффект и удержать внимание. Сделать экран интуитивно понятным и не заставлять думать, как работает сайт. Сохранить простоту входа в мобильный банк для клиентов с айфонами. Суть «серых квадратов» в том, чтобы быстро накидать большое количество решений и оценить взаимодействие с интерфейсом. Чтобы не обсуждать картинки и цвет кнопок — не добавляйте их. Я подготовил большое количество вариантов: от переиспользования классических приёмов и акцента на поиске в духе Яндекса до захода в историю с чат-ботом на главном экране. Решения казались перегруженными и во время обсуждений никого не могли зацепить. Было ощущение, что я хочу повторить всем знакомые сайты из других ниш бизнеса. Но чудо произошло. В какой-то момент мне в голову пришла гипотеза, что если переиспользовать паттерны мобильных приложений, получится максимально привычный интерфейс для пользователей, ведь в приложениях мы проводим основное экранное время. К тому же, такое решение будет уникальным для веба. С учётом референсов я проработал разные форматы экранов. В итоге появился вариант, который устроил всех: видеобаннер на весь экран привлекает к основному офферу, и шторка снизу открывает контент страницы, имитируя мобильное приложение. Так первый экран задал стиль всему концепту, и мне осталось проработать остальные блоки по такой же логике. Итог работы над прототипами — зафиксированная структура страницы, список основных блоков, которые вошли в MVP-версию главной, и бэклог, пополненный идеями для будущих экспериментов. На прошлом шаге я получил скелет страницы, предстояло детально проработать блоки, зафиналить механики взаимодействия и сформировать общий визуал. Это ответственный этап. На нём можно надолго застрять с бесчисленными референсами и поиском того самого стиля, который понравится стейкхолдерам, пользователям и самому себе, конечно, будь проклят, дизайнерский перфекционизм. Собрал примеры сайтов и приложений, не ограничиваясь только финтехом. Отдельно собрал ощущенческий мудборд, чтобы понять вайб, который должен передаваться зрителю. Определился с палитрой и собрал мнения коллег, какой дизайн им нравится, чтобы не разрушить ожидания. Выделил смысловые блоки: первый экран, продукты, акции, новости, приложения. Подобрал и отсортировал рефы под каждый блок, описал закономерности и первые правила построения компонентов. После многих итераций дизайна и долгих обсуждений рабочей группы я закончил основные виджеты и презентовал полноценный концепт главной страницы. Помимо первого экрана, фундаментом концепта стала витрина предложений для быстрой навигации в продукты. И если раньше нужно было зайти на страницу бизнеса, прежде чем выбрать соответствующий продукт, теперь это возможно с главной страницы сайта. Важно убедиться в качестве решения, иначе ресурс и время разработки будет потрачено впустую. Концепт ушёл на исследования, чтобы пользователи подтвердили, что дизайн классный и ничего не ломает. К тому же, результаты исследования помогут заручиться поддержкой стейкхолдеров для ускорения разработки, в том числе с дополнительными ресурсами. С исследованием помогли коллеги из Alfa Research Center — нашей внутренней UX-лаборатории. Совместно с исследователями мы определили два сегмента аудитории: клиенты и не клиенты банка, предприниматели и обычные пользователи 25-55 лет с айфоном в кармане. Подготовили сценарий для глубинного интервью, составили список гипотез для подтверждения и продумали задания для теста. Исследование сделали в формате качественного юзабилити-тестирования прототипа с айтрекером, чтобы видеть, куда смотрит человек и какие эмоции испытывает при работе с сайтом. Респондент получал задание в духе оформить продукт или найти что-то на сайте, например: Представьте, что от ваших знакомых вы узнали, что в Альфа-Банке выгодный кэшбек по дебетовой карте, а карту можно заказать дистанционно. Вы решили зайти на сайт банка, посмотреть информацию про карту и оформить заказ. Дополнительно мы провели количественный тест старой и новой версии главной и сравнили скорость кликов на точки входа в продукты. А ещё тестировали эмоциональный отклик нескольких версий главного экрана, где оценивали первое впечатление и показатель когнитивной нагрузки от видео против статического изображения. Неделя тестов пролетела незаметно, и оставалось только дождаться анализа результатов из лаборатории. Небольшой лайфхак — даже если тесты проводит выделенный исследователь, вам полезно лично присутствовать на интервью. Во-первых, вы получите опыт и прокачаете навыки самостоятельного проведения интервью. Во-вторых, до презентации результатов, вы уже понимаете, насколько прототип оказался успешным, а с чем у пользователей трудности. Я понимал, что концепт зашёл, а результаты юзабилити-теста это подтвердили. Взаимодействие с новой главной не вызывает критичных затруднений и проблем. Новый интерфейс упрощает и ускоряет путь пользователя. Видео в главном баннере не отвлекает от целевых действий. Новая версия показывает более позитивное первое впечатление и интуитивную понятность интерфейса. Выявили проблемы с неймингом продуктов и описанием услуг. Точки входа в мобильный банк плохо считываются. Незаметен поиск по сайту. Респонденты не сразу разобрались, как взаимодействовать со шторкой, так как считали, что контент под ней, а не внутри. В новом интерфейсе продукты ищут быстрее. Так кредитную карту находили на 7 секунд быстрее, а точку входа в накопления — на 3,5 секунды быстрее. Разброс кликов стал меньше, точки входа в продукты более заметны. Новая версия показывает значимо меньшее напряжение и фрустрацию, лучшую концентрацию. С точки зрения эмоционального отклика новая версия отработала лучше: продукты стало проще находить, путь пользователя стал интуитивно понятнее. Я получил много фидбэка, интересных инсайтов и сформировал новые гипотезы для улучшения страницы в следующей итерации. По результатам тестов нам одобрили разработку с незначительными изменениями концепта. Одним из них стал отказ от шторки в пользу параллакс-эффекта. Так я решил возможные проблемы с механикой скролла страницы, и это оказалось проще в реализации. Спустя два месяца непростой разработки главная страница была готова выйти в люди. Но увидеть её было суждено не всем сразу. Все изменения на бою, будь то редизайн или маленькая фича, мы запускаем через AB-тесты. Хорошие результаты исследований на контрольной группе — это круто, но опыт пользователей на живой странице может отличаться от прототипа, к тому же взаимодействовать на бою будет гораздо больше людей. А ещё нам важно сравнить все метрики нового варианта со старым, чтобы на цифрах доказать бизнесу успех редизайна. Трафик наращивали постепенно, на старте новый дизайн видели лишь 5% посетителей, затем их число доросло до 50%, а после получения статистически значимых результатов оценить редизайн смогли все. на 3,84 п.п. увеличилось количество переходов в мобильный-банк на 1,74 п.п. увеличилось количество кликов по СТА в мейнбаннере на 1,64 п.п. увеличилось количество переходов в кредиты на 1,89 п.п. увеличилось количество переходов в кредитные карты на 1,21 п.п. увеличилось количество переходов для скачивания приложения на 2 п.п. уменьшилось количество отказов с главной Результаты были отличными, бизнес радовался новым показателям и современному дизайну, за который не было стыдно. А я радовался, что выполнил поставленные перед собой задачи, ведь что для UX-дизайнера может быть важнее счастья пользователя? Новый дизайн собрал положительные комментарии как от моих руководителей и коллег, так и от пользователей. На удивление, я ни разу не увидел комментов в духе «верните всё как было», и это потрясающе. Настройте коммуникацию со всеми заинтересованными в задаче сторонами. Периодически презентуйте и обсуждайте наработки, все должны быть в курсе, в какую сторону движется проект и как дела со сроками. Своевременное решение любого незначительного вопроса сохранит стабильность проекта Обсудите общее видение проекта. Каждый в команде должен понимать, что вы делаете, зачем, каких результатов хотите достичь и как будете их измерять. Договоритесь об ответственности каждого участника, например, что креативы для тестирования подготовят коммуникационные дизайнеры к такому-то числу, для начала работы им потребуется … Помните, что вы не только достигаете целей, поставленных бизнесом, но и решаете задачи пользователей. Учитывайте интересы обеих сторон, сформируйте и опишите отдельные проблемы для каждой стороны. Так вы достигнете большего успеха в долгосрочной перспективе. Соблюдайте дизайн-процесс и, по возможности, не пропускайте его этапы. Качественная работа на дискавери-стадии поможет закрыть большинство вопросов, вылезающих на этапе дизайна. С полностью проработанным концептом, спецификацией на каждый компонент и описанными состояниями системы разработчики эффективно выполнят свою часть без изменений по ходу процесса. Исследуйте, тестируйте, разговаривайте с пользователями. Они конечные потребители продукта, их опыт определяет успех вашей работы. От того, насколько просто и удобно будет пользоваться вашим интерфейсом, зависит конверсия всей воронки продукта даже за пределами вашей платформы. Плохой результат — тоже результат. Негативный фидбэк пойдёт только на пользу, особенно если он собран до разработки. Итерация за итерацией вы закроете большинство проблемных мест своего решения, и оно быстрее окажется в релизе с минимальными рисками. Не останавливайтесь на достигнутом. Во время тестов вы соберёте много инсайтов, которые лягут в основу будущих улучшений продукта. Опишите правила и составьте гайды для других дизайнеров, чтобы масштабировать редизайн и соблюдать его консистентность. Иначе скоро вам придётся снова решать знакомые проблемы. На этом всё. Был ли у вас опыт редизайна и чего не хватает в моём чек-листе? Буду рад обсудить в комментариях.",
    "145": "В инфраструктуре заказчика имелся большой зоопарк систем, не объединенных единой логикой. Надо было навести порядок и наладить автоматизацию, особенно после того, как в этом уже поучаствовали сотрудники различных подразделений и сторонних компаний, не особо озабоченных единой концепцией. Нам повезло, что заказчик сам не до конца представлял, что именно хочет, поэтому в проекте было много пространства для творчества и возможности применить методологию DevOps, в том числе к системам на AIX. Ну а началось все с одного болезненного инцидента. Представьте себе огромную компанию примерно в 350 тыс. человек, где инженеры SAP управляют семью сотнями хостов. Хосты обрабатывают миллионы транзакций в день в основных подразделениях компании, на них же функционирует масштабная ERP-система, HR и несколько других им подобных. Инфраструктура включает как физические сервера, так и виртуальные машины на разных архитектурах виртуализации x86 KVM, IBM POWER Hypervisor с операционными системами AIX, Suse, RedHat, RHEL, SLES, AIX и даже RHEL on Power. Изначально весь этот «зоопарк» не был объединен общей логикой и разделялся по ландшафтам и регионам. В какой-то момент их собрали в одну общую инфраструктуру. Многие системы мигрировали из небольших ЦОД в централизованные, при этом произошло переключение на общий DNS. До определенного момента все работало без сбоев, пока не случился фатальный эпизод. В деталях нам не рассказывали, но, похоже, что-то приключилось с центральным DNS-сервером. То ли он отказал, то ли возникли некие сетевые проблемы, из-за которых частично или полностью пропал к нему доступ. Возможно, это случилось во время обновления оборудования или в процессе очередного переезда. Как итог – пошла цепная реакция, вызвавшая серьезную деградацию в работе основных сервисов. Простыми словами: в гигантской компании внезапно затормозились ключевые процессы. Тогда инженеры компании, сопровождающие SAP, решили основательно подстраховаться и выбрали файл /etc/hosts в качестве резерва для централизованного DNS. Причем отнеслись к этому с максимальной скрупулезностью, превратив локальный файл hosts в некое подобие выделенного DNS-сервера, расположенного локально на каждом из узлов SAP-систем. Спустя некоторое время эти системы были взяты нашей командой Unix-инженеров в поддержку. Тут то мы и столкнулись с тем самым “Франкенштейном”. Речь про файл hosts, который периодически редактировался вручную или с помощью рукописных bash-скриптов. В него добавлялись новые инстансы, удалялись старые, появлялись, исчезали, изменялись системы и целые ландшафты. В какой-то момент hosts превратился в настоящего монстра – стал слишком громоздким, с труднопонимаемой структурой. Дошло до того, что инженеры сопровождения, работавшие с заказчиком до нашего подключения к проекту, просто стали добавлять новые записи в конец файла, не принимая во внимание логическое разбиение систем и ландшафтов. Это сделало файл максимально неструктурированным и непригодным к обслуживанию. Первое время мы также редактировали записи в файле рукописным скриптом в полуавтоматизированном режиме. Но быстро поняли, что в текущем виде это обслуживать невозможно, и это точно не наш метод. Логика вместе с официальными рекомендациями SAP подсказывали, что для повышения стабильности системы надо вносить локальные записи для каждого инстанса в файл /etc/hosts. Изначально мы планировали для каждого ландшафта создавать свой локальный файл hosts тех систем, которые там требуются. Вроде все понятно. Но наша картина мира немного отличалась от требований заказчика и суровой реальности. Основной вызов состоял в том, что требовалось объединить все ландшафты в одном файле, тиражировать их на все системы, при этом соблюсти структуру, логику, последовательность, а также сохранить читаемость и надежность, при этом учесть различные исключения. Кроме всего вышеперечисленного нужно было навести порядок и выстроить логику в списках хостов, применив системный подход. В итоге задача трансформировалась в выработку процедуры автоматизированного редактирования и контроля за файлами. Так началась история внедрения автоматизированного подхода к управлению инфраструктурой (Infrastructure as Code). Тут стоит оговориться, что наша позиция – использовать правильные и эффективные инструменты (DNS), но в тот момент у нас не было выбора. Задача была жестко сформулирована заказчиком (hosts) – поэтому пришлось решать именно ее. Так как результат необходимо было предоставить в максимально сжатые сроки, приняли решение начать с доступа в Git и установки Ansible. Ansible был выбран как наиболее подходящий инструмент для управления разнородным составом серверов, поскольку для него не требуется установка агентов. Кроме того, в нашей команде инженеров поддержки Unix-систем имелась достаточная экспертиза, чтобы писать код в Ansible и поддерживать его работу, а также богатый набор уже написанных ролей и плейбуков. Казалось, нет ничего сложного в том, чтобы написать плейбук, создающий из темплейта требуемые записи в файле hosts. Подобное упражнение дается на экзамене RHCE. Но в нашем случае задача усложнялась требованиями внести данные не только обо всех системах и их сетевых интерфейсах. Имена систем имели у заказчика разную логику формирования. Плюс дополнительные разнообразные вспомогательные внутренние и виртуальные адреса, соответствующие неким определенным названиям, также должны были быть описаны в /etc/hosts. Особая головная боль – различные группы хостов с исключениями из правил, о которых упоминалось выше. Например, предполагается, что на всех узлах должна быть одинаковая копия файла. Но в одной из десятков групп хостов есть порядка восьми подгрупп, где одна определенная запись должна иметь отличный от других IP-адрес. Также есть определенные группы хостов, на которых требуется, например, добавить некоторые записи, не нужные на всех остальных системах. В итоге возникла первая версия Ansible-роли с темплейтом, которая переупаковывала его в красивый и структурированный файл. В роли была реализована сложная логика, способная обработать все варианты префиксов, постфиксов, исключений и вспомогательных IP-адресов, требуемых для полноценного файла hosts. Но, самое главное, теперь все было в одном месте: мы запротоколировали все системы в файле инвентори с их адресами и дополнительными атрибутами. В процессе презентации решения заказчику наибольшие усилия пришлось приложить в преодолении непонимания работы темплейта Ansible, который создавал для каждой управляемой машины свой файл, а не тиражировал один единственный и неповторимый «золотой стандарт» в неизменном виде. Возврат к «золотому стандарту» означал бы продолжение хаотичного наполнения файла без унифицированной логики, повышенные риски сбоев и потеря времени на разбор. Чтобы помочь заказчику побыстрее принять решение, мы добавили в Ansible-роль многочисленные вспомогательные функции, такие как: наборы предварительных проверок, генерация предустанавливаемого файла hosts перед процедурой тиражирования, его валидация и даже интеграции с системой SAP Landscape Management. Часть этих функций и производимых действий была избыточна, зато мы показали возможности, да и вообще заказчику было приятно, когда у него автоматически все проверилось, забэкапилось и зедеплоилось по ландшафтам, а в интерфейсе их SAP Landscape Management загорелись зеленые индикаторы того, что все в порядке. Лишние хлопоты иногда стоят того, чтобы люди себя чувствовали спокойно и не сопротивлялись полезным изменениям. Далее начался процесс внедрения. Разумеется, в процессе внедрения приходилось дорабатывать логику работы темплейта, вносить изменения в код для оптимизации и ускорения. Нам удалось запараметризовать максимальное количество вариантов для создания записей в файле hosts и вынести их в group_vars. Это упростило управление и сократило время работы скрипта. На первой стадии внедрения мы ограничились запуском плейбуков с Ansible jump-хоста. В дальнейшем удобство использования Ansible и наш энтузиазм вылились в создание все большего числа инфраструктурных ролей, с помощью которых мы стали упрощать всевозможные типовые задачи, такие как настройка NTP (ntpd и chrony в зависимости от операционной системы), установка и конфигурация Splunk и Zabbix-агентов, сетевые настройки, установки различных пакетов, выполнение обновлений операционных систем, управление firewall (iptables на Linux и genfilt в AIX), управление параметрами ядра, пользователями, группами, паролями и многое другое. Отдельное место в развитии проекта заняла адаптация ролей под AIX. Поскольку Ansible чаще используется с Linux, то в разных библиотеках модулей почти всегда можно найти тот, который бы отвечал заданным требованиям. Для IBM AIX, по сути, есть только одна поддерживаемая библиотека Ansible-модулей. И для многих, казалось бы простых функций, реализованных модулями Ansible для linux, в ней нет подходящих решений. Например, в процессе написания некоторых функциональных ролей нам пришлось писать свои модули для аналога locale_gen (Locale в Unix – набор параметров, определяющий региональные настройки пользовательского интерфейса, такие как язык, страна, часовой пояс, формат вывода даты и пр.) и для управления RPM пакетами. В общем, под AIX тоже можно делать DevOps, если очень захотеть. В итоге все это стало экономить нам большое число человеко-часов. Допустим тиражирование файла /etc/hosts занимало день, потому как нужно было подготовить скрипт, кастомизировать, дописать строчки, удалить строчки, протестировать (и помолиться, чтобы все прошло удачно на проде). Сейчас вся подготовка занимает порядка десяти минут. В последствии, с увеличением вовлеченности инженеров техподдержки в использование Ansible-репозитория, одного jump-хоста стало недостаточно. Пришло время доработать проект и довести все до логичного завершения – полноценные пайплайны для автоматизированного развертывания. В результате появился объединенный репозиторий с множеством Ansible-ролей наподобие локального ansible-galaxy, способных управлять инфраструктурой заказчика и конфигурировать системы в разных ландшафтах независимо от их специфики. Репозиторий хостится в корпоративном GitLab и работает по рельсам пайплайнов нажатием кнопки в веб-интерфейсе. Все это максимально страхует от человеческого фактора, экономит кучу времени и позволяет вести нормальную совместную работу. Вся эта история скорее про то, что любые систематические трудности и специфические требования заказчика можно решить с помощью автоматизации. В описанном случае нам остается лишь добавлять в локальный galaxy новые роли, совершенствовать существующие плейбуки, оптимизировать и добавлять сценарии их выполнения. Многие воспринимают системное администрирование как выполнение рутинных задач. А инфраструктуру на больших мейнфреймах с проприетарными операционными системами, как недостаточно гибкую и плохо поддающуюся автоматизации. Но даже с такой специфической инфраструктурой, как системы IBM Power Systems под управлением AIX, можно создать Devops-подход для автоматизации рутины по принципу Infrastructure as a Code.",
    "146": "Приветствую всех читателей Хабра!Меня зовут Белоусова Александра, я развиваю направление по обучению и стажировкам аналитиков в «Автомакон». У меня довольно разнообразный профессиональный опыт: была и бизнес-аналитиком, и системным, и аналитиком данных, руководила проектами и командами разработки. За все это время провела 100+ собеседований различного уровня специалистов в сфере IT-технологий (от кандидатов без опыта и еще студентов до архитекторов и сеньоров с опытом более 20 лет). Полгода назад в компании мы запустили направление по обучению (стажировки). Ежедневно я провожу интервью с 2-3 кандидатами. После 50+ собеседований заметила несколько довольно грустных и негативных, на мой взгляд, трендов: при этом уровень профессионализма этих свежеиспеченных спецов оставляет желать лучшего. В первую очередь я связываю это с тем, что они получают очень поверхностные знания, если сами не занимаются саморазвитием; многим таким специалистам хочется получать много денежек, так как они уже много вложили в обучение. Плюс обещания, что после окончания курсов у них будут «миллионные заработки». Это маркетинг — ничего личного 🙂 Приходят ко мне на собеседованиях они примерно так: *В связи с необходимостью использования в статье словосочетаний Аналитик Данных, Бизнес Аналитик и Системный Аналитик, для удобства ввела несколько сокращений: АД, БА и СА. Расскажу подробнее, с чем приходится довольно часто сталкиваться на собеседованиях. Несколько ключевых замечаний, от которых действительно больно: Понимание у окончивших курсы аналитиков, кем они будут работать, складывается только из того с какими инструментами будут работать (как правило, это стандартный набор: SQL, Python, Базы данных и прочее). Только вот c таким набором инструментов работают сейчас все аналитики и не только! Согласитесь, довольно сложно просто на инструментах объяснить, кем ты будешь, чем и как сможешь помогать бизнесу. Подобные специалисты ищут работу по принципу «что умею, там и пригожусь». На популярных платформах поиска работы, например, hh.ru кандидаты просматривают вакансии со знакомыми словами или настраивают фильтры требований по тем инструментам, которым думают, что обучились, и откликаются на все подряд. Многие затрудняются с ответом на вопрос: «Кто же такие БА и СА и в чем их главная задача?» Если кандидат может различать АД, БА и СА, едем дальше. Если же не повезет, то читаю на собеседовании часть лекции из стажировки. «Переводчик с языка бизнеса на язык разработчика», «докопаться до сути проблемы заказчика и передать ее разработчику на его языке» — все эти определения очень близки к правде и действительно отражают функционал БА и СА. Но когда прошу объяснить, что это значит, и привести пример, кандидат буквально цепенеет. Затем следует что-то по типу «просто прочитали или услышали». Кандидаты видят в вакансии слово аналитик и даже не понимают, что аналитики бывают разными и, возможно, то, на что они откликаются, не является желаемой вакансией. Вопрос на собеседовании: «Почему системная аналитика?» также вводит в ступор, так как они и не знали, что аналитика бывает разной и что вообще есть какие-то другие аналитики, кроме аналитиков данных. Я понимаю, что все мы себя «продаем» на рынке труда, но как же это грустно и печально, когда ты занимаешься не тем, что тебе интересно, а тем, чему обучили и это «стильно, модно, молодежно». Как говорят сейчас: «IT — это дорого-богато», «Все деньги в IT», «Лишь бы войти в IT, а там разберемся». Понимаю, что довольно тяжело найти актуальную информацию, не только сухие факты, а «жизу», то, как реально обстоят дела. Надеюсь, это прочитают и правильно воспримут те, кто действительно хочет разобраться и понять, чем отличаются одни аналитики от других. Особенно будет полезно и своевременно для тех, кто только планирует сделать первый шаг и выбирает между направлениями. АД или Аналитики данных (или как я их называю — аналитики по отчетности) включают в себя: Data-аналитик, Data Science аналитик (Data Scientist), BIG-data аналитик, аналитик машинного обучения, продуктовый аналитик, BI — аналитик, ETL специалист, финансовый аналитик, маркетолог-аналитик и все остальные ребята, чья работа связана с (внимание!) обработкой информации из баз данных (База данных — это упорядоченный набор структурированной информации или данных, которые обычно хранятся в электронном виде в компьютерной системе, а значит под них попадают и SQL, и Access, и Excel, и, не побоюсь этого слова, но все же встречалась 🙂 Google-таблицами, и пр.). Эти крутые ребята сами себе и заказчики, и аналитики, и  (!) разработчики. Они сами пишут алгоритмы, кодят и тестируют. формирование данных для анализа базы данных (подготовка, нормализация, чистка, подгон до необходимого формата и пр.); формирование выводов, рекомендаций, прогнозов на основании данных, которые они проанализировали и добавили к этому здравую щепотку логики, статистики, сезонности и др. влияющие на итоги показатели; создание и анализ отчетности для компании (показательной, удобной и учитывающей потребности и пожелания Заказчика), конверсии (по-простому – процент успешных заявок), воронки продаж, A/B тестирование и др. Обычно АД работают для подразделений продаж и маркетинга; создание дашбордов (красочных и обязательно ПОКАЗАТЕЛЬНЫХ для оперативной работы топ-менеджмента Компании). И еще одна очень важная вещь именно для АД — они не могут работать без БД. Их работа основана на наличии данных для анализа. Если данных нет, то и работы у них нет! Зато она появляется у БА и СА!))) и сейчас разберем, что же тут по различиям и функционалу. БА или Бизнес Аналитик — специалист, работающий с БИЗНЕСОМ, иначе с бизнес-процессами. Работающий означает создающий, проводящий анализ, оптимизацию и, если его скиллы позволяют, то и автоматизацию бизнес-процессов для сокращения издержек (трудовых, временных ресурсов и пр.) и повышения эффективности работы бизнеса. Обращаю особое внимание на то, что оптимизация и автоматизация — это разные понятия. Автоматизацией как раз занимается СА. СА или Системный аналитик — это специалист, работающий с СИСТЕМОЙ, иначе с бизнес-процессами в системе (согласно своей специализации, например: 1С, SAP, CRM-системы, сайт, приложения и т.д.). Работающий означает все то же самое, что и у БА, но с углубленным пониманием этих процессов в системе (если типовая система, то уже есть какие-то шаблоны функционала для реализации бизнес-процесса в системе), как они будут взаимодействовать внутри архитектуры, платформы, какие необходимо создать или изменить объекты ПО для работы, как простроить интеграцию и другую внутрянку системы. По сути, это один и тот же специалист с большим или меньшим налетом технической подкованности. В компаниях сейчас редко встречаются БА в чистом виде, так как они уже давно переросли в СА. Если вы решаете взять БА вместо СА, то весь глубокий анализ, который по-хорошему должен быть на аналитике, придется взвалить на себя разработчику (чему он точно не будет рад). Это значит, что БА будет стоит компании меньше, чем СА из-за набора скиллов. активное взаимодействие с Заказчиком и командой (обсуждение задач, сбор/уточнение/обсуждение требований, проведение интервью, постановка задач в различных трекерах и др.); другие задачи, сопровождающие разработку, внедрение и поддержку/сопровождение ПО (например, участие в оценке задач проекта, декомпозиции, проектирование архитектуры в качестве помощи архитектору и рост до него в будущем, администрировать проект, взаимодействовать с подрядчиками и др.). и многое другое. Чтобы АД смог сделать отчет по прибыли и дать по ней рекомендации и прогнозы, БА или СА должен придумать как она (информация по суммам заказа/продажи, оплатам) будет попадать от менеджеров по продажам в Базу данных удобным способом, а именно куда (в каком инструменте, ПО) данные по доходу, затратам, заказам, клиентам и прочая инфа для качественного анализа будет удобно вноситься (выбираться из справочника), храниться, создавать зависимости. А еще как прикрутить к этому процессу других участников, например, бухгалтерию (для ведения взаиморасчетов и автоматических напоминаний о платежах), документооборот (возможно, через ЭДО), финансы (для планирования затрат на год), производство (для своевременной подготовки продукции на продажу) и др. И самое важное во всем этом (!) — даже если ты технически МЕГАподкован, но не умеешь «докопаться до сути, первопричины, боли Заказчика», то, к сожалению, «грош цена тебе, как аналитику». А что же это действительно значит — большая отдельная тема, о которой я опишу в следующей статье. Исходя из изложенной информации, можно понять, почему же в отсутствии данных для работы АД появляется работа у БА и СА. Почему? Потому что их работа заключается в том, чтобы создать все условия для того, чтобы эти данные начали попадать в БД посредством различных автоматизаций работы специалистов и внедрения в их ежедневную рутину ПО (используя которое, они могут вносить информацию для ее дальнейшей аналитики). Иногда для своей работы они используют и функционал АД, и должны «включать их в себя». Таким образом, БА и СА на 50% АД. Но не все АД могут перейти в БА и СА (хотя у меня получилось😊 – устала ждать, когда данные для анализа кто-то начнет вносить в систему с их листочков и блокнотов и пошла создавать и автоматизировать бизнес-процессы сама). Благодарю всех, кто дочитал и, надеюсь, нашел ценную информацию для себя. Если вы выберите путь в аналитике, то, возможно, с этой информацией будет попроще)) Рада буду обсудить ваше мнение в комментариях. А еще мы расширяем штат кураторов стажировок. Если, ты системный аналитик, который готов помогать обучать “свеженьких” стажеров своему ремеслу, делиться своим опытом: бизнесовым и техническим, то очень тебя ждем! Откликнуться можно здесь.",
    "147": "Всем привет! Меня зовут Максим, я тимлид деливери-менеджеров в Тинькофф Кассе. Расскажу, из чего состоят изменения, а также про одну методологию работы с ними. Будет кейс, как мы применяли ее на практике. Будет немного про провалы изменений, про структурный подход и факторы успеха любого изменения. Главная мысль, которую я хотел бы до вас донести: системная работа повышает вероятность успеха изменений. Добро пожаловать под кат разговаривать про изменения! Контекст изменений — это OКR, Objectives and Key Results. Мы внедряли его в Тинькофф Кассе и даже не один раз, а два. Первый раз не очень удачно, второй раз получше. Давайте подумаем, почему могут проваливаться изменения в организации? Например, низы не могут, а верхи не хотят. Закидывайте ваши ответы в комментарии, а вот мои варианты провала организационных изменений: просто нет. Мне нравится вариант «просто нет» — не покатило, не получилось. Нормально общались, но потом «нет». Важно управлять человеческой стороной изменений и системно подходить к изменениям, чтобы они были успешными. Мы попробовали применять методологию Prosci для управления изменениями. Prosci предлагает зрелый подход, ее результативность проверена опытом и подтверждена результатами исследований. В этих исследованиях люди отвечают на вопросы про изменения: что влияет и не влияет, что для них важно и не важно. Техническая сторона — что-то вроде проектного менеджмента. Например, если нужно заменить лампочки в здании, проект состоит в том, чтобы купить и установить лампочки. Если здание из 60 этажей — это будет посложнее, если из одного — полегче. Но при этом покупаем лампочки, устанавливаем, и изменение завершено: появились новые лампочки. Человеческая сторона  — это как с OKR. Общая идея заключается в том, что пока люди изменение не примут, оно не будет успешным. Нельзя щелкнуть — и все начнут целеполагать. Нужно вовлекать, чтобы изменения принимали и действительно использовали. Вот важнейшие факторы успеха любого изменения в исследованиях Prosci. Спонсор — обычный человек, который недоволен чем-то или хочет что-то изменить. У него есть цель, ресурсы, но самое главное, определенная власть это изменение производить. Он может делегировать свою власть, ему не все равно, и он будет участвовать. Сотрудники ждут коммуникации по изменениям — кто и о чем должен с ними поговорить. От генерального директора, президента компании или их заместителей ждут видения, как будет выглядеть TO BE, что мы хотим увидеть в итоге и почему нам всем туда надо. На других уровнях находятся другие спонсоры — локальные менеджеры и руководители разных уровней. Вы знаете, что, если не вовлечь менеджеров среднего уровня в изменения, высоки шансы, что они могут саботировать его? Могут сопротивляться, убеждать своих сотрудников в нем не участвовать или просто не будут поддерживать. Это может происходить потому, что изменения несут риски потери контроля, дополнительную нагрузку, недовольство рядовых сотрудников. Часто высокое руководство или агенты изменений не помогают менеджерам среднего звена в реализации изменений, игнорируют дополнительную нагрузку, не вовлекают их. В таком случае закономерно начинается отторжение изменений и попытки сохранить текущие правила и свое положение. Задача спонсора — активное и видимое участие. Нужно, чтобы человек не только говорил, что что-то меняется, но и помогал изменениям. Спонсоры должны обязательно формировать коалицию. Мы понимаем, что один генеральный директор не может поддержать любое изменение. Ему нужно собрать, например, директора по маркетингу, директора по логистике и директора по ИТ. На всех уровнях его идею должны поддержать, каждый должен быть за. И еще один момент: для всех вовлеченных в изменения важно, чтобы спонсор был открыт для разговоров со всеми. Когда идет большое организационное изменение, нужно общаться. Нужно иметь возможность говорить с человеком, который его начал, и узнавать новости от первого лица. Не слышать информацию через десяток менеджеров, которые стоят по цепочке, а узнавать все напрямую. Главный риск со спонсорами: они могут перестать вовлекаться, если принимают решения не на своем уровне. Или спонсор вообще не принимает решений. Вы рассказываете ему, как все происходит, и от него ничего не просите. А зачем он тогда? Спонсор должен работать там, где может. Долой все мысли про то, что, если к нему не прийти, это сэкономит его время. Спонсор должен обязательно участвовать. При работе со спонсорами потребуются вложения. Мы вкладываемся в прозрачность через метрики, фидбэк, сводки. Вкладываемся в коммуникацию, обязательно размечаем границы и работаем с тем, чтобы выстроить ожидания. Если спонсор понял и оценил вложения, он ответит децентрализацией. Спонсоры привлекают дополнительных людей, быстро согласовывают и эскалируют на какие-то недоступные нам уровни. Вложения в спонсоров всегда окупаются. Поэтому корректное и заинтересованное спонсорство — один из важных компонентов успешного изменения. Prosci провели исследование на тему ожидания сотрудников — от кого они ждут сообщения о проведении изменений. Менеджер изменений — это владелец продукта. Изменения — очень запутанный контекст, и здесь нет верных ответов. Нужно постоянно быть готовым работать с неопределенностью и что-то менять, поэтому вперед выходят метрики и гипотезы. Мы постоянно проверяем что-то и смотрим, как это отражается на метриках. Я рассказывал о менеджерах изменений и их работе в докладе: Метрики в Тинькофф Кассе с OKR — это просмотры страниц про него. Самая важная метрика, на которую я опирался, когда мы только начинали внедрение OKR, — ADKAR. Само название ADKAR — мнемоническая техника, каждая буква — определенный критерий, который мы измеряем либо с помощью субъективной оценки, либо с помощью опроса. И смотрим, как с ними работать. В методике ADKAR считается, что первая по порядку оценка ниже 4 баллов 3 это барьер. Барьер — некий критерий, с которым нужно работать прямо сейчас. Если барьер на понимании, нет смысла накачивать людей каким-то желанием. Объясните им, что происходит и почему. Если барьер на желании, нужно работать именно там. На все остальное можно временно не обращать внимания. Очень часто с самого начала работают со знанием. Например, нужны изменения — приходи на тренинг, сейчас будем тебя учить. Лови книжку — почитай. Такая ситуация — один из распространенных антипаттернов ADKAR. Другой антипаттерн, когда люди не способны выполнять работу по новому. Например, они понимают зачем, они хотят, но здесь вступает в силу мысль «Я не могу, мне страшно. Я был умным до изменения, а сейчас стал глупым». Человек — профессионал, но теперь не понимает, как ему работать. Мы периодически проводим опросы у себя в Тинькофф Кассе. Результаты потом оцениваются, и мы работаем с этими оценками, анализируя, сколько людей в какой барьер уперлись. Мы получаем оценки по буквам и работаем с этапом, на котором больше всего блокировок. Сначала люди не понимают, что происходит, а значит, с этим нужно работать. Тренинг на этом этапе бесполезен. Нужно объяснить людям, зачем производятся изменения. Мы объясняли, и картинка к октябрю и февралю изменилась. Постепенно доля барьеров по Awareness снижается и появляются барьеры сначала по знаниям, потом по желанию. На этом уровне много работаем с руководителями среднего звена. Я сам ни к кому не хожу, потому что мне никто не поверит. От меня никто не ожидает такой информации. Ждут от руководителей или топ-менеджеров, которые придут и скажут, зачем мы это делаем. В процессе использования ADKAR важно вкладываться в сбор обратной связи по опросу и распространять понимание, для чего мы этот опрос проводим. Нужно давать понятную интерпретацию. Например, многие люди думают, что опрос анонимный. Нужно донести, что это не так. Но почему не анонимный и почему это безопасно, тоже нужно объяснить. Все это дает и список гипотез, и вовлеченность, и доверие. Когда мы говорим, что руководство поддерживает изменения, люди это проверяют. Если руководство поддерживает только на словах и не работает по-новому, люди тоже не принимают изменения и вовлеченность падает. Мы письменно согласовывали принципы, по которым ведем изменения. Собирали принципы, формулировали их, дискутировали на эту тему с группой руководителей. В финальной версии, когда принципы были приняты, мы создали документ, попросили каждого руководителя подписать, что он с ним согласен. А потом сделали открытую презентацию и всем рассказали, что руководители согласны. Такой подход возымел эффект. Люди приходили ко мне и говорили, что принципы изменений работают, мы применяем их на своих встречах, вспоминаем про них. Коммуникация создает для всех единый контекст внедрения изменения. Я упоминал, что системная работа повышает вероятность успеха изменений. Для этого еще важно понять, что такое успех изменений в каждом конкретном случае. Я не знаю, как измерить успех, но у меня есть допущение, что интерес к информации коррелирует с успешностью изменения. Если люди приходят читать про нашу новую систему, скорее всего, они более вовлечены, чем если никто никогда про это не читает. С таким допущением посмотрим несколько графиков. Изменения начинаются с того, что какой-то большой руководитель сказал: «Ну-ка, взялись и поехали». И все интересуются, что происходит. А потом идет спад. Затем высокие руководители вспоминают, что что-то происходило, и идут поинтересоваться, как там дела. Помните, мы что-то начинали и, значит, что-то должно происходить. Общая идея графика заключается в том, что просмотры страниц, которые сначала растут, потом падают, отражают политическую волю руководства. Руководство железным кулаком бьет по столу и говорит смотреть. Это работает, но не долго. Интерес к прижившемуся изменению имеет другой паттерн. На графике видно, как новый процесс начинает жить своей жизнью и люди читают документацию не потому что их заставили. Изменение перестает отражать политическую волю отдельных людей и начинает отражать себя. Это не кто-то сказал делайте. Это просто ритм изменения. На графике видны периоды постановки OKR и проведения чекина: Люди больше интересуются изменениями, когда с ними общаются системно, когда к этому более системный подход. Системная работа повышает вероятность успеха изменений, повышает вероятность того, что вернутся инвестиции, время, деньги, которые вы в это вкладываете. И я хочу оставить вас с фразой моего любимого специалиста по менеджменту изменений Питера Сенге: «Люди не сопротивляются изменениям. Люди сопротивляются тому, что их изменят». Как менеджер изменений, прошу вас, помогайте людям проходить через изменения. Сделайте так, чтобы они не сопротивлялись. Сделайте так, чтобы им не было страшно. И на прощание полезная ссылка на блоги и статьи — можно почитать, чтобы лучше погрузиться в тему. Читаю и пишу https://t.me/ireadshitbooks",
    "148": "Привет, Хабр! Меня зовут Кирилл, я работаю в IT более 13 лет. Сначала инженером по внедрению, потом DevOps, потом SRE, также работал руководителем группы сопровождения. Сейчас SRE в VK Рекламе, поэтому знаю, как важно делать правильные инструменты для анализа проблем. В любом проекте и компании я иногда сталкивался, а иногда сам создавал проблему: огромное количество дашбордов. Вспомните ситуацию, когда вы в Grafana ищете какой-нибудь дашборд, пишете, например, «Tarantool», и вам выпадает огромный список дашбордов, которые кто-то до вас насоздавал. Это могут быть кастомные дашборды, которые кто-то делал для какого-нибудь инцидента, или просто созданные другими специалистами. Часто бывает, что половина этих дашбордов нерабочие или на них нет чего-то полезного. Как правило, обилие дашбордов создаёт ряд проблем: информационную перегрузку, потерю фокуса, сложность восприятия, а самое главное, затруднение исследований инцидентов. Попробуйте себе честно ответить на вопрос: глядя на свой дашборд, вы можете понять, работает ваша система или нет? Если нет, то читайте дальше. За всё время работы у вас наверняка накопилось огромное количество интересных сообщений в чате. На самые разные темы, например: «Есть ли проблема с сервисом X?» Или кто-то врывается в чат с предсказанием: «Я думаю, что сервис X работает сегодня медленно». Как мы можем ответить на эти вопросы в моменте? Чаще всего смотрим графики в Grafana, анализируем тренды и пытаемся найти ответ. Но сделаем шаг назад и разберёмся, а как вообще появляются дашборды? Думаю, стандартный для многих процесс выглядит так: берём в Grafana дашборд — это может быть дашборд сервисов, инфраструктурный дашборд и т. д. — и допиливаем его под себя: удаляем лишние графики, обогащаем своими данными. Он становится огромным. С точки зрения мониторинга какого-то маленького компонента сервисов или инфраструктуры это вполне допустимо, потому что в целом мы покрываем 70-80 % запросов, которые нам нужны от мониторинга. На таких дашбордах чаще всего мы видим техническую составляющую, а не пользовательский опыт. Это стандартные request, latency, топики в Kafka, лаги и т. д. Такие дашборды трудно воспринимаются человеком, не погружённым в проект. Тут вы можете меня спросить: «А зачем человеку, не погружённому в проект, смотреть на наши дашборды? » Ответ очень простой. Мы не живём в вакууме, наши сервисы взаимодействуют с другими компонентами и платформами, и мы должны понимать, всё ли корректно работает и у нас, и в сервисе другой команды. Эксплуатация/L2. Это люди, которые занимаются поддержкой нашего production, бдят 24*7, либо по запросу реагируют на наши проблемы. Они менее погружены в проект, чем команда разработки. Команды разработки, в том числе текущего проекта и внешние команды, от которых мы так или иначе зависим, или которые зависят от нас. Менеджеры, которым необходимы графики, чтобы отслеживать проблемы. Пользователи. Вы можете мне смело возразить: «Какие пользователи, что ты несёшь?!» На самом деле пользователи могут быть не только внешние, но и внутренние, например, пользователи наших API и внутренних сервисов. И вообще, история с внешней наблюдаемостью, когда люди могут посмотреть на то, как наш проект работает, мне кажется очень классной. Пользователи тоже очень важны с этой точки зрения. Мы хотим получить некий единый дашборд здоровья сервисов. Он должен быть не только красивый, но ещё и понятный, причём не только людям, которые делают этот дашборд, но и тем, которые используют каким-то образом наши сервисы и хотят узнать, работает ли вообще наш проект. Прежде чем перейти к проектированию и исследованию, предлагаю не придумывать велосипед, а посмотреть на другие сферы нашей жизни, которые могут помочь в проектировании и создании такого дашборда. Однажды я обратил внимание на маленький прибор на панели в самолёте, который называется основной пилотажный дисплей. Этот небольшой дисплей находится постоянно в поле зрения пилота и показывает огромное количество информации. Пилот, смотря только на один этот прибор, может принять решение о том, как чувствует себя самолет в воздухе, какие у него угол крена и тангажа, высоту, скорость и другие параметры полёта. Возьмём отсюда идею компактности подачи информации. Понятное дело, что такой основной пилотажный дисплей будет не единственным в нашей системе. Есть ещё огромное количество различных графиков. Как нам получить доступ к какой-то дополнительной информации, к текущим графикам? Я обратил внимание на процесс, как мы принимаем решение обратиться к врачу. Если мы чувствуем какую-то боль, то обычно идём к терапевту. Он пытается продиагностировать наше состояние, назначает лечение или направляет к узкому специалисту — хирургу, аллергологу, дерматологу и т. д. Это второй уровень исследования нашей проблемы. Сверху — уровень сервисов, на котором нет ничего, кроме светофора: есть ли проблема с различными сервисами. Если мы понимаем, что она есть проблема (на иллюстрации проблема с сервисом D), то можем провалиться на уровень ниже. Там мы получаем уже более детальные графики нашего сервиса. Это может быть Latency, Four Golden Signals, USE, RED, какие-то кастомные метрики зависимых компонентов этого сервиса, и т. д.. В нашем случае мы используем Four Golden Signals (Latency, Traffic, Error, Saturation). Если эти графики нам не помогают, мы видим, что по ним у нас всё более-менее ровно, но проблема ещё существует, мы можем обратиться на уровень ниже и посмотреть, что происходит с инфраструктурой: потребление памяти, процессора, сети, диска, а при работе хостов на физических машинах, мы можем посмотреть метрики Cube API, и т. д. То есть мы «проваливаемся» в дашборды для получения более точной диагностической информации. Централизованный мониторинг. С помощью такой панели мы можем быстро и удобно отслеживать работоспособность, производительность и доступность наших компонентов.. Улучшенная видимость. Мы объединяем огромное количество данных из источников, можем быстро выявлять какие-то закономерности, когда что-то не работает. Эффективное сотрудничество. IT держится на коммуникациях. Мы должны уметь общаться с другими командами, с другими коллегами, и делать это эффективно. Дашборд нужен как раз для этого. Мы не даём внешним командам кучу непонятных графиков, возможно, красивых, но непонятных. Мы даём им единый дашборд, где они могут посмотреть состояние сервисов на текущий момент. Оптимизация поиска проблем. Наличие необходимой информации в одном месте позволяет достаточно быстро находить проблему и оптимизирует время устранения инцидентов. Мы примерно определились с тем, что хотим. Но сказать — не значит сделать, нужен план. Определение целей и показателей. Разработка макета. Сбор технических показателей. Пороговые значения. Управление инцидентами. Тестирование и обратная связь. Непрерывное улучшение. Но как понять, работает ли сервис? Мы уже говорили о том, что он не существует в вакууме, взаимодействует с другими платформами, передаёт и запрашивает данные. Можно ли, например, считать, что всё хорошо, если открывается главная страница? Конечно, нет. Идея в том, чтобы использовать понятие Критический Пользовательский Путь. Это подход, который описывает ключевые взаимодействия между пользователями и продуктом. CUJ — это путь (иногда частичный), который проходит пользователь в нашем продукте. И этот путь создаёт наибольшую ценность: финансовую, репутационную и т. д. Если на каком-то из этапов CUJ есть сбои, то мы должны знать об этой проблеме. Сегодня рассмотрим такой сценарий: критический пользовательский путь для нашего мониторинга. Что мы должны сделать? Первое — разбить CUJ на технические компоненты, которые так или иначе участвуют в показе статистики. У нас есть пользователь, который в личном кабинете запрашивает статистику. Запрос уходит в сервис статистики, а от него в кеш-коллектор. Тот обращается в Redis и смотрит, есть ли там нужные данные. Если нет, то идёт в СУБД. А как данные туда попадают? Есть некий сервис producer, который пишет данные в Kafka, откуда их читает сервис consumer и перекладывает в СУБД актуальную статистику. сервис быстрой статистики. После того, как мы определили технические компоненты, которые так или иначе участвуют в пользовательском пути, необходимо принять измеримые решения, необходимые для лучшего SLO. Availability — количество доступных запросов, которые мы можем обработать. Latency — задержка до обработки запроса. Quality — количество запросов, обработанных без ухудшения качества. Correctness — количество корректно отображаемых валидных данных. Freshness — количество обновлённых корректных данных ранее какого-то порога. Throughput — скорость обработки данных. Coverage — охват. Completeness — полнота. Durability — вероятность потери данных в хранилище. В нашем случае мы принимаем, что будем измерять SLO в Availability, Latency и Freshness. Дальше посмотрим, почему. После того, как мы определились с техническими компонентами, которые так или иначе участвуют в нашем процессе, и с теми SLO, которые будем накладывать, переходим к следующему пункту — это заполнение таблички, которая говорит о том, какие компоненты мы измеряем, какие типы SLO накладываем на эти продукты. Это мы принимаем для себя как доступный уровень обслуживания. На самом деле проектирование дашборда можно сделать нулевым шагом. Как правильно подойти к этому, какие инструменты мы используем? Рисуем на бумаге. Для себя я понял, что это как с презентацией. Когда готовишь её, открываешь чистый лист и не понимаешь, что туда разместить. Прежде чем начать проектирование чего-то, например, в Grafana, хорошо бы нарисовать это либо на бумажке, либо в Mirro, поместить все свои желания, а дальше уже собрать что-то актуальное и красивое. Grafana — это уже де-факто стандарт визуализации метрик, многие компании ею пользуются. Мы используем несколько плагинов: Распределение по критичности CUJ. Важно, что на дашборде мы распределяем CUJ по уровню критичности. Например, в онлайн-магазине у нас есть три пользовательских пути, не все из них mission critical. Threshold как SLO, чтобы подсвечивать, делать светофор на наших дашбордах. Здесь все идеи, которые мы расписали для себя, а затем начали анализировать их и пытаться переложить в Grafana. После этого пришло время собрать технические метрики и показатели. Определяемся с источниками данных. Дорабатываем сервисы. Понятное дело, что мы явно заложили не все метрики, которые так или иначе у нас есть на текущий момент. Это тоже важный этап, про который нужно не забыть и проговорить его. Чтобы мы могли понимать, когда есть проблемы, нужно так или иначе подсветить их на дашборде. Уровни критичности CUJ. Первое, что мы определяем — это уровни критичности нашего пользовательского пути (critical, low, medium, high), всё зависит от сценария. В первом приближении мы закладываем в качестве SLO технические метрики, так как на них мы можем прямо повлиять. Вместе с тем, не стоит забывать о других типах метрик. Даже если мы сами не можем повлиять на них напрямую, крайне важно вовремя обозначить какие-либо проблемы коллегам. Поэтому будьте готовы к тому, чтобы включить бизнес-метрики в критический пользовательский путь. Правильный выбор канала оповещений. Это тоже очень важно. Понятно, что никто не будет непрерывно смотреть на графики. Есть люди, которым интересно получать оповещения по тем или иным проблемам. Здесь важно выбрать правильный канал оповещения. Коллега рассказывал про alert fatigue — выгорание от оповещений. Например, мы распределяем наши оповещения в зависимости от уровня канала и степени назойливости. Например, если инцидент mission critical, всё горит, нужно срочно решать, то это может быть звонок на телефон. Этот способ вырвет вас из домашней среды, из вашего work-life balance и поместит в среду решения проблемы. Все остальные проблемы (low, medium, high) можно передавать, например, в рабочий чат. Список открытых инцидентов. Представьте ситуацию, что у вас есть дашборд. К вам приходит пользователь, видит, что какой-то сервис красный, и начинает бить в колокола, кричать, что у меня проблема, ничего не работает. Но по факту ваша команда эксплуатации или вы уже заметили эту проблему, минут 15 назад уже подняли инцидент и начали его решать. Вам не нужно, чтобы кто-то к вам пришёл и пытался нарушить процесс решения инцидента. Поэтому на дашборд важно добавить информацию о текущих инцидентах, чтобы пользователь, который зашел на дашборд, увидел, что да, сервис D красный, пользовательский путь красный, но есть инцидент, открытый командой эксплуатации или вами, и в рамках него вы уже проводите какие-то работы, так что дополнительно оповещать об этом уже не нужно. Здесь мы просто добавляем список открытых инцидентов в дашборд. Это может выглядеть по-разному. У нас любой инцидент выглядит как оповещение в Grafana, и мы его отрабатываем. Либо это может быть какой-нибудь процесс через внутренние системы, либо ручное добавление. Всё очень сильно зависит от ситуации. После того, как мы всё это сделали, нельзя просто взять и отдать такие наработки в другие команды, нужно протестировать и дать обратную связь. Тестируем получаемые данные. Уверен, что 90 % ваших оповещений сначала будут false positive. Поэтому важно в течение одной-двух недель протестировать, когда вы просто проверяете работоспособность вашего дашборда, корректность данных, которые предоставляете, и что у вас нет false positive. Собираем обратную связь от пользователей. Как разработчики инструмента мы всё понимаем в нём, но вопрос — понятно ли людям, для которых мы этот дашборд делали? Поэтому отдаём его им и собираем обратную связь, понятен ли этот дашборд, выполняет ли он функцию, ради которой его делали. Продаём идею в другие подразделения, когда команд много. Вы можете сделать что-то в рамках своего подразделения, но надо это ещё показать другим, рассказать им. Здесь совет простой, буду капитаном: мы просто создаём дашборд в одной команде, потом делаем Proof of Concept, после этого приходим в другую команду, показываем, как это работает, зачем это нужно, пытаемся эту идею продать. Плохо, когда мы что-то делаем и никому не рассказываем — опять же это про коммуникации. Мы рассказываем командам, как они могут у себя применить такие дашборды и как мы можем им помочь такие дашборды сделать. Не конечный процесс. Следим за появлением новых критичных процессов. В конечном итоге важно помнить, что процесс создания такого дашборда, как в целом мониторинг и observability, — это не конечные процессы. У нас добавляются новые метрики, появляются новые бизнес-процессы. Важно, что мы в каждой точке наших итераций должны следить за актуальностью нашего дашборда, потому что метрики могут выпилить, или добавить новый пользовательский путь, или один пользовательский путь может уйти в разряд некритичных, другой перейти в разряд критичных. За всем этим надо следить, и это тоже достаточно важно для организации правильного мониторинга и общей видимости. Много неактуальных дашбордов. Каждый, кто приходит, пытается что-то создать для себя, потому что ему чего-то недостаточно на дашборде другой команды. Получается огромный беспорядок, как в описанном мной примере: ищешь Tarantool, тебе выпадает список из 10 разных дашбордов, половина из которых уже в принципе не работает, потому что Tarantool уже начал метрики в другом формате отдавать, или дашборд вообще старый и делался под какой-то конкретный инцидент, и т. д. Долгий процесс поиска проблемы. Это про 10 тысяч вкладок, открытых в Grafana, когда ты пытаешься найти, где у тебя что-то сломалось, что-то не работает. Понятно, это не конечный процесс, он дорабатывается всё время. Что мы получили: Единый дашборд «живости» сервиса и наших пользовательских путей. Повысили прозрачность работы сервисы. Коллеги могут зайти и посмотреть на наши пользовательские пути, как наши сервисы работают. Мы открыты перед ними, не скрываем проблемы. Уменьшили время исследования проблем, связанных с пользовательскими путями. Здесь ничего не понятно, все зелёное. Когда мы понимаем, что проблема с сервисом, мы проваливаемся на один уровень ниже: Здесь получаем стандартные метрики Four Golden Signals (Latency, Traffic, Error, Saturation). Если и здесь непонятно, что с сервисом не так, то проваливаемся ещё ниже, на дашборд инфраструктуры: Смотрим метрики по latency, по памяти, перезапуски подов и другие, то есть проваливаемся ещё глубже и пытаемся анализировать проблему. Обогащаем дашборд сценариями. Мы покрыли пока малую часть того, что у нас в целом есть, потому что в нашем продукте огромное количество пользовательских сценариев. Добавляем синтетические транзакции. Классно мониторить тем путём, о котором я рассказал, но, если у вас есть какая-то транзакция, которая пройдёт весь пользовательский путь от начала до конца и выдаст финальный результат в конце — это ещё ценнее. В эту сторону мы двигаемся.",
    "149": "Важно не забывать про безопасность при разработке. По мере усложнения сценариев и архитектуры в онлайн и екоммерс сервисах риск возникновения ошибок возрастает. Поэтому мы обучаем разработчиков основам безопасности в вебе и регулярно проводим стороннее пенетрационное тестирование перед запуском. В этом материале расскажем о безопасности на примере мультиязычного e-commerce сервиса – интернет-магазина с аккаунтом покупателя. Проект построен на NextJS, где часть бекенда на JS и пишется фронтендерами. Поэтому помнить о безопасности в таком случае приходится более старательно, держа в уме случаи, о которых расскажем в этой статье. Пентесты обычно проводят перед запуском сервиса. Есть ряд известных уязвимостей, которые проверяют чаще всего, но отличие хороших пентестов как раз в том, чтобы попробовать найти уязвимости не только по методичке. Как уже упомянули выше, сервис, о котором мы говорим, построен на NextJS. Немного об архитектуре. Рендеринг страниц происходит на NextJS, а подтягивание контента и пользовательских данных сделаны через API на Umbraco (относительно недавно Umbraco зарелизила Content Delivery API для разработки headless решений). Пользовательские сессии так же создаются в Umbraco и потом используются в NextJS. Еще есть сторонний сервис Shopify, который реализует e-commerce функциональность, и NextJS сайт взаимодействует с ним через API. За счет этих архитектурных особенностей бекенд ответственность за безопасность размывается на бекенд и фронтенд разработчиков. Поэтому security issues могут возникать и там, и там. Фича: в личном кабинете пользователь может просмотреть свои текущие подписки. Данные тянутся POST-запросом из my account API на Umbraco, которая тянет активные подписки из Shopify. Тестировщик перехватил этот запрос, изучил его содержимое, убрал из него cookie пользовательской сессии, а также передал пустое тело запроса. В результате ему вернулись все подписки всех пользователей. При разработке подобного рода фич, всегда стоит держать в голове общую архитектуру приложения и думать о том как бэк и фронт в таких случаях коммуницируют между собой.  Глядя на диаграмму выше, представляем, как выглядела последовательность получения подписок: Клиентское JS приложение запрашивало подписки текущего пользователя из NextJS website, передавая cookie текущей сессии пользователя и email текущего пользователя. Собственно, те данные, которые тестировщик и вырезал в рамках теста. NextJS website, зная секретный Application token Umbraco, запрашивал подписки из Umbraco, и в качестве фильтра прокидывал тот самый email пользователя. Umbraco, зная секретный Application token Shopify, запрашивало подписки из Shopify, снова прокидывая email пользователя из параметров. Также оказалось, что Shopify API, когда ему передают пустой email в качестве параметра, возвращает все подписки всех пользователей, то есть параметр email в API выступает, как фильтр, и если он пустой – возвращается все. Сочетание этих факторов и привело к найденной проблеме. Из клиентского JS приложения нельзя передавать email и другие пользовательские данные, как параметры запроса. Вместо параметров email пользователя нужно брать из контекста сессии текущего пользователя, заодно эту сессию и проверив на валидность. На сайте есть возможность поменять пароль. Тестировщики перехватили запрос на change password и убрали identification cookie, отправили пустой запрос только с телом, где передаем старый и новый пароль на бэк. В одном месте безопасность такой запрос пропустила, а в другом проверила. Когда делаем запрос на изменение пароля, информацию о залогиненном пользователя нужно брать из identification cookies, а не из параметров. Другой случай подобного характера: на сайте есть секция project builder. Что-то вроде избранного, где пользователь может разложить понравившиеся продукты по папкам-проектам. Перехватили отправленный запрос, убрали identification cookie, но оставили тело запроса, в котором передается информация о том, для какого пользователя создать коллекцию и из каких продуктов. В итоге без аутентификации можно было обновить избранное другого пользователя. Ни один из уровней безопасности не отработал такое проникновение. В обоих случаях одна и та же уязвимость: данные передаются параметрами в запрос на API, а не берутся из сессии текущего пользователя. На сайте могут быть ситуации, когда пользователь (особенно справедливо для юзер порталов) может создать какую-то сущность, которую сайт потом будет показывать. Например, вышеупомянутые папки-проекты. В некоторых случаях в качестве имени проекта сайт разрешает вставить не только буквы, но и кусочек скрипта, который, например, будет красть пользовательские куки. Когда злоумышленник это делает, в алерте можно увидеть пользовательские куки или отправить их куда-нибудь. Тут справедливо можно заметить: я же сам себе создаю проект, значит никто эти данные кроме меня и не видит. На самом деле эта функциональность может получить дальнейшее развитие: в день релиза отображение имени проекта доступно только уникальному пользователю, а потом, в процессе доработки продукта, возможно появление роли админа. Тогда админский пользователь напорется на эту уязвимость, и уже его данные могут куда-то уйти. Другой вариант развития функциональности – добавление функции шэринга проектов между пользователями. Пользователь отдает проект другому, скрипт выполняется, и куки уходят. Редиректы. Пользователь, перемещаясь по сайту, может добавить продукт в корзину, а затем перейти на чекаут. Особенность имплементации сайта, о котором мы говорим, в том, что e-commerce сделан на Shopify – корзина отрисовывается на нашем сайте, но на чекаут – доставку и оплату – мы редиректим пользователя на Shopify портал. Получается, если пользователь залогинен на нашем сайте, это же залогиненное состояние нужно сохранить и при редиректе на Shopify. В терминологии Shopify такая имплементация Single-SignOn называется Multipass, и подробно про нее можно прочитать тут. Вкратце – чтобы сгенерировать редирект, NextJS отправляет запрос с URL для редиректа на Umbraco API, который шифрует этот URL секретным ключом от Shopify, а также подставляет идентификатор пользователя, под которым нужно залогинить при редиректе. Как оказалось, код нашего Umbraco API позволяет в URL с редиректом подбросить вообще любой URL, его не проверяет при редиректе и Shopify тоже. Это опасно тем, что можно попасть на фишинг. Злоумышленник может подготовить фишинговый сайт, который будет выглядеть как ваш сайт, первый домен будет валидным, а по факту при нажатии на ссылку пользователь перейдет на фишинг, и, доверившись изначальной картинке,  может выдать свои данные. Сделать так, чтобы логика редиректов обязательно проверяла редирект на предмет известного домена, перед тем, как этот редирект совершать. Во время логина на сайте пользователь вводит логин-пароль, после чего отправляется запрос на сервер. Бывает, что на сайтах нет ограничения на количество раз введения логина и пароля. Злоумышленник в таких условиях может подготовить скрипт для перебора паролей и запустить его на продолжительное время. Обычно на такие вещи делается пара уровней защиты. Такие формы логина могут быть скрыты под капчей — таким образом, при большом количестве повторений злоумышленнику придется автоматизировать и прохождение капчи. Если раздражает выбирать на картинках светофоры, то можно сделать невидимую капчу от Google, которая будет проверяться на бэке, прежде чем дальнейший код начнет выполняться. Второй способ защиты – rate limiting: с одного IP, браузера и так далее. Самое простое — после нескольких попыток логина с одного IP адреса, на последующие частые запросы возвращать ошибку. Также есть алгоритмы, которые кроме троттлинга еще и увеличивают время ожидания. То есть на первую попытку неверного логина сайт быстро произведет ответ, на вторую возьмет больше времени и так далее. Если время ответа увеличивать экспоненциально, то уже после нескольких неудачных попыток подбора пароля ждать придется очень долго. Представим, что кто-то все-таки взломал аккаунт пользователя, и от его имени начал что-то делать на сайте. Пользователь тут же сбрасывает пароль, но злоумышленник по-прежнему остается залогиненным, так как его сессия все еще активна. Помимо собственно сброса пароля, хорошей практикой является и сброс всех активных сессий, которые для этого пользователя созданы. Таким образом, если в нашем случае за создание и проверку сессий отвечает приложение NextJS, то и сброс всех активных сессий там тоже должен происходить при сбросе пароля пользователя, иначе мы получаем потенциальную дыру в безопасности. Для реализации этой логики потребуется где-то хранить активные сессии пользователя, так как сам по себе NextJS за вас этого делать не будет. В нашем случае мы организовали хранение активных сессий в базе данных Umbraco, и NextJS приложение запрашивало и обновляло данные о сессиях через API. В форме регистрации на сайте в поля имени и фамилии пользователя можно ввести скрипт и HTML-разметку. И если бекенд, как в классическом комиксе, предусмотрел защиту от SQL инъекций, то защита от фронтенд скриптов не всегда очевидна. Почему это опасно, ведь их вижу только я? Обычно данные регистрации, которые собирает сайт, вставляются еще и в письмо для подтверждения регистрации. Например, “Добрый день, Антон! Спасибо за регистрацию на нашем портале.” Пользователь может на стороннем сайте зарегаться от имени жертвы, которой придет письмо от валидного сайта со ссылками и скриптами, которые выполнятся, если жертва кликнет на ссылку в письме. Эта проблема устраняется введением проверки параметров на ввод при регистрации пользователя. (Не)правильная настройка Google Maps API keys (и любых других платных сервисов и API, на которых можно задать ограничение по домену использования). Если настройки доменов правильно не выставлены, эти ключи можно использовать на других сайтах и клиент будет платить больше денег. Бывают случаи, когда ключи уводили, и держатель сайта платил тысячи долларов. Помимо настройки ограничений для этих ключей, следует оптимизировать и само количество запросов к платным сервисам. Если код написан не очень эффективно, и получается очень много запросов в Maps API, конкуренты могут ходить тыкать и тратить ваши деньги. Как можем ограничить количество запросов?  С помощью уменьшения территории, по которой ищем, и уменьшения запросов в Google Places. Иногда бывает, что компонент карты в коде инициализируется на загрузку любой страницы, хотя сама карта либо глубоко внизу страницы, либо вообще скрыта на вкладке, которую еще надо переключить. Подобного рода проблемы специфичны вообще для разработки где угодно, не только на e-commerce платформах или похожих проектах. Дыры в безопасности часто возникают в приложениях на NextJS как раз из-за того, что ответственность между фронт- и бэк-задачами перераспределяется. Поэтому полезно помнить про эти юзкейсы, чтобы застраховать от неприятных сюрпризов и команду, и пользователей.",
    "150": "Привет, Хабр! Меня зовут Никита, я пентестер, специализируюсь на веб-тестировании. Наверняка многие из вас задумывались о подтверждении своей экспертизы с помощью некоторых сертификаций. Сегодня хочу поговорить о популярной сертификации от академии PortSwigger — BSCP, посвященной тестированию веб-приложений. Прежде чем приступить к изучению материалов для подготовки к BSCP, я уже имел хорошее представление об основных веб-уязвимостях из списка OWASP TOP-10. Также я знал, как эксплуатировать базовые уязвимости, такие как SQL-injection, XSS, Server-Side Template Injection и многие другие. Но на одном из этапов я задался вопросом: как всё-таки к нему эффективно подготовиться? В этой статье я поделюсь лайфхаками по подготовке к сертификации, покажу, как может помочь встроенный в Burp Suite сканер уязвимостей, и подробно разберу каждый из этапов самого экзамена. Начнем со списка советов, которые могут помочь при подготовке к сдаче экзамена. 1. Важно сосредоточить внимание на каждой новой функциональности на каждом этапе. Одинаковый интерфейс приложений в лабораторных и на экзамене позволяет быстро определить дополнительные возможности или отличия в функциях на различных этапах. Например, при получении доступа к учетной записи может появиться доступ к новому поиску по сайту. 2. Сканирование с помощью Burp Suite помогает при поиске точек вхождения в лабораторные. Также дополнительно можно ознакомиться с руководством по сканированию на сайте PortSwigger. 3. Многие вещи на экзамене и в лабораторных пересекаются, поэтому стоит обращать внимание на поиск документации и решений лабораторных PortSwigger. Также, конечно, могут встретиться и темы, которые не поднимались при изучении бесплатного материала. 4. Не стоит тратить много времени на тупиковый вектор и заострять внимание на одной функциональности, так как время на экзамене ограничено. 5. Необходимо обращать особое внимание на уязвимости, которые могут возникнуть на каждом этапе. Список уязвимостей, характерных для каждого этапа, будет показан далее. 6. В инструкциях к экзамену есть два словаря для подбора учетных данных пользователя — эти словари необходимо подготовить заранее. Так как данная уязвимость может встретиться в экзамене, это поможет сохранить время и нервы. 7. Решить достаточное количество MysteryLabs для формирования подхода к поиску вектора атаки. 8. Обязательно сдать рекомендуемые лабораторные и пробный экзамен. 1. Получение доступа к учетной записи пользователя с низким уровнем привилегий. 2. Повышение привилегий до уровня администратора. 3. Чтение файла /home/carlos/secret. Тестовый экзамен, так же как и реальный, состоит из двух уязвимых приложений, в которых существует шесть уязвимостей. Начнем с решения первой лабораторной машины. Для более быстрого поиска уязвимости можно воспользоваться встроенным в Burp Active Scan. Расширение ActiveScan++ добавляет возможности активного и пассивного сканирования Burp Suite. Оно разработано для добавления минимальной нагрузки на сеть и выявления поведения приложения, которое может помочь при поиске уязвимостей: 1. Возможные атаки на заголовок хоста (уязвимости сброса пароля, загрязнение кэша, DNS-переходы). 2. Client-Side уязвимости. 3. Обработка ввода XML. 4. Подозрительное преобразование ввода (например, 7*7 => '49', \\x41\\x41 => 'AA'). Identificate. Active Scan XSS-уязвимости на основе DOM обычно возникают, когда JavaScript получает данные из контролируемого злоумышленником источника, например, URL, и передает их в источник, поддерживающий динамическое выполнение кода, например, eval() или innerHTML. Это позволяет злоумышленникам выполнять вредоносный JavaScript-код, который, как правило, позволяет им захватывать учетные записи других пользователей. Дальше нам необходимо попробовать получить свой сессионный идентификатор, и при попытке это сделать с помощью обычного alert(document.cookie) наше действие попадает под фильтрацию и возвращает ответ «Potentially dangerous search term»: Для того чтобы обойти ограничение, нам понадобится вспомнить, что такое глобальные переменные JavaScript. Глобальная переменная — это переменная, определенная в контексте глобальной области видимости. Это означает, что к ней можно обращаться из любой другой области видимости, то есть глобальная переменная доступна в любой части кода. В JavaScript она представляет собой свойство глобального объекта. WAF предотвращает опасные фильтры и теги. Обойти фильтр WAF можно с помощью глобальных переменных JavaScript: Необходимо подобрать полезную нагрузку для того, чтобы украсть сессионный идентификатор обычного пользователя. Так как валидация входных данных не позволяет использовать document.cookie, ее можно обойти, используя полезную нагрузку: Воспользуемся конструкцией eval(atob()) для обхода фильтрации и получения сессионного идентификатора, а также полезной нагрузкой, чтобы получить собственный сессионный идентификатор. Но для начала разберемся, как это работает: - Метод eval() выполняет команду из аргумента. - Методы btoa() и atob() используются для кодирования и декодирования строк в формате base64.Если метод eval() заблокирован, то можно использовать альтернативы: Данная полезная нагрузка работает корректно, и мы можем получить ответ на свой сервер. Для последующей эксплуатации копируем ссылку, содержащую нашу полезную нагрузку, и отправляем пользователю: Благодаря данной уязвимости мы получили доступ к пользователю carlos. Можно идти дальше. Воспользуемся советом из начала статьи и обратим внимание на новую функциональность, доступную от имени обычного пользователя. В данном случае появилась возможность воспользоваться Advanced search: SQL-Injection — это процесс внедрения вредоносных запросов на языке SQL во входные данные, используемые веб-приложением для взаимодействия с базой данных. В результате можно получить доступ к конфиденциальным данным. Данная полезная нагрузка сравнивает значение первого элемента слова в базе данных с таблицей ASCII. Приложение выполняет сортировку постов в другом порядке при корректной обработке запроса. Прочитать значение version() можно с помощью Burp Intruder. Указываем в нём две точки для перебора и выбираем атаку Cluster bomb: Так как sqli возникает при сортировке постов на странице, воспользуемся Grep Extract, расположенным в настройках Intruder. Указываем место на странице, которое меняется при выполнении запроса: После окончания атаки в ответах от сервера можно заметить разницу в положении постов. Таким образом, можно собрать значение version() и перевести его из ASCII в читабельный вид, чтобы узнать версию используемой базы данных: Для постэксплуатации буду использовать инструмент sqlmap. Sqlmap — это инструмент с открытым исходным кодом, предназначенный для автоматизации тестирования на проникновение. Он специализируется на обнаружении и использовании уязвимостей SQL-инъекции, а также взломе серверов баз данных. Sqlmap включает в себя мощный механизм обнаружения уязвимостей и располагает разнообразными функциями для тестирования на проникновение. Этот инструмент позволяет решать задачи, от сбора информации о базах данных до выполнения команд в операционной системе через внеполосные подключения. В итоге мы получаем учетные данные администратора и можем переходить к поиску третьей уязвимости данного приложения. На данном этапе я воспользовался еще одним советом — не зацикливаться и обращать внимание на всё новое, что появляется в приложении. В данном случае единственное, что обновилось в приложении, — это сериализованный объект в файлах cookie на теперь доступной странице Admin Panel. Сериализация представляет собой процесс преобразования структуры данных в последовательность байтов. Противоположной операцией к сериализации является десериализация, которая заключается в восстановлении структуры данных из этой последовательности байтов. Java Deserialization Scanner — это расширение Burp Suite, которое дает возможность обнаруживать уязвимости Java-десериализации. Оно добавляет проверки как в активный, так и в пассивный сканер, а также может быть использовано в ручном режиме. Это расширение позволяет обнаружить и использовать уязвимости Java-десериализации с различными кодировками (Raw, Base64, Ascii Hex, GZIP, Base64 GZIP). Нюанс по работе Java Deserialization Scanner: для корректной работы данного расширения вам необходимо установить jdk8. Также вам понадобится установить Ysoserial — инструмент, который позволяет генерировать полезные нагрузки, эксплуатирующие небезопасную десериализацию объектов Java. Это позволит получить удаленное выполнение кода (RCE) при десериализации нагрузки на целевой системе. Во второй лабораторной также первым делом проверяем функциональность поиска с помощью встроенного сканера. Identificate. Active Scan. Для кражи сессионного идентификатора необходимо решить проблему с подбором полезной нагрузки, так как в данном случае блокируется вхождение символов (). Но с ней мне не удалось подобрать возможность захвата сессии. Изучив возможные обходы фильтрации, я подобрал полезную нагрузку, позволяющую обойти фильтрацию скобок: В итоге у нас получается обойти фильтрацию. Добавляем полезную нагрузку в ссылке на exploit-server и получаем сессионный идентификатор пользователя в приложении: Меняем свой сессионный идентификатор на полученный с помощью CookieManager, как в первой лабораторной, и переходим к следующему этапу. На этот раз необходимо обойти еще и WAF. Попробуем сразу перейти к эксплуатации с помощью sqlmap: Здесь встречается такая же уязвимость, связанная с Java Deserialization. Этапы эксплуатации уже были продемонстрированы, но здесь я попробую подробнее рассказать про Java Deserialization Scanner. Данный плагин включает в себя три отдельных компонента: 1. Интеграция с активным и пассивным сканером Burp Suite. 2. Ручной тестер для обнаружения уязвимостей десериализации Java в пользовательских точках вставки. 3. Инструмент Ysoserial, который использует уязвимости десериализации Java [frohoff ysoserial]. Такую уязвимость также можно выявить с помощью функционала ActiveScan. В сканере предусмотрены полезные нагрузки двух типов: 1. Полезные нагрузки, которые выполняют синхронную функцию sleep, чтобы проверить уязвимость на основе времени ответа от сервера. 2. Полезные нагрузки, которые разрешают DNS, чтобы проверить уязвимость с помощью Burp Suite Collaborator, интегрированного в Burp Suite. Эксплойт генерируется с помощью инструмента ysoserial, который создает необходимые полезные нагрузки. Каждый CommonsCollection имеет свою вариацию эксплойта. Нам нужно идентифицировать и определить, как конкретно работает уязвимость и какая полезная нагрузка нужна — это можно сделать с помощью встроенного в расширение сканера. В нашем случае уязвимость связана с DNS (JRE only): На этом наш экзамен окончен. Примерно так сдается экзамен. Буду рад, если мой опыт поможет вам успешно пройти сертификацию :)",
    "151": "Целочисленное линейное программирование может помочь найти ответ на множество реальных проблем. Теперь исследователи нашли гораздо более быстрый способ это сделать. Задача коммивояжера — одна из старейших известных вычислительных задач. Она заключается в поиске кратчайшего маршрута через определённый список городов. Несмотря на кажущуюся простоту, проблема, как известно, сложна. И хотя вы можете использовать перебор, чтобы проверить все возможные маршруты, пока не найдете кратчайший путь, такая стратегия становится несостоятельной, уже когда в списке всего лишь несколько городов. Вместо этого вы можете применить строгую математическую модель, называемую линейным программированием, которая грубо аппроксимирует задачу как набор уравнений и методично проверяет возможные комбинации, чтобы найти лучшее решение. Но иногда вам необходимо оптимизировать задачи, связанные с целыми числами. Какая польза от плана оптимизации завода, который производит 500,7 диванов? Для этого исследователи часто обращаются к варианту линейного программирования, называемому целочисленным линейным программированием (ILP). Он популярен в приложениях, требующих принятия дискретных решений, включая планирование производства, составление расписания работы экипажей авиакомпаний и маршрутизацию транспортных средств. «По сути, ILP — это хлеб и масло исследования операций как в теории, так и на практике», — сказал Сантош Вемпала, учёный из Технологического института Джорджии. С тех пор как учёные впервые сформулировали ILP более 60 лет назад, были открыты различные алгоритмы, решающие проблемы ILP, но все они были относительно медленными с точки зрения количества необходимых шагов. Лучшая версия, которую учёные смогли придумать (своего рода лимит скорости), исходит из тривиального случая, когда переменные задачи (например, посещает ли коммивояжер город или нет) могут принимать только двоичные значения (0 или 1). В этом случае ILP имеет сложность, которая экспоненциально масштабируется в зависимости от количества переменных, также называемом размерностью. (Если переменная всего одна, то для проверки всех возможных комбинаций и решения задачи потребуется всего два шага; две переменные означают четыре шага, три — восемь шагов и так далее.) К сожалению, как только переменные принимают значения, выходящие за пределы 0 и 1, время работы алгоритма значительно увеличивается. Исследователи уже давно задаются вопросом, смогут ли они приблизиться к тривиальному идеалу. Прогресс был медленным: рекорд был установлен в 1980-х годах, и с тех пор были достигнуты лишь небольшие улучшения. Но недавняя работа Виктора Рейса, который в настоящее время работает в Институте перспективных исследований, и Томаса Ротвосса из Вашингтонского университета, совершила самый большой скачок за последние десятилетия. Объединив геометрические инструменты для ограничения возможных решений, они создали новый, более быстрый алгоритм решения ILP почти за то же время, что и в тривиальном двоичном случае. Результат получил награду за лучшую статью на конференции Foundations of Computer Science 2023 года. «Этот новый алгоритм чрезвычайно интересен, — сказал Ной Стивенс-Давидовиц, математик и учёный из Корнелльского университета. — Это первое крупное улучшение в ILP почти за 40 лет». ILP работает путем преобразования заданной проблемы в набор линейных уравнений, которые должны удовлетворять некоторым неравенствам. Конкретные уравнения основаны на деталях исходной задачи. Но хотя эти детали могут различаться, основная структура проблем ILP остается прежней, что дает исследователям единый способ решения множества проблем. Нельзя сказать, что это лёгкая работа. Лишь в 1983 году математик Хендрик Ленстра доказал, что общая проблема вообще разрешима, и предложил первый алгоритм, который мог это сделать. Ленстра думал об ILP геометрически. Во-первых, он превратил неравенства, лежащие в основе ILP, в выпуклую форму, например, в любой правильный многоугольник. Эта фигура представляет ограничения отдельной задачи, которую вы решаете, будь то производство диванов или планирование полётов, поэтому внутренняя часть фигуры соответствует всем возможным значениям, которые могут решить неравенства и, следовательно, проблему. Ленстра назвал эту форму выпуклым телом. Размер задачи влияет на размер этой фигуры: с двумя переменными она принимает форму плоского многоугольника; в трех измерениях это платоново тело и так далее. Затем Ленстра представил все целые числа как бесконечный набор точек сетки, известный в математике как решётка. Двухмерная версия выглядит как море точек, а в трех измерениях — как точки соединения стальных балок в здании. Размерность решётки также зависит от размерности данной задачи. Ленстра показал, что для решения данной задачи ILP нужно просто искать место, где возможные решения встречаются с набором целых чисел: на пересечении выпуклого тела и решётки. И он придумал алгоритм, который мог бы изучить это пространство исчерпывающим образом, но чтобы быть эффективным, ему иногда приходилось разбивать задачу на части меньшего размера, добавляя множество шагов ко времени выполнения. В последующие годы несколько исследователей изучали, как заставить этот алгоритм работать быстрее. В 1988 году Рави Каннан и Ласло Ловас представили концепцию, называемую радиусом покрытия, заимствованную из теории кодов коррекции ошибок, чтобы помочь выпуклому телу и решётке эффективнее пересекаться. Грубо говоря, радиус покрытия гарантирует, что выпуклое тело всегда содержит хотя бы одну целочисленную точку, независимо от того, где вы разместите его на решётке. В результате размер радиуса покрытия также определяет, насколько эффективно вы сможете решить проблему ILP. Итак, всё свелось к определению размера идеального радиуса покрытия. К сожалению, это само по себе оказалось сложной проблемой, и лучшее, что могли сделать Каннан и Ловас, — это сузить возможное значение путём поиска верхних и нижних границ. Они показали, что верхняя граница — максимальный размер радиуса покрытия — линейно увеличивается с размером измерения. Это было довольно быстро, но недостаточно, чтобы значительно ускорить работу ILP. В течение следующих 30 лет другие исследователи смогли добиться лишь немного большего успеха. Что в конечном итоге помогло Рейсу и Ротвоссу добиться прорыва, так это несвязанный математический результат, который был сосредоточен исключительно на решётках. В 2016 году Одед Регев и Стивенс-Давидовиц фактически показали, сколько точек решётки может уместиться в определённой геометрической фигуре. Рейс и Ротвосс применили это к другим фигурам, что позволило им лучше оценить количество точек решётки, содержащихся в радиусе покрытия ILP, снизив верхнюю границу. «Последний прорыв произошёл с осознанием того, что вы в реальности можете создавать другие виды фигур», — сказал Регев. Эта новая уточнённая верхняя граница стала невероятным улучшением, что позволило Рейсу и Ротвоссу добиться значительного ускорения общего алгоритма ILP. Их работа доводит время выполнения до (log n)O(n), где n — количество запросов, а O(n) означает, что оно линейно масштабируется с ростом n. (У этого выражения «почти» такое же время выполнения, как и у бинарной задачи.) «Это триумф на стыке математики, информатики и геометрии», — сказал Дэниэл Дадуш из национального исследовательского института CWI в Нидерландах, именно он помог разработать алгоритм, который Рейс и Ротвосс использовали для измерения времени выполнения ILP. На данный момент новый алгоритм фактически не использовался для решения каких-либо логистических проблем, поскольку для его использования потребовалось бы слишком много работы по обновлению сегодняшних программ. Но для Ротвосса это не имеет значения. «Речь идет о теоретическом понимании проблемы, имеющей фундаментальное применение», — сказал он. Что касается возможности дальнейшего повышения вычислительной эффективности ILP, исследователи всё ещё надеются, что они будут продолжать приближаться к идеальной скорости, но не в ближайшее время. «Для этого потребуется принципиально новая идея», — сказал Вемпала. НЛО прилетело и оставило здесь промокод для читателей нашего блога:— 15% на заказ любого VDS (кроме тарифа Прогрев) — HABRFIRSTVDS.",
    "152": "Сегодня все крупные компании сохраняют и обрабатывают большие объёмы информации, причём стремятся делать это максимально эффективным для бизнеса способом. Меня зовут Мазаев Роман и я работаю в проекте загрузки данных на платформу SberData. Мы используем PySpark, который позволяет очень быстро распределённо обрабатывать данные в оперативной памяти узлов нашего кластера на базе Hadoop. Я поделюсь способом, с помощью которого можно снизить потребление ресурсов кластера за счёт перезапуска PySpark-приложений между выполняемыми Spark-задачами, и расскажу, как это делать правильно. PySpark — это API фреймворка Apache Spark с открытым исходным кодом на языке Python. Сам же фреймворк Apache Spark написан на Scala и Java. Для реализации API PySpark использует библиотеку PY4J, которая позволяет Python-приложению динамически получать доступ к Java-объектам внутри функционирующего JVM-процесса (Java Virtual Machine). Важно отметить, что общение между PVM- (Python Virtual Machine) и JVM-процессами в PY4J реализуется при помощи сокетов, а информация, которая кладётся процессами в сокет, должна соответствовать протоколу обмена информацией PY4J. Главной особенностью PySpark является то, что он предоставляет возможность выполнять Spark-задачи в памяти JVM-процессов, используя код на Python. Для этого необходимо работать только со структурированными данными и использовать доступные в PySpark методы их обработки. Например, Spark SQL или Dataframe API. В этом можно убедиться, обратившись к фрагменту исходного PySpark, представленному на рисунке 2. Здесь можно заметить, что при вызове метода union для некоторого экземпляра dataframe — происходит обращение к его закрытому атрибуту _jdf, который является экземпляром класса JavaObject в PY4J и ссылается на оригинальный dataframe на стороне JVM. Стоит добавить, что обработка данных с помощью RDD (распределённого набора данных) API или собственных реализаций функций на Python приведёт к значительным потерям производительности. Это связано с перемещением данных между JVM- и Python daemon-процессами, а также с накладными расходами на их сериализацию и десериализацию. Помимо прочего Apache Spark допускает два режима развёртывания — клиентский и кластерный, причём первый используется по умолчанию. В целом это влияет на то, где будет функционировать JVM-процесс драйвера (driver) и связанный с ним SparkContext. Драйвер (driver) управляет всей информацией о Spark-приложении, исполняет указанные команды, распределяет задачи между исполнителями и, при необходимости, может сохранять в своей памяти некоторый извлечённый набор записей от всех исполнителей (например, с помощью метода collect). Исполнители (executors) — это отдельные JVM-процессы, которые выполняют указанные драйвером задачи и хранят в своей памяти обрабатываемые данные. SparkContext — это создаваемый процессом драйвера объект, который представляет собой точку входа в Spark и реализует подключение к менеджеру кластера (standalone, Mesos, YARN, Kubernetes). В нём можно определить набор конфигураций, который будет использован в приложении, и с помощью него можно создавать RDD, широковещательные переменные и накопители. Это сердце Spark, которое существует в единственном экземпляре в программе драйвера. Если в качестве менеджера кластера используется YARN, то для каждого Spark-приложения в кластере создаётся отдельный процесс ApplicationMaster, который согласовывает ресурсы с YARN-менеджером ресурсов и взаимодействует с YARN-менеджером узлов при выборе наиболее подходящих хостов для функционирования Spark-приложения. В клиентском режиме развёртывания PySpark-приложения JVM-процесс драйвера и соответствующий ему SparkContext будет создан на той же машине, где функционирует основной Python-процесс. При этом Python-приложению будет предоставлен прямой доступ к данным, которые драйвер может извлекать в свою память. Вместе с этим PySpark-приложение будет жить до тех пор, пока функционирует основной Python-процесс, создавший его. При кластерном режиме развёртывания PySpark-приложения JVM-процесс драйвера и соответствующий ему SparkContext будут созданы на одном из узлов кластера, причём где именно — решает менеджер кластера. Чтобы использовать этот режим, понадобится вручную запустить программу spark-submit. Например, это можно сделать, выполнив следующую терминальную команду: sh /bin/spark-submit --master yarn --deploy-mode cluster --queue your_queue your_script.py Однако в таком случае весь контроль над PySpark-приложением перейдёт к созданному процессу your_script.py, который будет запущен на одном из узлов кластера независимо от основного процесса. И только это приложение будет иметь прямой доступ к процессу драйвера и к сохранённым в его памяти данным. Если в качестве менеджера кластера используется YARN, то в этом режиме развёртывания программа драйвера запускается в рамках процесса ApplicationMaster. В проекте нам ежедневно приходится иметь дело с десятками, сотнями, тысячами гигабайтов информации, совершенно отличающихся друг от друга источников. Мы стремимся загружать каждый из них за минимальное время, потребляя как можно меньше ресурсов общего кластера. При создании SparkContext можно определить ряд конфигураций, который будет использован в Spark-приложении. Самый популярный набор для конфигурации Spark-приложения обычно напрямую связан с потреблением ресурсов. Приведу в качестве примеров основные из них: spark.driver.memory — резервируемая память для процесса драйвера (по умолчанию используется 1 Гб); spark.executor.memory — резервируемая память для одного процесса исполнителя задач (по умолчанию используется 1 Гб); spark.executor.cores — резервируемое количество ядер для одного процесса исполнителя задач (по умолчанию используется 1); spark.executor.instances — количество создаваемых исполнителей задач (по умолчанию используется 2). Для оптимизации обработки данных источников можно было бы реализовать вариант, при котором для каждого из них заранее определяется оптимальный набор конфигураций для PySpark-приложения. Например, для больших источников можно было бы резервировать больше ресурсов (если их будет недостаточно, то могут возникнуть проблемы из-за нехватки памяти), а для небольших источников можно было бы резервировать меньше. Однако у такого подхода есть существенный недостаток: мы забираем ресурсы у кластера до тех пор, пока наше PySpark-приложение полностью не завершит свою работу. А ведь в процессе обработки данных источника вполне может быть ряд промежуточных Spark-задач, которые не нуждаются в использовании такого количества ресурсов. Приведу абстрактный пример проблемной ситуации: Создаём SparkContext с заранее определённым набором конфигураций потребления ресурсов. Вычисляем максимальные и минимальные значения некоторого поля и сохраняем их в памяти драйвера. В зависимости от полученных результатов определяем, необходимо ли идти дальше и обрабатывать полученные данные. Партиционируем данные по значению применённой функции к некоторому полю (например, хеширования). В зависимости от полученных результатов определяем, какие данные не нужно обрабатывать и их достаточно просто перенести из одной директории в другую, а какие данные нужно объединить с уже существующими для дальнейшей фильтрации. Определяем удаляемые и изменяемые записи в обрабатываемых данных, после чего сохраняем их в отдельной таблице истории. Создаём резервную копию основной таблицы через физическое копирование файлов из одной директории в другую (например, для HDFS это могла бы быть distcp‑команда, запускающая MapReduce‑задачу). Определяем актуальное состояние данных и сохраняем их в целевой таблице. Проставляем необходимые статистики в свойствах целевой таблицы. Как видите, ресурсы Spark-приложения могут быть полноценно использованы только при выполнении пунктов 4, 6 и 8. При этом до 4 пункта алгоритм обработки данных может вовсе не дойти, а между пунктами 6 и 8 может выполняться отдельная MapReduce-задача, для которой также необходимо выделять отдельные ресурсы, и которая может работать довольно долго. Становится очевидно, что резервировать ресурсы кластера для выполнения задач, выполняя пункты 1-9, неэффективно и нужно найти способ, при котором для каждой Spark-задачи будет выделяться только необходимое количество. Как известно, изменять конфигурацию драйвера и исполнителей после инициализации объекта SparkContext не имеет смысла, ведь для них PySpark уже создал отдельные JVM-процессы. Единственный способ изменять потребление ресурсов драйвером и исполнителями — создавать их как новые. И тут есть два пути: Для каждой Spark‑задачи создавать отдельные Python‑подпроцессы, которые будут инициализировать уникальные PySpark‑приложения с необходимой конфигурацией ресурсов и выполнять заранее запрограммированные действия. Перезапускать PySpark‑приложение, которое будет заново создавать процессы драйвера и исполнителей с уникальной конфигурацией потребления ресурсов. Если идти по первому пути, то можно столкнуться с некоторыми трудностями. Во-первых, создаваемые подпроцессы нужно отслеживать и прерывать по мере необходимости. Во-вторых, для выполнения некоторых Spark-задач может понадобиться некоторая информация, которая хранится в основном Python-приложении или которая извлекается в рамках выполнения других Spark-задач. В связи с этим придётся разработать механизм для обмена информацией между PySpark-приложениями и основным процессом, что может привести к дополнительным накладным расходам. В-третьих, это может значительно усложнить и без того непростую архитектуру приложения, ведь под каждую Spark-задачу придётся написать собственное PySpark-приложение. По второму пути невозможно идти, если используется кластерный режим развёртывания Spark-приложения c YARN-менеджером ресурсов, описанный выше. В этом случае пересоздание процесса драйвера приведёт к уничтожению ApplicationMaster, без которого Spark-приложение просто не сможет функционировать. Несмотря на это ограничение, далее мы рассмотрим именно второй путь, поскольку он проще и эффективнее, и большинство PySpark-приложений сможет себе позволить идти по нему. До выхода версии 2.0 в Apache Spark SparkContext был единственной точкой входа в Spark, а RDD API был основным. При этом для использования других API необходимо было инициализировать отдельный контекст, будь то HiveContext или SQLContext. Начиная с версии 2.0 Apache Spark предлагает новую точку входа при помощи объекта SparkSession, включающий в себя создаваемый экземпляр SparkContext и предоставляющий доступ ко всем функциям Spark, которые ранее можно было использовать только в рамках отдельных API. Для инициализации объекта SparkSession в PySpark-приложении и реализации входа обычно используется его внутренний подкласс Builder. Для доступа к нему нужно использовать открытый атрибут builder, являющийся его экземпляром. В отличие от SparkContext, в PySpark-приложении может быть создано несколько экземпляров SparkSession, правда, для этого придётся создавать их через внутренний метод newSession(). Каждый из них будет иметь изолированную конфигурацию SQL, зарегистрированные временные таблицы и представления, а также зарегистрированные пользовательские функции, известные как UDF. При всём этом каждый объект SparkSession будет ссылаться на единственный экземпляр SparkContext и иметь общее хранилище кешированных данных. На момент написания статьи Spark предоставляет единственный способ остановки приложения: вызов метода stop() у экземпляра Scala-класса SparkContext. Он сбрасывает состояние некоторых атрибутов и задействованных объектов в рамках инициализации, закрывает прослушиваемые порты, а также сообщает менеджеру кластера о необходимости завершения Spark-задачи в кластере и высвобождении всех занимаемых ресурсов JVM процессами исполнителей. Чтобы обратиться к методу stop() Scala-класса SparkContext из PySpark-приложения, достаточно вызвать метод stop() у экземпляра SparkContext. Важно отметить, что при инициализации PySpark-приложения с помощью объекта SparkSession метод stop() необходимо вызывать именно у него. В таком случае он самостоятельно вызовет метод stop() у единственного экземпляра SparkContext, но дополнительно также сбросит состояние некоторых собственных атрибутов и других задействованных объектов в рамках инициализации SparkSession. Этот метод действительно выполняет ряд необходимых действий для корректного завершения Spark-приложения, однако если появится необходимость в его повторном перезапуске, этого будет недостаточно. И вот почему. Во-первых, метод stop() объекта SparkSession в текущей реализации не сбрасывает состояние внутреннего атрибута builder. То есть если вы захотите заново инициализировать Spark-приложение с новым набором конфигураций, то старые не изменённые или не удалённые конфигурации будут задействованы повторно. Во-вторых, этот метод не завершает JVM-процесс драйвера, не высвобождает его ресурсы и не сбрасывает состояние внутренних атрибутов, обеспечивающих связь с ним. Это означает, что даже после остановки Spark-приложения JVM-процесс драйвера продолжит функционировать вместе с Python-процессом, создавшим его, а значит для него не получится переопределить конфигурацию потребляемых ресурсов. Всё это можно самостоятельно проверить, последовательно запустив два PySpark-приложения с разным набором конфигураций. Например, это можно было бы реализовать следующим образом: Для первого PySpark-приложения определяются конфигурации памяти драйвера (5 Гб) и исполнителей (10 Гб). В Spark Web UI (см. рисунок 11) в столбце Storage Memory всегда будет указано меньшее значение, поскольку только часть выделенной памяти Spark отводится на обработку и сохранение данных. Подробнее про это написано в официальной документации Spark в рамках описания конфигурации spark.memory.fraction. Для второго PySpark-приложения изменяется конфигурация памяти драйвера (15 Гб) и определяется количество используемых исполнителями ядер (2). Однако, как можно заметить, после перезапуска PySpark-приложения изменилось только количество используемых исполнителями ядер. Память исполнителей не изменилась потому, что не было сброшено состояние внутреннего атрибута builder, поэтому была повторно задействована конфигурация памяти исполнителей из предыдущего запуска. Память драйвера не изменилась потому, что PySpark просто переиспользовал ранее созданный JVM-процесс драйвера. Также негативным следствием переиспользования JVM-процесса драйвера является тот факт, что с каждым перезапуском PySpark-приложения его память заполняется новым набором кешированных данных, что может привести к ошибке Out Of Memory (OOM). В этом можно самостоятельно убедиться, выполнив следующий Python-скрипт: Здесь для драйвера по умолчанию Spark самостоятельно выделяет 1 Гб памяти, однако уже после 10 запусков её начинает не хватать. И это даже при том, что между запусками не происходит никаких вычислений. Если же начать извлекать в память драйвера какие-либо данные, то OOM-ошибку можно получить значительно раньше. Вполне очевидно, что вышеупомянутые недостатки не позволяют в полной мере использовать алгоритм перезапуска PySpark-приложений для оптимизации потребления ресурсов, а значит необходимо самостоятельно их устранить. Для исправления первой проблемы достаточно перед перезапуском PySpark-приложений атрибут _options объекта builder приравнивать к пустому словарю. Этот грубый метод позволит удалить все ранее установленные конфигурации. _options — это словарь, содержащий все заданные пользователем конфигурации при инициализации PySpark-приложения с помощью Builder’а. Для решения второй проблемы можно все атрибуты класса SparkContext приводить в исходное состояние перед каждым перезапуском. Это заставит PySpark каждый раз инициализировать новый экземпляр класса SparkContext и создавать связанный с ним JVM-процесс драйвера с новой конфигурацией потребляемых ресурсов. Этого можно добиться, например, выполнив следующий сценарий: Но не всё так просто, ведь в таком случае PySpark начнёт создавать множество отдельных JVM-процессов, которые не будут завершены, пока функционирует основной Python-процесс. Это означает, что одного сброса состояния атрибутов класса SparkContext для реализации механизма перезапуска PySpark-приложений недостаточно. Перед созданием нового JVM-процесса драйвера предыдущий необходимо завершать. Как мы уже знаем, в PySpark используется библиотека PY4J для взаимодействия PVM- и JVM-процессов, а обмен информацией между ними реализован при помощи сокетов. То есть, когда Python-приложение кладёт в сокет некоторую информацию, JVM-процесс драйвера её обрабатывает и при необходимости исполняет. Это означает, что нам достаточно положить в сокет сообщение, которое инициировало бы выполнение команды System.exit(0) на стороне JVM. Доступ к методу exit класса System в JVM через PySpark можно получить следующим образом: Со стороны PY4J данный метод является экземпляром класса JavaMember, который позволяет формировать сообщения для его вызова на стороне JVM. Инициализация экземпляра класса JavaMember выглядит следующим образом: Логика вызова данного метода определена в реализации магического метода __call__. Он позволяет формировать и отправлять сообщение для исполнения на стороне JVM. Однако, как можно заметить, в реализации магического метода __call__ после отправки сообщения в JVM-процесс от него ожидается получение ответа (вызов функции send_command). Но мы не можем получать ответ от процесса, который уже был завершён. В связи с этим не получится напрямую использовать вызов метода spark._jvm.System.exit(0). Это означает, что нам необходимо самостоятельно сконструировать сообщение и отправить его в JVM-процесс, не ожидая получить ответ. Для сборки сообщения достаточно переиспользовать часть логики, которая уже была представлена в магическом методе __call__. Используемые атрибуты proto.CALL_COMAND_NAME и proto.END_COMMAND_PART являются константами, которые определены в модуле protocol.py библиотеки PY4J. Для отправки сообщения нам придётся погрузиться в логику реализации метода send_command экземпляра класса GatewayClient, чтобы в дальнейшем переиспользовать её без ожидания ответа. Класс GatewayClient отвечает за управление подключением к мосту, соединяющему PVM- и JVM-процессы. В реализации метода send_command класса GatewayClient можно заметить, что на самом деле отправка сообщения в JVM-процесс происходит в рамках вызова аналогичного метода send_command, но уже для экземпляра класса GatewayConnection, который можно получить, вызвав метод _get_connection(). Этот метод разделён на две основные части: отправку и получение сообщения от JVM-процесса. Именно он пробрасывал исключение, когда мы пытались напрямую использовать вызов spark._jvm.System.exit(0). Для нас важна только первая часть представленной логики, которая используется для отправки некоторого сообщения в JVM-процесс драйвера. Помимо прочего, после завершения JVM-процесса драйвера необходимо корректно завершить все открытые соединения с ним. Для этого достаточно вызвать метод shutdown() у экземпляра класса JavaGateway, доступ к которому можно получить через атрибут _gateway класса SparkContext. Собрав всю полученную информацию воедино, мы можем реализовать функциональность, позволяющую перезапускать PySpark-приложение и создавать новые процессы драйвера и исполнителей с уникальной конфигурацией потребления ресурсов. Сделать это можно, например, следующим образом: В предложенном выше сценарии на 12 перезапуске PySpark-приложения мы определяем три уникальные конфигурации: spark.executor.memory, spark.executor.cores и spark.driver.memory. Уже на 13 перезапуске мы убираем конфигурацию spark.executor.cores, а для двух других специально изменяем значение. Ожидается, что на 13 перезапуске мы сможем изменить потребляемую память JVM-процессов драйвера и исполнителей, а конфигурация spark.executor.cores не будет задействована повторно. Как можно заметить, в любой момент времени функционирует только один JVM-процесс драйвера с уникальным набором конфигураций. При этом, как и ожидалось, во время 13 перезапуска конфигурация spark.executor.cores не была задействована повторно. Мы рассмотрели способ оптимизации потребления используемых ресурсов кластера, заключающийся в перезапуске PySpark-приложений между выполняемыми Spark-задачами. По ходу анализа доступных методов остановки Spark-приложения были обнаружены существенные недостатки, не позволяющие в полной мере использовать алгоритм перезапуска. Для их устранения мы погрузились в анализ исходного кода Apache Spark и библиотеки PY4J, необходимой для функционирования инструмента PySpark. Получив всю нужную информацию, мы реализовали алгоритм, который дополняет существующий метод остановки Spark-приложения и устраняет все обнаруженные недостатки, что доказывает проведённое в конце тестирование.",
    "153": "Боитесь, что тесты пропадут, если компьютер сломается? Хотите видеть историю изменений? Вынуждены запускать тесты в отпуске, потому что у других членов команды нет к ним доступа? Не можете одновременно работать над написанием и прогоном? Знакомы эти проблемы, хотите избавиться от них раз и навсегда? Тогда вам необходимо использовать Vanessa Automation вместе с Gitlab. И я готов показать этот процесс на максимально простом примере. Меня зовут Дмитрий, я занимаюсь тестированием 1С Зуп в команде HR Tech Самолет. В сфере 1С я уже 7 лет, работал консультантом, аналитиком и программистом. А в тестирование перешёл, чтобы уберечь галактику от ошибок ПО. Поехали! В нашем примере не используется Onescript, Vanessa Runner, другие дополнительные библиотеки, не разбираются нюансы запуска раннеров, джобы не падают, нет использования переменных gitlab-а. Минимальный уровень знаний: вы прошли и поняли всё из бесплатного курса Виталия Онянова по Vanessa Automation. В статье описан тестовый пример со схемы выше. Поясню, что на ней изображено. В нашем примере три действующих лица: Собственно, мы. Мы работаем за ПК №1 и пишем тесты на Vanessa Automation. Потом с помощью Vanessa Automation и git отправляем тесты в gitlab. Vanessa Automation здесь выступает в качестве графической оболочки для git. Gitlab. Сервис, который все вышеперечисленные возможности предоставляет. Хранит файлы, их историю, может запускать тесты на других компьютерах, если на них установлены gitlab-runnerы. Разумеется, gitlab умеет ещё много всего, но об этом здесь не будем говорить. В нашем примере используется Saas-версия gitlab — та, что в облаке на сайте gitlab.com. Все тоже самое можно сделать и с локальной версией, но тогда вам нужно дополнительно устанавливать еще gitlab pages. Компьютер ПК №2, на котором установлен gitlab-runner. Gitlab-runner — это программа, которой Gitlab выдает задания для выполнения. Она забирает наши тесты из Gitlab в свою папку, выполняет консольные команды для запуска тестов и формированию отчета Allure и потом всю эту информацию отправляет обратно в Gitlab. Нагляднее этот пример выполнять на двух компьютерах. Но можно все шаги проделать и на одном, просто установив на него gitlab-runner. Тогда схема для тестового примера будет выглядеть так: База из поставки Демонстрационная конфигурация \"Управляемое приложение\" (если пример будете воспроизводить на 2х ПК, то неважно будет ли это одна база, доступная обоим ПК по сети, или 2 разные базы). ОС Windows 10 Pro Версия 22H2 (сборка ОС 19045.4046) качал с официального сайта. Платформа 1С у нас 8.3.23.2040 — на момент создания статьи это последняя версия платформы, которая работает с комьюнити-лицензией. В описании конфигурации Демонстрационная конфигурация \"Управляемое приложение\" указано, что необходимо использовать платформу версии 8.3.24 и выше. Наша платформа этим критериям не соответствует, но у меня все запустилось нормально. Базу я брал здесь. Если что, данная страница гуглится фразой Демонстрационная конфигурация \"Управляемое приложение\". Vanessa Automation брал здесь. В моем примере используется версия 1.2.041.1. Дополнительно в процессе мы установим Java, Git, Allure, Gitlab-runner, никакого другого софта нам не нужно. Если для примера вы будете использовать один компьютер, то необходимо выполнить все действия на нём. Если два компьютера, то в скобках для вас будет указано, где нужно выполнить тот или иной шаг. Называйте проект как удобно, единственный нюанс — снимите галку Initialize repository with a README. Мой будет называться vanessaTestProject1. Галочку снимаем, чтобы файл README.md не был создан в удаленном репозитории. Без него удобнее регистрировать git. Java нужна, чтобы работал Allure. После установки Java необходимо перезагрузить компьютер. Здесь никаких нюансов нет. Качаем с официального сайта для вашей версии системы. Вот прямая ссылка для скачивания по состоянию на март 2024 года. Выберем папку, где будут храниться тесты наших проектов. В моем случае C:\\vanessaTestProject\\tests. Запустим командную строку или Powershell из этой папки. Введем в консоль команды, которые подключат наш локальный репозиторий к удаленному, создадут файл README.md и отправят его в gitlab. Для моего проекта: Все команды кроме одной взял с gitlab, а именно — вместо touch README.md — echo test >README.md . Поясню:touch README.md — это команда, которая создает пустой файл на Linux — она у нас работать не будет. Будет работатьecho test >README.md — команда, которая создает файл со строкой test на Windows. Команды для регистрации git создаются gitlabом автоматом именно под ваш проект — это удобно, ничего гуглить дополнительно не надо. После команды git clone необходимо будет ввести логин и пароль, под которыми вы заходите на gitlab.com. Можно также воспользоваться окном авторизации в gitlab, но не факт что оно у вас в первый раз появится. Для этого запускаем  блокнот под админом и добавляем в файл conf.cfg строку DisableUnsafeActionProtection=.* . Это нужно, чтобы в 1С не всплывало окно предупреждения безопасности: Защиту от опасных действий отключаю для удобства. На проде такое делать нельзя! Заходим в раздел Settings, потом в CI/CD, потом в раздел Runners. Выключаем Instance runners. Жмем кнопку New project runner. Ставим переключатель на Windows, ставим галку Run untagged jobs. Жмем кнопку Create runner. По итогам этого шага в Gitlab появился новый gitlab-runner. Чтобы открыть инструкцию, нажмем на гиперссылку How do I install Gitlab Runner. Создадим на диске C папку Gitlab-Runner. Запустим из папки C:\\Gitlab-Runner командную строку или Powershell. Введем команду для регистрации gitlab-runner. Эта команда уже содержит адрес расположения нашего gitlab и токен нашего gitlab-runner: Enter the GitLab instance URL (for example, https://gitlab.com/): Ничего не вводим — жмем Enter. Enter a name for the runner. This is stored only in the local config.toml file: Ничего не вводим — жмем Enter. Enter an executor: ssh, docker+machine, kubernetes, docker-autoscaler, custom, shell, parallels, virtualbox, docker, docker-windows, instance: Пишем в поле ввода Shell и жмем Enter. У нас на компьютере команды будут запускаться через Powershell. Результат шага — на нашем компьютере зарегистрирован gitlab-runner. Для запуска перейдем в папку, где лежит файл gitlab-runner.exe. Вызываем командную строку или Powershell в этой папке, вводим: Зайдем в файл C:\\Gitlab-Runner\\config.toml и отредактируем значение для атрибута shell. При установке устанавливается значение pwsh. Если мы его оставим — команды выполняться не будут, gitlab-runner будет ругаться и выдавать ошибку, что pwsh в качестве исполнителя не найден. Вместо pwsh установим значение powershell. Важный момент: Gitlab-runner должен быть запущен не как служба, а как приложение. Если gitlab-runner будет запускаться как служба, то скриншотов ошибок не будет, т.к. не будет подключена графика. Также не все шаги будут работать. Чтобы при старте системы наш gitlab-runner запускался как приложение, создадим bat-файл и добавим его в автозагрузку. Чтобы открыть папку автозагрузки, вызовем утилиту Выполнить и введем команду shell:startup. Первая переходит в папку с gitlab-runner, вторая его запускает. В папку нужно переходить обязательно, иначе будет ошибка, что config.toml не найден. Дистрибутив доступен на официальной странице проекта. Установка сводится к распаковке архива. Для удобства папку с Allure копируем в C:\\vanessaTestProject. откроем окно свойство системы с помощью ввода команды sysdm.cpl; добавим в переменную PATH путь до файла allure — в нашем случае — C:\\vanessaTestProject\\allure-2.27.0\\bin. Блок настроек закончен, дальше только интересное. Часть шагов отмечено на схемах №1 и №2 для наглядности. Добавим первый тест через форму работы с каталогами. Добавьте первый тест. Откроем форму работы с git в Vanessa Automation. Откроем форму работы с Git и выберем нашу папку C:\\vanessaTestProject\\tests\\vanessatestproject1, куда мы уже добавили один тест. Видим, что Vanessa Automation подсказывает нам, что в нашей папке есть файлы, которые отсутствуют в удаленном репозитории, но есть в локальном. Vanessa Automation выступает здесь в качестве графической оболочки над git. Сохраним еще два теста в наш каталог. Для наглядности сценарий «Создание записи» в регистре сведений Курсы валют у нас будет падать. Сценарии «Создание элемента справочника Контрагенты» и «Установка пометки удаления» для элементов справочника Валюты будут проходить. Переносим тесты в Gitlab. Открывайте Gitlab и проверяйте, что  тесты действительно доставлены в облако. Добавление пайплайна в Gitlab. Если говорить простыми словами, то «пайплайн в gitlab»— это рецепт приготовления готовой программы, описание которого указано в файле .gitlab-ci.yml. В этом файле указаны стадии приготовления нашей программы (stages), например: сборка из исходников (build), запуск автотестов (test), установка на сервер (deploy). Каждая стадия в свою очередь может состоять из нескольких работ (джоб или jobs), в которых описываются уже конкретные действия, необходимые для выполнения (script) или описывается последовательность работы с файлами (artifacts). Добавим файл.gitlab-ci.yml в наш проект: Теперь у нас есть пайплайн, который состоит из одного этапа — test, для которого есть одна джоба — test, единственной целью которой является включение кириллицы. Но это пока. Проверяем результат работы созданной на предыдущем этапе джобы. Зайдем в раздел Build — Jobs. Провалимся в нашу первую джобу. В ней можно увидеть процесс выполнения команды chcp 65001. Результат Active code page: 65001, как и ожидалось. Команда успешно выполнилась, статус джобы поменялся на Passed. На нашем компьютере в папке C:\\Gitlab-Runner\\builds\\zU3fFBCGu\\0\\dimaSecret\\vanessatestproject1 появились те же файлы, что и в Gitlab. При каждом запуске нашего пайплайна содержимое папки будет актуализироваться. Указываем дополнительные настройки Vanessa Automation. На вкладке Основные для Системных каталогов укажем папку C:\\Gitlab-Runner\\builds\\zU3fFBCGu\\0\\dimaSecret\\vanessatestproject1 -в ней будут хранится тестовые сценарии, которые приезжают к нам из gitlab. На вкладке Скриншоты проставим галки Делать скриншоты по требованию, а также Делать скриншоты компонентой VanessaExt. Создадим и укажем в настройках каталог для скриншотов C:\\vanessaTestProject\\screenshots. На вкладке Отчет о запуске сценариев проставим флаг Формировать отчет в формате Allure. Создадим и укажем в настройках каталог для скриншотов C:\\vanessaTestProject\\allure-results. Сохраним настройки в файл VAParams.json. Для удобства создадим отдельный каталог C:\\vanessaTestProject\\settings. Получим строку запуска Vanessa Automation для Powershell. Давайте разбираться, что это за строка  у нас сформировалась. Разобьем ее на части и проанализируем: \"C:\\Program Files\\1cv8\\8.3.23.2040\\bin\\1cv8c\" — путь до ярлыка запуска 1С. /N\"\" — аргумент для указания имени пользователя. В нашей базе пользователя нет, поэтому между кавычек пусто. /TestManager — 1С будет запущена в режиме менеджера тестирования. /Execute \"C:\\vanessaTestProject\\vanessa-automation-single.epf\" — после запуска 1С будет запущена обработка, в нашем случае Vanessa Automation. /IBConnectionString \"File=\"\"\\DESKTOP-5UB7GS9\\Users\\fluMac\\Documents\\InfoBase\"\"; — путь до базы. \" /C\"StartFeaturePlayer; — будут загружены сценарии. QuietInstallVanessaExt; — будет выполнена установка компоненты VanessaExt (будет нам делать скрины ошибок). VAParams=C:\\Temp\\VAParams.json — Vanessa Automation будет запущена с дополнительными настройками. Соответственно, если эту команду запустить в командной строке, то у нас запустится 1С в режиме менеджера тестирования с определенной базой под определенным пользователем, в которой Vanessa Automation с определенными настройками будет запускать тесты. Файл настроек лежит не в папке Temp, так что адрес надо поменять на C:\\vanessaTestProject\\settings\\VAParams.json. Исполнителем для gitlab-runner является Powershell, там такая команда не сработает — это особенности синтаксиса консоли. Перед командой необходимо поставить &. Аргумент /N\"\" в Powershell будет воспринят неправильно, возможно это связано с экранированием. В общем, 1С попытается запуститься с пользователем /TestManager. Поэтому /N\"\" для нашего примера мы вообще из строки уберем. Откорректируем наш пайплайн с учетом настроек Vanessa Automation. Теперь у нас есть все данные, чтобы дописать файл.gitlab-ci.yml. Сейчас в нашей стадии test одна джоба — test. И пока в ней только одна команда включения кириллицы. Мы добавим к ней команду запуска Vanessa Automation из предыдущего этапа. Но для получения результатов в нашем примере недостаточно просто прогнать тесты. Нам также необходимо хранить где-то результаты тестирования — наши отчеты Allure. Для этого создадим еще одну джобу, я назвал ее screenshots. Сейчас я бы назвал ее по-другому, т.к. название не соответствует ее содержанию, но уж сделал как сделал. В этой джобе у нас всего одна команда, которая создает нам отчет Allure на основе файлов, которые подготовила для него Vanessa Automation. Рассмотрим строку раздельно: allure generate — сгенерировать итоговый файл отчета Allure. --report-dir C:\\Gitlab-Runner\\builds\\zU3fFBCGu\\0\\dimaSecret\\vanessatestproject1\\allure-results — папка куда наш отчет Allure будет сохранен. Т.к. нам нужно, чтобы gitlab-runner данный отчет забрал в gitlab для сохранения, мы и сохраняем его в папку с нашим проектом. --single-file — отчет формируется в виде одного файла. Для тестового примера удобней именно эта опция. Правда это лишает нас возможности смотреть историю прохождения этих тестов в предыдущих прогонах. C:\\vanessaTestProject\\allure-results — папка откуда Allure берет файлы, подготовленные Vanessa Automation по итогу прохождения тестов. Еще нам нужно указать, что по итогам прохождения тестов, нам необходимо забрать файл отчета. Мы укажем эту информацию в нашей джобе в разделе artifacts. Тут будет прописан относительный путь до папки с нашим отчетом. Вы спросите, а почему бы нам все команды в одну джобу не засунуть? Ответ — в нашем примере gitlab-runner выполнит сразу все команды и будет формировать отчет Allure еще до окончания выполнения тестов. И мы получим пустой отчет Allure. После изменения файла .gitlab-ci.ymlзапустится выполнение пайплайна. Посмотрим на джобу тестов. После изменения файла .gitlab-ci.yml запустятся джобы нашего пайплайна. Gitlab-runner запустит Vanessa Automation. Откроем список джоб в разделе Build — Jobs. Видим, что джоба test выполнилась успешно. Посмотрим джобу отчётов Allure. Откроем список джоб в разделе Build — Jobs. Видим, что джоба screenshots выполнена успешно. К ней также прикреплен Allure отчет, который gitlab разместил у себя на сервисе Gitlab Pages. Отчет можно скачать или открыть для просмотра по кнопке Browse. Для этого никаких настроек делать не надо, все отработает само. В нашем тестовом примере пайплайн будет запускаться всегда при изменении/добавлении/удалении файлов в репозитории. Также можно запускать его отдельно в разделе Build — Pipelines по кнопке Run pipeline. Поздравляю, теперь у вас есть понимание как работает Vanessa Automation и gitlab! В Самолёте у нас иной стек и связка сильно сложнее: СППР + Vanessa Automation + Gitlab. Так что, здесь ознакомились лишь с базовым примером, который поможет чуть углубиться в индустрию тестирования. Если есть какие-то вопросы, пишите, постараюсь ответить в комментариях. Благодности: HR Tech Samolet: Дмитриев Дмитрий (Ведущий программист 1С Зуп), Кашаканов Баир (бывший техлид 1С Зуп), Козлова Инна (нынешний техлид 1С Зуп), Гарбуз Елена (руководитель отдела качества HR Tech Samolet), Бакулина Наталия (Ведущий программист 1С Зуп). Cообщество testspro1c: Леонид Паутов, Виталий Онянов, @Evgeniy_Chekushkin, @nixel2007, @Labotamy, @OLYRS.",
    "154": "Граница Солнечной системы определяется гелиосферой и гелиопаузой. Гелиопауза — это область, где межзвёздная среда останавливает исходящий солнечный ветер. Но только два космических аппарата, «Вояджер-1» и «Вояджер-2», когда-либо достигали гелиопаузы. Поэтому учёные не уверены в протяжённости гелиопаузы и других её свойствах. Некоторые учёные стремятся узнать больше об этом регионе и разрабатывают концепцию миссии для его исследования. Гелиосфера играет важнейшую роль в Солнечной системе. Гелиосфера Солнца — это защита от входящего галактического космического излучения, например, от мощных сверхновых. Гелиопауза обозначает предел защитной силы гелиосферы. За её пределами галактическое космическое излучение беспрепятственно. Не существует общего понимания формы и протяжённости гелиосферы и гелиопаузы. Новое исследование призвано решить эту проблему, разработав зонд, который отправится за пределы этого региона, чтобы найти необходимые ответы. Исследование под названием «Complementary Interstellar Detections from the Heliotail» опубликовано в журнале Frontiers in Astronomy and Space Sciences. Ведущий автор — Сара Спитцер, постдокторский научный сотрудник кафедры климатических и космических наук и инженерии Мичиганского университета. «Без такой миссии мы похожи на золотых рыбок, которые пытаются понять аквариум изнутри», — говорит Спитцер. Гелиопауза защищает всё, что находится внутри неё, от галактического космического излучения, включая наших астронавтов, которые покидают защитную магнитосферу Земли. «Мы хотим знать, как гелиосфера защищает астронавтов и жизнь в целом от вредного галактического излучения, но это трудно сделать, когда мы до сих пор не знаем даже формы нашего щита», — сказал Марк Корнблют, учёный из Бостонского университета и соавтор исследования. Гелиосфера формируется в результате взаимодействия солнечного ветра и местной межзвёздной среды (ММС). Последняя состоит из плазмы, пыли и нейтральных частиц. В нашей области космоса доминируют два облака: Местное межзвёздное облако и G-облако, в котором находится система Альфа Центавра. Два других облака, Облако AQL и Голубое облако, находятся неподалёку. Облака — это регионы, где ММС более плотная. Проблема, с которой сталкиваются учёные, заключается в том, что мы не можем узнать больше о форме гелиосферы и её связи с ММС и её облаками, не выходя за пределы гелиосферы. Хотя «Вояджер-1» и «Вояджер-2» превзошли самые смелые ожидания, продержавшись так долго и покинув гелиосферу, они уже близки к концу. Их приборы уже не работают так, как раньше, а ведь эти космические аппараты были построены в 1970-х годах. Само собой разумеется, что с тех пор технологии шагнули вперёд. Нам нужен специально построенный космический аппарат, который сможет покинуть гелиосферу, когда и где мы захотим. Конечно, это очень долгое путешествие, и по пути он будет выполнять другие научные задачи. В отличие от зондов «Вояджер», которые были отправлены для изучения планет и достигли ММС только благодаря своему упрямству. «Будущая миссия межзвёздного зонда станет нашей первой возможностью по-настоящему увидеть гелиосферу, наш дом, со стороны и лучше понять её место в местной межзвёздной среде», — говорит ведущий автор работы Спитцер. Эта идея вынашивалась уже давно. В 2021 году учёные разработали концепцию миссии для такого зонда. Они назвали его «Межзвёздный зонд» и заявили, что он отправится в 50-летнее путешествие в ММС. По их словам, он «… обеспечит первую реальную точку обзора нашей системы, несущей жизнь, извне». Он может стартовать в 2036 году и пролететь с пиковой скоростью 7 а.е. в год. Это около одного миллиарда километров в год. Точка выхода — критическое различие между предложением 2021 года и нынешним. В предложении 2021 года говорилось, что зонд должен «захватить боковой вид гелиопаузы, чтобы охарактеризовать её форму, предпочтительно вблизи 45° от направления на нос гелиопаузы в точке (7° с.ш., 252° в.д.) в эклиптических координатах Земли». Авторы новой работы утверждают, что команда Interstellar Probe ошиблась с точкой выхода. «Однако в этом отчёте предполагается, что оптимальной является траектория зонда, проходящая под углом 45 градусов от носа гелиохвоста, или переднего фронта направленного движения Солнца», — пишут они. Спитцер и её коллеги изучили этот вопрос и пришли к другому выводу. Они изучили шесть различных траекторий движения зонда — от носа до хвоста. Они пришли к выводу, что лучше всего использовать боковой вид. «Если вы хотите узнать, насколько далеко назад простирается ваш дом, то выйти через парадную дверь и сделать снимок с тротуара, скорее всего, не лучший вариант. Лучше всего выйти через боковую дверь, чтобы увидеть, насколько длинным является дом спереди и сзади, — говорит соавтор исследования Корнблейт. Такая точка обзора позволит получить наилучшие научные результаты и увидеть форму гелиосферы». «Понимание формы гелиосферы требует понимания гелиохвоста, поскольку его форма сильно зависит от гелиохвоста и его взаимодействий с ММС, — пишут авторы в своей работе. — Миссия Interstellar Probe — идеальная возможность для проведения измерений либо по траектории, проходящей через гелиохвост, либо через фланг…» Есть и ещё одна веская причина следовать по этой траектории. Исследователи считают, что плазма от ММС может попасть в гелиосферу через хвост из-за магнитного пересоединения. Если это так, то зонд может взять пробы ММС дважды: один раз внутри гелиосферы, а другой — за её пределами. Команда также предложила отправить два зонда за пределы гелиосферы. Один будет двигаться по носовой траектории, а другой — по гелиохвостовой. Это «... позволит получить более полное представление о форме гелиосферы и лучше понять её взаимодействие с ММС», — объясняют авторы в своей статье. «Этот анализ потребовал большого упорства. Он начался с малого и превратился в большой ресурс для сообщества», — говорит соавтор исследования Сьюзан Лепри. Команда, стоящая за этим предложением, говорит, что межзвёздный зонд будет рассчитан на 50 лет и пролетит 400 астрономических единиц. Потенциально он может преодолеть гораздо большее расстояние — до 1 000 астрономических единиц. По мнению исследователей, это позволит нам получить беспрецедентное представление о гелиосфере и ММС. Научпоп. Проповедую в храме науки.",
    "155": "Стоимость электронных компонентов и систем за последние несколько лет неоднократно снижалась, а затем снова повышалась. Особенно это было актуально во время дефицита электроники. Сейчас происходит примерно то же самое. Эксперты считают, что после многомесячного снижения стоимости твердотельных накопителей их цена начнёт расти в ближайшее время. Дело в том, что компании на протяжении многих месяцев были вынуждены снижать цены на SSD. Причины разные, включая снижение спроса, накопление складских запасов, увеличение объёмов производства, нарушение логистических цепочек и т. п. Соответственно, примерно с 2021 г. объём продаж SSD постепенно снижался, а где-то в середине 2022 года стали сильно падать и цены. Компании тогда потеряли достаточно крупные средства — или думали, что они их потеряли, поскольку во время пандемии коронавируса спрос на технику очень вырос. Тогда большие прибыли получали почти все, кто причастен к рынку электроники. Потом, после спада активности пандемии, спрос упал и долгое время не возвращался к прежним высотам, поскольку как корпорации, так и частные лица закупились электронными товарами, которые были нужны на то время. Сейчас же ситуация понемногу меняется. Рынок оживился, плюс компании вроде Samsung и Western Digital стали снижать объёмы производства своих систем. Сейчас, когда запасы SSD снизились, а спрос на них вырос, компании планируют начать повышать цены. Один из крупнейших вендоров, сокративший объём выпуска памяти, — южнокорейская корпорация Samsung. Одновременно она увеличила цены на кремниевые пластины с NAND-чипами на 10–20%. А это очень чувствительно для рынка, поскольку и повышение резкое, и компания большая. Ну а другой производитель, Phison, даже был вынужден вернуться к предоплате за модули NAND, которые заказывают у неё клиенты. Такая политика была актуальной во время дефицита электроники и компонентов в течение 2020–2021 годов. После того как дефицит закончился, Phison, а также некоторые другие производители отказались от предоплатного способа работы. Сейчас всё это понемногу возвращается. В целом стоимость флеш-памяти стала повышаться с августа 2023 года. Это, в частности, привело к росту контрактных цен на корпоративные твердотельные накопители. Вполне возможно, что увеличится стоимость и на другие типы памяти. Так, игроки рынка предполагают, что стоимость мобильной DRAM повысится сразу почти на 15–20%. Сейчас компания WD смогла выйти на прежние величины прибыльности. Корпорация собирается вернуть уровень производства до оптимального показателя, сохраняющегося в течение нескольких лет. Не отстают и другие компании, в частности Samsung. Специалисты TrendForce считают, что это будут корпорации, которым нужно много объёмных накопителей. Именно в этом секторе цены повысятся примерно на 25% — и это ещё не предел. Что касается пользовательского сегмента, то здесь стоимость SSD возрастёт не так сильно — на 10–15%. Скорее всего, считают эксперты другого агентства, WCCFTECH, в настоящее время нужно приобретать накопители. Просто потому, что пока ещё цены низкие, но вскоре всё может измениться. В приведённой выше таблице TrendForce представлен обзор и сравнение тенденций цен на флэш-память NAND за первый квартал (фактические цены) и второй квартал (прогноз) этого года. В таблице показаны цифры для мобильных продуктов на базе NAND (eMMC и UFS), клиентских и серверных твердотельных накопителей, а также пластин флэш-памяти NAND, на которых основаны эти продукты. Первый квартал 2024 года выдался довольно плохим для тех, кто планировал приобрести SSD-накопители по дешёвке. По оценкам исследования TrendForce, цены в этом сегменте выросли на 23–28%, и это добавляет примерно $100 к планам сборки или обновления ПК. По мнению TrendForce, цены не стабилизируются и не развернутся во втором квартале 2024 года. Вместо этого мы можем увидеть дальнейшее совокупное повышение цен на 10–15% с настоящего момента и до конца июня. В таблице ясно указано, что рост цен на клиентские твердотельные накопители опережает рост цен на ключевые пластины 3D NAND. TrendForce объясняет, что на стоимость SSD негативно повлияли такие факторы, как стратегии закупок производителей твердотельных накопителей, снижение продаж сопутствующей продукции и т. п. Возможно, выше 25% стоимость SSD и не поднимется. Ведь пока что политику цен меняют крупные производители. Мелкие и средние на этом фоне, возможно, захотят переманить покупателей к себе. Соответственно, слишком уж повышать стоимость никто не будет. При этом спрос на SSD может увеличиться, производители рано или поздно снова начнут наращивать объёмы выпуска продукции. Потом же цикл, вероятнее всего, повторится: слишком большой объём производства → снижение цен, демпинг и т. п. В целом, осталось ждать и надеяться на то, что цены не вырастут слишком сильно.",
    "156": "Растут или снижаются зарплаты в тестировании? Какие скилы наиболее востребованы? Сколько вакансий для джунов? Узнали у экспертов, как изменился рынок в 2023 году и какие тренды сохраняются в 2024. Привет! Это команда QA Studio. Несколько лет мы следим за статистикой вакансий и зарплат в тестировании. Используем для этого два ресурса — hh.ru и Хабр Карьера. Данные за январь, февраль и март мы разместили в нашем блоге. В этой статье обсудим тренды 2023 года и расскажем, что изменилось в первом квартале 2024. В 2023 году российские компании активно развивали собственные технологии и импортозамещение. Одна из причин — уход иностранных поставщиков и партнеров. Чтобы запускать новые проекты, необходимо расширять IT-отделы и нанимать специалистов на новые задачи. Больше продуктов — значит, требуется больше тестировщиков, которые будут отвечать за их качество. Но стоит отметить, что и во всем мире наблюдается повышенный спрос на QA-специалистов. Недавнее исследование PractiTest содержит информацию о том, что компании увеличивают штат тестировщиков. И всё реже встречается ситуация, когда ты единственный QA на проекте. В одном из видео мы более подробно обсуждали текущую ситуацию в IT — например, кого сейчас труднее всего нанять и стоит ли указывать зарплату в резюме. В период с 2023 по 2024 год медианная зарплата выросла у всех тестировщиков — будь то junior, middle или senior. Эти данные мы получили на основе информации, которую работодатели указывают в вакансиях на hh. Медианная зарплата — это уровень, выше и ниже которого получает доход одинаковое количество специалистов. Например, если медианная зарплата в команде из десяти человек — 150 000 ₽, это значит, что пять человек получают меньше 150 000 ₽, а пять — больше. Компании готовы платить больше за обеспечение качества своих цифровых продуктов — и это однозначно радует. При этом надо учитывать, что по-прежнему в большинстве вакансий (~ 80%) не указывается зарплата, а даже где она указана — чаще всего это зарплатная вилка. Как в 2023, так и в 2024 почти в половине вакансий требуются кандидаты с опытом от года до трех лет. Это связано с тем, что компаниям легче взаимодействовать с относительно самостоятельным специалистом, имеющим за плечами коммерческий опыт. Плюсом для работодателя является и то, что зарплатные ожидания в этой категории не такие высокие, как у тестировщиков с опытом от трех лет. Из историй наших учеников мы знаем, что иногда на такие вакансии могут взять после стажировки, при условии крепкой теоретической базы и развитых софт-скилов. Ну и без хорошего сопроводительного письма и резюме — никуда :) Постепенно растет и число компаний, которые готовы набирать джунов без опыта. В 2023 году таких вакансий было 3,6%, а по итогам первого квартала 2024 года — уже 4,8%. Почему так происходит? Во-первых, компаниям легче выделить бюджет под зарплатные ожидания начинающего специалиста. Во-вторых, иногда проще взять джуна и дорастить его до мидла, чем месяцами искать опытного сотрудника. Тренд на удаленку выстрелил в 2020 и немного просел к 2023 году: люди соскучились по работе в офисе и живому общению с коллегами. К 2024 году удаленный формат работы конкурирует с гибридным, когда сотрудник может работать полный день, но иногда делает это из дома, а иногда — из офиса. Такой подход позволяет быстрее адаптироваться на новой работе и быть на одной волне с коллегами. А еще держит тебя в тонусе — своеобразный «выход в люди» после недели удаленки в пижаме :) Мы изучили статистику с января по март, чтобы понять, как изменился рынок за это время. Собрали данные по количеству вакансий, резюме и зарплате в тестировании. Общее количество QA-вакансий с января по март 2024 года держится на одном уровне. При этом на рынке больше всего предложений для тестировщиков с опытом от года до трех лет, в среднем 50% от общего числа вакансий — сохраняется тенденция 2023 года, про которую рассказывали выше. Количество тестировщиков на рынке. Максимальное число наблюдалось в январе: на hh было больше 15 000 активных соискателей. А на 1 апреля таких тестировщиков уже меньше — 9629 человек. В сравнении с прошлым годом количество тестировщиков в активном поиске, у которых нет опыта, значительно уменьшилось. Например, 1 апреля 2023 года таких соискателей было 4929, а 1 апреля 2024 — уже 1750. Посмотрим соотношение открытых вакансий и тестировщиков в активном поиске на примере джунов. На 1 апреля на hh опубликовано 252 вакансии, где готовы взять без опыта, и 1750 соответствующих тестировщиков. Чтобы найти работу после курсов или самостоятельного обучения, нужно быть лучше, чем семь других соискателей. Насколько это сложно, можно оценить, сравнив свое резюме с другими. Спойлер: не все семь кандидатов будут достойными соперниками — часто резюме джунов не содержит описания освоенных инструментов и скилов, публикуется без фото, с орфографическими ошибками и опечатками. Так что уже на этом этапе можно заработать первые очки :) Поможет выделиться на фоне остальных и участие в стажировках. В этой статье мы собрали 25+ вариантов, где джуну можно получить первый коммерческий опыт. Уровень зарплаты тестировщиков. Для более точных цифр мы взяли данные из калькулятора зарплат на Хабр Карьере. Он рассчитывает актуальный доход на основе зарплат, которые указывают сами тестировщики. При этом на Хабре есть отдельная информация об инженерах по обеспечению качества и инженерах по автоматизации тестирования. У таких специалистов зарплата еще выше — что вполне объяснимо уровнем ответственности / знанием языков программирования, необходимых для работы с автотестами. Чтобы проверить, насколько достоверны эти данные, мы самостоятельно опросили 383 человека в закрытом сообществе по тестированию. В результате оказалось, что их зарплаты (данные за март 2024) соответствуют той информации, которая публикуется на Хабре. Но нужно держать в голове, что часть специалистов, участвующих в подобных опросах, могут работать на зарубежные компании. Это напрямую сказывается на уровне их зарплат. При этом кто-то работает удаленно из России, а кто-то готов релоцироваться. Для сравнения: зарплата тестировщиков в феврале 2024 года в США, Канаде и Великобритании составила от 680 000 до 1 180 000 ₽ в месяц до выплаты налогов. Расчеты грубые, но дают примерные представления о разнице зарплат тестировщиков в России и за рубежом. А вот здесь собраны реальные рассказы о том, сколько зарабатывают тестировщики в России и за рубежом и на что они тратят свою зарплату. А что на этот счет говорят зарубежные исследования? Самые востребованные навыки для тестировщика — умение общаться, автоматизировать функциональные проверки и тестировать API. Самые популярные языки программирования для автоматизации, как в 2023, так и в 2024 году, — Java, Python и JavaScript. На их долю приходится более 70% всех упоминаний в вакансиях. Среди средств для автоматизации тестирования Web UI лидируют Selenium, Selenide и Playwright. Список самых востребованных фреймворков для unit-тестов тоже не меняется за последние пару лет. В лидерах — pytest, JUnit и TestNG. Самые популярные инструменты для тестирования API — Postman, Swagger и SoapUI. На их долю приходится больше 90% всех упоминаний в вакансиях на hh. Для компаний не так важно, есть ли у тестировщика высшее образование. Больше смотрят на реальные скилы, опыт и мотивацию. На 1 апреля в 86% вакансий не требуется высшее образование. А еще тестирование перестало быть лишь «легким входом в айти». Согласно исследованиям, более 60% QA-инженеров через пять лет по-прежнему видят себя именно в тестировании.",
    "157": "Я не специалист, и это список моих идей для улучшения работы языковых моделей. К сожалению хорошо проверить это не имею возможности. Нигде не встречал таких идей. Интересно узнать мнения о них. 1. Новая функция потерь для минимизации переобучения на пре-тренировке. Предполагается что достаточно контролировать только \"нужные\" токены. Остальные выровняться косвенно. 2. Правка весов токенов в конкретных позициях ответа. Можно использовать для быстрого исправления конкретных косяков. Пример:Есть: 12345 -> Надо: 12045 -> Обучающий пример: 12 -> Веса в словарях ответа принимаем за истину кроме веса интересующих токенов. Для вероятности 0 или 3 используем нужные значения. <eos> не используется. 3. Модификация потерь для токенов подстрок в ответе. Позволяет сосредоточиться на фрагментах которые важно исправить. 4. Считать ошибку только для не совпавших токенов(наиболее вероятного и целевого). 5. Не обновлять весы с градиентами меньше порога. Установив минимальный порог для градиентов, можно предотвратить незначительные обновления. Поможет вносить в веса модели только значимые изменения и бороться с общим дрейфом модели. 6. Сжатие предложения в один вектор встраивания позволяет освободить место в контексте. Также его можно хранить в векторной базе данных вместе с текстом. 7. При генерации модель определяет предпочтительную температуру для каждого токена. Позволяет модели регулировать свой уровень креативности в зависимости от контекста. Особенно полезно для смешанных задач. 8. topT(Threshold) - Отбрасывание всех токенов с весом ниже порога. topK, topP ненадёжны потому что в выборку могут попадать мусорные токены. 9. Исправление ответа на лету если сэмплер внесёт серьёзную ошибку. Есть подозрение что уверенность ИИ в последующих токенах заметно уменьшится. Это можно отследить по уменьшению разброса вероятностей и откатить плохой токен. Хорошо бы всё это появилось в API всяких сервисов. 10. Мои наблюдения показывают что количество нейронов важней количества параметров. Поскольку Dense слои жрут квадратично от размера, то придумал такой заменитель. Жор должен быть линейным от размера. С этим ширина слоя может быть в сотни раз больше. gather тут плохо подходит. Но как реализовать быстрей на фреймворках не придумал. Только если Cuda ядро делать. Да и вообще добавили бы Vulkan уже во фреймворки. Хватит поддерживать монополию Nvidia. При обучении оценивается только первое несовпадение токенов. Предсказанный токен понижается а ожидаемый повышается. Это исключает переобучение, совместимо с пре-тренировкой и настройкой, максимально дёшево, быстро обучается, и даёт наилучшую производительность.",
    "158": "Я — Денис, Middle Android-разработчик в «Лайв Тайпинге». В этой статье я расскажу об инструменте тестирования accessibility в Android приложениях. Исследования показывают, что максимальный процент выявленных проблем с accessibility составляет 40-50%. Поэтому ручное тестирование всегда необходимо. Но для этих 40-57% есть несколько инструментов, которые можно использовать. Один из них — Accessibility Scanner, и в этой статье я расскажу о том, как протестировать с его помощью ваше приложение. Давайте начнем с того, что это такое. Accessibility Scanner — полезный инструмент для полуавтоматического тестирования доступности приложения. Он может выявить проблемы в следующих категориях: контрастность текста и изображения. Он также предлагает исправления для некоторых из обнаруженных проблем, и предоставляет дополнительную информацию о них. Тем не менее, он не находит все возможные проблемы доступности и, таким образом, не гарантирует хорошей доступности вашего приложения. Ручное тестирование все еще необходимо. Замечательно то, что вы можете использовать Accessibility Scanner с любым приложением для Android — это может быть нативное приложение для Android или созданное с использованием кроссплатформенных технологий, таких как Flutter или React Native. Или это может быть даже PWA — Progressive Web App. Чтобы воспользоваться Accessibility Scanner, его нужно сначала загрузить из Google Play. После запуска сканера будет предложено его активировать в настройках специальных возможностей. На телефонах Samsung нужно выбрать пункт «Установленные приложения» и выбрать «Сканер доступности». После вам необходимо дать доступ к управлению работой экрана. И нажать на «тогл» для запуска сканера. Accessibility Scanner предоставляет две возможности — сканировать один экран или записывать несколько экранов. Перейдите к приложению, затем нажмите кнопку Accessibility Scanner на экране. Откроется меню, в котором вы можете выбрать запись, снимок (один экран) или отключить сканер доступности. Нажмите кнопку записи, чтобы записать поток или несколько экранов. Затем перемещайтесь по экранам. Если в вашем телефоне включена вибрация, вы должны ощущать легкую вибрацию каждый раз, когда приложение делает снимок экрана. Завершить запись можно нажатием той же кнопки, которая теперь превратилась в кнопку «Стоп». Если она не видна на экране, то можно, например, открыть Быстрые настройки (сдвиньте их вниз от верхней части экрана). Итак, вы сделали снимок или записали поток. Следующий шаг — посмотреть результаты и интерпретировать их. Вы можете перемещаться по экранам и видеть выделение возможных проблем. Если вы предпочитаете видеть предложения в виде списка, вы можете найти его в правом верхнем углу, в значке списка. Найденные проблемы группируются либо по экранам, либо по категориям. При нажатии на элемент на экране отображается скриншот проблемного элемента: Как правило, в описании проблем также содержатся идеи о том, как их устранить. Подробнее о различных проблемах, их возможных причинах и способах устранения вы можете узнать из материалов, упомянутых в следующем разделе. Например, приложение для покупки шаурмы. Найденные проблемы и вариант их решения можно посмотреть кликнув на выделенный элемент или в списке категорий. Но даже без сканера можно узнать, на сколько ваш UI accessibility friendly. Достаточно открыть XML или Compose вёрстку и «провалиться» в меню предупреждений/ошибок. Тут IDE сама подскажет у какого элемента какая проблема. И даже выдаст ссылку на документацию. Google создал материалы, позволяющие узнать больше о сканере доступности. Есть два хороших текстовых ресурса: результаты сканирования доступности. А если вы предпочитаете смотреть видео, у них также есть видео на YouTube. Если вы нашли неточности/ошибки в статье или просто хотите дополнить её своим мнением — то прошу в комментарии! Или можете написать мне в Telegram — t.me/MolodoyDenis.",
    "159": "Эффективное обеспечение информационной безопасности невозможно без знания основ законодательства в области информационных технологий. В этой статье мы рассмотрим так называемые компьютерные статьи уголовного кодекса. При этом, мы не будем погружаться в юридические тонкости и нюансы, а попробуем простым языком с примерами разобраться за что и как наказывают за те или иные правонарушения. Статья 272. Неправомерный доступ к компьютерной информации. Статья 273. Создание, использование и распространение вредоносных компьютерных программ. Статья 274. Нарушение правил эксплуатации средств хранения, обработки или передачи компьютерной информации и информационно-телекоммуникационных сетей. Статья 274.1. Неправомерное воздействие на критическую информационную инфраструктуру Российской Федерации. Статья 274.2. Нарушение правил централизованного управления техническими средствами противодействия угрозам устойчивости, безопасности и целостности функционирования на территории Российской Федерации информационно-телекоммуникационной сети \"Интернет\" и сети связи общего пользования. Статья 272 (не путать с “экстремистской” статьей 282) посвящена неправомерному доступу к компьютерной информации. В случае, если неправомерный доступ к охраняемой законом компьютерной информации повлек уничтожение, блокирование, модификацию либо копирование компьютерной информации, вас могут наказать штрафом до двухсот тысяч рублей в лучшем случае, или лишением свободы на срок до двух лет в худшем случае. При этом, если преступление совершено группой лиц по предварительному сговору и повлекло тяжкие последствия, то есть вероятность присесть на срок до семи лет. Здесь сразу стоит обратить внимание на формулировку, а именно, на то, что отсутствие перечисленных последствий исключает наличие состава преступления, предусмотренного данной статьей. Вот такой пример удалось найти на одном из юридических ресурсов: “Гражданка И., желая проверить верность ей гражданина П., посетив сайт электронного почтового сервиса, используя ранее полученный неправомерным путем логин и пароль гражданина П., осуществляет визуальный просмотр содержимого его почтового ящика. Никаких действий по копированию, изменению уничтожению информации И. она не предпринимает.” В таком случае состав преступления отсутствует. Но если бы она скопировала хотя бы одно письмо, например для сбора доказательств, то был бы состав 272. Следующая компьютерная статья это 273. В ней говорится о создании, использовании и распространении вредоносных компьютерных программ. Так создание, распространение или использование компьютерных программ либо иной компьютерной информации, заведомо предназначенных для несанкционированного уничтожения, блокирования, модификации, копирования компьютерной информации или нейтрализации средств защиты компьютерной информации, наказываются штрафом до двухсот тысяч рублей или же сроком до семи лет в случае сговора и тяжких последствий. Ну казалось бы, здесь все понятно: написал вредонос – получи срок. Но в статье говориться не только о создании, но и об использовании и распространении и здесь все несколько сложнее. Приведем пару примеров. “Один сисадмин скачал в интернете крякер для активации офисного пакета программ Microsoft и записал их на флешку. Для этого ему понадобилось на время отключить антивирусники. Пограничные файерволы установленные на компьютерной сети завода, где работал данный гражданин, обнаружили несанкционированный исходящий трафик с компьютерной сети завода. Передача данных шла непродолжительное время, и сама прекратилась, конечной точкой получения информации были некие серверы в США. Факт был установлен сотрудниками подразделений безопасности предприятия, данные об инциденте передали в УФСБ по Пермскому краю, где возбудили и расследовали дело.” Товарища оправдали по 274.1 (об этой статье мы поговорим далее), но приговорили по 273, так как скачанные им файлы содержали вредоносный код. Это был пример “неудачного” применения вредоноса. Посмотрим также, как можно “неудачно” распространить вредонос. На просторах сети можно найти множество “зоопарков” – ресурсов, с которых можно скачать артефакты – образцы вредоносов. На этих сайтах артефакты находятся в безопасном виде: запароленный архив, расширение, формат и т.д. Но при желании подобный артефакт можно привести во вполне рабочее состояние. Зачем это может понадобится в законопослушных целях? Например, в целях тестирования средств защиты. В таком случае, при не слишком аккуратном использовании артефактов есть ненулевой риск стать распространителем вредоносных приложений. Если предыдущие две статьи предназначались прежде всего для умышленных правонарушителей, то есть тех, кто сознательно осуществлял несанкционированный доступ или распространял вредоносы, то теперь мы будем говорить о том, как могут наказать несознательный обслуживающий персонал – администраторов и инженеров, который не слишком хорошо обеспечили работу ИТ систем. Статья 274 посвящена нарушениям правил эксплуатации средств хранения, обработки или передачи компьютерной информации и информационно-телекоммуникационных сетей. Так нарушение правил эксплуатации средств хранения, обработки или передачи охраняемой компьютерной информации либо информационно-телекоммуникационных сетей и оконечного оборудования, а также правил доступа к информационно-телекоммуникационным сетям, повлекшее уничтожение, блокирование, модификацию либо копирование компьютерной информации, причинившее крупный ущерб наказывается от штрафа в размере до пятисот тысяч рублей до лишения свободы на срок до пяти лет. С появлением ФЗ №187 “О безопасности критической информационной инфраструктуры Российской Федерации” в уголовном кодексе появилась еще одна похожая статья, посвященная именно критической инфраструктуре. Статья 274.1. Неправомерное воздействие на критическую информационную инфраструктуру Российской Федерации. В этой статье отдельными пунктами рассматривается создание, распространение и (или) использование программ, заведомо предназначенных для неправомерного воздействия на критическую информационную инфраструктуру Российской Федерации. Также, рассматривается нарушение правил эксплуатации средств хранения, обработки или передачи охраняемой компьютерной информации, содержащейся в КИИ. Примечательно, что максимальное наказание по данной статье может достигать десяти лет лишения свободы. Можно найти несколько примеров использования данной статьи. Например, если кто не помнит, года три назад была актуальна повестка, связанная с пандемией, вакцинацией и соответствующими сертификатами о прививках. Так вот, одним из примеров применения 274.1 является внесение в базу информации о прививках без выполнения таковых на самом деле. Дело в том, что по мнению следствия при внесении ложных записей в информационные системы КИИ, обвиняемые лица нарушают целостность этой информационной системы, в результате чего, циркулирующие в системе сведения теряют объективность, достоверность и актуальность. Другой пример по данной статье. Гражданин работал в большой железнодорожной компании. Ему необходимо было сдать тест на знание правил работы с оборудованием. Он не придумал ничего лучше, как скачать кряк к тестирующей программе для того, чтобы тест выдал правильные ответы. Однако, компьютер, на котором проводилось тестирование оказался частью КИИ, и в результате получился состав 274.1. Опять все те же кряки, и компьютеры являющиеся частью КИИ. Так что, прежде чем устанавливать или лечить какое-либо ПО на рабочие компьютеры, проверьте не являются ли они частью критической инфраструктуры. И завершит наш сегодняшний набор компьютерных статей, провайдерская статья 274.2. Нарушение правил централизованного управления техническими средствами противодействия угрозам устойчивости, безопасности и целостности функционирования на территории Российской Федерации информационно-телекоммуникационной сети \"Интернет\" и сети связи общего пользования. Здесь речь идет о нерадивых провайдерах, которые поставили “сбоку” оборудование для обеспечения работы суверенного интернета, то есть не выполнили требования, предъявляемые регуляторами по установке данного оборудования. Здесь разброс наказаний также от штрафов до трех лет лишения свободы. Информации о применении данной статьи в свободном доступе найти не удалось. В этой небольшой статье мы на простом языке рассмотрели какие компьютерные статьи есть в российском уголовном кодексе. Будьте аккуратны, чтобы не столкнуться с этими статьями в реальности. Своими знаниями в области информационной информации эксперты OTUS делятся в рамках практических онлайн-крусов. С полным каталогом курсов можно ознакомиться по ссылке.",
    "160": "Задумывались ли вы о том, как изменить конфигурацию сразу на нескольких сетевых устройствах? Что, если нужно сделать это на всей сети с сотнями и тысячами единиц оборудования? А что, если приходится делать это каждый месяц на железе от пяти разных производителей? Очевидное решение для подобных задач — автоматизация. Но реализовать её можно не одним способом, а в процессе наткнуться не на одни грабли. Меня зовут Вадим Воловик, и я руковожу проектами разработки в Yandex Infrastructure. Наша команда NOCDEV отвечает за автоматизацию сетей всего Яндекса. Давно хотелось рассказать о задачах такого масштаба, но по ходу написания материала стало понятно, что тема тянет на целый цикл. Так что мы с коллегами расскажем о самых интересных примерах автоматизации в отдельных постах. В этой статье проведём небольшую экскурсию по нашему сетевому «хозяйству» в десятки тысяч устройств и остановимся подробнее на том, как при таком объёме мы автоматически обновляем конфигурации. Будет интересно не только сетевым инженерам, но и бэкенд‑ и фронтенд‑разработчикам, которые занимаются автоматизацией работы с самым разным оборудованием. В настоящее время наша сеть представляет собой совокупность дата‑центровых, магистральных и офисных сетей (включая склады и лавки). Так как сервисы Яндекса георезервируются, то и сеть у нас геораспределённая. Основными местами расположения наших сетевых устройств являются дата‑центры. Каждый дата‑центр — это десятки тысяч серверов и тысячи L3-коммутаторов. Дата‑центры взаимодействуют через резервируемую магистральную сеть хай‑фай‑класса. В дата-центрах есть специальная сеть управления (Management Network), включающая консольные серверы и IPMI-коммутаторы. Отдельно достойна упоминания офисная сеть с её тысячами WiFi‑точек доступа, L2-коммутаторами, системами видеоконференцсвязи и SIP‑телефонией. Суммарно команда эксплуатации сетей, или Network Operations Center (NOC) Яндекса обслуживает более 20 000 устройств. Используется практически весь стек сетевых технологий: OpenVPN для доступа к офисной сети, IPSec‑туннели для связей между офисами. Команды NOC каждый день сталкиваются с задачами масштабирования и поддержания сети в рабочем состоянии, периодически устраняя аварии на сети. Поэтому бок о бок с ними трудится команда NOCDEV — профессиональных программистов, разрабатывающих собственный стек автоматизации сетевой инфраструктуры. Совместными усилиями направлений NOC и NOCDEV решаются задачи ввода оборудования в эксплуатацию, обновления программного обеспечения, деплоя конфигурации, real‑time‑мониторинга и другие. Всего у нас более 20 сервисов, а технологический стек выглядит следующим образом: Так что если вы умеете писать код и интересуетесь инфраструктурными DevOps/NetOps‑задачами, то в этой статье для вас найдётся много любопытного. На рисунке ниже вы видите нашу высокоуровневую архитектуру автоматизации. Она не сильно отличается от других проектов сетевой автоматизации (например, можно посмотреть на ONAP). Users — вся автоматизация делается для наших пользователей (сетевых и не только инженеров и разработчиков). User Interfaces — этот уровень состоит из Web‑интерфейсов, CLI, TF‑провайдеров, API и не нуждается в особом представлении. Inventory — система инвентаризации аппаратного и программного обеспечения. Automation — именно здесь работают модули для автоматизированного распространения настроек на сетевое оборудование. Monitoring — набор модулей, позволяющих отслеживать состояние сети в различные моменты времени. Executors — этот уровень представлен разнообразными библиотеками и фреймворками общения с сетевым оборудованием. Network — и, конечно же, всё это опирается на сети и немного на облака. NOCDEV Яндекса разрабатывает и поддерживает все перечисленные компоненты автоматизации. Если вы думаете, что нашими пользователями являются только сетевые инженеры, то ошибаетесь. Сеть — это один из важнейших инфраструктурных компонентов любой IT‑компании. Соответственно, напрямую или косвенно пользователями являются все сотрудники и все пользователи сервисов компании.В нашем цикле под пользователями мы будем подразумевать технический персонал инфраструктурных подразделений. На рисунке можно увидеть статистику распределения возрастов этих пользователей. Так как основной возраст пользователей до 35 лет, то важно создавать современные Web‑интерфейсы для решений. Мы активно вкладываемся в UI/UX наших сервисов, так что в команде всегда найдётся место для фронтенд‑разработчиков и особенно для дизайнеров. Также мы понимаем, что зачастую наши пользователи — бэкенд‑разработчики других команд и сервисов, так что мы предоставляем API (REST, gRPC, GraphQL) и Terraform‑провайдеры в качестве пользовательских интерфейсов. и ещё сотни атрибутов. Система Inventory не является одним монолитом. Она состоит из нескольких сервисов, объединённых API Gateway. К ним мы относим: Racktables (опенсорс) для хранения общих данных об оборудовании и адресном пространстве. Источник истины о программном обеспечении сетевого оборудования. Источник истины о сетевой связности (устройство‑устройство, устройство‑сервер). Источник истины о внешней BGP‑связности. Источник истины о SIP‑телефонии. Inventory API. К Inventory‑системе предъявляются достаточно жёсткие требования по отказоустойчивости, доступности и скорости работы API. Именно поэтому мы используем в этом направлении как современные, так и проверенные временем решения. Например, мы широко применяем технологию GraphQL, а в качестве API Gateway используем GraphQL Router. Для тестирования изменений у нас есть специально разработанный Inventory Staging, который поднимает Docker‑контейнер, где разработчик может безопасно для прода тестировать изменения. В Inventory хранится информация именно о целевом состоянии сети. Когда она достаточно полна, то с использованием систем автоматизации можно привести состояние сети к целевому. Наш стек автоматизации состоит из нескольких сервисов: Оркестратор пользовательских сценариев. Сервис расчёта разницы между целевым состоянием сети и текущим. Фреймворк генерации конфигурации оборудования (annet). Сервис хранения текущей конфигурации устройства. Сервис ввода устройства в эксплуатацию. Благодаря этому стеку мы можем позволить себе еженедельно производить десятки тысяч операций на сетевом оборудовании. Служба эксплуатации сети, независимо от разработчиков, может реализовать разнообразные сценарии обслуживания сети. Наш стек чем‑то напоминает Ansible, Temporal, Apache AirFlow, но с Яндексовым шармом и является собственной разработкой. Почему мы идём тут своим путем? Наш коллега Дмитрий Липин высказывался на эту тему в статье на примере RTC (Runtime Cloud). При наличии такого «зоопарка» оборудования и разнообразных сценариев мы не можем себе позволить блокироваться о сторонние решения. Наш цикл автоматизации зависит только от новых моделей и софта производителей сетевого оборудования. Также выбор собственной реализации системы для выкатки конфигурации вызван тем, что производители оборудования не нацелены на унифицированный подход к конфигурации сетевых устройств. Даже внутри одной линейки оборудования H‑вендора можно встретить пять реализаций языка взаимодействия с оборудованием. При этом выкатка конфигурации по своей сути напоминает процесс деплоя кода: драфт конфигурации, ревью, автотесты, выкатка с возможностью rollback. В индустрии принято самостоятельно реализовывать инструменты общения с оборудованием. Также мы активно работаем над сервисом‑абстракцией над сетевым оборудованием. Цель проста — свести язык общения со всем разнообразием нашего сетевого оборудования к одному (об этом мы тоже обязательно расскажем когда‑нибудь). После того как конфигурация применена, хочется понять всё ли ок с оборудованием, а если что‑то не получилось — узнать, что именно. Для этого мы адаптировали сервисы мониторинга, позволяющие в реальном времени смотреть, что происходит с сетью. Стек мониторинга сети сталкивается с задачами быстрого опроса десятков тысяч устройств (буквально за минуты). Поэтому активно применяются технологии высоконагруженных сервисов. Используются разные протоколы взаимодействия с оборудованием: CLI, SNMP, Netconf, gRPC. Сервисы мониторинга представлены сборщиками данных, фреймворком регистрации аварий на сети. Мониторинг — глаза эксплуатации сети, сервисы автоматизации — руки, а системы Inventory — сердце. Важно, чтобы всё работало синхронно, и NOCDEV способствует гармоничному взаимодействию всех частей. Быстро сказка сказывается, да не скоро дело делается.Опираясь на эту народную мудрость, мы решили вести наш рассказ поэтапно и шаг за шагом освещать компоненты и модули сетевой автоматизации. А помогать нам в этом будет вымышленный персонаж — Милош. С ним вместе будем рассматривать задачи автоматизации, приближенные к реальным.И в первой вводной задаче приступим к рассмотрению задачи массовой конфигурации оборудования — одной из ключевых задач автоматизации сети. Условия задачи. Сетевому инженеру Милошу необходимо актуализировать подписи интерфейса (description) в формате {REMOTE_HOSTNAME}_{REMOTE_PORTNAME} на всех клиентских и сетевых портах. Вопрос. За сколько дней Милош справится с актуализацией 20 000 устройств на сети, состоящей из коммутаторов и устройств Cisco, Huawei? При том, что одна операция актуализации порта занимает в среднем 5 минут на одно устройство. Как кажется, самое простое решение — вручную изменять конфигурацию на устройствах.Плюсы — низкий порог входа для сетевого инженера.Минусы — очень долго. Для 20 000 устройств потребуется примерно 500 человеко‑дней. Не помешает сотня‑другая Милошей.Есть подводные камни: неизбежны ошибки. Вывод: смотрим другие варианты. Подключение к устройству (Open Session). Сбор информации о интерфейсах устройства (Send Command). Парсинг данных (Parse Data). Генерация конфигурации (Configuration Generation). Деплой конфигурации (Configuration Deploy). Для того чтобы подключиться к устройству, необходимо знать IP‑адрес управления, логин, пароль (или ключик) и протокол подключения. Также стоит выяснить, с устройством какого производителя (Vendor) и какой моделью (Model) мы имеем дело. Vendor и Model полезно знать, чтобы заранее понимать, на каком «языке» нужно общаться с оборудованием. Обычно в арсенале сетевого инженера имеется система инвентаризации, хранящая информацию о устройстве. Предположим, что в компании Милоша существует такая система, например, NetBox, Racktables, или NOC Project. Знания, которые хранятся в Inventory, позволяют Милошу автоматизировать чтение и запись данных на устройство. При решении данной задачи в роли Inventory‑системы будут выступать файлы. В примере мы будем использовать: importInventory.json для импорта данных о сетевых устройствах exportInventory.cvs для экспорта данных о сетевых устройствах В статье на нашем примере разберём, как экспортировать данные об устройстве в этот формат, а дальше покажем, как экспортировать инвентарную информацию о сетевом оборудовании и как использовать эту информацию для генерации конфигурации. Для подключения к устройству и ведения с ним диалога Милошу необходима библиотека, которая реализует чтение информации с устройства и запись на него. Одной из известных библиотек является Napalm, написанная на Python. Милош понимает, что он столкнётся с задачами масштабирования, так что смотрит в сторону библиотеки написанной на Golang. Мы в Яндексе разработали и выложили в опенсорс библиотеку gnetcli. Она написана на Go, с учётом нашего опыта работы со схожими библиотеками на Python. В статьях будем давать примеры использования именно gnetcli.Более подробно о том, почему мы разрабатываем собственную библиотеку общения с оборудованием, и какие ещё есть сценарии использования — мы обязательно расскажем в следующий раз. Для начала надо убедиться, что устройство активно. Для этого можно использовать незамысловатую функцию: Далее импортируем основные данные об устройствах из importInventory.json: Для подключения к устройству нам потребуются address, login, password. Ниже приведён пример, возвращающий ssh streamer для работы с устройством. Он будет использоваться в основной функции этого скрипта — отправка команд на устройство и предоставление результата их выполнения. Как видим, изначально предусмотрена возможность взаимодействия с устройствами разных производителей. Чтобы полностью решить задачу, Милошу необходимо узнать, как именно устройства соседствуют друг с другом. То есть, необходимо знать связь типа: Эта информация доступна на устройстве благодаря протоколу LLDP. Соответственно, необходимо запросить информацию о соседствах на устройство и получить вывод с устройства. Вспомним нашу топологию из условий задачи. Используя функцию SendCommand, соберём информацию об интерфейсах с устройств. Также Милошу потребуется информация о LLDP‑соседях. Соберем её, той же функцией. Данные с устройства получены, и Милош готов к следующей части — парсинг полученных данных. Прежде чем парсить список соседей, вначале надо иметь список доступных на устройстве интерфейсов. Обратите внимание, что FastEthernet0/0 не равно Fa0/0, хотя это один и тот же порт. Тут мы наступаем на проблему ShortName и LongName интерфейса, потому что команда show ip interface brief отдаёт длинные имена, а show lldp neighbors — короткие. Поэтому необходимо хранить оба значения имени. Научившись собирать интерфейсы, можно приступать к парсингу соседей. Пример функции парсинга LLDP Neighbors: За этим следует задача синхронизации реального состояния оборудования с Inventory. Одна из важных задач, которая включает в себя асинхронную многопоточную реализацию похода на устройства, стабильный и регулярный синк этих данных и многое другое из мира Software Engineering. В текущей статье мы ограничимся самой простой реализацией — скриптом. А подробнее об этом поговорим в статье про систему инвентаризации. Помимо оборудования Cisco, у Милоша на сети присутствуют устройства Huawei. Тут его ожидает новая проблема — разные производители имеют разные форматы команд. Как видим во втором шаге, формат данных об LLDP‑соседях у Cisco и Huawei разный. Отличаются форма и содержание вывода команд, сами команды тоже разные. Поэтому одним парсером тут, к сожалению, не обойтись. Поэтому появляются парсеры и для Huawei. Тут мы бы могли углубиться в одну из фундаментальных тем автоматизации сети: задача об универсальном языке общения с оборудованием. Многие сетевые инженеры и разработчики ПО пытались и пытаются решить эту задачу. Один из опенсорс‑проектов в этом направлении — OpenConfig. В одной из следующих статей более подробно остановимся на этом вопросе. Мы собрали необходимую информацию с устройства, теперь её можно экспортировать в файл exportInventory.csv. Для этого используем формат Netbox и преобразуем данные в формат CSV. База готова — пора приступать к генерации конфигурации. Простой вариант. Реализовать генерацию конфигурации в скрипте на gnetcli. Сложный вариант. Использовать фреймворк для генераторов. В данном примере мы исходим из предположения, что description интерфейса может быть перезаписан, поэтому нас не интересует, что до этого было на порту, и поэтому мы не рассчитываем разницу между желаемой и реальной конфигурацией устройства. Об этом мы будем говорить в другой части нашего цикла. Милош готов к заключительному шагу этой задачи. Для того чтобы настроить description на порту интерфейса Cisco, необходимо ввести следующие команды: Давайте напишем функцию, которая будет возвращать необходимую конфигурацию. На этом Милош приступает к завершающему этапу. Если переиспользовать функцию SendCommands и подавать на вход команды из генератора GenInterfaceDescription, Милош может реализовать функцию применения конфигурации на устройствах. Она максимально простая и идеально подходит для этой задачи. Однако для более сложных манипуляций с устройством потребуются дополнительные инструменты — поговорим об этом в одной из следующих частей нашего цикла. Успех! Значения description успешно выкачены на устройства. Задача решена. и реализовал выкатку конфигурации на сеть. Это первый шаг на пути к автоматизации настройки сети! Мы и дальше будем следить за успехами Милоша. В этой статье мы вместе с ним постепенно погрузились в мир автоматизации сети и на достаточно простом примере подняли такие вопросы, как: В следующий раз подробнее остановимся на особенностях генерации конфигурации. Спасибо за внимание. Полезные ссылкиВсе примеры из этой статьи рабочие и доступны в GitHub."
}
