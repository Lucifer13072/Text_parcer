{
    "1": "Всем привет! Меня зовут Андрей, я работаю дата аналитиком в Data Team продукта Dialog.X5/Insights в X5 Tech. Мы предоставляем аналитику по продажам и покупательскому поведению на данных X5 Group.  Для обработки больших объёмов данных в продукте используется  СУБД (система управления базами данных) Greenplum. Greenplum – это распределённая СУБД с массово-параллельной архитектурой (Massive Parallel Processing). Она построена на основе PostgreSQL и хорошо подходит для хранения и обработки больших объёмов данных. В статье рассмотрим ресурсоёмкую операцию для распределённых системCOUNT(DISTINCT)и два способа оптимизации. Для предварительного погружения в планы запросов можно прочитать вот эту хорошую статью. Сегмент. Сегменты в Greenplum представляют собой экземпляры PostgreSQL. Каждый сегмент представляет собой независимую базу данных PostgreSQL, где хранится часть данных. Сегмент обрабатывает локальные данные, отдавая результаты мастеру. Мастер, в свою очередь, – это сервер, где развёрнут главный экземпляр PostgreSQL, к которому подключаются клиенты, отправляя SQL-запросы. Сами данные хранятся на серверах-сегментах. Перераспределение данных. Операция в плане запроса (Redistribute Motion), когда каждый сегмент Greenplum повторно хеширует данные и отправляет строки в разные сегменты в соответствии с хеш-ключом. Распределение таблицы по полю/списку полей. Хранение таблицы на различных сегментах кластера.Конкретный сегмент для хранения записей выбирается на основе хешей, которые рассчитываются по указанным полям. Таблица распределена по полю receipt_id достаточно равномерно и партицирована по полю receipt_dttm. Объём данных исчисляется в терабайтах. исходя из вышеперечисленных утверждений, метрика количество чеков аддитивна по времени и по группам магазинов. Рассмотрим расчёт количества чеков для групп магазинов (это может быть сегментация по регионам, сетям и т. д) и для групп товаров (это может быть сегментация по брендам, производителям и т. д). Так как таблицы-параметры небольшого размера относительно таблицы чековых фактов, для таблиц-параметров выбран тип распределенияREPLICATED. Таблицы с распределением REPLICATED дублируются в полном объёме на всех сегментах кластера и при соединении с нимиJOIN происходит локально. DISTINCT нужен, так как разные plu_id одной и той же группы товаров могут встречаться в одном чеке. GROUPING SETS используется, чтобы не делать несколько обращений к чековой таблице фактов для расчёта разных группировок. Упрощённый план запроса.  Комментарии к узлам плана пронумерованы.Читаем снизу вверх: Перераспределение данных по ключу группировки. DISTINCT ключ группировки + receipt_id. COUNT(receipt_id). Переданные в запрос группы товаров и группы магазинов явно не равномерны. После перераспределения данных (шаг 1) на одном или нескольких сегментах может оказаться слишком много данных, возникнет так называемый перекос. Соответственно, некоторые сегменты будут более нагружены, и выполнение запроса будет упираться в обработку данных на этих сегментах. Чтобы посмотреть, сколько строк пришло на сегмент или сколько строк обработал сегмент, можно включить параметрSET gp_enable_explain_allstat = ON; передEXPLAIN ANALYZE. Путём нехитрого парсинга можно получить список сегментов. В элементах списка последнее значение будет равным количеству строк, обработанных сегментом. Ключ группировки распределился по 58 сегментам, виден явный перекос на одном из сегментов. На 179 сегмент поступило около 269 млн строк, а на 129 сегмент поступило около 31 млн. На 179 сегмент поступило в 9 раз больше строк, а если сравнивать с другими сегментами, то разница будет ещё более ощутимой. Вышеуказанный запрос выполняется около одной минуты на периоде 1 месяц, в зависимости от нагрузки на кластере. Рассмотрим пару вариантов оптимизации такого запроса. Для текущей версии нашего кластера параметр optimizer_force_multistage_agg установлен в off. Значение от версии к версии может изменяться. Для просмотра значения параметра можно воспользоваться командой: В документации написано, что данный параметр указывает оптимизатору GPORCA на выбор многоступенчатого агрегатного плана для операций типа COUNT(DISTINCT). Когда это значение выключено (по умолчанию), оптимизатор GPORCA выбирает между одноэтапным и двухэтапным агрегированным планом в зависимости от стоимости SQL-запроса. Включаем параметр SET optimizer_force_multistage_agg = on;Приказываем оптимизатору выбирать двухэтапный агрегированный план. Перераспределение по ключу группировки + receipt_id.Это уменьшает перекос, так как количество уникальных значений receipt_id велико. DISTINCT по ключу группировки + receipt_id. Это уменьшает количество данных для следующего оператора перераспределения. Перераспределение по ключу группировки. COUNT(receipt_id). В этом плане двухэтапность выражается в шагах 1, 2. Происходит дополнительное перераспределение иDISTINCTпо ключу группировки + receipt_id. Вышеуказанный запрос отрабатывает от 3,5 до 4,5 раз быстрее, в зависимости от нагрузки на кластере. Не стоит включать параметр на всю базу – это может изменить поведение других запросов. Локально на уровне сессии можно ускорить проблемный запрос, а далее вернуть значение параметра в исходное состояние командойRESET. При использовании параметра (хинта) появляется дополнительное перераспределение с более оптимальным ключом распределения, в расчёте начинают участвовать все сегменты кластера. Это повышает нагрузку на сеть в кластере, но кратно уменьшает время исполнения запроса. Метрика количество чеков аддитивна по времени. Можно посчитать количество чеков по дням и потом сделать доагрегацию. Добавление в ключ группировки дня позволит увеличить количество ключей группировки до 30 раз. Если равномерности данных в ключе всё ещё не будет хватать, можно задействовать другие поля таблицы, отвечающие аддитивности метрики количества чеков.Переписанный запрос: Существенного перекоса не возникает, так как расширенный ключ группировки задействует все сегменты кластера, и нагрузка становится более равномерной. Данный запрос выполняется от 7 до 9 раз быстрее по сравнению с изначальным запросом без хинта, в зависимости от нагрузки на кластере. Если предположить, что каждый оператор перераспределения перемещает 100% строк, то в данном запросе происходит меньшее по объёму перераспределение данных в сравнении с запросом с хинтом, и нагрузка на сеть уменьшается до двух раз. Расширяя ключ группировки за счет аддитивности метрики “количество чеков” по времени, мы уменьшаем перекос в данных в ключе группировки и задействуем все сегменты кластера для расчёта. При разработке важно понимать природу данных. Хороший алгоритм в большинстве случаев лучше, чем использование параметров оптимизатора. Если ключ группировки отличается от ключа распределения таблиц, то операция группировки ведёт к перераспределению данных на кластере. В данных ключа группировки может быть перекос, и важно уметь диагностировать такие случаи. Малое количество ключей группировки ведёт к неполному задействованию сегментов кластера. Расширяя ключ группировки, можно увеличить использование сегментов, чтобы снизить время расчёта Мы погрузились немного в мир Greenplum, рассмотрели, как СУБД выполняет запросы. Узнали о перекосе и методах борьбы с данным явлением. Надеюсь, было полезно и интересно. Хочу поблагодарить за консультации Даниила Недумова и за помощь в подготовке статьи – Антона Денисова.",
    "2": "Привет. Делюсь лайфхаком по уменьшению размеров Docker-образов. Как-то нам попалась на поддержку и развитие CRM-система, написанная на Ruby. Пришли со словами: предыдущий разработчик не передал исходный код, но систему нужно развивать. Я уверен, что по условиям контракта передавали исходный код, но заказчики всегда относятся попустительски: им присылают архив на почту, а они потом стирают старое барахло, чтобы ящик почистить. Так вот, зайдя на продакшен-сервер, я нашел развернутую платформу, да ещё и с .git папочкой. Ура, у меня были исходники с историей (она потом мне ни разу не понадобилась). Загрузил в нашу репу исходники, поизучал. В ходе контракта нужно было изменить деплой с rsync на контейнеризацию и перетащить все на Alt Linux (или Astra, уже не помню). Обновили Ruby-пакеты (gems), обновили под них код и написали Dockerfile. Первая сборка была удручающей: образ в 2Гб. Это нормальный размер, если ты собираешь образ со всями там Torch и другой ML-штуковиной, но CRM - нет. В результате дальнейших действий, удалось сократить размер образа до 200Мб. Multistage. Избавление от ненужный gem-ов. Неизменение прав на файлы. Если вы вздумаете выполнить команду RUN chmod +x /app/something, то создастся новый слой с копией этого something, но с другими разрешениями. Это актуально для больших папок и файлов. Установка пакетов ОС и удаление кэша пакетов в этом же слое. С помощью dive (крайне рекомендую этот tool для того, чтобы посмотреть, что находится в каждом Docker-слое) выяснилось, что примерно 1 Гб занимает gem-пакет для генерации PDF. При чуть более пристальном взляде оказалось, что ребята загружают бинарники для 10 различных ОС и, даже, для Windows, а потом используют только один бинарник. Естественно, в том же слое, где ставился этот пакет, мы подчистили с помощью rm -rf этот ненужный хлам. Еще в пакеты любят пихать документацию и тесты, их тоже можно и нужно удалять, если вы настолько же параноидальны по поводу размеров. RUN mount - это способ монтирования данных из других stage в multistage без их копирования, а также выполнения действий в одном слое RUN. Если хотите, посмотрите официальную непонятную документацию https://docs.docker.com/reference/dockerfile/#run---mounttypebind, но лучше посмотрите пример ниже. Обычно, в первом stage мы делаем компиляцию, а потом результат копируем в результирующий stage. COPY --from=builder /app/public ./public Периодически возникает потребность поставить пакеты в финальном слое, но инсталляторы для этого нам не нужны. Давайте приведу пример Dockerfile простого Python-проекта, тот проект на Ruby показывать не могу: Мы не копируем папку /app/wheels в /wheels, а используем как временную. Это позволяет выполнить установку пакетов и сразу же забыть о существовании папки . Конечно, можно копировать уже установленные пакеты из python3.11/site-packages, а не wheels, но это уже попахивает костылём. Затем проверяем с помощью dive - да! папки /wheels не существует ни в одном слое! Только на этой простой команде экономия сразу в 100Мб в моем случае. Надеюсь, данные способы помогут вам в сокращении размеров Docker-образов. PS. Это будет моя 10-я (я удалил несколько, чтобы не привлекать внимание санитаров) статья здесь, поэтому, в честь юбилея, если вам будет интересно почитать про работу CTO в небольшой компании, подписывайтесь на мой новый Telegram-канал https://t.me/cto_podsekin.",
    "3": "Привет, Хабр! Меня зовут Роман Остапчук, я директор по техническому развитию РТК-Сервис. Одной из важных моих задач является взаимодействие с нашими заказчиками – телеком-операторами разного профиля и из разных сегментов рынка. В этой статье я постарался собрать воедино основные принципы хорошего ТЗ от оператора связи партнеру по сервисному обслуживанию телеком-инфраструктуры. Причем не просто хорошее ТЗ, а которое сэкономит заказчику до 20% стоимости контракта и гарантирует качественное исполнение услуг. Операторы связи обслуживают свою телеком-инфраструктуру, в основном привлекая для этого сервисных партнеров. В настоящее время это ИКТ-компании, которые в свое время обучили специалистов, а в последние годы активно разобрали к себе инженеров, ранее работавших у покинувших РФ западных вендоров. К слову, эти же спецы сейчас отлично разбираются уже в новых решениях от российских производителей. А еще в наличии у партнеров – собственные лаборатории для тестирования оборудования, склады запчастей и другие (недокументированные) возможности. И все равно по моему опыту, именно люди – главный ресурс. В 90% случаев от их рук и светлых (и, конечно, мотивированных) голов зависит результат. До 2022 года крупные операторы связи и корпоративные заказчики иногда заключали прямые контракты на техническую поддержку с производителями. Но подавляющее число компаний в России имели договорные отношения с российскими интеграторами, которые уже в свою очередь заключали договора с производителями на критичные услуги и услуги, которые могли предоставить только вендоры. После ухода иностранных производителей оборудования из России вся система пошла на слом. Перед нашими заказчиками встала задача переоценки правил и методов работы с внешними поставщиками услуг технической поддержки. Потребность же в этом не только не сократилась, но стала еще сильнее и сложнее в реализации. Во многих компаниях есть несколько центров компетенций, которые имеют свои очерченные зоны ответственности и показатели эффективности. Вопросы юридической чистоты, финансовой эффективности и технической компетенции как правило имеют различных владельцев, и мы видим, как сложно дается нашим заказчикам выработка итогового компромисса. Зачастую ключевым критерием для победы в тендере является самая низкая стоимость контракта, и у технического блока заказчика не всегда хватает аргументации в спорах с закупками или финансистами, чтобы оценивать будущего сервисного партнера по более глубоким компетенциям. Но если смотреть только на цену, это в 99 случаев из 100 грозит непредвиденными ситуациями в процессе взаимодействия. Поэтому, надеюсь, моя статья будет полезна с точки зрения раздельного анализа всех компонентов услуг технической поддержки в текущих условиях. Подробнее читайте под катом. Для того, чтобы посчитать стоимость услуг по контракту, нужно оценить масштаб требуемых ресурсов и рисков, которые будут сопровождать проект. Чем качественнее и оперативнее происходит общение с заказчиком на этом этапе, тем эффективнее с экономической и технической точки зрения складывается дальнейшее сотрудничество. Мы стараемся прозрачно и предсказуемо для партнеров выстроить весь процесс общения. Чтобы помочь заказчикам быстрее погрузиться в наш подход, мы предлагаем заполнить несложный специальный опросник, который прояснит все нюансы и позволит прозрачно забюджетировать услуги по техподдержке. По этому опроснику мы работаем со всеми телеком-операторами и испытали его в действии десятки раз – на проектах постгарантийного обслуживания. Спойлер – в львиной доле случаев мы сами заполняем все опросники, пишем и выверяем все ТЗ, но опросник формулирует ключевую информацию, и ее нужно уточнить и верифицировать с заказчиком. Бывает, что заказчик уже приходит с детально проработанным ТЗ, но, как ни странно, в текущей ситуации в таком подходе есть много подводных камней. Иногда ТЗ делается на основе предыдущего контрактного опыта, без учета реалий последних двух лет. Иногда — на основе опыта общения с единственным интегратором и по формализованным процедурам. Но анкеты и опросники, которые нужно заполнить, очень разные от интегратора к интегратору по своей глубине и степени проработки, не факт, что в больших лонгридах корректно расставлены приоритеты, существует описание возможных вариантов поведения, когда у заказчика большая инфраструктура с пробелами в инвентаризации и документировании. А ведь этот фактор влияет на результат. Недостаток данных может привести к недоразумениям и непониманию запроса, а избыточные требования — к удорожанию контракта. Итак, содержание ТЗ. Поехали по пунктам. 1. Подробная спецификация оборудования В техническом задании должна быть отражена спецификация оборудования, которое установлено у заказчика, и это первый пункт опросника, который мы заполняем подробно. Если спецификация содержит перечень и описание оборудования в точном и полном виде, то дальнейший расчет ведется с использованием этих данных. Если мы получаем на руки только перечень моделей устройств, то уточненную спецификацию со всей информацией по оборудованию составляем мы совместно с заказчиком. Спецификация оборудования - один из основных источников оптимизации себестоимости и ценообразования потенциального контракта в целом, так как мы можем предложить различные варианты исполнения непосредственно сервисных работ и оптимизированные способы использования подменных фондов оборудования с учетом возможных ограничений и рисков. Очень многие наши заказчики, решая задачи развития и повышения качества сервиса, чуть меньше времени и ресурсов уделяют поддержанию актуальной информации о своих активах. Но важно отметить, что любая совместная работа по уточнению информации дает хороший результат и положительно сказывается на точности проработки затрат на реализацию. Например, обоснованная экспертная оценка служб эксплуатации о типовом составе карт в шасси или примерном распределении таких типовых вариантов на сети. 2. Адресный план установки оборудования Львиная доля проектов в нашем портфеле требует проведения тех или иных работ с привязкой к адресу установки оборудования. Например, это касается отправки отремонтированного или восстановленного оборудования, забора вышедшего из строя оборудования с адреса установки. Москва или Хабаровск – согласитесь, есть разница. Поэтому для точного расчета себестоимости услуги необходимо точно знать, где стоит оборудование. Этот пункт долго расписывать не будем, обычно его заполняют максимально точно, нужно только учитывать, что адресный план может меняться, если в период действия контракта заказчик ставит дополнительное оборудование на сетях. Нас это, конечно, не слишком беспокоит - мы работаем с территориально распределенными заказчиками по всей стране. 3. Статистика обращений в техподдержку На этом пункте предлагаю остановиться подробнее. Как говорил ранее, очень важно для подготовки лучшего конкурентного предложения максимально точно сформулировать и разложить по полочкам все неизвестные вводные и специфицировать риски. Мы пытаемся погрузиться в то, что происходит в заказчике, в его традиции эксплуатации и развития своей сети. Например, в распределение задач между собственным персоналом и подрядчиками, в политики информационной безопасности, специфику расширения сети и т.п. Как показывает практика, двух одинаковых операторов с этой точки зрения не бывает. 1.     Количество обращений за информационной поддержкой в части специфики эксплуатации оборудования, предоставления актуальной документации и т.п. 2.     Количество обращений за технической поддержкой при проведении аварийно-восстановительных работ (АВР): количество кейсов 1-го и 2-го приоритета (с полным, критическим или существенным ущербом для сервисов), количество минорных консультаций и диагностик. 3.     Количество аппаратных неисправностей в год. Идеально выделить статистику выходов из строя от блоков питания и вентиляторов до основных модулей оборудования. Обращения мы детализируем по вендорам, линейкам оборудования и категориям инцидентов. Инциденты, в свою очередь, разбиваем на четыре группы по следующим параметрам: ·  АВР оборудования с существенной деградацией сервисов (25% и более). Да-да, мы помним, что у вас есть SLA с приоритетами, но на данном этапе распределение АВР по типам не играет значительной роли. ·  АВР оборудования без существенной деградации сервисов. ·  АВР, повлекшие замену аппаратных компонентов. ·  Запросы на информационную поддержку и предоставление документации. ·  объема замен там, где ремонт не будет успешным. Прогнозирование параметров для расчёта зависит от конкретных условий по эксплуатации оборудования, технических политик эксплуатации оборудования персоналом, политик ИБ и других факторов, которые специфичны от заказчика к заказчику. Поэтому максимально точно рассчитать параметры для конкретного оператора связи можно и нужно с использованием актуальной статистики, которая уже отражает динамику происходящего на сети заказчика. Плюс статистика всегда хорошо раскрывает экспертную оценку этих же вопросов специалистами заказчика. Для договоров технической поддержки и постгарантийного ремонта оборудования вендоров, с которыми у заказчика сохраняются отношения, важно избежать повторного бюджетирования работ в части 3-ей линии технической поддержки, замен или ремонтов. Поэтому обязательно укажите, если на 2-ю линию сдаётся комплексная сеть, в том числе оборудование с текущими контрактами на замену оборудования, на предоставление доступа к программному обеспечению. Партнёр по технической поддержке заложит использование существующих договоров и исключит свои затраты на данное направление работ. 4.   Техподдержка неаварийных работ на сети Тут мы просим указать, какого плана поддержка неаварийных работ нужна и как часто. Например, заказчик хочет раз в год планово обновлять программное обеспечение, и требуется присутствие эксперта с нашей стороны для консультации или иной помощи. Или необходима помощь в процессе модернизации оборудования, обычно тоже есть какой-то график или его видение на текущий год или календарный год вперед. Бывают запросы на расширенную техническую поддержку, граничащую с эксплуатацией. Например, вводится в строй узел связи, на котором требуется установить определенное оборудование, или нужно подключить большее количество новых пользователей, добавить линейных карт и т.п. Внедрение каких-то дополнительных фич может влиять на работу оборудования другого вендора оборудования, и в этом случае тоже может потребоваться дополнительная консультационная поддержка или даже стейджинг. Или, наоборот, требуется конкретный эксперт, который будет погружен в специфику дизайна и развития конкретной сети конкретного заказчика. Он должен оказывать техническую поддержку в случае сложных, системных кейсов, которые требуют изменения дизайна сети. Проводить выделенные воркшопы с эксплуатацией заказчика по данным проблемам. Руководить или лично проводить аудит сети, в том числе с оценкой производительности и мер отказоустойчивости. Регулярно выдавать рекомендации по улучшению эксплуатационных характеристик отдельных узлов и сетей связи целиком. 5. Обновление ПО на сетях заказчика Специфика обновлений софта от производителей, прекративших поддержку оборудования на территории РФ, изменилась, и для уточнения стоимости постгарантийной поддержки требуется расширенная информация об используемом ПО. Все помнят, как было раньше: после покупки оборудования производитель предоставлял доступ к порталу, где были выложены маркетинговые материалы, а также техническая документация и доступ к обновлениям ПО. Политика выдачи доступов разнилась от вендора к вендору – некоторые давали информацию только по приобретенному оборудованию. Другие давали доступ к базе данных по всей линейке оборудования без каких-либо ограничений, в особенности, если покупатель имел статус золотого партнера. В реальности цепочка была куда сложнее: оборудование покупал не только конечный пользователь, но и дистрибьютор, реселлер, а иногда пользователи перепродавали свое железо. Так что контроль за доступами и раньше был не так прост, а сейчас все сильно усложнилось. По официальным каналам документацию не достать. Если такой канал удается получить, то он стоит конкретных денег, я бы даже сказал, что теперь это очень заметная статья расходного бюджета. Для достижения лучшей цены проекта, заказчику нужно в первую очередь сформулировать, сколько раз в год ему нужно обновлять ПО и с какой целью. Возможно, есть сегменты, где это вообще не требуется. Такая информация позволит поставщику техподдержки серьезно оптимизировать бюджет в части затрат на поддержку софта. В качестве источников вдохновения для этой работы предлагаем использовать следующие факторы: 1. Специфические требования к ИБ при обновлении ПО. Даже если ПО приобретено по легальным каналам, никто не может гарантировать отсутствие рисков в части ИБ в связи с местом производства софта (Индия, страны НАТО и пр.), ведь оно может быть подконтрольно недружественным странам. 2. Политика обновления ПО на сети: требования к регулярному обновлению ПО и сопровождению данных работ. Раньше у всех операторов связи были определенные политики, которые регламентировали объем обновлений, например, в привязке к официальным релизам. Сегодня слепое следование таким политикам – это дополнительные расходы. Например, мы часто видим на сетях заказчиков End-of-Support оборудование, с ПО версии на 1-3 ниже итогового GA релиза модели. Рекомендуем принять итоговое решение, обновится разово до финального или остаться на существующей версии, но не вносить в ТЗ общие туманные требования «поставки ПО по запросу», под которые исполнитель всегда закладывает худший для себя вариант развития событий. Что касается сопровождения работ с ПО – также надо подумать, нужна ли только консультация или же непосредственный доступ к ПО и его установка, нужна ли работа с инцидентами по результатам установки нового ПО. Из этого пункта вытекает следующий. 3. SLA для работ c ПО оборудования. Доступ к новому программному обеспечению теперь не просто ограничен, не только стал дороже, но еще и дольше. Даже при наличии бюджета сейчас это история, растянутая во времени – на получение нужного обновления может потребоваться до 1 месяца. Это связано с необходимостью документального оформления, с представлением обоснований для доступа к ПО. Дополнительные данные может запросить вендор, у которого есть свой compliance. Продолжение следует….",
    "4": "Исследование — это не просто первая стадия дизайна, а фундамент для создания успешного продукта. Этот этап позволяет понять аудиторию, ее болевые точки, предпочтения и потребности. Результатом исследования должен стать путь клиента Эта диаграмма в дальнейшем должна стать основой для определения необходимых функций и дизайна в целом. Анализ конкурентовИзучение чужих продуктов помогает узнать лучшие практики и понять, как выделить свой продукт на фоне остальных. Но главная цель этого этапа: сформулировать список гипотез, которые вы сможете точечно проверить во время интервью с потенциальным клиентом. Исследование ЦА. Различают 2 вида первичных исследований: Количественное: опросС помощью опроса можно оценить верность гипотез, однако трудно выяснить более широкий набор болей, именно это мешает построить путь клиента только из количественного исследования. Качественное: проблемное интервьюИнтервью, в свою очередь, позволяет не только проверить список гипотез, но и добавить туда новые, чтобы простроить детальный клиентский путь. Лайфхак: попросить открыть собеседника сайт конкурента это абсолютно нормально и наоборот повышает эффективность интервью, так вы можете отслеживать путь клиента в реальном времени. Канонически, нужно упомянуть: записывайте ответы респондентов. После проведения исследования важно расставить приоритеты в функциональной части продукта. Выделение ключевых функций преследует 2 цели: помогает избежать перегрузку интерфейса и не застрять в разработке на долгие месяцы. Если пропустить этот шаг, то вполне возможно у вас получится продукт как айфон ниже. RICE и ICE позволяет определить, какие функции имеют высокий приоритет, оценивая их охват (только для RICE), ценность и сложность реализации, также все эти показатели взвешиваются на уровень уверенности. Для обоих методов каждый из показателей оценивается от 1 до 10, при наличии данных  по охватам(Reach) его оценивают в абсолютных значениях. Соответственно, чем выше итоговая оценка, тем приоритетнее функция. Индивидуальные методы приоритизацииХотя существует огромное количество методов приоритизации, наиболее эффективным будет собственная модель. Особенность собственного метода в основном заключается во взвешивании отдельных показателей, в набор показателей добавлю рентабельность. Например: для малого бизнеса Effort может иметь гораздо больший вес чем охваты, а для крупного бизнеса более важным показателем может стать рентабельность от внедрения функции/продукта. Суммируя вышесказанное на этом шаге важно не ограничиваться шаблонами. UX/UI дизайн стал неотъемлемой частью цифрового опыта, с которым мы сталкиваемся каждый день. От приложений до веб-сайтов и интерфейсов устройств. Качество сайта/приложения стало отдельным фактором для принятия решения пользователем, после опыта использования Ozon покупатель не готов искать одежду на сайте, на котором нет фильтров и подобный принцип работает почти в каждой отрасли. Главный принцип массового дизайна в 21 веке: минимализм Улучшение юзабилити. Минималистичный дизайн упрощает процесс навигации и снижает когнитивную нагрузку на пользователя. Быстрая загрузка. Чем меньше элементов на странице, тем быстрее загружается интерфейс, что положительно влияет на пользовательский опыт. Фокус на главном. Минимализм позволяет пользователям сосредоточиться на основных функциях и контенте, избегая отвлекающих деталей. Лидеров  в вашей отраслиКрупные компании уделяют много внимания интерфейсу продукта, детально опробуйте их сайт/приложение самостоятельно, выделите слабые и сильные места, а также постарайтесь использовать отличные от них цвета для индивидуальности. Лидеры разных сферНасмотренность - важный фактор в любой сфере, дизайн не исключение. Чтобы исследовать  интерфейсы мировых лидеров можно использовать сайты-агрегаторы, такие как mobbin, dribbble и behance В шрифтах самое важное обращать внимание на схожие по написанию буквы, чтобы они не сливались и адекватную читаемость маленького текста. Не будем долго говорить об этом, примеры шрифтов можете увидеть ниже Из списка выше можно использовать любые шрифты кроме Times New Roman, лично мои фавориты: Ubuntu для неофициального стиля и большинства B2C продуктов и Verdana для более официальной сайтов/приложений. Для начала определимся сколько оттенков необходимо выбрать. Нужно выбрать: цвет фона, текста, основной и вспомогательный цвета. На основной цвет лучше добавить пару оттенков, например с помощью снижения прозрачности. Разберемся по порядку: если вы неопытны в дизайне не усложняйте интерфейс и делайте белый фон и черный текст, для неосновного текста можно добавить еще один оттенок, зачастую это серый. Желательно на этапе первичного формирования дизайна уже планировать светлую и темную тему, учитывайте это при выборе цветов. Правильное расположение элементов интерфейса — один из ключевых факторов, определяющих успешность UX/UI дизайна. Интуитивная компоновка улучшает навигацию и повышает удовлетворенность пользователя. Закон Фиттса: элементы, которые чаще всего используются, должны быть расположены ближе к пользователю и легко доступными. Принцип близости: элементы, связанные друг с другом, должны быть сгруппированы для создания логической связи. Недостаточно пространства: пока вы заняты дизайном конкретного элемента пространство между объектами может казаться вам слишком большим, но для пользователя, который смотрит на весь интерфейс целиком это не так. Рекомендация: начните с расположения блоков впритык после чего попробуйте spacing 28 пикселей, оцените, уменьшите пространство между схожими по смыслу блоками. Для первичного подбора, используйте матрицу ниже Для того чтобы лучше разобраться в расположении элементов рекомендую видео (на английском). На самом деле есть еще одна категория исследований, которое проводиться уже с готовым дизайном. Юзабилити-тестирование помогает выявить проблемные точки в прототипах и макетах интерфейсов. Тестирование Проведение интервью или тестов с конечными пользователями для получения обратной связи по удобству использования и выявлению проблем.  Например: A|B тестирования и проблемные интервью вы можете провести их прямо на прототипе в Figma Анализ метрик Сбор и анализ данных о поведении пользователей для оценки эффективности дизайна и нахождения точек роста. Этот метод подойдет для уже готовых сайтов, вы можете использовать как базовые метрики: посещения, переходы по ссылкам, целевые действия, так и продвинутые: heatmap интерфейса, карта кликов с разделением по категориям пользователей. Завершение дизайна исследованием помогает учиться на ошибках и улучшать продукт в будущем, обеспечивая его эволюцию и адаптацию к меняющимся потребностям пользователей. UX/UI дизайн — это непрерывный процесс, который начинается с исследования и заканчивается анализом результатов. Понимание пользователей, расстановка приоритетов, следование общей концепции и внимание к деталям — все это ключевые аспекты успешного проектирования интерфейсов. В современном мире, где каждая деталь важна, создание удобного и эстетичного интерфейса может стать основой успеха вашего продукта.",
    "5": "Всем привет! Меня зовут Константин Малолетов, я архитектор облачных сервисов в компании Arenadata. Сегодня хочу рассказать, как мы решаем задачу эффективного размещения ресурсоёмких систем, таких как Arenadata DB, в облаке. массивно-параллельная аналитическая СУБД, основанная на открытом проекте Greenplum. В настоящее время, в связи с архивацией проекта Greenplum в мае 2024 года, ADB переходит на новый open source проект Greengage DB в качестве основы. Как решение для корпоративных хранилищ данных, построенное на архитектуре MPP, ADB требует высокой производительности вычислительных ресурсов, сети, дисковой подсистемы и стабильности работы каждого узла кластера. В статье рассмотрим несколько сценариев использования вычислительных ресурсов и их влияние на работу ADB, а также поделимся результатами проведённых тестов. Целью исследования было определить, как плотность размещения виртуальных машин (и, соответственно, уровень конкуренции за вычислительные ресурсы) на физическом сервере (в данном случае имеется в виду физический сервер с установленным гипервизором, позволяющим запускать виртуальные машины) влияет на производительность и стабильность кластера Arenadata DB. Чтобы плавно перейти от плотности размещения виртуальных машин (далее VM — Virtual machine) к исследуемым сценариям, придётся сделать небольшое теоретическое отступление. Здесь нужно вспомнить, как работает планировщик вычислительных ресурсов Linux с учётом технологии SMT, в частности Intel Hyper-threading. Подробно на технологии останавливаться не буду, напомню лишь, что каждое физическое ядро процессора представляется в операционной системе двумя логическими процессорами, на каждый из которых планировщик может назначить свой поток выполнения. Пока количество потоков выполнения, требующих процессорного времени, меньше или равно доступному количеству физических ядер, планировщик будет распределять эти потоки на логические процессоры, принадлежащие разным физическим ядрам (планировщик знает, какому физическому ядру принадлежит тот или иной логический процессор). То есть фактически каждому потоку выполнения выделяется в пользование физическое ядро целиком. Когда количество потоков, требующих процессорного времени, превышает доступное количество физических ядер, планировщик начинает задействовать те логические процессоры, соседи которых по физическому ядру уже заняты другим потоком. Таким образом, он начинает распределять по два потока выполнения на одно физическое ядро. Подобное поведение планировщика справедливо в том числе для распределения потоков выполнения VM, если мы говорим о планировщике физического сервера. В операционной системе, установленной на VM, также есть свой планировщик, который распределяет потоки выполнения внутри виртуальных машин на выделенные им виртуальные процессоры (vCPU). Стоит отметить, что всё описанное выше — это стандартное поведение планировщика. При размещении Arenadata DB в облаке мы рекомендуем использовать технологию CPU pinning / processor affinity. Она модифицирует стандартное поведение планировщика и позволяет VM не зависеть от количества и нагруженности соседних по физическому серверу виртуальных машин, а также получать стабильно одинаковую производительность каждого узла кластера, что важно для MPP-систем. К сожалению, если говорить о публичных российских облачных провайдерах, эта технология в большинстве случаев не реализована. Сценарий № 1: «Идеальный». В идеальном сценарии суммарное количество vCPU всех VM, размещённых на физическом сервере, не должно превышать количество физических ядер на этом сервере. Этим сценарием мы хотели эмулировать ситуацию, когда каждому потоку выполнения VM выделяется в пользование физическое ядро целиком. Server — физический сервер. На практике идеальный сценарий можно получить, например, взяв в аренду выделенные серверы либо кластеры серверов и техническими или административными средствами соблюсти условие: сумма всех vCPU меньше суммы всех физических ядер. Таким образом, можно ожидать производительность, близкую к производительности baremetal инсталляции. Именно поэтому сценарий условно назван идеальным. Сценарий № 2: «Базовый». В базовом сценарии суммарное количество vCPU всех VM, размещённых на физическом сервере, должно было быть больше, чем количество физических ядер на этом сервере, но меньше, чем количество логических процессоров (или удвоенное количество физических ядер). В этом сценарии мы эмулировали ситуацию, когда часть потоков выполнения VM делит одно и то же физическое ядро. У подавляющего большинства облачных провайдеров функционал hyper-threading включён и один vCPU равен одному логическому процессору. Планирование двух потоков выполнения на одно физическое ядро с hyper-threading мы не считали переподпиской. То есть на практике это облако без переподписки. Этот сценарий мы условно назвали базовым, потому что он удовлетворяет нашим рекомендациям об отсутствии переподписки и при этом легко реализуется в любом облаке. Сценарий № 3: «Переподписка». В сценарии «Переподписка» суммарное количество vCPU всех VM, размещённых на физическом сервере, должно превышать количество логических процессоров (удвоенное количество физических ядер). Так мы эмулировали переподписку, когда количество потоков выполнения VM, ожидающих процессорного времени, больше, чем количество логических процессоров в физическом сервере. На практике такая ситуация может сложиться, когда вы покупаете вычислительные ресурсы с переподпиской. Здесь стоит сделать оговорку, что схемы специально упрощены для наглядности размещения VM. Конечно же, мы не использовали в тестах однопроцессорный трёхъядерный сервер и VM с двумя vCPU. Описание реального тестового стенда приведу далее. В качестве площадки проведения исследования использовалось публичное облако одного из российских облачных провайдеров, построенное на базе связки Linux + Qemu + KVM. В каждом физическом сервере было по 48 физических ядер (pCore) с поддержкой технологии Hyper-Threading, что давало 96 логических процессоров (T). Если говорить о плотности размещения VM, то суммарные 96 vCPU — та граница, после которой начинается переподписка. На самом деле реальная граница даже ниже, если учитывать ресурсы процессора, которые использует для своих нужд сам гипервизор. 6.25.1_arenadata49_b2-1 enterprise Это типовая конфигурация для базового тестирования инфраструктуры облачных провайдеров. Да (1:1.33) Значение переподписки для сценария № 3 составляет 1:1,33 (128 vCPU / 96 T). Здесь вполне уместным может оказаться вопрос — почему было взято соотношение 1:1,33? Почему не 1:2, не 1:3? Ответ прост: такое значение получилось после размещения всего кластера Arenadata DB на одном физическом сервере. В качестве основного инструмента тестирования производительности и стабильности работы Arenadata DB мы используем свой форк отраслевого стандарта TPC-DS. Параметром сравнения производительности считаем время исполнения запросов в мультипользовательском режиме (параметр CONCURRENCY задаёт количество одновременно работающих пользователей). Исходный тест состоял из 99 запросов, выполняемых последовательно в рамках одной пользовательской сессии. В нашем форке мы выделили 51 самый быстрый запрос и обозвали этот тест lite. Этот тест позволяет в приемлемые сроки оценить производительность кластера под высокой параллельной нагрузкой (до 100 пользовательских сессий включительно). Можно сказать, что это стресс-тест для вычислительных мощностей физического сервера. Исходный же набор в 99 запросов мы обозвали full и используем его с целью нагрузить по полной подсистему ввода/вывода физического сервера. Для такого теста высокая параллельная нагрузка не нужна (тестируем до 8 пользовательских сессий). Для каждого сценария проводилось три замера серии тестов lite и три замера серии тестов full. Серия тестов — один замер с последовательным ступенчатым увеличением параметра CONCURRENCY. В таблице ниже приведены средние значения времени выполнения всех запросов при заданной CONCURRENCY. Трактовать значения времени просто — чем быстрее завершился тест, тем лучше. Для оценки нехватки вычислительных ресурсов также использовали дополнительный показатель: количество предупреждений о неудачной доставке датаграмм в журналах Arenadata DB. Дело в том, что ADB в качестве сетевого транспорта по умолчанию использует протокол UDP, а контроль доставки реализует сама. При отсутствии подтверждения доставки датаграммы от принимающего сегмента отправляющий сегмент повторяет её отправку, а когда таких неудачных повторов накапливается 100 штук подряд, фиксирует одно предупреждение в журнале событий. В виртуальной среде мы довольно часто видим подобные предупреждения на больших CONCURRENCY. Обычно они связаны с высокой конкуренцией за вычислительные ресурсы и их нехваткой для того, чтобы успеть обработать сетевой трафик за отведённые таймауты. Чем позднее (на больших значениях CONCURRENCY) появляются эти предупреждения и чем их меньше, тем лучше. В идеале их вообще быть не должно. За эталон был взят идеальный сценарий № 1. Относительно него высчитывали изменение времени в сценариях № 2 и 3. Положительные значения % означают увеличение времени исполнения пользовательских запросов. x — при CONCURRENCY = 24 завершился только один замер из трёх, начиная с CONCURRENCY = 28 успешно завершённые замеры отсутствуют. Ухудшение производительности в сценарии № 2 объясняется описанной выше логикой работы планировщика задач Linux. В этом сценарии всем vCPU (64) не хватало физических ядер (48), и поэтому активно использовалась многопоточность (hyper-threading). Стоит резонно заметить, что при размещении VM с 96 vCPU вместо 64 (всё ещё без переподписки) производительность упадёт ещё больше. В сценарии № 3 ситуация значительно ухудшается. Из 96 доступных логических процессоров одновременно запрашивается 128 (без учёта потребностей самого гипервизора). Потоки выполнения вынуждены простаивать в очереди за процессорными ресурсами, что выражается увеличением времени исполнения запросов в два-три раза! Более того, в сценарии № 3 в процессе запусков тестов lite виртуальные машины кластера становились недоступны, и требовалась ручная перезагрузка с последующей ребалансировкой сегментов для восстановления работоспособности кластера. При втором замере потребовалась полная копия данных для восстановления сегментов. При третьем все VM кластера перешли в состояние недоступности. Дополнительно стоит отметить, что с ростом плотности размещения VM на физическом сервере раньше начинают появляться предупреждения сети. Так, в сценарии № 1 они отсутствовали вовсе, в сценарии № 2 появились начиная с CONCURRENCY = 24, в сценарии № 3 — с CONCURRENCY = 20. Ниже приведены графики утилизации процессора CPU usage % и CPU steal, также в процентах. Если с CPU usage, я думаю, всё понятно, то про steal time вкратце позволю себе напомнить. Это время, в течение которого виртуальная машина не получает ресурсы процессора для выполнения, то есть она готова исполнять код и ожидает процессорные ресурсы, которые в данный момент заняты. Эта метрика отдаётся гипервизором внутрь VM, и изнутри VM мы её и снимали, как и CPU usage. На графиках видно, что CPU steal в сценариях № 1 и 2 показывают околонулевые значения, в то время как в сценарии № 3 можно наблюдать пики до 40%, что подтверждает дефицит процессорных ресурсов. Да (1:1.33) Сценарий № 1 обеспечивает наилучшую производительность и стабильную работу кластера Arenadata DB, а также одинаковую производительность каждого узла кластера. По этим показателям он наиболее близок к размещению на baremetal. Этот сценарий требует организации частного решения в облаке. Размещение VM по сценарию № 2 — это то, что можно ожидать в облаке без переподписки. Производительность кластера ниже, чем в сценарии № 1, но, во всяком случае, он работает стабильно, без падений. Производительность узлов кластера непостоянна, она зависит от наличия и «шумности» соседей по физическому серверу. Постоянство производительности узлов можно обеспечить дополнительными техническими средствами, например применением технологии CPU pinning. Сценарий № 3 с применением переподписки по вычислительным ресурсам существенно замедляет работу кластера Arenadata DB, а по достижении определённого порога нагрузки возможно его падение. Производительность узлов кластера также непостоянна. Исследование показало, что кластер Arenadata DB может быть запущен в облачной среде с переподпиской по вычислительным ресурсам и что он способен функционировать в такой среде. Однако подобное размещение приводит к значительному ухудшению производительности и стабильности работы кластера. Возможные падения системы и, как следствие, простои в обслуживании пользователей делают использование подобного подхода крайне рискованным. Такое поведение системы не позволяет предоставлять качественные услуги технической поддержки. В связи с этим настоятельно не рекомендуется применять переподписку по вычислительным ресурсам в продуктивных средах.",
    "6": "Генеральный директор Decagon Джесси Чжан утверждает, что их ИИ-сервис сравнивали с решениями от Salesforce, и Decagon выходил победителем. Дисклеймер : это вольный перевод заметки издания Forbes. Перевод подготовила редакция «Технократии». Чтобы не пропустить анонс новых материалов подпишитесь на «Голос Технократии» — мы регулярно рассказываем о новостях про AI, LLM и RAG, а также делимся полезными мастридами и актуальными событиями. Обсудить пилот или задать вопрос об LLM можно здесь. Джесси Чжан не стремился создать продукт, который, как он думал, людям понравится. Повторно создавая стартап, он хотел разработать программное обеспечение, которое компании могли бы быстро внедрить и извлечь из него реальную выгоду. В августе 2023 года Чжан объединился с Ашвином Сренивасом, с которым познакомился на встрече основателей в Юте, и опросил десятки стартапов, чтобы создать продукт, который действительно востребован и за который компании будут готовы платить — ИИ-система, способная решать рутинные задачи по обслуживанию клиентов. Вскоре после этого появилась компания Decagon. Чуть больше года спустя такие компании, как Notion, Duolingo, Rippling, Bilt и Substack, уже используют систему Decagon для ответов на вопросы о работе их приложений, обработки возвратов, отмены подписок, разрешения споров и замены кредитных карт. Однако получить этих клиентов было непросто, рассказывает Чжан Forbes. Почти все клиенты Decagon проводили так называемые «проверочные тесты», сравнивая её программное обеспечение с решениями конкурентов, таких как Salesforce, чтобы определить, какой чат-бот лучше решает задачи. В каждом из этих испытаний ИИ-чатбот Decagon неизменно выходил победителем, отметил Чжан. Подобные оценки становятся все более распространенными, поскольку компании тестируют различные ИИ-модели. Например, Intercom недавно перешёл с GPT от OpenAI на Claude от Anthropic после серии собственных тестов, сообщила Fortune. Сегодня Decagon объявила о привлечении $65 миллионов в рамках раунда Series B, возглавленного Bain Capital Ventures при участии инвестора Элада Гила и ведущих венчурных фондов Accel и Bond Capital. С $100 миллионами общего финансирования стартап теперь оценивается в $650 миллионов. Компания планирует использовать средства для расширения команды, привлечения новых клиентов и добавления голосовой функции в продукт. Свежие инвестиции поступают в момент, когда ИИ-агенты — программные средства, способные не только отвечать на запросы, но и выполнять конкретные задачи, — приобретают всё большую популярность. Руководитель OpenAI Сэм Альтман делает ставку на то, что уже в следующем году ИИ-агенты станут частью повседневной жизни. Новейшие технические достижения в области ИИ, такие как способность к многозадачности, приводят к созданию стартапов, разрабатывающих ИИ-агентов, которые могут писать код или выполнять сложные юридические исследования. «Если объединить высокую способность к логическому мышлению и использование других программных инструментов, например, веб-браузеров, то мы на пороге создания агентов, способных выполнять большую часть рутинной работы, от которой люди обычно хотят избавиться», — отметил Ареф Хилали, партнёр Bain Capital Ventures, в интервью Forbes. Компании, использующие чатботы Decagon, уже корректируют свои планы по найму. Некоторые, как например Bilt, поставщик карт и программ лояльности, сократили свои команды поддержки с сотен контрактеров до 65 человек, что позволило сэкономить миллионы долларов. Другой клиент, которого Чжан не назвал, сократил свою команду поддержки на 80%. ИИ-агенты Decagon базируются на модели больших языковых моделей от таких компаний, как OpenAI, Anthropic и Cohere, и обучены на данных предприятий: инструкциях, блогах и прошлых диалогах с клиентами. Основной вызов в обучении заключается в том, что информация часто устаревает, поэтому Decagon разработала систему, позволяющую поддерживать данные в актуальном состоянии. В результате чатбот действует почти как хороший человеческий агент, отметил Чжан. 27-летний Чжан утверждает, что успех их программного обеспечения заключается не только в предоставлении достоверной информации, но и в возможности для компаний контролировать и настраивать ответы ИИ на запросы клиентов. «Я считаю, что они грамотно создали прозрачную систему управления, чтобы их ИИ-агенты не были чёрными ящиками», — отметил инвестор Элад Гил в интервью Forbes. «Клиент может видеть, что происходит, понимать это и изменять, так что система выполняет именно те функции, которые нужны». Автоматизация обслуживания клиентов — не новая идея. Это одно из первых приложений для интеграции ChatGPT. Это также одна из наиболее проблемных сфер для потребителей, которые часто хотят перейти на общение с человеком после нескольких минут взаимодействия с ИИ. Некоторые компании, такие как Discover, предлагают клиентам возможность связаться с человеком в любое время. Однако компании, как Decagon, верят, что ИИ-агенты, в отличие от старых чатботов с запрограммированными ответами, способны решать проблемы без вмешательства человека. «Раздражение, которое испытывали люди, объясняется тем, что системы не работали должным образом. Теперь они работают», — отметил Хилали из Bain Capital Ventures.",
    "7": "Офисные работники засыпают, просыпается выгорание. Выгорание делает свой выбор среди неспящих, засидевшихся допоздна сотрудников. Выгорание сделало свой выбор. Все просыпаются офисными работниками. Все, кроме Олега. Олег просыпается уличным художником в Амстердаме. Как говорится, в любой шутке есть доля шутки. Давайте поговорим про выгорание в команде. Про то, как стать максимально продуктивным, перестать выгорать и в целом избавиться от ощущения «опять эта работа». Меня зовут Евгений Идзиковский. Я начинал аналитиком, продолжил IT-карьеру в Radmin’е, а затем кардинально сменил профессию. Сейчас я — психолог и «депрограммирую» людей от того, что мешает жить. А когда отдыхаю от депрограммирования, пишу нейросети на R и изучаю Python. Эта статья написана по мотивам топ1 доклада на TeamLeadConf 2022. Контент близок, но не дублируется, поэтому если тема близка, можно посмотреть выступление. На видео есть ответы на вопросы из зала, так что там может быть ответ на вопрос, который появится у вас. Большую часть жизни я ощущал себя в ловушке чужих решений. Школа, университет, первая работа — всё это было не моим выбором. Меня вели по этому пути, словно по уже проложенной тропе, с которой невозможно свернуть. И всё заканчивалось одинаково — разочарованием и чувством, что моя жизнь происходит как-то не так. Не потому, что сама тропа была ложной, и в институт ходить не стоило — всё намного хуже. Это было скучно. Некоторые предметы вызывают откровенное недоумение. Слушать лекции невозможно, в реальной жизни на х2 не поставишь, назад не отмотаешь. Да и текст на 95% дублирует учебник. Домашнее задание… ну, вы и сами всё знаете. Потеря мотивации, рутина, серые будни и выгорание в конце каждого цикла. Общаться весело, ходить в одно и то же место и заниматься какой-то ерундой — нет. А когда началась работа, выгорание приобрело новые краски. Это было не просто апатичное состояние, которым мы привыкли прикрываться, а настоящий барьер. Перед ним опускались руки. Не хотелось просыпаться по утрам. Самое страшное — было понимание, что жизнь становится хуже, но не было сил что-либо сделать. Это момент, когда любые способы мотивации перестают работать. И кажется, что так будет всегда и у всех. Присмотрелся к людям вокруг, послушал разговоры. Все кругом недовольны: начальники не ценят, работать тяжело… Каждый второй будто избивает себя кнутом, чтобы принудить к труду. И тогда я задал себе вопросы: почему мы делаем то, что не хотим? И не хотим делать то, что делаем? Почему мы принимаем страдание на работе как норму? Это было давным-давно. Задолго до того, как начали использовать слово выгорание… но сейчас всё это на слуху. Про выгорание слышали все, но не все, наверное, понимают, как с этим бороться. Особенно у себя в команде. У нас есть система вознаграждения, в которой большую роль играет дофамин. Здесь, для интересующихся, ссылки на более точное описание работы системы вознаграждения. Я позволю себе сильно упростить модель, и в слово «дофамин» вложить всю внутреннюю систему оценки, которая, конечно, им не ограничивается, но на существо дела это не влияет: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010340 - дофамин, движение и мыши https://www.brown.edu/carney/news/2021/05/13/study-finds-systematic-wave-pattern-dopamine%E2%80%99s-signaling-reward-based-learning - дофамин, вознаграждение и ошибка предсказания (и тоже мыши) https://www.mdpi.com/2227-9059/11/9/2469 - дофамин, система вознаграждения, ангедония (близко к выгоранию, очень близко). Люди. https://www.nature.com/articles/s12276-020-00532-4.pdf Стресс, хронический стресс, система вознаграждения. Акцент на биохимии. https://newsroom.wakehealth.edu/news-releases/2023/12/research-shows-human-behavior-guided-by-fast-changes-in-dopamine-levels Компьютерные игры (!) поведение, дофамин, ошибки предсказания. Люди. Вы ведь не ждали, что тут будут мыши, да? https://zuckermaninstitute.columbia.edu/new-study-sheds-light-how-brain-learns-seek-reward Назначение мозгом наград за деятельность. Мыши, исследован процесс обучения и переобучения. Перед любым действием: от банального «поесть» до запуска сложного проекта, мозг оценивает, что он получит на выходе, и принимает решение, стоит ли игра свеч. Причем вердикт, который выносит мозг, слабо связан с логикой, прибылью или значимым результатом в объективной реальности. Он зависит от приза, назначенного системой вознаграждения. Задним числом, конечно, мы себе объясним, почему «я это хочу, а это не хочу». Но наши объяснения — всего лишь следствие выбора, а не его причина (речь идёт про быстрые выборы, а не про результаты размышлений, когда наши мысли уже могут влиять на наши переживания). Приведу пример: вы в новом проекте, и вами движут драйв, любопытство и интерес: «Что же будет дальше?». Вы практически не чувствуете усталости. Рабочий день все длиннее, понедельник начинается в субботу, а побед все больше. Но все временно, и через несколько месяцев рост замедляется. Появляется «вязкая рутина», достижения приедаются, мельчают, и у вас возникает ощущение, что так было всегда. Работа уже не приносит такого удовольствия. Все чаще хочется подольше поспать, побольше поесть и в целом сделать что-то, что отвлечёт от рутины и повседневных задач. Здесь важно отличить прокрастинацию от выгорания, у которого тоже есть своя классификация. Чаще всего к IT-специалистам в дверь стучится когнитивное, эмоциональное и социальное выгорание. Это когда не получается/не хочется концентрироваться, общаться с людьми и проявлять эмоции. В выгорании много опустошенности, в прокрастинации — страха. И вдруг звонит товарищ и говорит: «Я знаю, какой ты крутой спец. Мне нужна твоя помощь! Вот тебе в 3 раза больше денег, чем ты зарабатываешь сейчас, и условия, в которых тебе будет максимально комфортно (тут можете себе придумать идеальный оффер и удвоить его). Сможешь помочь?» Сразу появляется энтузиазм, загораются глаза, и мозг начинает генерировать тысячу новых идей. Вы возвращаетесь в начало цикла. Это поможет. Но не навсегда. Потому что мозг оценивает не текущее положение вещей, а изменения. Можете вспомнить все разы, когда вам повышали зарплату. Через некоторое время, часто непродолжительное, вы начинали воспринимать новую зарплату как норму. А заметно повышать зарплату каждые 4-8 месяцев мало реалистично :( Не так просто решить проблему со стороны внешних факторов. Посмотрим изнутри. Происходит ли что-то интересное в психике, если обстановка участвует слабо? Оказывается, происходит. Причем на самом деле эмоции и установки могут влиять намного сильнее! Возьмём за пример классического перфекциониста. Когда-то я задавался вопросом — если перфекционисты это люди, которые хотят всё делать хорошо, почему они прокрастинируют? И как-то меня озарило! Центральная мысль: «Я должен делать все на отлично!». Из-за неё перед каждой задачей психика перфекциониста достраивает два варианта развития событий: Получилось: «Вроде так и должно быть, это мой стандарт». Не получилось: «Все ужасно! Я — плохой специалист. Со мной никто не захочет работать, а я буду вспоминать этот провал всю жизнь». На чаше весов оказываются две сущности: «Так и должно быть» против «Я никчемен» / «Обо мне будут думать плохо» и т.д. То есть в жизни либо ничего не изменится, либо станет сильно хуже. Я называю этот парадокс безвыигрышной лотереей. Выиграет страх, и задача будет висеть, пока не появятся новые стимулы. Неожиданный вывод: нужно хвалить команду и себя. Желательно чаще, чем ругать, иначе ситуация ухудшится. Подчеркну: хвалить не только потому, что сделали хорошо, но и для того, чтобы дальше психика помогала нам работать, а не мешала. Чаще всего энтузиазм гибнет после сильного и систематического стресса/усталости. Есть эффективное, но не долгосрочное и не особо выгодное решение — отпуск. Каждый месяц отправлять себя или сотрудников на отдых не получится, а причины не стресса обычно не исчезают. Ремарка: часто после отдыха усталости намного меньше, а вот система вознаграждения, наоборот, считает, что сидеть и работать намного скучнее, чем развлекаться. Поэтому работать хочется ещё меньше, чем до отпуска. Синдром самозванца, перфекционизм, неуверенность в себе, тревога — все это забирает силы у работника. Да, проработанным людям легче жить и расти, но не все люди готовы идти в терапию. Иногда надо решать кризис прямо сейчас. Нет времени решать проблемы, нет денег, негативный опыт работы с психологом, причин может быть много. Мы примерно разобрались, как работает система вознаграждения. Всё время бороться с ней или подкупать, очевидно, не вариант. Можем ли мы как-то договориться с ней на поведенческом уровне? Ответ: Да! Мозг «начисляет» больше дофамина за сложность задач и то, как мы их выполняем. Если я могу получить награду прямо сейчас — отлично. Метафорически это «быстрый дофамин». Про него сейчас пишут на каждом заборе. Полистать ленту, почитать новостные сайты, выпить кофе, съесть сахар, поиграть в компьютерные игры, посплетничать… в идеале одновременно. Награда невелика, но легкодоступна. Решить сложную задачку. Дооолго-долго думать. Подбирать. Ошибаться, приходить всё ближе и ближе к решению… чем мы ближе, чем больше мы напрягаемся, тем больше мозг нас стимулирует продолжать. При этом баланс во втором варианте довольно тонкий. Сложнее, чем я могу? Думаю, что не справлюсь? Не продолжай, брось, говорит психика. Слишком легко? Я делаю, и у меня выходит? Рутина, скучно, брось. А первый вариант уже подстроился под нас. Те потоки информации, которые не вовлекали людей — отпали. Искусственная эволюция. Остаются те соцсети и сайты, которые подают информацию ровно в таком формате, чтобы максимально удерживать наше внимание. Потому что они его монетизируют. Беда даже не в том, что у нас есть постоянная конкуренция между сложными задачами и легкими наградами, а в том, что мы привыкли, что есть легкий источник вознаграждения. Зачем трудиться, если можно сделать себе хорошо прямо сейчас? (Если более точно, то психика считает, что там лучше соотношение между усилием и наградой. Пусть даже фактической награды в реальном мире нет, психика всё же создавалась не для человека, в дикой природе всё по-настоящему). Мы, кстати, догадываемся об этом. Но реализовать сложно. Всё время что-то отвлекает. А мозг пока вовлечется, пока разгонится… Предлагаю довольно интересную технологию с условным названием «капсула». Это моя любимая игрушка последнюю пару лет. Работает как волшебная таблетка. Одна капсула = 2 часа. Вы можете экспериментировать и вставлять несколько капсул во время рабочего дня. Первый час вы тратите на важную задачу, которую выбрали заранее, при этом ни на что не отвлекаетесь. Следующие 30 минут вы тратите на обслуживание инфраструктуры: общение с коллегами, изучение новостей и так далее. Последние 30 минут — отдых, но не развлечения. Вот несколько примеров, которые помогают мне и моим клиентам: прогулка на свежем воздухе, сон, неформальное общение с коллегами и другие активности, которые не будут задействовать слишком много рецепторов дофамина. Позвольте своему мозгу отдохнуть. На всех конфах я регулярно задаю один и тот же вопрос. «Если бы вы могли работать в полном погружении и не отрываясь, сколько часов в среднем понадобилось, чтобы сделать вашу недельную работу?». Ответы обычно в диапазоне 7-15 часов. И я не назову респондентов лентяями и бездарями. Наоборот, чаще всего это сильные сеньоры, тимлиды, архитекторы и СТО. Опрос на TeamLeadConf (на котором я представлял этот доклад) показал средний результат в 13 часов. И это действительно талантливые и успешные люди. Эта технология даёт 20 (!) часов чистого времени и сверху ещё 10, чтобы решать инфраструктурные задачи. х1.5-х2 прирост производительности. При том, что не накапливается усталость — 4 полноценных получасовых отдыха! В обычном режиме в конце рабочего дня, мы часто пониманием, что совсем не отдыхали, вертелись, как белка в колесе, а значимых подвижек в работе нет. Почему такие тайминги? Почему не Помодоро? Потому что сложные задачи требуют погружения. Чем выше уровень абстракций, тем дольше подгружать всё в «оперативку». Невыгодно прерываться через 25 минут. А вот через час уже становится тяжело поддерживать концентрацию. Мы не хотим максимизировать продуктивность за цикл, мы хотим повышать эффективность. На дистанции месяцев, выгоднее переключиться и давать себе отдыхать. Помодоро не даёт полноценно отдохнуть, не даёт переключить мозг в другой режим (но об этом в другой раз). Важно! Это не священная корова. Попробуйте применить так, а дальше адаптируйте под свои нужды. Наша цель в этой схеме — максимизировать время в концентрации. Именно оно даёт результаты: самоуважение, рост скиллов. Именно за него, кстати, платит работодатель. Инфраструктура — неизбежное зло. Мы встраиваем время на неё, потому что у большинства такие задачи копятся в течение дня. Если нечего делать, можно отдохнуть. Отдых сокращать не рекомендую. В идеале к концу рабочего дня должно быть ощущение «офигенно поработал, могу ещё». На этом этапе главное — среда, которая поощряет сосредоточение. Не нужно требовать срочных ответов в мессенджерах или тушения пожаров от сотрудника, который занят сложной задачей, которая требует концентрации. На внедрение нужен ресурс. Любые изменения для человека и команды не бесплатны. Что делать? Пробовать внедрять 1 капсулу в день 5-7 рабочих дней и наблюдать за своим состоянием. Большинство отмечают рост эффективности, что мотивирует внедрять дальше. Если есть серьезные внутренние проблемы, они не исчезнут. Ни перфекционизм, ни синдром самозванца…  Что делать? Прорабатывать или закладываться на это. Не все проблемы надо решать, как-то же вы работали до этого. У многих проблемы с концентрацией, её приходится тренировать. Другие люди часто не считают ваше время и ваши границы важными, они хотят вас дёргать. Плохо, если это клиенты и начальство, которым сложно отказать. Что делать? Постепенно перестраивать коммуникации. Специфические бизнес процессы, митинги и т.д. Что делать? Здесь индивидуальные решения, обычно удаётся выделить какое-то время в течение дня, когда можно ограничить входящий поток.",
    "8": "За 15 лет работы red_mad_robot база знаний компаний сильно масштабировалась. Появление новых артефактов и рост количества проектов усложнили актуализацию знаний для сотрудников. Времени на обновление данных часто не хватает, поиск материалов стал сложнее, а часть информации вообще канула в лету вместе с ушедшими сотрудниками. В итоге пересылка документов в чатах и многочисленные гугл-таблички стали самым простым, но не самым удобным и тем более безопасным вариантом. Но мы ведь роботы, и там, где белковые пересылают документы в чатах, мы создаём умные сервисы. Так родилась база знаний Smarty. Бизнес-юнит NDT, выросший из пет-проекта разработчиков, взял на себя задачу создания Smarty — умной базы знаний red_mad_robot. На основе RAG-архитектуры команда разработала систему, использующую облачные и локальные большие языковые модели для генерации ответов и поддержки диалогов. Мы регулярно тестировали интерфейс и дорабатывали дизайн, а вскоре после запуска дополнили веб-версию телеграм-ботом, который стал полноценным участником рабочих чатов. Сначала точность ответов не превышала 40%. Точность мы измеряем на основе составленных Q&A вопросов для нашего датасета. Мы постоянно проходимся по датасету и данным методом RAGAS и дополняем секцию Q&A тестовыми вопросами для анализа данных. Что такое RAG и RAGAS мы расскажем ниже в тексте. В итоге у нас получилось поднять точность ответов до 95%, значительно сократив время поиска информации и уменьшив нагрузку на службу поддержки. Это довольно высокий показатель среди аналогичных продуктов на рынке. Мы спроектировали Smarty на основе on-premise-модели — это самый безопасный вариант. Чтобы добиться наилучшего результата, мы ускоряли LLM, автоматизировали, уменьшали размеры, выбирали лучшие. Помимо языковой модели, Smarty использует ещё три: классификатор Guardrails (запрещает LLM общаться на темы, несуществующие в датасете и запрещённые законом). В red_mad_robot все знания, начиная от того, как уйти в отпуск, заканчивая оборудованием дочерних компаний, хранились в Confluence. Если в его поиске задать вопрос «Как уйти в отпуск?», результатом будет 1082 страницы разной степени неактуальности. Это приводит пользователя в ужас. Так работает индексируемый поиск по полному вхождению слова, в том числе и поисковики вроде Google. База знаний на основе RAG позволяет получить чёткий сформулированный ответ на свой вопрос со ссылками на источники. Но как это работает? RAG (Retrieval Augmented Generation) — метод загрузки данных в векторное пространство и последующее использование их в ответах большой языковой модели (LLM) в момент генерации. Обычный индексируемый поиск разбирает запрос пользователя на отдельные слова и ищет максимально релевантные вхождения среди своих данных, предлагая ссылки на источники в качестве ответа. В поиске, основанном на RAG, всё работает так: Запрос пользователя попадает в семантический механизм поиска, который ищет «близость» со всеми документами или знаниями, которые есть в базе. Документы, вместо полного вхождения в слова, имеют вектор — набор чисел в векторном пространстве. Запрос при этом тоже превращается в вектор. Специальный механизм поиска ищет максимально релевантные по вектору ответы. То есть буквально сравнивает углы между векторами в пространстве. Если угол маленький, значит знания семантически похожи. Чем меньше значение вектора, тем ближе по смыслу информация к запросу пользователя. Ты — умная база знаний, тебе будет задан вопрос пользователя и релевантные куски текста из базы данных. Проанализируй их и сформулируй краткий ответ на вопрос. LLM генерирует конечный ответ для пользователя и даёт ссылку на все источники. Smarty — это мультиагентная сеть, которая самостоятельно создаёт базу знаний, тестирует её и валидирует без участия человека. Есть даже агент, который смотрит на результаты остальных агентов и даёт им рекомендации о том, как увеличить точность. Документ попадает в систему авторазметки. Затем система кладёт документ в векторную базу данных и сама создаёт себе вопросы. После этого отправляет ответы человеку на валидацию. Если человека устраивает результат, он даёт команду запустить тест, система его запускает и даёт рекомендации. Затем человек смотрит, что ещё нужно исправить. Система исправляет, запускает новый тест, и в итоге мы получаем нужные метрики. Это самообучающаяся система, которая может превратить один текстовый документ в домен знаний. Агент-авторазметчик позволяет разбить огромное полотно текста на логические блоки, не теряя смысла и не переписывая слова, по такой схеме: А дальше весь размеченный текст преобразуется в векторы с помощью модели эмбеддинга и хранится в векторной базе данных. При использовании продвинутого RAG-поиска, на котором мы и построили Smarty, система выбирает более 30 релевантных запросу кусков текста из векторной базы данных, реранжирует их и отправляет в LLM топ-5 подходящих от этих 30. Затем LLM формулирует ответ, который проходит ещё один цикл проверки Guardrails и RAGAS (автотестирования качества). Таким образом достигается точность ответов выше 95% не только на вопросы, контекст которых система уже знает, но и на вопрос, контекст которого системе вообще не знаком. Конечно! Если у вас есть лишний $1млн, огромное количество токенов и вычислительных мощностей, то можно. Но даже при этом — не факт, что сработает. А разработанное нами решение на основе RAG стоит от 7,5 млн рублей. Мы кастомизируем агентов для разметки, тестирования и создания датасета и промпт для LLM, чтобы модель сохраняла tone of voice компании. Мы зашили в RAG-модель основы ToV и редакционной политики red_mad_robot, и теперь Smarty отвечает так же, как это сделал бы любой другой сотрудник компании — дружелюбно, лаконично и с уместным юмором. Все модели, с которыми мы работаем, подняты на нашем железе, автомасштабируются, кластеризируются. В общем, из разных кубиков этого RAG-конструктора мы собрали Smarty для себя и теперь создаём индивидуальные решения и для наших клиентов. 95% правильных ответов. Значительно сократилось время на поиск информации. Уменьшилась нагрузка на техподдержку. Ассистент научился давать краткие и расширенные ответы. Он умеет задавать наводящие вопросы и давать ссылки на материалы. Создали гибкую систему, работающую как on-cloud, так и on-premise. Создать поиск по неявным знаниям, которые «сидят» на сотрудниках — научить умную базу знаний давать советы, к кому обратиться по тому или иному вопросу. Вшить в базу глубокие знания по процессам и компетенциям. А ещё мы прорабатываем возможность вытаскивать такие знания через аудиосообщения — это минимальные трудозатраты для сотрудников, всё остальное сделают агенты. Создать систему, которая сама «достаёт» нужные знания из людей, чтобы она определяла, чего ей не хватает и спрашивала у сотрудников. Тестировать на Smarty разработки в области создания ИИ-агентов, чтобы в перспективе умная база знаний стала полноценным напарником в работе. иллюстрации — Петя Галицкий.",
    "9": "Всем привет, с вами снова Павел Бузин. Нобелевская неделя принесла несколько замечательных новостей, которые окажут серьезное влияние на развитие отраслей, связанных с машинным обучением и искусственным интеллектом. Первая новость — Нобелевская премия по физике 2024 года присуждена Джону Хопфилду (John J. Hopfield) и Джефри Хинтону (Geoffrey E. Hinton) за исследования в области нейронных сетей. Вторая новость — Нобелевская премия по химии 2024 года присуждена Дэвиду Бейкеру (David Baker), Демису Хассабису (Demis Hassabis) и Джону Джамперу (John M. Jumper). Оба лауреата удостоены премии за исследования белков с применением AI-технологий, а по сути — с применением нейронных сетей. Третьей новостью можно считать то, что эти решения практически не вызывают возражений в условиях неоднократных обвинений Нобелевского комитета в предвзятости и ангажированности. Достойная работа ученых получила заслуженные высокую оценку и награду. Как аналитик компании-провайдера облачных и AI-технологий, который работает с нейросетями каждый день, предлагаю обратиться к первоисточникам и посмотреть на суть достижений нобелевских лауреатов по физике, а конкретно — на физическую составляющую открытий и их дальнейшее применение. Про химию поговорим чуть позже. Как известно, регламент присуждения Нобелевских премий не раскрывает кандидатов, и процессы отбора наивысших научных достижений по разным направлениям независимы. Такие решения Нобелевского комитета буквально юридически фиксируют, что нейронные сети стали важной частью знаний и технологий человеческой цивилизации. Давайте отмотаем время назад и посмотрим, с чего же все начиналось. Начало нейронным сетям положили идеи Уорена МакКаллоха (Warren S. McCulloch) и Уолтера Питтса (Walter Pitts). В 1943 году они опубликовали статью A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY («Логическое исчисление идей, присущих нервной деятельности»), в которой применили математический аппарат логики для описания и формализации процессов, происходящих при взаимодействии нейронов. неизменность нейронной сети со временем. Дополнительно авторы ввели понятия временнóе пропозициональное выражение (temporal propositional expression, TPE) и временнáя пропозициональная функция (temporal propositional function, TPF), т. е. функции и выражения, позволяющие описать высказывания, состоящие из нескольких логических утверждений, и их изменение со временем. Также МакКаллох и Питтс доказали несколько теорем, в том числе подтвердили, что временнóе пропозициональное выражение может быть реализовано сетью нулевого порядка и описали свойства TPE. Проще говоря, ученые установили, что можно задавать логические функции с помощью сети нейронов. Следующей значительной вехой развития нейросетей стала математическая модель нейрона, предложенная в 1957 году Фрэнком Розенблаттом (Frank Rosenblatt). Фрэнк Розенблатт предложил алгебраическую модель нейрона, получившую название перцептрон. Перцептрон связывает поступающие на вход нейрона сигналы и выходной сигнал с помощью линейных алгебраических функций. Компоненты входного сигнала xi суммируются с весами wi, и к полученному результату применяется функция активации, имитирующая прохождение сигнала от нейрона в возбужденном состоянии и отсутствие сигнала при уровне возбуждения ниже порога. Математически это записывают так: Если описанное выше кажется слишком сложным, давайте упростим: Фрэнк Розенблатт предложил модель, которая имитирует работу нейрона. Он работает как настраиваемый «черный ящик», принимая на вход разные сигналы. Каждому сигналу присваивается «вес»: у звонка от начальника вес выше, чем от очередных спамеров, например. Затем перцептрон складывает сигналы с учетом их важности. Есть специальное условие — порог. Если общая важность сигналов превышает этот порог, перцептрон выдает сигнал. Если нет — он молчит. Такая модель позволила создавать математическое описание наборов нейронов, связанных последовательно в виде многослойных структур, которые получили название «нейронные сети». Сам перцептрон для простоты именуют нейроном, имея в виду именно математическую модель. повышение точности при увеличении числа слоев. Возможность обучения появляется в тот момент, когда мы начинаем менять веса нейронов так, чтобы выходной сигнал был ближе к ожидаемому. Процесс получил название «обучение с учителем», эта тема широко известна и описана во многих источниках. Поэтому сразу перейду к менее очевидным, но интересным свойствам. Способность многослойного перцептрона уже при двух слоях реализовывать базисный набор команд булевой логики AND, OR, NOT и XOR открыла возможность строить модели, эмулирующие любой процесс формальной математической логики практически неограниченной сложности, а также создавать нейросети, имитирующие поведение непрерывных функций. Ученые были удивлены, когда обнаружили, что многослойные нейронные сети, содержащие 10 и более слоев, стали успешно справляться с распознаванием изображений. Этот эффект был экспериментально подтвержден в 2021 году командой с участием Алекса Крижевского, Джеффри Хинтона и Ильи Суцкевера (также известного своей работой в OpenAI над моделями GPT) и получил название Deep Learning. Разработанная ими архитектура сверточной нейронной сети под названием AlexNet вызвала взрывной рост успехов, интереса и инвестиций в области нейронных сетей и машинного обучения. В многослойные нейронные сети на основе перцептрона Розенблатта нашли очень широкое применение при создании сверточных и рекуррентных нейронных сетей, решения задач регрессии, классификации и многих других. Каждый, хотя бы немного знакомый с обучением нейронных сетей, знает про обратное распространение ошибки — backpropagation. Автором этого метода является Джеффри Хинтон, который в 1986 году вместе с Дэвидом Румельхартом (David E. Rumelhart) и Рональдом Вильямсом (Ronald J. Williams) описал новую процедуру обучения нейронных сетей в статье Learning representations by back-propagating errors. Высокая вычислительная эффективность алгоритма обуславливается тем, что для производной этой сигмоиды выполняется равенство: y' = y*(1-y). Это значит, что вместо нахождения производной численными методами (считать функцию во многих последовательных точках, повторять так много раз, уменьшая шаг по оси x) мы вычисляем ее через арифметические операции со значениями самой функции (в одну операцию по формуле, где нужно всего лишь перемножить пару значений функции). Дальнейшее развитие этих идей позволило найти другие эффективные функции активации, включая ступенчатую функцию, гиперболический тангенс, ReLU и другие. Помимо широко используемого в настоящее время метода обратного распространения ошибки Джеффри Хинтон исследовал модели машины Больцмана, Mixture-of-Experts (привет OpenAI и GPT-4) и многое другое. И небольшое историческое отступление. Интересная подробность, которую раскопали историки — Джеффри Хинтон является правнуком Джорджа Буля (George Boole), создателя математической логики и булевой алгебры. В 1988 году финский исследователь Тейво Кохонен (Teuvo Kohonen) предложил на выходе многослойной нейронной сети ставить дополнительный слой, работающий по принципу Winner-take-all (победитель получает все). При анализе алгоритма работы сети Кохонена финальное решение выглядит так: остальные выходные сигналы — зануляются. Такая архитектура позволяет эффективно решать задачи кластеризации, а подход Кохонена получил название «векторное квантование». Многослойные перцептроны и сети Кохонена обладают одним важным свойством: при работе сети сигнал движется последовательно от входного слоя до выходного, без петель и циклов. Такие сети получили название Feedforward Neural Network — сети прямого распространения. При этом существует множество задач, когда необходимо знать значения в предшествующий момент времени. Самая важная их часть — задачи обработки естественного языка. Например, значение следующего слова в предложении зависит от предыдущего, а также от семантики конкретного языка. Особенно ярко это выражено в английском языке. Сравните два выражения: John called Mark и Mark called John. Кто первый в предложении — тот и позвонил другому. Но y feedforward-сетей нет возможностей различить эти два случая — вектора John, called и Mark поступают на вход одновременно. Для анализа этих и других выражений нейросеть должна иметь возможность учитывать значения сигнала на предыдущем шаге, а, значит, появляется необходимость подать на вход нейрона выходные сигналы этого и других нейронов. В связях нейронов появляются петли и циклы, а такие архитектуры получили название рекуррентные. Работы Джона Хопфилда 1982 года — создание архитектуры сети с ассоциативной памятью. Работы Майкла Мозера (Michael C. Mozer) 1988 года и его последователей, Зеппа Хохрайтера (Sepp Hochreiter) и Юргена Шмитхубера (Jürgen Schmidhuber), которые привели в 1997 году к созданию ячейки с памятью LSTM. Появление в недрах Google архитектуры трансформеров 2017 году, с которой стартовала эпоха генеративных и больших языковых моделей. Посвященная трансформерам статья Attention Is All You Need является, по разным оценкам, самой цитируемой статьей в области нейронных сетей и машинного обучения. Результаты развития идей Джона Хопфилда многие читатели используют уже сейчас в виде различных версий ChatGPT, GigaChat, YandexGPT и других моделей. В свете сказанного не вызывает сомнений полезность открытий и Джона Хопфилда, и Джеффри Хинтона, их роль и влияние на развитие науки, технологий машинного обучения и нейросетей. Нобелевский комитет оценил это по достоинству с формулировкой for foundational discoveries and inventions that enable machine learning with artificial neural networks («за фундаментальные открытия и изобретения, сделавшие возможным машинное обучение для искусственных нейронных сетей»). Как я упомянул выше, это не вызывает возражений. Но у внимательного читателя возникнет закономерный вопрос: а при чем тут физика? Вторым вопросом может быть — за что именно получил премию Джеффри Хинтон? Давайте чуть подробнее. В этом пункте я опишу некоторые физические модели, которые позволили создать математические методы, примененные в машинном обучении. Эта часть для физиков-скептиков. Если же вы не очень знакомы с физикой и просто хотите узнать, в чем же там дело с Нобелевской премией, можете пропустить этот пункт без потерь. Один из фундаментальных законов природы — закон сохранения энергии. Он помогает вычислить, как система будет меняться со временем и каким будет ее конечное состояние, а также определить стабильные (равновесные) состояния, в которых эта система может находиться. Другим важным принципом является принцип наименьшего действия. Действие как физическая величина определяется через функцию Лагранжа L(φi) (еще ее называют лагранжиан), где φi — обобщенные координаты (координаты, скорости, угловые скорости и т. д, соответствующие степеням свободы системы). — вектор обобщенных скоростей системы. Другой подход к описанию, предложенный Уильямом Гамильтоном (William Hamilton) в 1833 году, опирается на описание системы в терминах обобщенных координат и импульсов. Функция Гамильтона (гамильтониан) для системы с n степеней свободы. H(p, q, t), где p – вектор обобщенных импульсов, q — вектор обобщенных координат. Гамильтониан и лагранжиан системы связаны уравнением Лежандра: И нам не нужно последовательно следить за каждым шагом эволюции системы. В 1925 году Эрнст Изинг (Ernst Ising) опубликовал статью «Вклад в теорию ферромагнетизма», в которой исследовал поведение систем, каждый элемент которых может принимать одно из двух четко выраженных состояний (спин атома). При этом вероятность принять одно или другое состояние зависит от энергии атомов (~ температуре T) и определяется распределением Больцмана. где Ji,j — описывает поведение системы (J > 0 — ферромагнетизм, «магнитный», все спины выстраиваются в одном направлении, J < 0 — антиферромагнетизм, соседние спины противоположны и противодействуют друг другу, J = 0 — спины распределяются случайно и не взаимодействуют). Величина σi — описывает направление спина и принимает одно из двух значений — {-1, +1}. Модель Изинга описывает случай короткого взаимодействия спинов (взаимодействуют только соседи). Развитие модели Изинга для кристаллических решеток (дальнодействие) получило название модели Поттса по имени создателя Ренфри Поттса (Renfrey Potts). Другую физическую задачу, которую необходимо упомянуть, касается метода градиентного спуска. Этот метод был разработан Коши (Augustin-Louis Cauchy) в 1847 году и применяется для решения задачи о нахождении минимума энергии системы во многих разделах физики, в том числе для многомерных случаев. И вот здесь-то и началось сближение физики и науки о данных. В работе 1982 года Джон Хопфилд, исследуя динамическую модель ассоциативной памяти, обнаружил коллективные эффекты в поведении нейронов. Нейроны обновляли свое состояние Vi, переходя из одного порогового значения в другое, а переход зависел от значения сигналов на входе нейрона Tij: Матрица взаимосвязей Tij — симметричная с нулевыми значениями на диагонали. Стационарные состояния нейронов можно было трактовать как состояние ячейки памяти, хранящей информацию. Здесь мы видим потенциальную энергию сети как составляющую Лагранжиана. Дальнейшие операции по решению уравнений были к этому моменту очень хорошо разработаны физиками-теоретиками. Про лежащие на поверхности аналогии вроде спинового стекла или применения для решения задач дискретной оптимизации я предлагаю изучить читателю самостоятельно, тем более сейчас появится очень много различных публикаций на эту тему. В 1983 году Джеффри Хинтон с коллегами Дэвидом Экли (David Ackley) и Террeнсом Сейновски (Terrence Sejnowski) применили стохастический подход к модели Хопфилда. А именно: они предложили задавать состояние нейрона в соответствии с распределением Больцмана (напомню, что этому распределению подчиняется кинетическая энергия частиц в газах и плазме). где, а 𝜃𝑖 — смещение (bias), а в физическом мире его роль выполняет локальное поле (например, поле тяготения). Описанная архитектура получила название «машина Больцмана», и для нее выполняются закономерности, ранее найденные при решении различных задач с помощью модели Изинга, включая алгоритм имитации отжига. Я надеюсь, что мне удалось вам показать, как тесно связаны машинное обучение и физические процессы. Основы машинного обучения и нейросетей, такие как машины Больцмана из конца XIX века и модель Изинга из XX века, стали важными для развития информатики. В начале XXI века эти идеи получили признание в виде Нобелевских премий по физике и химии как ценные инструменты для понимания природы. Благодарю старшего архитектора-исследователя Кирилла Кучкина за продуктивное обсуждение идей и фундаментальных физических принципов, которые привели к написанию этой статьи. Идеи и лежащие в основе статьи оригинальные материалы были собраны задолго до решения Нобелевского комитета, а само решение лишь подтвердило важность и актуальность темы. 1736 – 1813 — годы жизни Луи Лагранжа, создателя вариационного исчисления. 1837 год — Ян Пуркине открывает нейроны мозга. 1844 – 1906 — годы жизни Людвига Больцмана. 1847 год — метод градиентного спуска Коши. 1897 год — Чарльз Шеррингтон исследует роль синапсов в обмене сигналами между нейронами. 1925 год — Эрнст Изинг, статья «Вклад в теорию ферромагнетизма». 1943 год — искусственные нейронные сети Уорена МакКаллоха и Уолтера Питтса. 1957 год — перцептрон Розенблатта. 1982 год — нейронная сеть Джона Хопфилда с ассоциативной памятью. 1983 год — Джеффри Хинтон с коллегами — стохастические подходы и машина Больцмана. 1986 год — обратное распространение ошибки Джеффри Хинтона, Дэвида Румельхарта и Рональда Вильямса. 1988 год — Майкл Мозер предлагает архитектуры нейросетей с памятью. 1988 год — нейронная сеть Кохонена. 1997 год — Зепп Хохрайтер и Юрген Шмитхубер предложили ячейку LSTM. 2021 год — нейросеть AlexNet стала победителем конкурса ISLVRC-2012. Взрывной рост интереса к машинному обучению. 2017 год — появление архитектуры трансформеров в недрах Google. Начало эры GPT. 2024 год — Нобелевские премии по физике за исследования в области нейронных сетей и по химии за применение нейронных сетей. А еще 24 октября приглашаю вас на IT-конференцию GoCloud Tech — сможете послушать доклады про изнанку русского AI и облачных решений, обсудить собственные идеи с коллегами и экспертами. https://link.springer.com/article/10.1007/BF02478259 https://home.csulb.edu/~cwallis/382/readings/482/mccolloch.logical.calculus.ideas.1943.pdf https://www.pnas.org/doi/pdf/10.1073/pnas.84.23.8429 https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554 https://gwern.net/doc/ai/nn/rnn/1995-mozer.pdf https://www.researchgate.net/publication/13853244_Long_Short-term_Memory https://www.nature.com/articles/323533a0 https://direct.mit.edu/neco/article-abstract/3/1/79/5560/Adaptive-Mixtures-of-Local-Experts https://cdn.aaai.org/AAAI/1983/AAAI83-087.pdf https://www.cs.toronto.edu/~hinton/absps/fahlmanBM.pdf https://www.sciencedirect.com/science/article/pii/S0364021385800124 https://onlinelibrary.wiley.com/doi/epdf/10.1207/s15516709cog0901_7 https://arxiv.org/pdf/1706.03762 https://www.nobelprize.org/prizes/physics/ https://www.nobelprize.org/prizes/physics/2024/press-release/ https://www.nobelprize.org/uploads/2024/09/advanced-physicsprize2024.pdf https://www.hs-augsburg.de/~harsch/anglica/Chronology/20thC/Ising/isi_fm00.html Математика прекрасного. Как создать красивую картинку, если ты дилетант, художник или нейросеть?",
    "10": "Mark Shiloh, Влад - наш бывший соотечественник, более 20 лет живущий в Новой Зеландии (Окленд ), профессиональный музыкант, мульти-инструменталист с большим практическим опытом, ныне преподаватель и директор музыкальной школы... Более 30 лет назад мы вместе работали в Новосибирской консерватории. Когда в моей команде разрабатывали программу морфинга параметров синтеза для Yamaha DX7, Влад приобрел редкий и крутой SY99. Этот одаренный и энергичный парень поражал меня тем, что до мельчайших деталей штудировал толстенную инструкцию SY99 и задавал такие вопросы, которых я никогда больше не слышал ни от одного музыканта. Я бы сказал, что он, благодаря своему пытливому характеру, к профессии музыканта добавил квалификацию sound-инженера... Сейчас мы в контакте и я, как и другим знакомым, периодически посылаю ему примеры, которые сгенерил в Suno... Две песни Влада \"зацепили\". У меня, честно говоря, отношение к генерациям более спокойное, особенно, когда их 50 в неделю и больше... Вот, пишет: \"Сколько стоят эти твои песни?\" ( Странная девушка v2.1 и Бледно-розовый оттенок v2.1 ). Я ему: \"У меня нет на них прав, т.к. аккаунт Free\" ... Потом, раз в неделю он писал/говорил о каких-то фишках во 2-й: \"Посмотри, как здесь пропето ..., а вот здесь гитара\" и т.п. Ну, на самом деле, мне не до этих нюансов - слишком много текущей работы... и я предложил ему записать наш разговор - получится что-то вроде интервью 1,2, наверное, кому-то будет интересно. Он сказал \"окей\" и мы созвонились 14 октября 2024 г. 1 я этот же текст выложил на сайте. 2 мои вопросы/реплики пронумерованы. Качество аудио фрагментов невысокое - я просто записал разговор на второй телефон. Мы говорим о коленках. Бледно-розовый оттенок ... Бледно-розовый оттенок v2.1 (video) Самое главное, что босса-нова не загружена перкуссионными инструментами, как это было, скажем, в 80-м году. Но есть совершенно яркая, выраженная \"босса-новость\". Барабанная партия, не барабанная, а перкуссионная, она как бы на втором плане... Очень мягко сделано, скорее всего, кахон-бокс, я имею ввиду по звуку. Очень мягкий бас, все очень современно... Само настроение музыки в целом, когда песню прослушиваешь, оно такое очень легкое из-за того, что она не перегружена инструменталом. Это очень большой вкус проявлен, если бы мы о человеке говорили. Это сделано по принципу \"отсечь все лишнее\", поэтому, конечно большой опыт. Когда музыкант только формируется, он старается показать все, на что он способен - стукнуть там лишний раз, брякнуть на клавишах. А здесь оставлены только самые необходимые вещи, то есть минимум средств выразительности, которые приводят к такому хорошему спокойному чувству босса-новы. Тут гитара играет совершенно по-другому и она удивительно сочетается с басом. И это очень сложно сделать на самом деле. Вот я занимаюсь неделю, я понимаю головой, как это сделано, все-таки образование, какой-то опыт... Ну, конкретно я тебе говорю: 1-й, 2-й, 3-й, 4-й такт. В 4-м такте зависает функция и ты начинаешь свой вокал на паузе, т.е. инструментал просто останавливается. \"Светло-розовый оттенок\", вот эта 1-я фраза просто в пустоте сделана, там никто не мешает, и даже нет сустейна какого-то на инструментале. Т.е. просто есть пауза, где человек начинает петь. Потом, во 2-м такте этой музыки сделан такой ход басовый, который висит на ноте Ми, а функция уже поменялась. И вот это очень привлекает, ну, до безобразия. Вот смотри, как получается играет - 1-й аккорд [guitar] ..., ставится 2-й аккорд и ... бас, по идее, должен был уйти сюда [guitar] ... вот на эту ноту. Но этого не происходит. И он этим привлекает, потому что он играет вот отсюда [guitar] ..., но берет из предыдущего аккорда ноту басовую. Получается [guitar] ... #4 Ну,... ты мне со звуком, о каких-то деталях ... Можно сказать, что это высокого уровня профессионализм, как-бы тонкое чувство? Да, человек, если бы это человек создавал, было бы ощущение, что он не только очень хорошо ориентируется в гармонии, но привнес какую-то ... свежесть. При этом у него, скажем, очень хорошее чувство меры, вкуса. Аранжировка сделана действительно профессионально, с ощущением, что человек очень долго попрактиковал босса-нову в латиноамериканском вот этом стиле. Впереди у нас три гитары. Первая гитара четко играет ритм. Вторая гитара, которая выдает вот это соло - вступление и развернутое соло. И бас-гитара. И вот эти три инструмента вместе, они в основном и делают эту картину музыки. #5 Слушай, а скажем, из известных артистов, из того, что ты слышал, это ближе всего к чему? Ведь фишка то в том, что Suno делает это все на основе реальных треков, значит где-то существует прототип. Думаю это бразильская система, явно бразильские музыканты, путешествующие музыканты... Я их вижу у себя здесь, они около океана играют. Играют вот именно в этом стиле, это именно бразильский такой вариант музыки... Та босса-нова, которая классическая, Жобимом3 была сделана. Она была очень насыщенная перкуссией, там и пианисты играли, и другие... А вот такую выжимку из босса-новы я услышал в первый раз именно в твоей песне. И то, что это именно бразильская стилистика у меня никаких сомнений нет. У меня много есть таких примеров, когда играет 2-3 гитариста и звучит такая похожая латино-американская красивая музыка и танцевальная, и живая... Ну, у меня просто есть разные отзывы от разных людей. Одному знакомому послал, врачу, но серьезно музыкой занимается, говорит \"... какая удивительная поэзия и какой мелодизм... \" он просто был в восторге. В таком, знаешь, так по-детски, искреннем... 3 Antônio Carlos Jobim (примеч. AU) #6 Хорошо,... про поэзию - спорно, это была шутка, я же не поэт. Давай про мелодизм. Да, мелодизм в этой песне меня, честно говоря, приятно удивил. Особенно тем, что он использует альтерированные ноты в таком совершенно уверенном ключе. Вот в соло гитаре есть ноты альтерированные, но они звучат как будто музыканты экспериментируют. Потому, мне кажется, они не были на 100% прочитаны стилистически. А вот мелодия, особенно там, где припев. Вот ... ноты передо мной, вот 4-я строка: \"в Википедии, возможно, есть подробная статья\"... Там Соль# стоит нота. \"подробная статья\" - вот это отпевание, это абсолютно, ну, как бы не типично. Если это человек сочиняет, можно сказать, что он из джаза. Потом, вот это \"у тебя, не у подруг\" - там тоже стоит Ми-b, Ре, Ми-b, Ре... и Ми-b, Ми-бекар(!) и только потом последняя нота. То есть вот это \"пятый год я замечаю у тебя, не у подруг\". Ну, вот бл@ха-муха, извини, ведь это именно так спеть трудно человеку неподготовленному! #7 Мне кажется, подобное встречается в русских романсах, когда наши классики писали, а не то, что просто в народе. Ну, и у советских сильных авторов, может немного в \"Подмосковных вечерах\" Соловьева-Седого... Да, есть такое... Вот, пониженные ступени, опевания, т.е. это уже говорит о более изысканном владении материалом. Это не просто A,B,C ... это очень по-музыкантски... Подожди, я сейчас подойду просто к инструменту, смогу проиллюстрировать, как это действительно красиво звучит... А вот мелодия, особенно там, где припев. Вот ... ноты передо ... вот эта мелодия [piano]... \"Я не знаю, кто здесь болен, может ты, а может я, в Википедии, возможно, есть подробная статья\" и дальше вот это [piano]... последний момент, но мне это просто охренительно нравится [piano]... Вот это вот: [piano]... \"... твоих щёк и нежных рук\"... \"твоих щёк и нежных рук пятый год я замечаю у тебя не у подруг\", ну это вообще, ну не знаю даже, как это назвать [piano]... Т.е. это использование низкой 5-й ступени [piano]... \"у тебя не у подруг\". [piano]... \"Я не знаю, кто здесь болен...\" Т.е. это просто говорит о том, что у этой программы есть очень хороший ресурс, музыкальных фрагментов, образцов, который она проанализировала. Чтобы так сочинять, это действительно нужно ... Во-первых в этом легко запутаться, когда появляются такие альтерированные ноты. Это либо дело вкуса тончайшего. Либо человек по случайности что-то такое напортачил, но воспринимается это не очень хорошо на слух, а здесь все сделано с мерой, очень красиво, музыкально. Эстетически это очень красиво сделано... вот эти мелизмы - \"у тебя не у подруг\". Причем, спето очень точно, я даже не включал пониженную скорость на Logic'е, чтобы прослушать - считал прямо в реальном времени. Только зачем ты сделал 88,1 bpm? #8 Я вообще к темпу, да и к музыке не имею никакого отношения! Может косвенно, я задал стиль и вбил текст, а Suno уже генерирует, исходя из задания... Ну, я еще выбрал версию, т.е. генерацию, которая мне понравилась. Кстати, там их всего (у меня на сайте), кажется, 12-ти в разных стилях ... А, ... ну я два слова скажу по этому поводу. Эта система предохраняет от того, чтобы не гоняли это вместе с музыкантами, чтобы музыканты под это не играли. Они специально записывают на несколько центов либо выше, либо ниже, чтобы не строило. Чтобы музыканты напрямую не могли это использовать. Да, конечно, но я когда это увидел, я подумал, что думаю, ну, значит, они тоже так делают, как фирменные фонограммы. Ок... Вообще-то, знаешь, ощущение от музыки очень сильное. А уж если она у меня в голову засела, и я вот больше месяца её кручу и не устал от этого... Уже сам аранжировок, черновых, штук 5 сделал на нее... думаю, а как бы привнести что-нибудь свое и сказать, что это Алекс мне разрешил исполнять? Я же это вот придумал тут, в аранжировке. #10 Да, ... мне все-равно, точнее, приятно. Исполняй! Но я на бесплатном аккаунте, т.е. без коммерческого использования и по-любому надо указывать, что это Suno. Платный не дорого, только платить от нас сложно... Нужна опция типа \"выкупа отдельной песни\"... Есть у знакомого платный, я бы его попросил, но нет функции \"сочинить также \"... Ну,... знаешь, тут с этим, т.е. с копирайтом очень строго. Вот один местный перец, педагог, скопи-пастил пол-страницы для своей статьи, накатали (коллеги, похоже) на него заяву, комиссия разобралась и ... штраф в $10К, не слабо так! Ладно, еще хотел по голосоведению сказать... машина эта, в гитаре очень хорошо следит за мелодией - тот, кто это создавал, просто в совершенстве знает гитарные позиции. Чтобы в этом разобраться, надо перебрать много примеров... т.е. очень хорошо голосоведение сделано... Голосоведение гитары на 100% зависит от мелодии, это очень, очень красиво. Ясно то, что я не могу так сочинить, если бы я так мог, мне было бы приятно. #11 Понятно, спасибо. А вокал тебя не смущает, у них во всех песнях какой-то хоровой, немного ансамблевый характер. Нет, наоборот. Сейчас так модно, 4 года назад заказывал аранжировку тут для одной местной певицы... да, в Новосибирске заказывал. Так мне парень ваш такой же эффект сделал... Смешано три вокала там, одинаковых. Без искажений и без обработки центральный вокал, т.е. с панорамой 0, потом одновременно, с небольшой задержкой, несколько миллисекунд, буквально, левый. И правый канал прописан тоже с задержкой небольшой. Они как бы немножечко отстают от оригинала и объем, плотность добавляют... Ну, может ревер где-то совсем чуть-чуть. #12 А я думал, что у Suno просто так генерация вокала работает, что это или косяк, или как признак, что синтез от Suno, или чтобы немного скрыть первоисточник, или что это из-за микширования моделей разных голосов - типа один мужской из трех... Хотя, конечно, проще один трек синтезировать, а потом с ним уже играть. Не знаю про Suno, я почитал других там музыкантов, они говорят, да сейчас это так модно. Все западники делают так, и в Америке так пишут вокал. Программа, которая музыку сочиняла, она в курсе всех модных дел, это 100%. #13 Ага, в курсе... Ну, что, спасибо за твое время, успехов. Как что-то доделаешь с \"коленками\" - присылай :-) Конечно пришлю, когда что-то закончу... И тебе спасибо, не часто удается по таким моментам с кем-то тут пообщаться... Действительно хорошо поговорили... Хочу подчеркнуть по сути. Аранжировка сделана настолько со вкусом, что она не мешает вокалу, чётко следует за вокалом в голосоведении аккордов и мастерски передаёт стиль, именно стиль... Да, если будет что новое по авторским - пиши/звони. #14 Хорошо,... бай-бай! Хорошо, bye-bye... P.S. Самые последние мои записки по Suno у меня в Дневнике на сайте.",
    "11": "Как вы знаете, я приглашаю на свои подкасты знаменитых людей, которые продвинули программирование вперед: профессор доктор Бертран Мейер, Марк Симан, Джеффри Рихтер, Ребекка Вирфтс-Брок, и мой октябрьский гость — Роберт С. Мартин. На моем следующем подкасте—легендарный Роберт Мартин, известный всему миру как \"Дядя Боб\"! 🤯 Автор культовых книг Clean Code и Clean Architecture, он откроет нам свой взгляд на принципы разработки, которые формируют будущее IT. Но это еще не всё—у вас есть возможность задать свои вопросы самому Роберту Мартину! Оставляйте их в комментариях, и самые интересные мы обсудим на подкасте. Не упустите свой шанс задать вопрос одному из самых влиятельных людей в мире программирования!",
    "12": "Анализ производительности процессоров 2024 года раскрывает захватывающие тенденции в архитектурных решениях и технологических процессах. Ожесточенная конкуренция между AMD и Intel привела к значительному прогрессу в многоядерных конфигурациях и частотных характеристиках, и сильно изменила расстановку сил на рынке. В этом обзоре мы детально рассмотрим топовые модели, их микроархитектурные особенности и реальную производительность в разных сценариях и попробуем определить, какой процессор лучше. Данный рейтинг процессоров 2024 года основан на тестировании производительности в рабочих приложениях и играх, где использовалась видеокарта PNY RTX 4090 Epic-X OC. Все тесты выполнялись на системе с Windows 11 и актуальными драйверами. Места в рейтинге распределялись по совокупности показателей, которые включали в себя производительность процессора как в рабочих приложениях, так и в играх. Оценка в 100 баллов соответствует лучшему результату в каждой категории. Тестовый набор включает современные игры, такие как Baldur's Gate 3, F1 24, Forza Motorsport, Ghost of Tsushima, Horizon Forbidden West, Lords of the Fallen, Starfield, Anno 1800, Cyberpunk 2077, а также профессиональные приложения: CineBench R23/R24, Handbrake, 7-Zip, V-Ray 6, Corona 10. Посмотрим, что из этого получилось. Intel Core i9-14900K - флагманский процессор Intel демонстрирует впечатляющую производительность благодаря архитектуре Raptor Lake Refresh, поэтому закономерно стоит на первом месте. 24 ядра (8P + 16E) и 32 потока с максимальной частотой до 6 ГГц на P-ядрах устанавливают рекорд для массовых процессоров. А улучшенный техпроцесс Intel 7 позволяет достичь высокой энергоэффективности при TDP 125 Вт, хотя в режиме турбо потребление может достигать 253 Вт. Оптимизированная кэш-иерархия с увеличенным объемом L2 и L3 кэша снижает задержку в многопоточных задачах. Плюс технология Intel Thread Director второго поколения позволяет процессору более эффективно распределять нагрузки между P- и E-ядрами. Поэтому в синтетике i9-14900K показывает себя в среднем до 15% лучше предшественника. Особенно хорошо он себя чувствует в однопоточных задачах, где процессор рвет всех благодаря высоким частотам и IPC. А энтузиасты оценят улучшенный разгонный потенциал. AMD Ryzen 9 7950X3D выделяется применением технологии 3D V-Cache, увеличивающей объем кэш-памяти L3 до впечатляющих 128 МБ. 16 ядер Zen 4 на 5-нм техпроцессе TSMC покойно берут отметку 5,6 ГГц. При этом TDP в 120 Вт говорит о высокой энергоэффективности, хотя в пике он может достигать и 162 Вт. Ryzen 9 7950X3D отлично показывает себя в играх, где критичен большой объем кэша. Особенно это заметно в таких тайтлах, как Cyberpunk 2077 или Baldur's Gate 3. В многопоточных задачах процессор немного уступает i9-14900K, но берет верх при работе с большими базами данных или в научных расчетах. Переход на архитектуру Zen 4 позволил AMD добавиться значимого прироста IPC в Ryzen 9 7950X3D по сравнению с Zen 3. А улучшенный предсказатель переходов и расширенный буфер переупорядочивания команд обеспечили высокую эффективность исполнения кода в сложных приложениях для профессионалов. Предшественник i9-14900K в лице Intel Core i9-13900K по-прежнему остается мощным решением, поэтому без особого труда поднялся на третью строчку самых быстрых процессоров. 24 ядра (8P + 16E) работают на частоте до 5,8 ГГц, что отражается на производительности в самом широком спектре задач - от игр до профессиональных приложений. Процессор отлично справляется с такими задачами, как рендеринг и компиляция. Например, в Cinebench R23 он набирает более 40 000 баллов в многопоточном режиме, примерно на 15% обходя процессор предыдущего поколения. По части базового TDP, который равен 125 Вт, i9-13900K не отличается от Intel Core i9-14900K. То же самое и на максималках: его энергопотребление может достигать тех же 253 Вт, что и у модели актуального поколения. Это обязательно надо учитывать при сборке системы и опираться только на качественную систему охлаждения. Поддержка DDR5-5600 и PCIe 5.0 делает i9-13900K совместимым с высокоскоростной памятью и позволяет ему работать быстрее. Благодаря этому в игровых сценариях он лишь немного уступает своему преемнику, и часто разбег fps между ними не превышает 3-5%. Intel Core i7-14700K относится к среднему сегменту и выдает отличные для своей цены цифры производительности. 20 ядер (8P + 12E), 28 потоков спокойно берут 5,6 ГГц в пике. При прямом сравнении i7-14700K показывает значительный прирост к скорости относительно показателей i7-13700K, особенно в многопоточных сценариях. В реальных задачах, таких как рендеринг в Blender или компиляция крупных проектов, i7-14700K нередко приближается к производительности более дорогих моделей. Например, в тесте Cinebench R23 он отстает от i9-13900K примерно на 10-15%, что очень и очень неплохо. Но и стоит этот процессор совершенно других денег. Флагман AMD на архитектуре Zen 5 - целиком про впечатляющий прогресс в рабочих задачах и энергоэффективности. Тем удивительнее видеть его не на первом, а на пятом месте. Однако это не ошибка. AMD Ryzen 9 9950X выдает максимальную производительность в профессиональных приложениях, ощутимо проседая лишь в играх, которые и откинули его назад. На его стороне также поддержка DDR5-6000 и заточка под многопоточные задачи. В Blender или V-Ray процессор Ryzen 9 9950X показывает себя в среднем на 5-10% лучше конкурентов, поэтому в лидерах оказался он, а не они. Улучшенная система управления питанием и тепловыделением позволяет процессору эффективнее распределять доступную мощность между ядрами. Например, при работе с Adobe Premiere Pro, где нагрузка может резко меняться в зависимости от применяемых эффектов, Ryzen 9 9950X демонстрирует стабильно высокую производительность. i7-13700K также остается высокопроизводительным решением, и по-прежнему входит в десятку самых быстрых процессоров на 2024 год. Он выдает высокую производительность в рендеринге и компиляции. А в тестах Blender или V-Ray i7-13700K нередко показывает результаты, близкие к более продвинутым моделям. В играх он тоже показывает себя очень достойно. А еще процессор хорош в обработке 4K-видео или работе с большими наборами данных в научных расчетах и может полностью раскрыть потенциал быстрых SSD и оперативной памяти нового поколения. Правда, при работе под нагрузкой его TDP может достигать 253 Вт, так что хорошее охлаждение ему строго показано. AMD Ryzen 9 7950X, построенный на архитектуре Zen 4, предлагает высочайшую производительность благодаря 16 ядрам и 32 потокам, сохраняя высокую энергоэффективность при TDP, равным 170 Вт, хотя в пиковых нагрузках потребление может быть и выше. при этом лидирует в рендеринге и в целом довольно хорош в сценариях, предполагающих работу с 3D-графикой, монтажом видео или сложными симуляциями. Кроме того, технология AMD Precision Boost Overdrive 2 позволяет процессору динамически адаптировать свою производительность в зависимости от текущей нагрузки и температуры, что обеспечивает оптимальный баланс между производительностью и энергопотреблением. Этот процессор представляет собой оптимизированное решение для игровых систем и является полной противоположностью Ryzen 9 9950X. В играх он - настоящий дока, а вот в рабочих приложениях несколько уступает иным процессорам той же ценовой категории. Впрочем, по части технического оснащения это вполне себе добротное решение: 8 ядер и 16 потоков на архитектуре Zen 4 дополнены 96 МБ кэша L3 благодаря технологии 3D V-Cache. Частоты до 5,0 ГГц позволяют выдавать высокую производительность в однопоточных задачах. А TDP в 120 Вт делает процессор привлекательным выбором для компактных систем. В играх Ryzen 7 7800X3, часто обходит более дорогие модели без 3D V-Cache. В таких тайтлах как Cyberpunk 2077 или Baldur's Gate 3 процессор демонстрирует прирост до 15-20% по сравнению с обычными моделями Ryzen 7000. Более того, именно в играх он обходит даже модель 7900X3D за счет особенностей чиплетной конфигурации: одна из двух его матриц CCD оснащена 3D V-Cache, а вторая - нет. Это позволяет операционной системе более эффективно распределять задачи между ядрами в зависимости от их требований к объему кэша. Да, в многопоточных задачах он уступает топовым моделям, но остается конкурентоспособным в своем ценовом сегменте. В Cinebench R23 Ryzen 7 7800X3D показывает результаты на уровне процессоров предыдущего поколения с большим количеством ядер. AMD Ryzen 9 7900X3D сочетает высокую многоядерную производительность с преимуществами технологии 3D V-Cache. 12 ядер и 24 потока на архитектуре Zen 4 разгоняются до 5,6 ГГц. 140 МБ общего кэша (включая 96 МБ L3) дают отличную производительность в задачах, чувствительных к объему кэш-памяти. TDP в 120 Вт позволяет использовать процессор в высокопроизводительных системах без чрезмерных требований к охлаждению. Ryzen 9 7900X3D демонстрирует впечатляющие результаты в играх и профессиональных приложениях, но при этом уступает младшему 7800X3D. В игровых тестах процессор часто обходит даже более дорогие модели без 3D V-Cache. Например, в Starfield или Ghost of Tsushima прирост производительности может достигать 15-20% по сравнению с обычным Ryzen 9 7900X. Это делает 7900X3D привлекательным выбором для энтузиастов, стремящихся получить максимальную игровую производительность. В многопоточных задачах он уступает 16-ядерным моделям, но превосходит их в некоторых специфических сценариях благодаря большому объему кэша. Например, при работе с большими наборами данных в приложениях для 3D-моделирования или при анализе геномных последовательностей Ryzen 9 7900X3D может показывать неожиданно высокие результаты. Интересной особенностью является возможность точной настройки работы 3D V-Cache через BIOS. Продвинутые пользователи могут экспериментировать с различными режимами работы кэша для оптимизации производительности под конкретные задачи. Intel Core i5-14600K предлагает отличное соотношение цена/производительность. 14 ядер (6P + 8E) и 20 потоков обеспечивают хорошую многозадачность. P-ядра разгоняются до 5,3 ГГц, что гарантирует высокую производительность в однопоточных задачах. Архитектура Raptor Lake Refresh и 10-нм техпроцесс Intel 7 позволили достичь хорошей энергоэффективности при TDP 125 Вт. i5-14600K демонстрирует значительный прирост производительности по сравнению с предшественником, особенно в многопоточных сценариях. В игровых тестах процессор часто не уступает более дорогим моделям, особенно при использовании с видеокартами среднего класса. Например, в Cyberpunk 2077 или Baldur's Gate 3 разница в fps между i5-14600K и i7-14700K может не превышать 5-7% при игре в разрешении 1440p. В профессиональных приложениях i5-14600K показывает себя достойно для своей ценовой категории. В тестах рендеринга, таких как Blender или V-Ray, он обеспечивает около 70-80% производительности топовых моделей при существенно меньшей цене. Отдельного внимания заслуживает улучшенная система управления питанием, которая позволяет процессору эффективнее распределять энергию между P- и E-ядрами. Это особенно заметно в сценариях смешанных нагрузок, например, при стриминге игр или одновременной работе с несколькими ресурсоемкими приложениями. У различных процессоров наблюдается значительное расхождение между игровой производительностью и производительностью в приложениях. Это обусловлено спецификой архитектуры и оптимизацией процессоров под разные типы задач. Например, AMD Ryzen 7 7800X3D демонстрирует максимальный показатель в играх (100.0), но значительно отстает в приложениях (57.8). Это объясняется наличием технологии 3D V-Cache, которая существенно улучшает игровую производительность за счет увеличенного объема кэш-памяти, но менее эффективна в задачах, требующих высокой многопоточной производительности. Напротив, AMD Ryzen 9 9950X показывает лучший результат в приложениях (100.0), но уступает в играх (92.6). Это связано с большим количеством ядер и высокой частотой, что идеально подходит для многопоточных задач, но не всегда полностью утилизируется в играх. Intel Core i9-14900K демонстрирует наиболее сбалансированные показатели (99.1/87.9), что делает его универсальным выбором для различных сценариев использования. Intel сохраняет лидерство в однопоточной производительности благодаря высоким частотам и оптимизированной архитектуре Raptor Lake Refresh. Модели i9-14900K и i7-14700K демонстрируют впечатляющие результаты в высоконагруженных сценариях и профессиональных приложениях. AMD, в свою очередь, делает ставку на технологию 3D V-Cache, которая обеспечивает существенное преимущество в играх и задачах, чувствительных к объему кэш-памяти. Ryzen 9 7950X3D и Ryzen 7 7800X3D являются отличным выбором для игровых систем высокого уровня. Новое поколение процессоров AMD на архитектуре Zen 5 (серия 9000) демонстрирует значительный прирост производительности и энергоэффективности благодаря использованию 4-нм техпроцесса TSMC. Модели Ryzen 9 9950X и Ryzen 9 9900X конкурируют с топовыми решениями Intel в многопоточных задачах. Для пользователей со средним бюджетом обе компании предлагают отличные варианты: Intel Core i5-14600K и AMD Ryzen 7 9700X обеспечивают высокую производительность в играх и многозадачных сценариях. Выбор оптимального процессора зависит от конкретных задач и бюджета пользователя. Для максимальной производительности в играх рекомендуется обратить внимание на модели с 3D V-Cache от AMD. Для высоконагруженных профессиональных приложений оптимальным выбором будут топовые модели Intel или AMD с большим количеством ядер. Пользователям со средним бюджетом стоит рассмотреть процессоры среднего сегмента, которые обеспечивают отличное соотношение цена/производительность.",
    "13": "Недавно в MWS начался новый бесплатный образовательный курс «Основы сетевых технологий». Мы уделяем большое внимание теме распространения знаний и много пишем о книгах, онлайн-курсах и вебинарах, которые помогают в самообразовании. Открытые знания дают свободный доступ к информации, но с этой инициативой не все так просто. Сторонники открытых знаний считают, что информация должна быть бесплатной, приносить пользу обществу и использоваться в образовании и науке. Но не все авторы готовы работать бесплатно. Это приводит к конфликту: одни выступают за свободный доступ к знаниям, другие — за защиту авторских прав. Поговорим о том, как возникло движение Open Knowledge, а также предложим пару примечательных книг по облачной тематике с открытых платформ. Считается, что история концепции берет начало еще в XIX веке с появлением Британской библиотеки. Тогда Антонио Паницци, работник Британского музея, выступил перед парламентским комитетом с призывом обеспечить свободный доступ к книжному фонду для всех. С тех пор любой желающий может воспользоваться услугами библиотеки. Но инициативой Паницци дело не ограничилось, и в 1850 году парламент Соединенного королевства одобрил закон, закрепляющий право на бесплатный доступ к литературе и важным источникам информации. Тогда по всей стране стали открываться публичные библиотеки. Подобный акт также был подписан и в Российской Империи, где к концу 1899-го число посетителей читальных домов достигло 16,5 тыс., что довольно много для того времени. В XX веке библиотечное дело продолжило развиваться, а с наступлением эпохи интернета, знания стали доступнее. Неудивительно, что появились цифровые ресурсы, следующие парадигме Open Knowledge. Одним из них стал arXiv, основанный профессором Корнельского университета, который хотел наладить быстрый и бесплатный обмен научными статьями. Со временем электронный архив расширил свою аудиторию и стал одной из главных платформ для научных публикаций. Стремление к свободному распространению знаний в сети не могло долго оставаться неорганизованным. В 2002 года была принята «Будапештская инициатива», которая определила принципы открытого доступа и подходы для его достижения. С тех пор было предложено несколько определений термина «открытые знания». В частности, определение Open Definition 1.0 гласило, что контент является открытым, если любой человек может свободно использовать и распространять его при условии сохранения цельности и указания авторства. За прошедшие двадцать лет это определение было несколько раз пересмотрено. Последняя версия — Open Definition 2.1 — была опубликована в 2015 году. Согласно ей, информацию или данные можно считать открытыми, если люди могут использовать и делиться ими без юридических, технологических или социальных ограничений. Знания в некоторой степени можно рассматривать как товар, а это значит, что конкуренция, так или иначе, будет препятствовать свободному распространению ценной информации. Сегодня можно встретить мнение, что нужно пересмотреть определение Open Knowledge; уточнить понятия «данные» и «контент» в контексте развития систем искусственного интеллекта. Нейросети упростили доступ к знаниям, но системы ИИ часто галлюцинируют, и контент, который на первый взгляд, кажется, адекватным, на самом деле не соответствует действительности и требует верификации. Ответы нейросети также зависят от обучающей выборки, а разработчики не всегда используют информацию из проверенных и «белых» источников. Пока нейросети не перестанут галлюцинировать, в вопросах открытых знаний лучше опираться на курируемые источники и подборки. Например, доступ к такой литературе предлагают в организации OAPEN. Она занимается хранением, распространением и индексацией книг. Изначально проект запустили как 30-месячный эксперимент для построения устойчивой модели публикации академических работ по гуманитарным и социальным наукам. После организация продолжила свою деятельность в качестве независимого фонда. Крупные исследовательские институты также формируют собственные библиотеки. Наиболее полным источником информации о существующих организациях является OpenDOAR — международная инициатива, целью которой стал сбор всех доступных репозиториев открытых знаний в мире. Другой крупный центр распространения Open Knowledge — MIT Press. В рамках проекта Direct to Open, запущенного в 2021 году, издательство открыло доступ к более чем ста шестидесяти монографиям и книгам в первые два года работы. Плюс в прошлом году MIT выложили дополнительные восемьдесят две книги в открытый доступ. Открытые библиотеки знаний охватывают самые разные темы — от физики до дизайна, от философии до технологий. Мы выбрали несколько интересных книг, посвящённых облачным вычислениям и инфраструктуре в широком смысле. MIT Press выпустили исчерпывающее руководство по работе с облачными технологиями в исследовательских целях, рассчитанное на студентов и начинающих ученых. Под обложкой собрана теория по управлению данными в облаке, показано, как развернуть виртуальные машины и контейнеры для проведения научных экспериментов и аналитики. Одна из версий книги находится в свободном доступе под лицензией Creative Commons на сайте Cloud4SciEng.org. Также там можно найти Jupyter Notebooks и другие доп. ресурсы. Книга, подготовленная крупным китайским телекомом и доступная для скачивания в библиотеке OAPEN. Она состоит из восьми глав, каждая из которых охватывает различные аспекты облачных технологий — от архитектуры облачной инфраструктуры до применения Docker и Kubernetes. Авторы подробно разбирают серверную виртуализацию и связанные с ней решения, такие как KVM. В книге содержатся материалы по развёртыванию и управлению облачными ресурсами, с акцентом на принципы серверной виртуализации c практическими примерами настройки. Этот материал — обзор технологии MEC, которая объединяет вычислительные ресурсы и мобильные сети. MEC позволяет обрабатывать данные рядом с источником, что ускоряет приложения и снижает задержки. Автор книги, Ян Чжан, — исследователь в области мобильных технологий. В своей работе он объясняет ключевые принципы MEC и показывает, как эта архитектура поможет в разработке 6G. Чжан также рассматривает интеграцию MEC с блокчейном, системами искусственного интеллекта и машинного обучения. Читатель также сможет узнать о принципах работы MEC и его применении в таких областях, как промышленный интернет вещей и управление чрезвычайными ситуациями. Книга адресована широкой аудитории, включая студентов старших курсов, аспирантов, преподавателей, ученых и инженеров. Стоит отметить, что работа входит в серию Simula SpringerBriefs on Computing от издательства Springer и исследовательской организации Simula, которая занимается популяризацией научного знания в области ПО, машинного обучения, коммуникационных технологий и криптографии. Книга показывает, как современные вычислительные системы трансформируют города. Основное внимание уделено концепции Urban OS — совокупности практик, обеспечивающих управление мегаполисами. Urban OS — это не отдельный продукт, а экосистема, включающая цифровые платформы, социальные сети, мобильные приложения, облачные решения, программное обеспечение, датчики и гражданские инициативы. Важнейшую роль здесь играют сбор, анализ и использование данных, потоки которых связывают частные и публичные облака. В работе рассматривается, как такой подход к управлению мегаполисами может вытеснять традиционные формы городского администрирования. Книга будет полезна всем, кто интересуется воздействием цифровых технологий на городскую среду. Это — сборник статей о влиянии компьютерных инноваций на общество. Авторы анализируют, как современный технологический ажиотаж соотносится с утопическими идеями, появившимися еще в 1960-х годах. Так, в статье The Art of Work: «Bürolandschaft» and the Aesthetics of Computation [стр. 103] автор исследует взаимосвязь между эстетикой вычислительных технологий и дизайном офисных пространств в стиле Bürolandschaft (в переводе с немецкого означает «офисный пейзаж»). Он анализирует, как теории, заложенные в основу проектирования рабочих пространств, изменились с появлением компьютеров, что повлияло на восприятие условий интеллектуального труда. Для тех, кто интересуется облачными вычислениями, особенно примечательна статья All that Is Solid Melts into the Cloud [стр. 305]. Там автор описывает дата-центры, противопоставляя их внешний футуристичный облик и потребление ими ресурсов с идеей «облака» — символом легкости и мобильности. Получите доступ к надёжной виртуальной инфраструктуре и запустите облачные сервисы без начальных затрат — бесплатно на два месяца.",
    "14": "Перед началом погружения в UMA стоит отметить, что оракул в Polymarket является второй важной составляющей сервиса. Первую часть мы разобрали в предыдущей статье “Токенизация рынка предсказаний: Gnosis Conditional Token Framework”. UMA - это децентрализованный оракул, который специализируется на записи любых данных в блокчейн за исключением тех, которые невозможно проверить. Этот оракул называется оптимистичным, потому что считается, что если данные не оспорены, то они полностью верны. Он имеет собственную арбитражную систему для разрешения споров. Оракул предоставляет данные для проектов, таких как кроссчейн мосты, протоколы страхования, рынки предсказаний и деривативы. Тип данных может варьироваться: от цен на криптовалюты до спортивных или политических событий. Однако все данные относятся к реальному миру и могут быть проверены в определенный момент времени. Илон Маск сделает твит о криптовалютах до 31-го августа. Трамп скажет слово \"тампон\" в своем интервью. В 2024 случится эпидемия обезьяньей оспы. Токен Solana будет больше 140$ на 23 августа. Псевдоним Сатоши будет раскрыт в 2024 году. Рассмотрим упрощенный процесс получения оракулом данных, которым можно доверять. Под данными мы понимаем те примеры, которые были описаны выше. В статье мы будем называть эти данные, с которыми работает оракул, разными терминами: statement, утверждение, событие, предсказание и так далее. Все эти термины описывают данные для разных предметных областей в разные моменты их жизненного цикла. Statement. Утверждение добавляется в оракул вместе с наградой. Предполагается, что награду сможет забрать тот, кто сможет оспорить утверждение. Challenge period. Любой может оспорить утверждение за этот период. Или не оспорить. Если время выйдет, утверждение будет считаться готовым к окончательной фиксации. Это будет означать, что оно верное и ему можно доверять. Dispute. И все-таки любой участник протокола может оспорить утверждение с целью получения награды или исполнения своего \"гражданского долга\". Это шутка конечно. На практике это происходит достаточно редко. Согласно теории игр, большинство участников ведет себя в экосистеме протокола честно. Voting. Если спор был начат, то его разрешают держатели токена UMA. Это токен протокола, который позволяет голосовать для разрешения споров и получить за это вознаграждение. Settle. Последним этапом является процесс settle или фактическая фиксация данных, после которых можно считать, что предложенное утверждение гарантированно является истиной. Посмотрим этот процесс на примере. Василий, слава Богу не Теркин, знает, что на евро 2024 по футболу победила Испания. Факт достаточно известный и Василий решает научить оракул UMA этому. Для этого он передает информацию о событии в оракул. Это этап statement. Проходит challenge period. Никто это не оспорил, потому что это бессмысленно. Тогда последний этап settle окончательно зафиксировал в оракуле, что Испания стала чемпионом. Теперь оракул может всегда отвечать по этому событию и выдавать победителя. Василий воодушевлен успехом и планирует передать всю таблицу результатов евро 2024 по футболу. Но Василий в этом турнире болел за команду Франции, которая заняла третье место. И он решает смухлевать, переместив ее на второе место. Disputer это замечает и открывает спор. Надо сказать, что и Василий и disputer ставят на кон не только свою репутацию, но и некоторую сумму токенов. Процесс голосования прошел, подлог Василию сделать не удалось, так как арбитражная система оракула отработала штатно и выдала результат, что событие неверное. Тогда на последнем этапе settled, оракул выдал часть токенов Василия спорщику в качестве вознаграждения. Василий потерял свои токены, disputer приумножил. Примерно таким образом можно описать работу оракула UMA по утверждению новых данных из реального мира. UMA оракул позволяет смарт-контрактам быстро запрашивать и получать информацию. Однако до того момента пока информация станет подтвержденной, пройдет определенный момент времени и отработает целый процесс верификации данных. Asserter - участник, который опубликует в системе некоторое утверждение из реального мира. Например \"Курс юаня на сегодняшний день равняется одному доллару\". Disputer - участник, который может усомниться в утверждении, которое публикует asserter. DVM - программный модуль для разрешения споров между asserter и disputer. Однако использование этого модуля опционально и может быть заменено собственной системой разрешение споров. voter - участник, который будет своим голосом решать кто прав, asserter или disputer. Посмотрим на схему из официальной документации на которой изображено взаимодействие всех компонентов системы, описанных выше. Asserter публикует некоторый statement и передает некоторую сумму залога в токене, который разрешен протоколом. На протяжение challenge period disputer может открыть спор, если он не согласен с утверждением asserter. Для инициализации спора disputer тоже передает протоколу залог. Если на протяжение разрешенного периода statement не оспаривается, то он считается верным. После этого в DVM начинается процесс независимого голосования всеми держателями токена UMA. Этот процесс включает в себя стадии commit и reveal, когда сначала никто не знает голосов других участников, а потом каждый участник раскрывает результат своего голосования. После завершения процесса голосования, если прав был asserter, то он получает половину от залога disputer, вторая половина остается в протоколе. Если побеждает disputer, то наоборот, он забирает половину залога asserter. Для того, чтобы voter смог принять участие в голосовании, необходимо, чтобы его токен UMA был застейкан на протоколе. Это значительное отличие между версиями DVM v1.0 и v2.0. Мотивацией для стейкинга является возможность получить вознаграждение. В целом это стандартная история, которая используется многими протоколам и здесь UMA не предлагает ничего нового. Voter принимает участие в голосовании на двух этапах: передача голоса в закрытую и раскрытие своего голоса. Так называемая схема \"commit/reveal\", где на первой стадии смарт-контракту передается результат хеширования голоса, а потом его раскрытие. Это необходимо для того, чтобы никто не видел результатов голосования участников. Считается, что если участники не знают результата голосования других, то это гарантирует действительно независимое голосование. Также есть один очень интересный механизм, который называется slashing. Согласно этому механизму происходит перераспределение застейканных балансов от участников не принимающих участие в голосовании, или делающих это неправильно, к участникам, которые голосуют правильно. Здесь стоит определить \"правильных\" и \"неправильных\" участников. Неправильный участник - это участник, который отдал голос и в результате оказался в группе меньшинства. Например, система предлагает событие \"Зимбабве завоюет на олимпийских играх ровно 10 золотых медалей\". Событие маловероятно, однако, звезды сошлись, и Зимбабве выиграло ровно 10 золотых медалей. Казалось все просто, мы голосуем, что событие верное, но большинство участников, по какой-то причине, видимо попали на источник с ошибкой, проголосовали, что событие не наступило, подумали, что Зимбабве вообще не выиграло медалей. В этом случае, хоть мы и все сделали честно, мы будем названы \"неправильно\" проголосовавшими и подвергнемся процессу slashing. Стоит еще раз отметить, что неправильный участник, этот тот, который отдал свой голос вместе с меньшинством. Однако пример маловероятен, потому что участники не знают голосов других до момента раскрытия, все голосуют секретно. Поэтому никто из голосующих не захочет оказаться в группе меньшинства, а значит будут голосовать правильно. Таким образом, исключается экономическая мотивация вести себя в системе некорректно или саботировать процесс голосования. В документации это называется штрафом за не участие в голосовании. OOV3 - это кодовое название смарт-контракта Optimistic Oracle V3. С этим смарт-контрактом будет необходимо делать интеграцию, если есть необходимость подключить UMA протокол. finder. Смарт-контракт, который хранит адреса всех актуальных смарт-контрактов протокола. Позволяет эффективно использовать его в любом стороннем смарт-контракте. defaultCurrency. Адрес токена в котором протокол будет принимать залог. defaultLiveness. Время за которое есть возможность начать спор по statement. Assert truth. Предлагает событие/утверждение из мира для фиксации утверждения Dispute assertion. Дает возможность оспорить событие Settle assertion. Фиксирует событие, как правдивое, точное или правильное Для утверждения события есть две публичных функции: assertTruthWithDefaults() и assertTruth(). По сути первая вызывает вторую с дефолтными параметрами, поэтому посмотрим на вторую функцию в облегченном виде. После создания утверждения начинается отсчет времени в течении которого есть возможность открыть спор. Для этого любому несогласному необходимо вызвать функцию disputeAssertion(). Это небольшая функция, посмотрим на нее внимательнее. После того, как время спора прошло, можно окончательно фиксировать утверждение для публичного использования. Для этого необходимо вызвать функцию settleAssertion(). Посмотрим на нее подробнее. Пришло время получше познакомиться с системой разрешения споров DVM. В репозитории эта часть кода лежит в отдельной папке под названием data-verification-mechanism. Основной контракт с которого удобно начинать разбирать систему называется Voting.sol. Ниже мы будем разбирать его вторую версию: смарт-контракт VotingV2.sol. Это уже система побольше, чем OOV3, но мы постараемся охватить наиболее важные моменты. В центре схемы находится сам смарт-контракт VotingV2.sol и перечислены интерфейсы от которых он наследуется: OracleInterface, OracleAncillaryInterface, OracleGovernanceInterface, VotingV2Interface. Для того, чтобы держатели токена UMA могли голосовать в спорах, им необходимо застейкать эти токены. За это отвечает отдельный смарт-контракт Staker.sol, от которого VotingV2.sol наследуется. На схеме он отмечен зеленым цветом. Не то желтым, не то оранжевым цветом отмечены смарт-контракты, которые используются внутри основного VotingV2.sol. Все они так или иначе подключают дополнительную логику. Для некоторых из них необходимо передать их адреса в constructor VotingV2.sol при деплое, что само собой подразумевает их отдельный, преждевременный деплой. Перечислим и скажем, за что каждый из них отвечает: VoteTiming.sol. Определяет временные интервалы для голосования держателей токена UMA. Правильнее сказать порядок commit/reveal операций. Помним, что голосование проходит в два этапа. Сначала голосующие отправляют хеш своего голоса, а потом раскрывают этот хеш. Finder.sol. Отвечает за хранение адресов других смарт-контрактов, которые используются во всем протоколе и за его пределами. FixedSlashSlashingLibrary.sol. Отвечает за перераспределение токена UMA между честными и нечестными участниками в пользу первых. VotingToken.sol.. Это и есть токен протокола UMA. Его адрес передается в VotingV2.sol вместе с деплоем. Именно его должны стейкать пользователи для получения доступа к голосованию. ResultComputationV2. Очень важная библиотека, которая занимается подсчетом голосов. Previous voting contract. Адрес предыдущей версии смарт-контракта Voting.sol. Используется только для получения ревардов со старой версии или хранящейся на нем информации. Фиолетовым цветом на схеме перечислены группы функций смарт-контрактов, которые реализует смарт-контракт Voting.sol: Думаю здесь нет смысла расписывать, что за функции в каждой группе делают. Названия говорят сами за себя. Зная все это, ты можешь самостоятельно пошариться в смарт-контракте VotingV2.sol. Отдельно стоит поговорить про механизм slashing. Мы уже определили его, как основной механизм для стимуляции держателей токена UMA принимать участие в разрешение споров. Чтобы понять, как этот механизм работает, надо найти в коде его начало. Удобно его рассматривать от внутренней функции _updateTrackers() в смарт-контракте VotingV2.sol. Посмотрим на нее. Эта функция используется много где по коду. Но если присмотреться, то большинство мест - это действие пользователя: stake(), unstake(), withdrawRewards(). Есть еще парочка мест: как говорится, найди их сам. Получается, что любое действие голосующего будет для него делать slashing. И избежать его он уже не сможет. То есть не получится застейкать токены, накопить вознаграждение за стейкинг, при этом не участвуя в голосовании, а потом вывести вознаграждение. В момент вывода пользователь сам, собственной транзакцией, запустит для себя slashing и получит штраф. Единственный способ избежать штрафа, принимать участие в голосовании и делать это честно. Помимо этого протокол дает возможность кому угодно и для кого угодно запустить slashing в любой момент времени. То есть потенциально, даже просто отсидеться до лучших времен, когда протокол отменит slashing через DAO, не получится. Кто-нибудь обязательно запустит для такого пользователя slashing. Для этого достаточно вызывать публичную функцию updateTrackers(). А вот она уже под капотом вызовет одноименную внутреннюю функцию. Если не хочется прогонять пользователя по всей истории голосований, то можно использовать функцию, которая позволит определить максимальное количество событий в глубину для slashing. Отвечает за это функция updateTrackersRange(). Остается подытожить когда пользователь попадает на slashing. Посмотрим на схему. Единственное, что мы не рассмотрели, это каким образом написан код для slashing. Оставляю это в качестве домашнего задания. Подсказка: смотреть функцию _updateAccountSlashingTrackers(). Важно! Ранее мы упоминали библиотеку FixedSlashSlashingLibrary.sol. При ее детальном рассмотрении, можно понять, что она всего лишь отвечает за количество токена, которое будет снято с пользователя при получение штрафа за неучастие в голосовании или за неправильное голосование. Prediction market. Реализует рынок предсказаний, где проверка предсказаний делается при помощи Optimistic Oracle V3. Insurance contract. Реализует договор страхования, который позволяет получить один полис с возможностью получить выплату в случае наступления страхового случая. A generic data assertion framework. Позволяет делать утверждения на произвольные данные, подтверждая их через Optimistic Oracle V3. Разберем наиболее интересный пример \"Prediction market\". Я выбрал именно этот пример, потому что знаю один популярный сервис среди рынков предсказаний Polymarket, который использует UMA протокол. Как работает рынок предсказаний? Сервис наполняется предсказаниями с возможностью проголосовать за один из его исходов. Например: \"Завтра биткоин будет стоить 100000$\". Пользователь в праве согласиться с этим утверждением или не согласиться и отдать свой голос за положительный или отрицательный исход. Можно на это смотреть как на сервис ставок. Мы ставим на определенный исход события. Когда событие произойдет, ставка либо сыграет и мы заработаем, либо все проиграем. Обычно исходов у предсказания два, три, но встречается и больше. Под каждый исход создается взаимозаменяемый токен. Для того, чтобы сделать ставку, нам необходимо купить этот токен. Полученный токен можно обменять на других площадках на более известный токен, если он там листится. Часто возможность для обмена реализуется прямо в самом сервисе предсказаний. Начнем сразу с места в карьер. Смотрим на конструктор: Основная задача рынка предсказаний создавать различные бинарные утверждения, где пользователи могут оставлять свое мнение \"согласны\" или \"не согласны\" они с утверждением. Такие штуки в смарт-контракте называются возможными исходами (outcome) утверждения или предсказания. Для хранения утверждений есть специальный маппинг, который называется markets. Зная, какие структуры данных описывают рынок предсказаний, можно смотреть функцию initializeMarket(), которая создает рынки на смарт-контракте. Следующим этапом, после того, как рынок предсказаний создан, и пользователи уже сделали свои ставки \"за\" или \"против\" и предсказанное событие произошло - необходимо добавить предполагаемый исход в UMA протокол для того, чтобы утвердить результат. Наш смарт-контракт для этого реализует функцию assertMarket(). Здесь может возникнуть резонный вопрос, а каким образом мы теперь на смарт-контракте PredictionMarket.sol узнаем, когда оракул рассчитает событие и будет готов сообщить нам, что наше утверждение верно. Время вспомнить про callbacks, которые оракул умеет делать. Поэтому нам достаточно реализовать один из таких callback, под названием assertionResolvedCallback(). Когда утверждение будет проверено оракулом, он сделает обратный вызов. Весь порядок вызовов, который мы описали выше можно представить на схеме для закрепления. На этом основная логика взаимодействия смарт-контракта PredictionMarket.sol c оракулом UMA заканчивается. Мы не рассмотрели и намерено опустили историю, которая касается работы самого рынка предсказаний. Часто для каждого исхода рынка создается взаимозаменяемый токен. Когда пользователи выбирают исход, они по сути покупают эти токены под капотом и более того, могут обменивать его (этот функционал вынесен за границы примера). Зачатки этой логики также есть в нашем контракте-примере. За это отвечают функции: createOutcomeTokens(), redeemOutcomeTokens(), settleOutcomeTokens(). Раньше, для того, чтобы получить право принять участие в голосовании достаточно было держать токены UMA на кошельке. Протокол использовал механизм снепшотов, фиксирующий балансы пользователей, и это являлось \"входным билетом\". В новой версии для доступа к голосованию уже необходимо застейкать токены UMA на смарт-контракт протокола. На момент написания статьи пользователям предлагается APR равный 30.1%. Но это не означает, что можно застейкать и пассивно получать вознаграждение. Помним, что система имеет механизм slashing, который может перераспределять балансы стейкеров. первые 24 часа позволяют зашифровать голос. Стадия commit. вторые 24 часа позволяют раскрыть голос. Стадия reveal. Награды за стейкинг накапливаются постоянно. Забрать их можно в любой момент. Однако есть нюанс, когда потребуется анстейкнуть токены обратно. Придется сначала сделать заявку, после этого необходимо выждать период времени, токены не будут приносить вознаграждение и только после этого, отдельной транзакцией можно забрать застейканные токены. Важно! На момент написания штраф за пропуск голосования: 0,05% от застейканного баланса. Токен протокола UMA помимо того, что нужен для голосования при утверждения событий из реального мира, также используется для управления протоколом. Процесс голосования разделен на два этапа commit и reveal голоса и длится 48 часов. Это нам уже знакомо. Протокол придумал свой стандарт предложений по образу и подобию EIP в Ethereum. Называются предложения UMIPs (UMA Improvement Proposals). Стоит отметить, что протокол использует прогрессивный подход в голосовании. Что это означает? Это означает, что голосование не сразу проводится ончейн, а стартует со специального сервиса snapshot.org. В итоге голосование выглядит следующи образом: Post to Discourse. Первым делом необходимо разместить предложение в \"https://discourse.uma.xyz/\". Предложение описывает ключевую идею. На этом этапе сообщество имеет возможность обсудить предложение. Snapshot Vote. Когда предложение готово, автор создает голосование в snapshot длительностью в пять дней. On-chain Vote. Если в snapshot голосование имеет положительный результат, то голосование проводится ончейн. На момент написания статьи протокол в snapshot имеет всего 71-го участника. Оптимистичный оракул UMA гарантированно стоит особняком от других оракулов. В классической модели, оракулы стараются максимально исключить человеческий фактор, вводя децентрализацию по агрегированию данных, определению лидера, который доставит эти данные. Для этого применяют целые блокчейны, дополнительные консенсусы, несколько независимых источников данных и так далее. В UMA ничего подобного нет, этот протокол идет совершенно другим путем, полностью полагаясь на свое сообщество в определении достоверности данных. Такой взгляд открывает совершенно новые варианты использования, которые прекрасно подходят рынкам прогнозов, страхованию и многим другим областям, где скорость поставки данных не так приоритетна и они могут позволить себе подождать пока пройдет полный цикл по верификации данных внутри UMA протокола. Однако это позволяет оракулу поставлять совершенно неограниченные типы данных, по сути с полуавтоматическим подтверждением их целостности и правдивости. По традиции мое личное мнение такое. Это интересный протокол, который хорошо подумал о системе мотивации и сдерживанию своих участников. Прекрасно понимает свою нишу, где он может быть полезным и затачивает под это свой сервис. Протокол точно заслуживает внимания, потому что решение является достаточно простым и элегантным технически, но при этом полностью непохожим на классическую модель оракулов. Практический же успех зависит от активности сообщества, которому необходимо стейкать, голосовать, решать спорные утверждения. Но это конечно отдельная история. Спасибо, что дочитали статью до конца! Надеюсь, она была для вас полезной. Буду рад обсудить ваши вопросы и мнения в комментариях. Наши собственные мысли о мире блокчейн в телеграм-канале. MetaLamp. Разработчик смарт-контрактов на Solidity",
    "15": "Привет, Хабр! Мы в HFLabs не унываем  продолжаем исследовать тему российских адресов. Уже рассказывали о том, почему нельзя просто взять и выгрузить список городов из Государственного адресного справочника (ГАР), и разбирались, что представляет собой нормативка по адресам. А сегодня я расскажу, как один и тот же объект может иметь различные написания адреса. Такие разночтения заставляют делать ручной разбор адресов, а в некоторых случаях и вовсе рушат аналитику. Дальше будет о том, в чём причина и что с этим делать. Итак, по версии ГАР (ФИАС), существуют два адресных деления — административное и муниципальное. Адреса в них могут выглядеть по-разному. Один и тот же адрес в разных форматах (guid д.257 — 19704d91-68d5-4023-afd6-dcbbce6bbf14): Муниципальное деление: Самарская область, городской округ Самара, город Самара Кировский внутригородской район, проспект Кирова, д.257 А еще есть картографические сервисы — например, 2ГИС и Яндекс карты. И в них наш адрес может выглядеть совсем иначе. Например, иногда там встречаются улицы, которых нет в справочнике ГАР (ФИАС). Расскажем об этом подробнее на примере. Есть объект — дом 3. Его идентификатор по справочнику ГАР — 0643ed6e-79ef-4d7a-b2f7-c908d6e61739. Дом находится в СНТ под названием 70 лет Октября. Это Татарстан, город Зеленодольск. Guid — be39502f-a814-405d-aed2-603780308a40. Всего СНТ подчинены 9 999 домов. Адрес нашего дома по справочнику ГАР такой: Республика Татарстан (Татарстан), Зеленодольский р-н, г Зеленодольск, тер. СТ 70 лет Октября, д. 3. Но есть, как говорится, нюанс: если посмотреть Яндекс Карты, то выяснится, что дом стоит на 1-й Кольцевой улице. Как же так получилось? Откуда в Яндексе взялась улица? Чтобы разобраться в этом, мы написали в поддержку Яндекс-карт с таким вопросом: «Согласно справочнику ГАР, дома (9999) в этом СНТ подчиняются напрямую СНТ. А в Яндекс Картах у них есть улица-родитель. Подскажите, чем вызвано такое расхождение?» В садовых товариществах часто используется нумерация и по самому СНТ, и по улицам. Адреса по самому СНТ ищутся при запросе так же, как по улицам. Яндекс Карты обновляются каждый день: появляются новые дороги, дома, скверы — таких обновлений каждый месяц происходит больше миллиона. Чтобы оперативно отражать мир вокруг, сервис использует разные источники. За актуальностью карты следят  штатные картографы и сообщество пользователей в редакторе Народная карта. Также в переписке специалисты Яндекс Карт пояснили, что названия улицам могут присваивать как сотрудники картпроизводства, так и пользователи сервиса «Народная карта». Делать это они могут, основываясь на данных из разных источников и личных знаниях местности. Правки пользователей проходят обязательную модерацию сотрудниками картпроизводства сервиса или опытными пользователями «Народной карты», которые имеют права модератора. Проще говоря, любой пользователь может нанести название дороге, но его правка пройдёт через модератора. Если она некорректная, ее отклонят. Разные объекты на карте имеют разные условия проверки достоверности. К некоторым из них предъявляются более строгие требования для попадания в Карты, к другим — упрощённые. Дальше мы написали тот же вопрос в 2ГИС. И получили такой ответ: «В сервисе 2ГИС дома в СНТ «70 лет Октября» также привязаны к СНТ. По данным сервиса, собственники домов в СНТ используют названия улиц в качестве ориентиров, что следует из официальной группы СНТ: https://vk.com/club154445682». - нет улиц, подчиненных населённому пункту. Получилось 57 162 (!) объекта. Часто это деревни с небольшим количеством домов. Но есть и более густонаселённые объекты. Ниже несколько примеров с достаточно большим количеством домов: Что из этого следует? Создавая адресные формы, не заставляйте людей обязательно указывать улицу при вводе адреса, её реально может не быть. А ещё в России есть объекты, где подчинение у домов смешанное — часть домов подчинены самому населённому пункту, часть — улицам. Например, 17c781b0-0fa4-432e-b53c-848b88b819b5. Пермский край, Кудымкарский р-н, деревня Гурина Согласно ГАР, в этой деревне только одна улица. Все остальные 24 актуальных объекта — дома, причём некоторые подчинены напрямую деревне, а не этой улице. А на Яндекс-картах опять все иначе: и улица не одна, и домов больше 24. Возникает вопрос: просто не успели занести новые улицы и подчинения в ГАР? Или что-то другое?.. Представим, что компания использует для ввода адреса подсказки Яндекса, а потом хочет привести адресные данные к формату налоговой, чтобы сдать отчётность. Сюрприз — так просто не получится! Потому что в Яндексе улицы у этого объекта есть, а в ГАР — нет. Сопоставить адрес из источника заказчика и адрес из ГАР может адресная мастер-система. Но и тут без дополнительной ручной валидации, скорей всего, не обойтись, так как только человек сможет подтвердить, что адрес без улицы в ГАР и адрес с улицей из Яндекса — это одно и то же. Поэтому, если у вас есть адресная мастер-система со встроенными подсказками по ГАР, используйте их для помощи ввода людям, чтобы сразу получить адрес «государственного образца». Кстати, приведение адреса к эталонному значению, в основе которого справочник ГАР, поможет системам понимать друг друга лучше. Ведь если в одной системе адреса заводятся по ГАР, а в другой приезжают из стороннего сервиса, то сопоставить их сложно. Другой пример: бизнес может делать аналитику по допродажам, чтобы понять, сколько населённых пунктов не охвачено его услугами. Если слепо верить данным ГАР, в отчёт попадут умирающие деревни, которые вообще не интересны. Или наоборот: есть населённый пункт, в ней улица. Но домов на ней, с точки зрения ГАР, нет. И они не попадут в отчёт. Опять же, в адресных мастер-системах можно хранить признак подтверждённого адресного объекта — если бизнес проверил, что такой дом или населённый пункт точно существуют и представляют для него интерес. В мире адресных данных много неоднозначных задачек. Если столкнулись с какой-то из них, пишите в комментариях — попробуем разобраться вместе.",
    "16": "Всем привет! В данной статье рассмотрим такой javascript модуль как HMPL и как он может заменить HTMX в проекте. Также, рассмотрим в чём их отличия, преимущества и недостатки. При дальнейшем сравнение двух модулей стоит учесть, что один является языком шаблонов, когда как другой является набором инструментов для работы с HTML, реализуемых через атрибуты и не только. Начнём с общей концепции для двух модулей. Модуль HMPL схож по концепции с HTMX. Мы также можем брать HTML с сервера по API, заменяя тем самым современные фреймворки и библиотеки для создания UI. Возьмём небольшой пример, иллюстрирующий работу HMPL и HTMX, а также такого фреймворка как Vue.js: На примере простого кликера можно увидеть (с небольшими оговорками по данным на стороне сервера и на стороне клиента, а также по html и js разметке, но не в это главная идея), что мы получаем одинаковый интерфейс, хотя размеры файлов на клиенте будут совершенно разными. В этом как раз и основное преимущество подхода создания уже готового, или шаблонного компонента UI на стороне сервера, чтобы пользователь сайта загружал его быстрее с сохранением результата. Теперь, вспомним, насколько большие приложения сегодня, ну, или по крайней мере раньше (когда server-side rendering не был ещё настолько популярным), могли получаться при использовании фреймворков и библиотек. Тот же SPA (Single Page Application), генерирует весь контент через javascript, когда в html у нас буквально одна строчка, но в том и прикол, что при 10 килобайтном html мы получаем javascript файл размером в несколько десятков мегабайт. Такой сайт, первом заходе, пользователи могут грузить долго. К примеру, если потенциальный клиент захочет по быстрому заказать себе цветы, то он не будет ждать 10-15 секунд, пока сайт магазина доставки прогрузится, он перейдёт на другой сайт, где сайт будет грузиться быстрее. Есть ещё много прикладных примеров работы сайтов, которые могут влиять на воронку продаж. Но, суть в том, что главное - это скорость и удобство интерфейса, а вот тут уже есть различия в подходах. Но, лучше сделать это в отдельной статье. Тут же рассматривается сравнение HMPL и HTMX. При схожей идее сокращения кода, два модуля отличаются в понятиях. В случае HTMX, с одной стороны, мы получаем удобный инструментарий работы с уже существующим DOM, но с другой, всё это происходит через HTML и обновляется буквально в режиме реального времени. Мы с большим трудом, через нетипичиные решения, можем работать более ли менее через javascript, а по сути, работа с javascript почти полностью отсутствует. В случае HMPL, с одной стороны, мы без труда можем работать с javascript; генерировать кастомный RequestInit, создавать тысячи отдельных DOM узлов с той же поддержкой серверного UI, что и на HTMX, но с другой - всё таки работа происходит с кодом, что не всегда удобно, когда хочется создавать проекты быстрее. Возьмём пример кода: На данном примере чётко видно, что HTMX - это больше про максимальное убыстрение и укорачивание кода, когда как HMPL - это нечто комбинированное между HTMX и современным фреймворком или библиотекой для создания UI. Можно сказать, что мы получаем несколько похожий результат, но с учётом того, что мы можем кастомизировать запрос на сервер. Синтаксис HMPL тоже является в своём роде преимуществом, потому что объекты запроса не привязаны к какому-либо тегу. При рендеринге они заменяются на коментарии, которые не засоряют DOM ненужными тегами. Пример синтаксиса: HMPL полностью построен на fetch заросах, которые ввели как стандарт в 2015 году. HTMX для поддержки IE13 по дефолту использует XMLHTTPRequest, который был введён в 2000 году. Функция fetch позволяет использовать современные возможности javascript в браузерах, такие как AbortController, signals и другое И, есть ещё достаточное количество преимуществ, по типу отдельного расширения файла .hmpl при работе с вебпаком и другие, но, на мой взгляд, те, которые я выделил, являются наиболее важными. Вебпак конфиг: У HMPL пока нету поддержки WebSocket, что может усложнить внедерение кода в проект. В HTMX данная поддержка присутствует. Так как происходит использование fetch, то в некоторых старых версиях браузера вёрстка поддерживаться не будет. HTMX - это годами протестированная технология, когда как HMPL - это новый модуль. Язык шаблонов HMPL может заменить HTMX в тех случаях, когда требуется гибкая настройка запроса, а также непосредственная работа с узлами через javascript; Если вы, допустим, хотите создать цикл из 1000 одинаковых узлов, и при этом иметь преимущества серверо-ориентированного UI, то он также подойдёт для задачи. Если же цель стоит в полной минимизации работы с javascript, либо же в использовании устоявшегося протестированного модуля и простым подключением к серверу с минимальным количеством HTML кода, то тут HTMX хорош. https://hmpl-lang.github.io https://htmx.org",
    "17": "Всем привет! Меня зовут Сабина, я лидер команды исследователей данных во ВкусВилле. Мы помогаем бизнесу принимать решения, ориентируясь в том числе на данные. Сегодня я расскажу об одном таком случае. Статья будет полезна аналитикам, которые хотят перестать беспокоиться и начать использовать линейную регрессию из питоновской библиотеки stasmodels. Бизнес-контекстОтдел B2B прорабатывает новый сервис сборки и доставки. Важное изменение — возить будем не в пакетах, а в многоразовых контейнерах. Они представлены в нескольких вариантах по объёму, и нужно понять, какие и в каком соотношении заказать. Кроме того, необходимо учесть правила сборки: товары категории \"Кулинария\", \"ФРОВ\" и \"Заморозка\" складывать отдельно от всех. Аналитическая задачаНужно определить, какие объемы чаще всего возят курьеры, округлить вверх до объемов предложенных контейнеров и посчитать их соотношение. Подготовка данныхВ БД есть справочник товаров, однако данными по объему для большинства из них мы не располагаем. Мешать сборщикам в дарксторе, измеряя средний объем Манго Египет, или искать его в таблице плотностей, чтобы перейти от веса к объему, конечно, никто не собирается.Как известно, самый популярный товар среди покупателей розницы — это пакет. И клиенты B2B в этом с ними солидарны. А у пакета как раз есть объём. Тогда нужно подготовить таблицу, в которой строка — одна покупка, а поля: и для каждой автономной категории по два поля: кол-во единиц штучных товаров и суммарный вес товаров весовых. Остаётся посчитать, какой объём забирает на себя килограмм и штука каждой автономной категории, и решение у нас в кармане (или в пакете). Будь у нас много заказов, в которых только одна автономная категория и только один юнит (кг или шт), то решалось бы всё запросом SQL. Но в данном случае это не так, поэтому применяем линейную регрессию. Исключаем незначимую переменную, запускаем еще раз. - какую долю дисперсии зависимой переменной объяснила математическая модель. Может принимать значения [0, 1]. Важный показатель, но ориентироваться только на него не стоит. И нет жёсткого трешхолда, после какого значения модель хорошая. Даже в публикуемых научных статьях он может быть 0,3. Р-value всей модели. Должно быть ниже уровня значимости (обычно 0,05) Коэффициенты при независимых переменных модели должны пройти проверку на адекватность. Например, чем больше категорий (cnt_cat), тем больше пакетов — зависимость прямая, коэффициент положительный. Модуль от t-статистики показывает, какой вклад в модель вносит конкретная переменная. Статистика Дарбина-Уотсона отражает наличие автокорреляции. Если автокорреляции нет, равна 2. Теперь то, для чего строилась модель: восстанавливаем количество пакетов на категорию, подставляя коэффициенты из модели. Интерсепт = 0.4136, но если его прибавлять при вычислении пакетов каждой категории, получится больше, чем задумывалось. То есть нужно в каждую категорию отдать часть интерсепта (делим его на количество категорий) 0.5183 – коэффициент при количестве категорий Интерсепт и количество категорий существуют всегда, поэтому, чтобы не выдавать лишний пакет, когда категории нет в заказе, умножаем на булеву переменную ее наличия. 0.0933 и 0.0595 — коэффициенты при ФРОВ_вес и ФРОВ_шт соответственно. ВыводДалее пакето-категории округляем вверх до объемов многоразовых контейнеров и считаем, в каком соотношении они встречались. Получилось, что почти 100% заказов можно привозить в первых трех вариантах объёмов: 14, 32 и 46 литров. Что и было рекомендовано заказчику. ДополнительноНовый сервис сделал доставку удобнее для клиента и курьера. Но работу сборщику мы усложнили: раньше он не задумываясь брал стандартный пакет, а теперь предстоит по составу корзины прикинуть, какой контейнер под автономную категорию взять.  Наша линейная регрессия поможет и сборщику, если ее зашить в интерфейс: полученная формула может на основании товаров в корзине подсказать, какие контейнеры взять под каждую автономную категорию."
}
